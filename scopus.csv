"Authors","Author full names","Author(s) ID","Title","Year","Source title","Volume","Issue","Art. No.","Page start","Page end","Page count","Cited by","DOI","Link","Affiliations","Authors with affiliations","Abstract","Author Keywords","Index Keywords","Correspondence Address","Editors","Publisher","ISSN","ISBN","CODEN","PubMed ID","Language of Original Document","Abbreviated Source Title","Document Type","Publication Stage","Open Access","Source","EID"
"Kathiria P.; Mankad S.H.; Patel J.; Kapadia M.; Lakdawala N.","Kathiria, Preeti (57205348007); Mankad, Sapan H. (55340255600); Patel, Jitali (57202255048); Kapadia, Mayank (59279292100); Lakdawala, Neel (59280856200)","57205348007; 55340255600; 57202255048; 59279292100; 59280856200","Assistive systems for visually impaired people: A survey on current requirements and advancements","2024","Neurocomputing","606","","128284","","","","0","10.1016/j.neucom.2024.128284","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85201600226&doi=10.1016%2fj.neucom.2024.128284&partnerID=40&md5=f0bba414ceb13763b3383dce99039602","Department of Computer Science and Engineering, Institute of Technology, Nirma University, Sarkhej-Gandhinagar Highway, Gujarat, Ahmedabad, 382481, India","Kathiria P., Department of Computer Science and Engineering, Institute of Technology, Nirma University, Sarkhej-Gandhinagar Highway, Gujarat, Ahmedabad, 382481, India; Mankad S.H., Department of Computer Science and Engineering, Institute of Technology, Nirma University, Sarkhej-Gandhinagar Highway, Gujarat, Ahmedabad, 382481, India; Patel J., Department of Computer Science and Engineering, Institute of Technology, Nirma University, Sarkhej-Gandhinagar Highway, Gujarat, Ahmedabad, 382481, India; Kapadia M., Department of Computer Science and Engineering, Institute of Technology, Nirma University, Sarkhej-Gandhinagar Highway, Gujarat, Ahmedabad, 382481, India; Lakdawala N., Department of Computer Science and Engineering, Institute of Technology, Nirma University, Sarkhej-Gandhinagar Highway, Gujarat, Ahmedabad, 382481, India","In this survey, we provide a comprehensive study on the assistive technological devices which help visually impaired persons in their day-to-day lives. With various forms of disabilities such as visual, auditory, mobility, or cognitive impairment affecting a significant number of people worldwide, the aim of this study is to enable the blind persons to achieve independence. The primary objective of this work is to provide a concise description of assistive technologies for the visually impaired, along with an exploration of approaches for object detection, text detection, and text-to-speech synthesis. We present detailed discussion on these approaches using both traditional as well as deep learning based techniques. Finally, we conclude with some open opportunities where further research can be carried out. © 2024 Elsevier B.V.","Assistive; Object detection; Text detection; Text-to-speech synthesis; Visually impaired","Assistive; Assistive system; Objects detection; On currents; On-currents; Text detection; Text to speech; Visually impaired; Visually impaired people; Visually impaired persons; adult; assistive technology; blindness; clinical article; cognitive defect; deep learning; diagnosis; female; human; male; middle aged; self help device; short survey; text-to-speech; visually impaired person; Assistive technology","S.H. Mankad; Department of Computer Science and Engineering, Institute of Technology, Nirma University, Ahmedabad, Sarkhej-Gandhinagar Highway, Gujarat, 382481, India; email: sapanmankad@nirmauni.ac.in","","Elsevier B.V.","09252312","","NRCGE","","English","Neurocomputing","Short survey","Final","","Scopus","2-s2.0-85201600226"
"Zhang Y.; Li Z.; Guo H.; Wang L.; Chen Q.; Jiang W.; Fan M.; Zhou G.; Gong J.","Zhang, Yan (57953649600); Li, Ziang (57954810700); Guo, Haole (57852848700); Wang, Luyao (57208610611); Chen, Qihe (57954112800); Jiang, Wenjie (57953192600); Fan, Mingming (26424940800); Zhou, Guyue (55608815200); Gong, Jiangtao (56447667300)","57953649600; 57954810700; 57852848700; 57208610611; 57954112800; 57953192600; 26424940800; 55608815200; 56447667300","""I am the follower, also the boss"": Exploring Different Levels of Autonomy and Machine Forms of Guiding Robots for the Visually Impaired","2023","Conference on Human Factors in Computing Systems - Proceedings","","","542","","","","9","10.1145/3544548.3580884","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85160006419&doi=10.1145%2f3544548.3580884&partnerID=40&md5=03e87c8c1f637e7bae8f249a0986286f","Institute for Ai Industry Research, Tsinghua University, Beijing, China; The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; The Hong Kong University of Science and Technology, Hong Kong","Zhang Y., Institute for Ai Industry Research, Tsinghua University, Beijing, China; Li Z., Institute for Ai Industry Research, Tsinghua University, Beijing, China; Guo H., Institute for Ai Industry Research, Tsinghua University, Beijing, China; Wang L., Institute for Ai Industry Research, Tsinghua University, Beijing, China; Chen Q., Institute for Ai Industry Research, Tsinghua University, Beijing, China; Jiang W., Institute for Ai Industry Research, Tsinghua University, Beijing, China; Fan M., The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China, The Hong Kong University of Science and Technology, Hong Kong; Zhou G., Institute for Ai Industry Research, Tsinghua University, Beijing, China; Gong J., Institute for Ai Industry Research, Tsinghua University, Beijing, China","Guiding robots, in the form of canes or cars, have recently been explored to assist blind and low vision (BLV) people. Such robots can provide full or partial autonomy when guiding. However, the pros and cons of different forms and autonomy for guiding robots remain unknown. We sought to fill this gap. We designed autonomy-switchable guiding robotic cane and car. We conducted a controlled lab-study (N=12) and a field study (N=9) on BLV. Results showed that full autonomy received better walking performance and subjective ratings in the controlled study, whereas participants used more partial autonomy in the natural environment as demanding more control. Besides, the car robot has demonstrated abilities to provide a higher sense of safety and navigation efficiency compared with the cane robot. Our findings offered empirical evidence about how the BLV community perceived different machine forms and autonomy, which can inform the design of assistive robots. © 2023 ACM.","control; guiding robot; level of autonomy; machine form; navigation; safety; trust; visual impairment","Control theory; Field studies; Guiding robot; Level of autonomies; Low vision; Machine form; Performance ratings; Switchable; Trust; Visual impairment; Visually impaired; Machine design","J. Gong; Institute for Ai Industry Research, Tsinghua University, Beijing, China; email: gongjiangtao2@gmail.com","","Association for Computing Machinery","","978-145039421-5","","","English","Conf Hum Fact Comput Syst Proc","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85160006419"
"Hebbar M.K.; Pullela P.K.","Hebbar, Maithili K (58918653700); Pullela, Phani Kumar (59227399800)","58918653700; 59227399800","Object Recognition System for the Visually Impaired: Leveraging IoT and Remote Server Integration","2023","2023 International Conference on the Confluence of Advancements in Robotics, Vision and Interdisciplinary Technology Management, IC-RVITM 2023","","","","","","","1","10.1109/IC-RVITM60032.2023.10434991","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85186665020&doi=10.1109%2fIC-RVITM60032.2023.10434991&partnerID=40&md5=89cb4ed309cf52bcc2d830fd1fb91f7c","Rv University, School of Computer Science Engineering (SOCSE), RV Vidyanikethan Post 8th Mile, Mysuru Road, Bengaluru, 560059, India","Hebbar M.K., Rv University, School of Computer Science Engineering (SOCSE), RV Vidyanikethan Post 8th Mile, Mysuru Road, Bengaluru, 560059, India; Pullela P.K., Rv University, School of Computer Science Engineering (SOCSE), RV Vidyanikethan Post 8th Mile, Mysuru Road, Bengaluru, 560059, India","This paper offers a comprehensive examination of advancements in assistive technology tailored for individuals with visual impairments, with a specific focus on the role of the Internet of Things (IoT) in enhancing their quality of life. The research encompasses a thorough review of diverse articles and studies related to navigation solutions, wearable devices, and IoT-driven object recognition systems, emphasizing the significance of user-centric design and potential improvements in detection precision, navigation capabilities, device robustness, weight, and cost-effectiveness. Additionally, the paper explores the profound impact of intelligent environments and advanced technologies on the daily lives of visually impaired individuals, showcasing the potential of IoT, embedded systems, and machine learning in innovative solutions. Ultimately, it concludes by highlighting the crucial need for ongoing research and development to address the challenges faced by the visually impaired community. © 2023 IEEE.","Assistive Technology; Blind; Internet of Things (IoT); Sensor; Visual Impairment","Cost effectiveness; Embedded systems; Internet of things; Object recognition; Wearable technology; Assistive technology; Blind; Comprehensive examination; Internet of thing; Navigation solution; Object recognition systems; Quality of life; Remote servers; Visual impairment; Visually impaired; Assistive technology","M.K. Hebbar; Rv University, School of Computer Science Engineering (SOCSE), Bengaluru, RV Vidyanikethan Post 8th Mile, Mysuru Road, 560059, India; email: maithilikh.btech22@rvu.edu.in","","Institute of Electrical and Electronics Engineers Inc.","","979-835030608-8","","","English","Int. Conf. Conflu. Adv. Robot., Vis. Interdiscip. Technol. Manag., IC-RVITM","Conference paper","Final","","Scopus","2-s2.0-85186665020"
"Beheshti M.; Naeimi T.; Hudson T.E.; Feng C.; Mongkolwat P.; Riewpaiboon W.; Seiple W.; Vedanthan R.; Rizzo J.-R.","Beheshti, Mahya (57208333398); Naeimi, Tahereh (57219892981); Hudson, Todd E. (7203069715); Feng, Chen (51461205400); Mongkolwat, Pattanasak (6602619011); Riewpaiboon, Wachara (8642392600); Seiple, William (7005218017); Vedanthan, Rajesh (24823609000); Rizzo, John-Ross (56527876700)","57208333398; 57219892981; 7203069715; 51461205400; 6602619011; 8642392600; 7005218017; 24823609000; 56527876700","A Smart Service System for Spatial Intelligence and Onboard Navigation for Individuals with Visual Impairment (VIS4ION Thailand): study protocol of a randomized controlled trial of visually impaired students at the Ratchasuda College, Thailand","2023","Trials","24","1","169","","","","2","10.1186/s13063-023-07173-8","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85149999175&doi=10.1186%2fs13063-023-07173-8&partnerID=40&md5=dad225d95f8a5be0bc14b077c156986c","Department of Rehabilitation Medicine, NYU Langone Health, New York, NY, United States; Department of Mechanical and Aerospace Engineering, NYU Tandon School of Engineering, New York, NY, United States; Department of Neurology, NYU Langone Health, New York, NY, United States; Department of Biomedical Engineering, NYU Tandon School of Engineering, New York, NY, United States; Department of Information and Communication Technology, Mahidol University, Salaya, Thailand; Ratchasuda College, Mahidol University, Salaya, Thailand; Lighthouse Guild, New York, NY, United States; Department of Ophthalmology, NYU Langone Health, New York, NY, United States; Department of Population Health, NYU Langone Health, New York, NY, United States; Department of Medicine, NYU Langone Health, New York, NY, United States","Beheshti M., Department of Rehabilitation Medicine, NYU Langone Health, New York, NY, United States, Department of Mechanical and Aerospace Engineering, NYU Tandon School of Engineering, New York, NY, United States; Naeimi T., Department of Rehabilitation Medicine, NYU Langone Health, New York, NY, United States; Hudson T.E., Department of Rehabilitation Medicine, NYU Langone Health, New York, NY, United States, Department of Neurology, NYU Langone Health, New York, NY, United States, Department of Biomedical Engineering, NYU Tandon School of Engineering, New York, NY, United States; Feng C., Department of Mechanical and Aerospace Engineering, NYU Tandon School of Engineering, New York, NY, United States; Mongkolwat P., Department of Information and Communication Technology, Mahidol University, Salaya, Thailand; Riewpaiboon W., Ratchasuda College, Mahidol University, Salaya, Thailand; Seiple W., Lighthouse Guild, New York, NY, United States, Department of Ophthalmology, NYU Langone Health, New York, NY, United States; Vedanthan R., Department of Population Health, NYU Langone Health, New York, NY, United States, Department of Medicine, NYU Langone Health, New York, NY, United States; Rizzo J.-R., Department of Rehabilitation Medicine, NYU Langone Health, New York, NY, United States, Department of Mechanical and Aerospace Engineering, NYU Tandon School of Engineering, New York, NY, United States, Department of Neurology, NYU Langone Health, New York, NY, United States, Department of Biomedical Engineering, NYU Tandon School of Engineering, New York, NY, United States","Background: Blind/low vision (BLV) severely limits information about our three-dimensional world, leading to poor spatial cognition and impaired navigation. BLV engenders mobility losses, debility, illness, and premature mortality. These mobility losses have been associated with unemployment and severe compromises in quality of life. VI not only eviscerates mobility and safety but also, creates barriers to inclusive higher education. Although true in almost every high-income country, these startling facts are even more severe in low- and middle-income countries, such as Thailand. We aim to use VIS4ION (Visually Impaired Smart Service System for Spatial Intelligence and Onboard Navigation), an advanced wearable technology, to enable real-time access to microservices, providing a potential solution to close this gap and deliver consistent and reliable access to critical spatial information needed for mobility and orientation during navigation. Methods: We are leveraging 3D reconstruction and semantic segmentation techniques to create a digital twin of the campus that houses Mahidol University’s disability college. We will do cross-over randomization, and two groups of randomized VI students will deploy this augmented platform in two phases: a passive phase, during which the wearable will only record location, and an active phase, in which end users receive orientation cueing during location recording. A group will perform the active phase first, then the passive, and the other group will experiment reciprocally. We will assess for acceptability, appropriateness, and feasibility, focusing on experiences with VIS4ION. In addition, we will test another cohort of students for navigational, health, and well-being improvements, comparing weeks 1 to 4. We will also conduct a process evaluation according to the Saunders Framework. Finally, we will extend our computer vision and digital twinning technique to a 12-block spatial grid in Bangkok, providing aid in a more complex environment. Discussion: Although electronic navigation aids seem like an attractive solution, there are several barriers to their use; chief among them is their dependence on either environmental (sensor-based) infrastructure or WiFi/cell “connectivity” infrastructure or both. These barriers limit their widespread adoption, particularly in low-and-middle-income countries. Here we propose a navigation solution that operates independently of both environmental and Wi-Fi/cell infrastructure. We predict the proposed platform supports spatial cognition in BLV populations, augmenting personal freedom and agency, and promoting health and well-being. Trial registration: ClinicalTrials.gov under the identifier: NCT03174314, Registered 2017.06.02. © 2023, The Author(s).","Accessibility; Adaptive mobility device; Assistive technology; Blind/low vision; Low- and/or middle-income countries; Navigation; Visual impairment","Humans; Intelligence; Quality of Life; Randomized Controlled Trials as Topic; Thailand; Universities; Vision, Low; adult; Article; clinical article; clinical feature; college student; controlled study; disease severity; female; human; low income country; male; middle income country; patient monitoring; randomized controlled trial; risk factor; Thailand; visual impairment; intelligence; low vision; quality of life; randomized controlled trial (topic); university","J.-R. Rizzo; Department of Rehabilitation Medicine, NYU Langone Health, New York, United States; email: johnrossrizzo@gmail.com","","BioMed Central Ltd","17456215","","","36879333","English","Trials","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85149999175"
"Ranganeni V.; Sinclair M.; Ofek E.; Miller A.; Campbell J.; Kolobov A.; Cutrell E.","Ranganeni, Vinitha (57195421508); Sinclair, Mike (7202410378); Ofek, Eyal (10139546600); Miller, Amos (57221318994); Campbell, Jonathan (57193548707); Kolobov, Andrey (57194409260); Cutrell, Edward (57203053744)","57195421508; 7202410378; 10139546600; 57221318994; 57193548707; 57194409260; 57203053744","Exploring levels of control for a navigation assistant for blind travelers","2023","ACM/IEEE International Conference on Human-Robot Interaction","","","","4","12","8","7","10.1145/3568162.3578630","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85150391784&doi=10.1145%2f3568162.3578630&partnerID=40&md5=56bc863c5b98ddb8d87b0b14fb04a685","University of Washington, Seattle, WA, United States; Microsoft Research, Redmond, WA, United States; Independent Researcher, Seattle, WA, United States","Ranganeni V., University of Washington, Seattle, WA, United States; Sinclair M., Microsoft Research, Redmond, WA, United States; Ofek E., Microsoft Research, Redmond, WA, United States; Miller A., Independent Researcher, Seattle, WA, United States; Campbell J., Microsoft Research, Redmond, WA, United States; Kolobov A., Microsoft Research, Redmond, WA, United States; Cutrell E., Microsoft Research, Redmond, WA, United States","Only a small percentage of blind and low-vision people use traditional mobility aids such as a cane or a guide dog. Various assistive technologies have been proposed to address the limitations of traditional mobility aids. These devices often give either the user or the device majority of the control. In this work, we explore how varying levels of control affect the users' sense of agency, trust in the device, confidence, and successful navigation.We present Glide, a novel mobility aid with two modes for control: Glide-directed and User-directed. We employ Glide in a study (N=9) in which blind or low-vision participants used both modes to navigate through an indoor environment. Overall, participants found that Glide was easy to use and learn. Most participants trusted Glide despite its current limitations, and their confidence and performance increased as they continued to use Glide. Users' control mode preferences varied in different situations; no single mode ""won""in all situations.  © 2023 Copyright held by the owner/author(s).","assistive navigation; robotics; user study","Assistive technology; Robots; Assistive navigations; Assistive technology; Current limitation; Guide dogs; Indoor environment; Learn+; Low vision; Mobility aids; Sense of agencies; User study; Navigation","","","IEEE Computer Society","21672148","978-145039964-7","","","English","ACM/IEEE Int. Conf. Hum.-Rob. Interact.","Conference paper","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85150391784"
"Ferdous J.; Lee H.-N.; Jayarathna S.; Ashok V.","Ferdous, Javedul (57223136514); Lee, Hae-Na (57218554453); Jayarathna, Sampath (36052654200); Ashok, Vikas (55633359800)","57223136514; 57218554453; 36052654200; 55633359800","Enabling Efficient Web Data-record Interaction for People with Visual Impairments via Proxy Interfaces","2023","ACM Transactions on Interactive Intelligent Systems","13","3","13","","","","3","10.1145/3579364","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85163766240&doi=10.1145%2f3579364&partnerID=40&md5=9f14a78cf9fe2b9084cb71c07456020b","Old Dominion University, Norfolk, 23529, VA, United States; Stony Brook University, Stony Brook, 11794, NY, United States","Ferdous J., Old Dominion University, Norfolk, 23529, VA, United States; Lee H.-N., Stony Brook University, Stony Brook, 11794, NY, United States; Jayarathna S., Old Dominion University, Norfolk, 23529, VA, United States; Ashok V., Old Dominion University, Norfolk, 23529, VA, United States","Web data records are usually accompanied by auxiliary webpage segments, such as filters, sort options, search form, and multi-page links, to enhance interaction efficiency and convenience for end users. However, blind and visually impaired (BVI) persons are presently unable to fully exploit the auxiliary segments like their sighted peers, since these segments are scattered all across the screen, and as such assistive technologies used by BVI users, i.e., screen reader and screen magnifier, are not geared for efficient interaction with such scattered content. Specifically, for blind screen reader users, content navigation is predominantly one-dimensional despite the support for skipping content, and therefore navigating to-and-fro between different parts of the webpage is tedious and frustrating. Similarly, low vision screen magnifier users have to continuously pan back-and-forth between different portions of a webpage, given that only a portion of the screen is viewable at any instant due to content enlargement. The extant techniques to overcome inefficient web interaction for BVI users have mostly focused on general web-browsing activities, and as such they provide little to no support for data record-specific interaction activities such as filtering and sorting—activities that are equally important for facilitating quick and easy access to desired data records. To fill this void, we present InSupport, a browser extension that: (i) employs custom machine learning-based algorithms to automatically extract auxiliary segments on any webpage containing data records; and (ii) provides an instantly accessible proxy one-stop interface for easily navigating the extracted auxiliary segments using either basic keyboard shortcuts or mouse actions. Evaluation studies with 14 blind participants and 16 low vision participants showed significant improvement in web usability with InSupport, driven by an increased reduction in interaction time and user effort, compared to the state-of-the-art solutions. © 2023 Association for Computing Machinery. All rights reserved.","Additional Key Words and Phrases: Web accessibility; blind; data records; low vision; screen magnifier; screen reader; visual impairment","Machine learning; Mammals; Additional key word and phrase: web accessibility; Blind; Data record; Key words; Key-phrase; Low vision; Screen magnifier; Screen readers; Visual impairment; Web accessibility; Websites","","","Association for Computing Machinery","21606455","","","","English","ACM Trans. Interact. Intelligent Syst.","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85163766240"
"Cai S.; Ram A.; Gou Z.; Shaikh M.A.W.; Chen Y.-A.; Wan Y.; Hara K.; Zhao S.; Hsu D.","Cai, Shaojun (56742658100); Ram, Ashwin (57203205706); Gou, Zhengtai (58237048300); Shaikh, Mohd Alqama Wasim (59151968800); Chen, Yu-An (59151315100); Wan, Yingjia (57203788955); Hara, Kotaro (55498092800); Zhao, Shengdong (8433003800); Hsu, David (34569904000)","56742658100; 57203205706; 58237048300; 59151968800; 59151315100; 57203788955; 55498092800; 8433003800; 34569904000","Navigating Real-World Challenges: A Qadruped Robot Guiding System for Visually Impaired People in Diverse Environments","2024","Conference on Human Factors in Computing Systems - Proceedings","","","44","","","","0","10.1145/3613904.3642227","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85194497102&doi=10.1145%2f3613904.3642227&partnerID=40&md5=509a205bd4916ef4e64db49d29107708","Department of Computer Science, National University, Singapore; NUS-HCI Lab, Department of Computer Science, National University of Singapore, Singapore; Xinya College, Tsinghua University, China; Department of Mechanical Engineering, Veermata Jijabai Technological Institute, India; Yale-NUS College, Singapore; Institute of Psychology, Chinese Academy of Sciences, China; School of Computing and Information Systems, Singapore Management University, Singapore; NUS-HCI Lab, National University of Singapore, Singapore; Department of Computer Science, National University of Singapore, Singapore","Cai S., Department of Computer Science, National University, Singapore; Ram A., NUS-HCI Lab, Department of Computer Science, National University of Singapore, Singapore; Gou Z., Xinya College, Tsinghua University, China; Shaikh M.A.W., Department of Mechanical Engineering, Veermata Jijabai Technological Institute, India; Chen Y.-A., Yale-NUS College, Singapore; Wan Y., Institute of Psychology, Chinese Academy of Sciences, China; Hara K., School of Computing and Information Systems, Singapore Management University, Singapore; Zhao S., NUS-HCI Lab, National University of Singapore, Singapore; Hsu D., Department of Computer Science, National University of Singapore, Singapore","Blind and Visually Impaired (BVI) people find challenges in navigating unfamiliar environments, even using assistive tools such as white canes or smart devices. Increasingly affordable quadruped robots offer us opportunities to design autonomous guides that could improve how BVI people find ways around unfamiliar environments and maneuver therein. In this work, we designed RDog, a quadruped robot guiding system that supports BVI individuals' navigation and obstacle avoidance in indoor and outdoor environments. RDog combines an advanced mapping and navigation system to guide users with force feedback and preemptive voice feedback. Using this robot as an evaluation apparatus, we conducted experiments to investigate the difference in BVI people's ambulatory behaviors using a white cane, a smart cane, and RDog. Results illustrated the benefits of RDog-based ambulation, including faster and smoother navigation with fewer collisions and limitations, and reduced cognitive load. We discuss the implications of our work for multi-terrain assistive guidance systems. © 2024 Copyright held by the owner/author(s)","assistive technology; navigation; orientation and mobility; robot guide dog; visual impairment","Assistive technology; Feedback; Multipurpose robots; Navigation systems; Assistive technology; Blind and visually impaired; Guide dogs; Guiding systems; Orientation and mobility; Robot guide dog; Robot guides; Robot guiding; Visual impairment; Visually impaired people; Machine design","S. Zhao; NUS-HCI Lab, National University of Singapore, Singapore; email: shengdong.zhao@cityu.edu.hk; D. Hsu; Department of Computer Science, National University of Singapore, Singapore; email: dyhsu@comp.nus.edu.sg","","Association for Computing Machinery","","979-840070330-0","","","English","Conf Hum Fact Comput Syst Proc","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85194497102"
"Bamdad M.; Scaramuzza D.; Darvishy A.","Bamdad, Marziyeh (58030736000); Scaramuzza, Davide (16550950000); Darvishy, Alireza (24723823200)","58030736000; 16550950000; 24723823200","SLAM for Visually Impaired People: A Survey","2024","IEEE Access","","","","","","","0","10.1109/ACCESS.2024.3454571","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85203518725&doi=10.1109%2fACCESS.2024.3454571&partnerID=40&md5=562d71bf5fc5c3e3508ddaddc26d5367","University of Zurich, Department of Informatics, Zurich, 8050, Switzerland; Zurich University of Applied Sciences, Institute of Applied Information Technology, Winterthur, 88400, Switzerland","Bamdad M., University of Zurich, Department of Informatics, Zurich, 8050, Switzerland, Zurich University of Applied Sciences, Institute of Applied Information Technology, Winterthur, 88400, Switzerland; Scaramuzza D., University of Zurich, Department of Informatics, Zurich, 8050, Switzerland; Darvishy A., Zurich University of Applied Sciences, Institute of Applied Information Technology, Winterthur, 88400, Switzerland","In recent decades, several assistive technologies have been developed to improve the ability of blind and visually impaired (BVI) individuals to navigate independently and safely. At the same time, simultaneous localization and mapping (SLAM) techniques have become sufficiently robust and efficient to be adopted in developing these assistive technologies. We present the first systematic literature review of 54 recent studies on SLAM-based solutions for blind and visually impaired people, focusing on literature published from 2017 onward. This review explores various localization and mapping techniques employed in this context. We systematically identified and categorized diverse SLAM approaches and analyzed their localization and mapping techniques, sensor types, computing resources, and machine-learning methods. We discuss the advantages and limitations of these techniques for blind and visually impaired navigation. Moreover, we examine the major challenges described across studies, including practical challenges and considerations that affect usability and adoption. Our analysis also evaluates the effectiveness of these SLAM-based solutions in real-world scenarios and user satisfaction, providing insights into their practical impact on BVI mobility. The insights derived from this review identify critical gaps and opportunities for future research activities, particularly in addressing the challenges presented by dynamic and complex environments. We explain how SLAM technology offers the potential to improve the ability of visually impaired individuals to navigate effectively. Finally, we present future opportunities and challenges in this domain.  © 2013 IEEE.","Navigation; SLAM; systematic literature review; visually impaired","","M. Bamdad; University of Zurich, Department of Informatics, Zurich, 8050, Switzerland; email: bamdad@ifi.uzh.ch","","Institute of Electrical and Electronics Engineers Inc.","21693536","","","","English","IEEE Access","Article","Article in press","All Open Access; Gold Open Access","Scopus","2-s2.0-85203518725"
"Yi D.","Yi, Ding (59259524600)","59259524600","Embedded Computer Vision for Obstacle Detection and Recognition in Intelligent Assistance System for Visually Impaired Individual","2024","2024 IEEE 4th International Conference on Electronic Communications, Internet of Things and Big Data, ICEIB 2024","","","","201","204","3","0","10.1109/ICEIB61477.2024.10602660","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85201233911&doi=10.1109%2fICEIB61477.2024.10602660&partnerID=40&md5=4315731c6c60a3e716ab83d539483aca","Qingdao Engineering Vocational College, Qingdao, China","Yi D., Qingdao Engineering Vocational College, Qingdao, China","A detection system for obstacles was developed using embedded computer vision techniques for mobile capability and protection for the blind. System implementation, performance monitoring, and model assessments were conducted to develop the system. The proposed system recognizes obstacles and is the most effective in the detection of vehicles. Despite the successful accomplishment, more research is needed to take advantage of new technologies and to fine-tune and perfect the system in terms of accuracy, robustness, usage, pertinence, and effectiveness.  © 2024 IEEE.","assistive technology; embedded computer vision; mobility; obstacle detection; visually impaired","Assistive technology; Computer vision; Assistance system; Assistive technology; Computer vision techniques; Detection system; Embedded computer vision; Intelligent assistances; Mobility; Obstacle recognition; Obstacles detection; Visually impaired; Obstacle detectors","D. Yi; Qingdao Engineering Vocational College, Qingdao, China; email: tinyii_QD@163.com","Meen T.-H.","Institute of Electrical and Electronics Engineers Inc.","","979-835036072-1","","","English","IEEE Int. Conf. Electron. Commun., Internet Things Big Data, ICEIB","Conference paper","Final","","Scopus","2-s2.0-85201233911"
"Bhaskar Nikhil S.; Sharma A.; Nair N.S.; Sai Srikar C.; Wutla Y.; Rahul B.; Jhavar S.; Tambe P.","Bhaskar Nikhil, S. (58745137100); Sharma, Ambuj (57210974634); Nair, Niranjan S. (58153404200); Sai Srikar, C. (58744925400); Wutla, Yatish (58744716900); Rahul, Bhavanasi (58745967400); Jhavar, Suyog (55873065800); Tambe, Pankaj (26023828800)","58745137100; 57210974634; 58153404200; 58744925400; 58744716900; 58745967400; 55873065800; 26023828800","Design and Evaluation of a Multi-Sensor Assistive Robot for the Visually Impaired","2024","Lecture Notes in Mechanical Engineering","","","","119","131","12","0","10.1007/978-981-99-5613-5_10","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85178591446&doi=10.1007%2f978-981-99-5613-5_10&partnerID=40&md5=dc5f94487c2c63abac9761d39fff24bc","VIT-AP University, A.P, Amaravati, India","Bhaskar Nikhil S., VIT-AP University, A.P, Amaravati, India; Sharma A., VIT-AP University, A.P, Amaravati, India; Nair N.S., VIT-AP University, A.P, Amaravati, India; Sai Srikar C., VIT-AP University, A.P, Amaravati, India; Wutla Y., VIT-AP University, A.P, Amaravati, India; Rahul B., VIT-AP University, A.P, Amaravati, India; Jhavar S., VIT-AP University, A.P, Amaravati, India; Tambe P., VIT-AP University, A.P, Amaravati, India","Visual impairment affects approximately 285 million people worldwide, the profound challenge faced by those who are visually impaired or blind lies in the intricate task of maneuvering through different surroundings. The intricate nature of navigating both indoor and outdoor spaces poses significant difficulties for people with visual impairments. This paper presents an innovative undertaking that strives to address this very challenge by developing a cutting-edge robot dedicated to guiding the blind securely through various environments. Employing a sophisticated amalgamation of diverse sensors and a high-resolution camera, the robot adeptly identifies obstacles and furnishes the user with real-time information pertaining to their immediate surroundings. Remarkably, this state-of-the-art robot can be seamlessly operated through voice commands, facilitated by a bespoke mobile application, thereby enabling users to effortlessly guide it to their desired destinations. Equipped with comprehensive audio feedback capabilities, the robot effectively communicates crucial details to the user, encompassing obstacles, directions, and more. This robot can help blind people to navigate unfamiliar environments and travel independently. © 2024, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Assistive robot; Multi-sensor; Obstacles avoidance; Visually impaired; Voice command","Machine design; Assistive robots; Cutting edges; Design and evaluations; Indoor space; Multi sensor; Obstacles avoidance; Outdoor space; Visual impairment; Visually impaired; Voice command; Robots","A. Sharma; VIT-AP University, Amaravati, A.P, India; email: ambujsharma08@gmail.com","Tambe P.; Jhavar S.; Huang P.","Springer Science and Business Media Deutschland GmbH","21954356","978-981995612-8","","","English","Lect. Notes Mech. Eng.","Conference paper","Final","","Scopus","2-s2.0-85178591446"
"Surendran R.; Chihi I.; Anitha J.; Hemanth D.J.","Surendran, Ranjini (57215723227); Chihi, Ines (54948272000); Anitha, J. (57204786853); Hemanth, D. Jude (35100335700)","57215723227; 54948272000; 57204786853; 35100335700","Indoor Scene Recognition: An Attention-Based Approach Using Feature Selection-Based Transfer Learning and Deep Liquid State Machine","2023","Algorithms","16","9","430","","","","1","10.3390/a16090430","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85172237886&doi=10.3390%2fa16090430&partnerID=40&md5=626df708954c5a70ff776c4ead4b1f95","Department of ECE, Karunya Institute of Technology and Sciences, Coimbatore, 641114, India; Department of Engineering, Faculty of Science, Technology and Medicine, University of Luxembourg, Luxembourg, 1359, Luxembourg","Surendran R., Department of ECE, Karunya Institute of Technology and Sciences, Coimbatore, 641114, India; Chihi I., Department of Engineering, Faculty of Science, Technology and Medicine, University of Luxembourg, Luxembourg, 1359, Luxembourg; Anitha J., Department of Engineering, Faculty of Science, Technology and Medicine, University of Luxembourg, Luxembourg, 1359, Luxembourg; Hemanth D.J., Department of ECE, Karunya Institute of Technology and Sciences, Coimbatore, 641114, India","Scene understanding is one of the most challenging areas of research in the fields of robotics and computer vision. Recognising indoor scenes is one of the research applications in the category of scene understanding that has gained attention in recent years. Recent developments in deep learning and transfer learning approaches have attracted huge attention in addressing this challenging area. In our work, we have proposed a fine-tuned deep transfer learning approach using DenseNet201 for feature extraction and a deep Liquid State Machine model as the classifier in order to develop a model for recognising and understanding indoor scenes. We have included fuzzy colour stacking techniques, colour-based segmentation, and an adaptive World Cup optimisation algorithm to improve the performance of our deep model. Our proposed model would dedicatedly assist the visually impaired and blind to navigate in the indoor environment and completely integrate into their day-to-day activities. Our proposed work was implemented on the NYU depth dataset and attained an accuracy of 96% for classifying the indoor scenes. © 2023 by the authors.","deep learning; DenseNet; fuzzy colour stacking; liquid state machine; transfer learning; world cup optimization","Classification (of information); Deep learning; Feature extraction; Learning systems; Deep learning; Densenet; Fuzzy color stacking; Liquid state machines; Optimisations; Scene understanding; Stackings; Transfer learning; World cup; World cup optimization; Color","D.J. Hemanth; Department of ECE, Karunya Institute of Technology and Sciences, Coimbatore, 641114, India; email: judehemanth@karunya.edu","","Multidisciplinary Digital Publishing Institute (MDPI)","19994893","","","","English","Algorithms","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85172237886"
"Ruiz-Serra J.; White J.; Petrie S.; Kameneva T.; McCarthy C.","Ruiz-Serra, Jaime (57904284100); White, Jack (57213605188); Petrie, Stephen (57217009609); Kameneva, Tatiana (24450762000); McCarthy, Chris (58761583900)","57904284100; 57213605188; 57217009609; 24450762000; 58761583900","Learning Scene Representations for Human-assistive Displays Using Self-attention Networks","2024","ACM Transactions on Multimedia Computing, Communications and Applications","20","7","204","","","","0","10.1145/3650111","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85193684992&doi=10.1145%2f3650111&partnerID=40&md5=7ca0ccfca65dc32e4b070ff9db362a04","Swinburne University of Technology, Hawthorn, Australia; University of Melbourne, Parkville, Australia","Ruiz-Serra J., Swinburne University of Technology, Hawthorn, Australia; White J., Swinburne University of Technology, Hawthorn, Australia; Petrie S., Swinburne University of Technology, Hawthorn, Australia; Kameneva T., University of Melbourne, Parkville, Australia; McCarthy C., Swinburne University of Technology, Hawthorn, Australia","Video-see-through (VST) augmented reality (AR) is widely used to present novel augmentative visual experiences by processing video frames for viewers. Among VST AR systems, assistive vision displays aim to compensate for low vision or blindness, presenting enhanced visual information to support activities of daily living for the vision impaired/deprived. Despite progress, current assistive displays suffer from a visual information bottleneck, limiting their functional outcomes compared to healthy vision. This motivates the exploration of methods to selectively enhance and augment salient visual information. Traditionally, vision processing pipelines for assistive displays rely on hand-crafted, single-modality filters, lacking adaptability to time-varying and environment-dependent needs. This article proposes the use of Deep Reinforcement Learning (DRL) and Self-attention (SA) networks as a means to learn vision processing pipelines for assistive displays. SA networks selectively attend to task-relevant features, offering a more parameter - and compute-efficient approach to RL-based task learning. We assess the feasibility of using SA networks in a simulation-trained model to generate relevant representations of real-world states for navigation with prosthetic vision displays. We explore two prosthetic vision applications, vision-to-auditory encoding, and retinal prostheses, using simulated phosphene visualisations. This article introduces SA-px, a general-purpose vision processing pipeline using self-attention networks, and SA-phos, a display-specific formulation targeting low-resolution assistive displays. We present novel scene visualisations derived from SA image patches importance rankings to support mobility with prosthetic vision devices. To the best of our knowledge, this is the first application of self-attention networks to the task of learning vision processing pipelines for prosthetic vision or assistive displays. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.","deep reinforcement learning; display algorithms; prosthetic vision; Vision processing","Augmented reality; Deep learning; Pipeline processing systems; Pipelines; Prosthetics; Visualization; Assistive; Deep reinforcement learning; Display algorithm; Prosthetic vision; Reinforcement learnings; Scene representation; Video frame; Vision processing; Visual experiences; Visual information; Reinforcement learning","","","Association for Computing Machinery","15516857","","","","English","ACM Trans. Multimedia Comput. Commun. Appl.","Article","Final","","Scopus","2-s2.0-85193684992"
"Gowthami M.; Shreya P.; Sripriya T.; Yazhini B.","Gowthami, M. (57189306474); Shreya, P. (57480524500); Sripriya, T. (58988503800); Yazhini, B. (58988361700)","57189306474; 57480524500; 58988503800; 58988361700","Cognitive Vision Companion: An AI-Enhanced Support System for the Visually Impaired","2024","2024 2nd International Conference on Computer, Communication and Control, IC4 2024","","","","","","","0","10.1109/IC457434.2024.10486214","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85190495503&doi=10.1109%2fIC457434.2024.10486214&partnerID=40&md5=d1287fcdfb319f256013498a618b7d84","Easwari Engineering College, Department of Computer Science and Engineering, Chennai, India","Gowthami M., Easwari Engineering College, Department of Computer Science and Engineering, Chennai, India; Shreya P., Easwari Engineering College, Department of Computer Science and Engineering, Chennai, India; Sripriya T., Easwari Engineering College, Department of Computer Science and Engineering, Chennai, India; Yazhini B., Easwari Engineering College, Department of Computer Science and Engineering, Chennai, India","In a world where a major portion of the populace suffer with daily challenges of blindness, this pioneering project introduces a comprehensive smart support system designed to aid the needs of people who are visually impaired. This advanced system is ingeniously integrated into a pair of headphones, combining various state-of-the-art technologies such as cameras, ultrasonic sensors, and cutting-edge machine learning algorithms. Its primary goals encompass recognizing objects, accurately estimating distances between objects and users, deciphering captured signs and indications, and offering real-time location-based navigation and directions. This multifunctional blind assistance device is designed to empower users by enhancing their spatial awareness and providing them the tools they need to navigate travel securely and on their own. It represents a remarkable leap forward in refining the overall quality of life and mobility for the blind people.  © 2024 IEEE.","machine learning; navigation; object detection; object distance estimation; visually impaired","Learning algorithms; Machine learning; Object detection; Ultrasonic applications; Advanced systems; Cognitive vision; Distance estimation; Machine-learning; Object distance; Object distance estimation; Objects detection; State-of-the-art technology; Support systems; Visually impaired; Ultrasonic cameras","M. Gowthami; Easwari Engineering College, Department of Computer Science and Engineering, Chennai, India; email: gowthami.m@eec.srmrmp.edu.in","","Institute of Electrical and Electronics Engineers Inc.","","979-835038793-3","","","English","Int. Conf. Comput., Commun. Control, IC4","Conference paper","Final","","Scopus","2-s2.0-85190495503"
"Ashmad Ahemed S.; Jayakumar D.","Ashmad Ahemed, S. (58988065800); Jayakumar, D. (57212310120)","58988065800; 57212310120","Voice Assisted Facial Emotion Recognition System For Blind Peoples With Tensorflow Model","2024","2024 IEEE International Students' Conference on Electrical, Electronics and Computer Science, SCEECS 2024","","","","","","","1","10.1109/SCEECS61402.2024.10481892","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85190556944&doi=10.1109%2fSCEECS61402.2024.10481892&partnerID=40&md5=4281e37669bc138557786508d779f6d4","Ifet College of Engineering, Department of Computer Science and Engineering, Villupuram, India","Ashmad Ahemed S., Ifet College of Engineering, Department of Computer Science and Engineering, Villupuram, India; Jayakumar D., Ifet College of Engineering, Department of Computer Science and Engineering, Villupuram, India","This research project presents the development of a novel Voice-Assisted Facial Emotion Recognition System tailored to address the unique needs of blind individuals. Leveraging the power of deep learning and the TensorFlow framework, the system employs computer vision techniques to detect and interpret facial emotions, enabling blind users to comprehend the emotional cues of those they interact with. The project encompasses data collection, preprocessing, model training, and evaluation, resulting in a robust TensorFlow model capable of real-time emotion recognition from facial images. A voice assistant integrates seamlessly into the system, offering spoken feedback to users, thus enhancing their understanding of emotional contexts during social interactions. The system prioritizes user accessibility and privacy, ensuring a user-friendly interface and adherence to data security standards. Extensive testing and user feedback are essential components of the project, allowing for continuous improvement and optimization to meet the specific needs of blind individuals. This research offers a promising solution to empower the visually impaired community by providing them with a valuable tool for enhanced social interaction and emotional understanding. Real-World Application, Emphasize the practicality and real-world applicability of the system, showcasing how it can make a meaningful impact on the daily lives of blind individuals by helping them navigate social and emotional contexts more effectively. Multimodal Interaction, Highlight the multimodal nature of the system, which combines visual facial emotion recognition with voice-based interaction to provide a comprehensive and accessible user experience.  © 2024 IEEE.","Convolutional Neural Network; Emotion identification; Tensorflow","Convolutional neural networks; Deep learning; Face recognition; Speech recognition; User interfaces; Blind individuals; Blind people; Convolutional neural network; Emotion identifications; Emotion recognition; Facial emotions; Real-world; Recognition systems; Social interactions; Tensorflow; Emotion Recognition","S. Ashmad Ahemed; Ifet College of Engineering, Department of Computer Science and Engineering, Villupuram, India; email: ash25032003@gmail.com","Kapri D.","Institute of Electrical and Electronics Engineers Inc.","","979-835034846-0","","","English","IEEE Int. Students' Conf. Electr., Electron. Comput. Sci., SCEECS","Conference paper","Final","","Scopus","2-s2.0-85190556944"
"Ben Atitallah A.; Said Y.; Ben Atitallah M.A.; Albekairi M.; Kaaniche K.; Alanazi T.M.; Boubaker S.; Atri M.","Ben Atitallah, Ahmed (8576049700); Said, Yahia (53867137900); Ben Atitallah, Mohamed Amin (57217948619); Albekairi, Mohammed (57763190200); Kaaniche, Khaled (57217928562); Alanazi, Turki M. (57201554813); Boubaker, Sahbi (36545771900); Atri, Mohamed (23017853700)","8576049700; 53867137900; 57217948619; 57763190200; 57217928562; 57201554813; 36545771900; 23017853700","Embedded implementation of an obstacle detection system for blind and visually impaired persons’ assistance navigation","2023","Computers and Electrical Engineering","108","","108714","","","","9","10.1016/j.compeleceng.2023.108714","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85153525072&doi=10.1016%2fj.compeleceng.2023.108714&partnerID=40&md5=15aee4edfb6b2ca39d16ac54cb4c9442","Department of Electrical Engineering, College of Engineering, Jouf University, Sakaka, Saudi Arabia; Remote Sensing Unit, College of Engineering, Northern Border University, Arar, Saudi Arabia; Laboratory of Electronics and Microelectronics (LR99ES30), University of Monastir, Tunisia; Laboratory of informatics, Gaspard-Monge, A3SI, ESIEE Paris, CNRS, Gustave Eiffel University, France; LETI, ENIS, University of Sfax, Sfax, Tunisia; College of Computer Science and Engineering, University of Jeddah, Jeddah, Saudi Arabia; College of Computer Sciences, King Khalid University, Abha, Saudi Arabia","Ben Atitallah A., Department of Electrical Engineering, College of Engineering, Jouf University, Sakaka, Saudi Arabia; Said Y., Remote Sensing Unit, College of Engineering, Northern Border University, Arar, Saudi Arabia, Laboratory of Electronics and Microelectronics (LR99ES30), University of Monastir, Tunisia; Ben Atitallah M.A., Laboratory of informatics, Gaspard-Monge, A3SI, ESIEE Paris, CNRS, Gustave Eiffel University, France, LETI, ENIS, University of Sfax, Sfax, Tunisia; Albekairi M., Department of Electrical Engineering, College of Engineering, Jouf University, Sakaka, Saudi Arabia; Kaaniche K., Department of Electrical Engineering, College of Engineering, Jouf University, Sakaka, Saudi Arabia; Alanazi T.M., Department of Electrical Engineering, College of Engineering, Jouf University, Sakaka, Saudi Arabia; Boubaker S., College of Computer Science and Engineering, University of Jeddah, Jeddah, Saudi Arabia; Atri M., College of Computer Sciences, King Khalid University, Abha, Saudi Arabia","Blind and Visually Impaired (BVI) persons encounter safety problems during their navigation. Therefore, assisting BVI must be addressed. Obstacle detection and avoidance in real scenes present very challenging tasks. To handle this challenge, we suggested developing a new obstacle detection system based on an enhanced YOLO v5 neural network. The improved network architecture increased both the network's speed and the detection accuracy. This was achieved by integrating the DenseNet into the YOLO v5 backbone, which impacted the reuse of features and data transfer with additional modifications. Aiming to ensure an embedded implementation of the proposed work on a ZCU 102 board, we applied two compression techniques: channel pruning and quantization. The performance of the suggested system in terms of detection and processing speed showed very encouraging results. In fact, it achieves a detection accuracy of 83.42% and a detection speed of 43 Frame Per Second (FPS). © 2023 Elsevier Ltd","Blind and visually impaired; Deep learning; FPGA; Obstacle avoidance; Vitis ai","Data transfer; Deep learning; Network architecture; Obstacle detectors; Blind and visually impaired; Deep learning; Detection accuracy; Detection speed; Embedded implementation; Obstacles avoidance; Obstacles detection systems; Safety problems; Visually impaired persons; Vitis ai; Field programmable gate arrays (FPGA)","A. Ben Atitallah; Department of Electrical Engineering, College of Engineering, Jouf University, Sakaka, Saudi Arabia; email: abenatitallah@ju.edu.sa","","Elsevier Ltd","00457906","","CPEEB","","English","Comput Electr Eng","Article","Final","","Scopus","2-s2.0-85153525072"
"Kuriakose B.; Shrestha R.; Sandnes F.E.","Kuriakose, Bineeth (57189239916); Shrestha, Raju (37016669000); Sandnes, Frode Eika (35594004500)","57189239916; 37016669000; 35594004500","Assessing the performance, usability and cognitive workload of an AI-based navigation assistant: a quantitative study with people with visual impairments","2024","International Journal of Human Factors and Ergonomics","11","2","","129","156","27","0","10.1504/IJHFE.2024.139166","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85197880930&doi=10.1504%2fIJHFE.2024.139166&partnerID=40&md5=cc0b4714e427a3a5d27d27b0b72bf09e","Department of Computer Science, Oslo Metropolitan University, Oslo, Norway","Kuriakose B., Department of Computer Science, Oslo Metropolitan University, Oslo, Norway; Shrestha R., Department of Computer Science, Oslo Metropolitan University, Oslo, Norway; Sandnes F.E., Department of Computer Science, Oslo Metropolitan University, Oslo, Norway","Recent advances in artificial intelligence (AI) have propelled the development of assistive navigation systems for people with visual impairments. Many studies focusing on the benefits of using AI in navigation assistants overlook user perceptions and responses. Human factors are crucial in these systems, aiming to demonstrate enhanced capabilities and gain user acceptance. Notably, involving users in evaluations is vital for a realistic understanding of system usability and performance – a critical aspect frequently neglected in existing systems. This paper presents results from a study assessing an AI-based smartphone navigation assistant for the visually impaired. In this quantitative study, 13 users evaluated the navigation assistant in natural environments. Results indicate that although the navigation assistant helps in avoiding obstacles, participants express reservations about relying solely on an AI navigation assistant. These findings emphasise the need for a user-centric approach in designing and implementing AI technologies to improve user acceptance and usability. © 2024 Inderscience Enterprises Ltd.","AI; artificial intelligence; assistive technology; blind; cognitive workload; deep learning; DeepNAVI; human factors; navigation assistant; performance; portable; quantitative study; smartphone; usability; user acceptance; user evaluation; user perceptions; visual impairments","","B. Kuriakose; Department of Computer Science, Oslo Metropolitan University, Oslo, Norway; email: bineethk@oslomet.no","","Inderscience Publishers","20457804","","","","English","Int. J. Human Factors Ergonomics","Article","Final","","Scopus","2-s2.0-85197880930"
"Kaushik G.; Arun E.; Pavan K.H.D.; Reddy G.S.D.; Ramesh D.","Kaushik, G. (58209996600); Arun, Embari (58308555900); Pavan, K. Hema Durga (58308338700); Reddy, G. Suman Datta (58308556000); Ramesh, D. (57217015772)","58209996600; 58308555900; 58308338700; 58308556000; 57217015772","Smart stick for visually impaired using Raspberry Pi","2023","AIP Conference Proceedings","2492","","020055","","","","0","10.1063/5.0130495","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85161434562&doi=10.1063%2f5.0130495&partnerID=40&md5=d6f5a8bca16255f277648fdd71f71fcc","Department of Electronics and Communication Engineering, MLR Institute of Technology, Hyderabad, India","Kaushik G., Department of Electronics and Communication Engineering, MLR Institute of Technology, Hyderabad, India; Arun E., Department of Electronics and Communication Engineering, MLR Institute of Technology, Hyderabad, India; Pavan K.H.D., Department of Electronics and Communication Engineering, MLR Institute of Technology, Hyderabad, India; Reddy G.S.D., Department of Electronics and Communication Engineering, MLR Institute of Technology, Hyderabad, India; Ramesh D., Department of Electronics and Communication Engineering, MLR Institute of Technology, Hyderabad, India","The main aim of this work is to help blind people by providing a smart stick to them. We know that blind people find a lot of difficulties in distinguishing obstacles while walking on the road. This work will provide safe communication and object recognizing for blind people by making use of Raspberry Pi. The mission of this work is to provide a voice based assistance to blind people. This system consists of ultrasonic sensors, Raspberry Pi, speaker, etc. This stick discovers an object around them and sends acknowledgement in the form of speech, danger alerts through audio by speaker for blind people. In ordinary stick, the obstacle detection is not performed and ordinary stick is not efficient for blind persons. Because the visually impaired person does not know what kind of obstacles or what type of things might come in front of him/her. It provides a low price and effective navigation and object detection assistance for blind people which provides a sense of artificial vision by giving information about the environment, things on all sides of them, so they can walk on one's own without any risk and difficulties. If any obstruction occurs in front of blind person, he/she can hear the sound produced by the speaker and can know about the obstacle or things. This stick is very useful for people who are blind and are frequently need help from others. © 2023 Author(s).","","","G. Kaushik; Department of Electronics and Communication Engineering, MLR Institute of Technology, Hyderabad, India; email: kaushikgundagani@gmail.com","Anand A.V.; Reddy M.V.; Gupta M.S.","American Institute of Physics Inc.","0094243X","978-073544438-6","","","English","AIP Conf. Proc.","Conference paper","Final","","Scopus","2-s2.0-85161434562"
"Fink P.D.S.; Doore S.A.; Lin X.; Maring M.; Zhao P.; Nygaard A.; Beals G.; Corey R.R.; Perry R.J.; Freund K.; Dimitrov V.; Giudice N.A.","Fink, Paul D.S. (57216695324); Doore, Stacy A. (26030371200); Lin, Xue (57205018638); Maring, Matthew (58528593400); Zhao, Pu (57792740400); Nygaard, Aubree (57226306483); Beals, Grant (58528721700); Corey, Richard R. (57190189925); Perry, Raymond J. (57222713824); Freund, Katherine (7006369785); Dimitrov, Velin (56028136100); Giudice, Nicholas A. (15724751900)","57216695324; 26030371200; 57205018638; 58528593400; 57792740400; 57226306483; 58528721700; 57190189925; 57222713824; 7006369785; 56028136100; 15724751900","The Autonomous Vehicle Assistant (AVA): Emerging technology design supporting blind and visually impaired travelers in autonomous transportation","2023","International Journal of Human Computer Studies","179","","103125","","","","2","10.1016/j.ijhcs.2023.103125","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85167414819&doi=10.1016%2fj.ijhcs.2023.103125&partnerID=40&md5=596b5f074456463da2e04429b463df72","Virtual Environment and Multimodal Interaction (VEMI) Lab, The University of Maine, Orono, 04469, ME, United States; School of Computing and Information Science, The University of Maine, Orono, ME, United States; INSITE Lab; Department of Computer Science, Colby College, Waterville, ME, United States; Northeastern University, Boston, MA, United States; ITNAmerica, Westbrook, ME, United States; Toyota Research Institute, Cambridge, MA, United States","Fink P.D.S., Virtual Environment and Multimodal Interaction (VEMI) Lab, The University of Maine, Orono, 04469, ME, United States, School of Computing and Information Science, The University of Maine, Orono, ME, United States; Doore S.A., INSITE Lab; Department of Computer Science, Colby College, Waterville, ME, United States; Lin X., Northeastern University, Boston, MA, United States; Maring M., INSITE Lab; Department of Computer Science, Colby College, Waterville, ME, United States; Zhao P., Northeastern University, Boston, MA, United States; Nygaard A., Virtual Environment and Multimodal Interaction (VEMI) Lab, The University of Maine, Orono, 04469, ME, United States; Beals G., Virtual Environment and Multimodal Interaction (VEMI) Lab, The University of Maine, Orono, 04469, ME, United States; Corey R.R., Virtual Environment and Multimodal Interaction (VEMI) Lab, The University of Maine, Orono, 04469, ME, United States; Perry R.J., Virtual Environment and Multimodal Interaction (VEMI) Lab, The University of Maine, Orono, 04469, ME, United States; Freund K., ITNAmerica, Westbrook, ME, United States; Dimitrov V., Toyota Research Institute, Cambridge, MA, United States; Giudice N.A., Virtual Environment and Multimodal Interaction (VEMI) Lab, The University of Maine, Orono, 04469, ME, United States, School of Computing and Information Science, The University of Maine, Orono, ME, United States","The U.S. Department of Transportation's Inclusive Design Challenge spurred innovative research promoting accessible technology for people with disabilities in the future of autonomous transportation. This paper presents the user-driven design of the Autonomous Vehicle Assistant (AVA), a winning project of the challenge focused on solutions for people who are blind and visually impaired. Results from an initial survey (n = 90) and series of user interviews (n = 12) informed AVA's novel feature set, which was evaluated through a formal navigation study (n = 10) and participatory design evaluations (n = 6). Aggregate findings suggest that AVA's sensor fusion approach combining computer vision, last-meter assistance, and multisensory alerts provide critical solutions for users poised to benefit most from this emerging transportation technology. © 2023 Elsevier Ltd","Accessibility; Autonomous vehicles; People with visual impairment","Accessibility; Autonomous Vehicles; Blind and visually impaired; Emerging technologies; Inclusive design; People with visual impairment; Technology designs; U.S. Department of Transportation; US Department of Transportation; Visual impairment; Autonomous vehicles","P.D.S. Fink; Virtual Environment and Multimodal Interaction (VEMI) Lab, The University of Maine, Orono, 04469, United States; email: paul.fink@maine.edu","","Academic Press","10715819","","IHSTE","","English","Int J Hum Comput Stud","Article","Final","","Scopus","2-s2.0-85167414819"
"Ramyadevi R.; Loganathan R.; Karthikeyan R.; Vijay A.","Ramyadevi, R. (59186842500); Loganathan, R. (57983876800); Karthikeyan, R. (59170295900); Vijay, A. (59170153200)","59186842500; 57983876800; 59170295900; 59170153200","Smart Blind Stick with Wristband: Obstacle Detection and Warning System","2024","Springer Proceedings in Mathematics and Statistics","421","","","725","733","8","0","10.1007/978-3-031-51167-7_69","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85195840109&doi=10.1007%2f978-3-031-51167-7_69&partnerID=40&md5=2ccbbb5bae86ebfc6f0ff9f8e88316b9","Department of Computer Science & Applications, SRM Institute of Science and Technology, Chennai, India","Ramyadevi R., Department of Computer Science & Applications, SRM Institute of Science and Technology, Chennai, India; Loganathan R., Department of Computer Science & Applications, SRM Institute of Science and Technology, Chennai, India; Karthikeyan R., Department of Computer Science & Applications, SRM Institute of Science and Technology, Chennai, India; Vijay A., Department of Computer Science & Applications, SRM Institute of Science and Technology, Chennai, India","The smart blind stick-connected wristwatch is a device designed to assist visually impaired individuals with independent navigation while also providing additional features to enhance their daily life. The device combines the functionality of a traditional blind stick with the convenience of a wearable wristwatch, making it a discreet and convenient option for users. The device includes sensors and technologies such as object recognition and environmental sensors to provide feedback to the user, allowing them to navigate safely and confidently. The object recognition feature uses machine learning algorithms and computer vision to identify obstacles in the user’s path, such as stairs or curbs, and provides feedback through vibrations or audio cues. The environmental sensors provide information about the weather or other environmental conditions to help the user plan their route. The device also includes an emergency response feature that allows the user to quickly call for assistance if needed, providing peace of mind and a sense of security. The blind stick–connected wristwatch provides a comprehensive solution for visually impaired individuals, combining navigation, connectivity, and safety features in a discreet and convenient wearable device. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.","Assistive technology; Reading/mobility aids; Wearable devices and systems","Assistive technology; Environmental technology; Feedback; Learning algorithms; Object recognition; Wearable computers; Assistive technology; Devices and systems; Environmental sensor; Mobility aids; Objects recognition; Obstacle warning systems; Reading/mobility aid; Visually impaired; Wearable devices; Wearable systems; Machine learning","","Lin F.M.; Patel A.; Kesswani N.; Sambana B.","Springer","21941009","978-303151166-0","","","English","Springer Proc. Math. Stat.","Conference paper","Final","","Scopus","2-s2.0-85195840109"
"Hsieh Y.-Z.; Ku X.-L.; Lin S.-S.","Hsieh, Yi-Zeng (16031160700); Ku, Xiang-Long (58309593000); Lin, Shih-Syun (35253775800)","16031160700; 58309593000; 35253775800","The development of assisted- visually impaired people robot in the indoor environment based on deep learning","2024","Multimedia Tools and Applications","83","3","","6555","6578","23","0","10.1007/s11042-023-15644-y","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85161577683&doi=10.1007%2fs11042-023-15644-y&partnerID=40&md5=b9896845056bf1c36fa42f97e092b6c1","Department of Electrical Engineering, National Taiwan University of Science and Technology, Taipei City, 106, Taiwan; Department of Computer Science and Engineering, National Taiwan Ocean University, Keelung City, Taiwan","Hsieh Y.-Z., Department of Electrical Engineering, National Taiwan University of Science and Technology, Taipei City, 106, Taiwan; Ku X.-L., Department of Electrical Engineering, National Taiwan University of Science and Technology, Taipei City, 106, Taiwan; Lin S.-S., Department of Computer Science and Engineering, National Taiwan Ocean University, Keelung City, Taiwan","The indoor positioning for visually impaired people has influence on their daily life in unknown indoor environment. This study designs the robot that can assist the blind walking safety and navigate in indoor environment by a single camera. The sense classification is proposed to position the blind in indoor environment by proposed convolutional neural network framework and integrate the semantic segmentation to find the road surface through a depth camera to guide the blind walking. The proposed vision-based sense classification method is compared with the traditional WiFi triangular-positioning method, and the average error of x-y coordinate position result as (9.25,3.65) is better. From the experiment, the designed robot can help the visually impaired people to indoor navigation in unknown indoor environment. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Convolutional neural network; Deep learning; Indoor positioning; Visually impaired people","Cameras; Convolution; Convolutional neural networks; Deep learning; Indoor positioning systems; Machine design; Semantic Segmentation; Semantics; Walking aids; Blind walking; Convolutional neural network; Daily lives; Deep learning; Indoor environment; Indoor positioning; Network frameworks; Single cameras; Study design; Visually impaired people; Robots","S.-S. Lin; Department of Computer Science and Engineering, National Taiwan Ocean University, Keelung City, Taiwan; email: linss@mail.ntou.edu.tw","","Springer","13807501","","MTAPF","","English","Multimedia Tools Appl","Article","Final","","Scopus","2-s2.0-85161577683"
"Palogiannidis D.","Palogiannidis, Dimitrios (57667988200)","57667988200","Development of a Real-Time Object Detection Algorithm with Voice Notification for Visually Impaired People","2024","Proceedings of the International Conference on Information Processes and Systems Development and Quality Assurance, IPS 2024","","","","40","42","2","0","10.1109/IPS62349.2024.10499607","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85191727717&doi=10.1109%2fIPS62349.2024.10499607&partnerID=40&md5=a9a9eb2ef6a96419d99680e11a8d2a77","Saint Petersburg State Electrotechnical University Leti, Department of Biotechnical Systems, Saint Petersburg, Russian Federation","Palogiannidis D., Saint Petersburg State Electrotechnical University Leti, Department of Biotechnical Systems, Saint Petersburg, Russian Federation","The number of visually impaired and blind individuals is large, however, the existing systems for their navigation do not match the technology of today. It is safe to say that there is need for improvement of the technology for the development of such systems, using modern means, such as Machine Learning and Computer Vision. The aim of this work is the improvement of such space navigation systems, by creating an algorithm for processing images of objects surrounding visually impaired people in a spatial navigation system, providing the expansion of information concerning their environment, improving the state of their navigation and their safety.  © 2024 IEEE.","Computer Vision; Distance Estimation; Gyroscope; LiDAR Sensor; Navigation Assistance; Object Detection; Speech Generator; Visual Impairment","Computer vision; Gyroscopes; Image enhancement; Object detection; Object recognition; Optical radar; Signal detection; Distance estimation; LiDAR sensor; Navigation assistance; Object detection algorithms; Objects detection; Real- time; Speech generator; Visual impairment; Visually impaired; Visually impaired people; Navigation systems","D. Palogiannidis; Saint Petersburg State Electrotechnical University Leti, Department of Biotechnical Systems, Saint Petersburg, Russian Federation; email: dimitris.palogiannidis@gmail.com","Shaposhnikov S.O.; Saint Petersburg Electrotechnical University �LETI�, Prof. Popov Str. 5, Saint Petersburg","Institute of Electrical and Electronics Engineers Inc.","","979-835037405-6","","","English","Proc. Int. Conf. Inf. Process. Syst. Dev. Qual. Assur., IPS","Conference paper","Final","","Scopus","2-s2.0-85191727717"
"Zhao L.; Zhang L.","Zhao, Lin (58074509200); Zhang, Lihang (58771610700)","58074509200; 58771610700","A Blind Navigation Algorithm Based on The PPYOLO Model","2023","ACM International Conference Proceeding Series","","","35","","","","0","10.1145/3632314.3632359","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85180150487&doi=10.1145%2f3632314.3632359&partnerID=40&md5=90d0d01ffe80e86710b2e0cb8dc8dbdb","School of Computer and Software, Dalian Neusoft University of Information, Liaoning, Dalian, 116023, China","Zhao L., School of Computer and Software, Dalian Neusoft University of Information, Liaoning, Dalian, 116023, China; Zhang L., School of Computer and Software, Dalian Neusoft University of Information, Liaoning, Dalian, 116023, China","Abstract: This study aims to address challenges faced by visually impaired individuals in daily travel, such as inability to perceive traffic lights, navigate obstacles, and locate tactile pavings. The goal is to develop a specialized travel assistance algorithm for visually impaired individuals. After thorough research on existing technologies including GPS, IrDA, AI-assisted technology, mobile applications, and object recognition, and analyzing their limitations, we proposed a blind navigation algorithm based on the PP-YOLO model. This algorithm combines traffic sign recognition, object recognition, and tactile paving recognition to achieve multi-task image recognition. Through dataset preparation, data annotation, and parameter adjustment, the algorithm was implemented on portable devices like mobile phones and wearable devices. Laboratory verification and field testing confirmed that the algorithm provides accurate navigation guidance and ensures safety for visually impaired individuals during travel. With a frame rate of 8fps, the algorithm achieved a recognition rate of 96.3% within an 8m range on portable devices; in low-light environments, the object recognition accuracy within the same range reached 94%. Even at complex intersections with a width of 30m, the recognition rate was 85%. This research outcome presents an innovative technical solution for travel assistance technology for visually impaired individuals and offers new research ideas for similar studies. The PP-YOLO model was chosen for its superior speed and accuracy in object detection tasks, crucial for real-time navigation. The model was streamlined for use on portable devices through lightweight design and hardware acceleration features such as NPU and GPU. The challenge with this approach involves some risk of recognition errors, similar to those in autonomous driving technology. However, considering the slower walking speed of visually impaired individuals, the consequences of these errors are mitigated. Thus, the project provides an additional mobility option for visually impaired individuals, providing more freedom than without it.  © 2023 ACM.","Artificial Intelligence; Blind Path Obstacle Detection; Deep Learning; Environmental Perception; Object Detection; Visual Processing","Deep learning; Navigation; Object recognition; Obstacle detectors; Blind navigation; Blind path obstacle detection; Deep learning; Environmental perceptions; Objects detection; Objects recognition; Obstacles detection; Portable device; Visual-processing; Visually impaired; Object detection","","","Association for Computing Machinery","","979-840070940-1","","","English","ACM Int. Conf. Proc. Ser.","Conference paper","Final","","Scopus","2-s2.0-85180150487"
"Raja J.; Kaviya N.; Koushika B.V.; Sushma R.","Raja, J. (7003515382); Kaviya, N. (58193253600); Koushika, B.V. (59185613600); Sushma, R. (59184701300)","7003515382; 58193253600; 59185613600; 59184701300","Integrated Image Processing Device For Visually Impaired","2024","2024 International Conference on Communication, Computing and Internet of Things, IC3IoT 2024 - Proceedings","","","","","","","0","10.1109/IC3IoT60841.2024.10550301","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85196721115&doi=10.1109%2fIC3IoT60841.2024.10550301&partnerID=40&md5=6fe0dd93dba5c8c0c5cd37fed9dcfc9a","Sri Sairam Engineering College, Department Of Electronics And Communication Engineering, Tamil Nadu, Chennai, India","Raja J., Sri Sairam Engineering College, Department Of Electronics And Communication Engineering, Tamil Nadu, Chennai, India; Kaviya N., Sri Sairam Engineering College, Department Of Electronics And Communication Engineering, Tamil Nadu, Chennai, India; Koushika B.V., Sri Sairam Engineering College, Department Of Electronics And Communication Engineering, Tamil Nadu, Chennai, India; Sushma R., Sri Sairam Engineering College, Department Of Electronics And Communication Engineering, Tamil Nadu, Chennai, India","In a world where technology is developing more quickly than ever before, many of these advances have been aimed at making people's lives easier. To aid with these efforts, we are creating an integrated picture processor for visually impaired people. The main forms of human communication nowadays are speech and text. A person must have the vision to view text-based information. Nonetheless, persons without the ability to see can still learn things by listening. The integrated image processor is an assistive text-reading tool that uses a camera to help the blind read text on labels, printed notes, and merchandise. It entails text extraction from an image using optical character recognition (OCR) and text-to-speech (TTS) conversion to turn it into speech. This method aids the blind in reading the text and serves as the foundation for the creation of a prototype that will enable the blind to identify objects in the real world. Using Jetson Nano, the text from product descriptions is retrieved and rendered as speech, with mobility as the primary consideration. It entails text extraction from an image using optical character recognition (OCR) and text-to-speech (TTS) conversion to turn it into speech. This method aids the blind in reading the text and serves as the foundation for the creation of a prototype that will enable the blind to identify objects in the real world. Using Raspberry Pi, the text from product descriptions is retrieved and rendered as speech, with mobility as the primary consideration. By including a battery backup, portability is made possible and could be used in future technology. The user can use the device at any time and any place because of its portability. The project also has a feature that uses OpenCV and TensorFlow to recognize cash notes. Additionally, this system incorporates deep learning-based object identification, which uses MobileNets and SSD methods for object detection, to recognize various items for recognizing barriers in front of the individual. The person can receive this information as helpful auditory feedback. © 2024 IEEE.","OCR; OpenCV; Tensorflow; TTS","Deep learning; Extraction; Object detection; Object recognition; Optical data processing; Rendering (computer graphics); Speech communication; Speech recognition; Images processing; Integrated images; Opencv; Processing device; Product descriptions; Real-world; Tensorflow; Text extraction; Text to speech; Text-to-speech conversion; Optical character recognition","J. Raja; Sri Sairam Engineering College, Department Of Electronics And Communication Engineering, Chennai, Tamil Nadu, India; email: raja.ece@sairam.edu.in","","Institute of Electrical and Electronics Engineers Inc.","","979-835035268-9","","","English","Int. Conf. Commun., Comput. Internet Things, IC3IoT - Proc.","Conference paper","Final","","Scopus","2-s2.0-85196721115"
"Rajalakshmi B.; Prathicksha S.; Harsha K.; Varshini M.A.; Anisha T.","Rajalakshmi, B. (57094955900); Prathicksha, S. (58965305200); Harsha, K. (58364622200); Varshini, M. Amritha (57210620750); Anisha, T. (58965713000)","57094955900; 58965305200; 58364622200; 57210620750; 58965713000","Third Eye","2023","2023 Intelligent Computing and Control for Engineering and Business Systems, ICCEBS 2023","","","","","","","0","10.1109/ICCEBS58601.2023.10448602","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85189154709&doi=10.1109%2fICCEBS58601.2023.10448602&partnerID=40&md5=778460b2fca037d32543036ced001ba7","Sri Sairam Enginnering College, Electronics and Communication Engineering, Chennai, India","Rajalakshmi B., Sri Sairam Enginnering College, Electronics and Communication Engineering, Chennai, India; Prathicksha S., Sri Sairam Enginnering College, Electronics and Communication Engineering, Chennai, India; Harsha K., Sri Sairam Enginnering College, Electronics and Communication Engineering, Chennai, India; Varshini M.A., Sri Sairam Enginnering College, Electronics and Communication Engineering, Chennai, India; Anisha T., Sri Sairam Enginnering College, Electronics and Communication Engineering, Chennai, India","This abstract presents an innovative approach to assist blind individuals in perceiving their surroundings through the integration of ultrasonic sensors and face recognition technology. By providing real-time object detection and facial recognition capabilities, the suggested system intends to improve the independence and mobility of visually impaired individuals. Ultrasonic sensors are utilized to detect objects within the user's vicinity. By emitting ultrasonic waves and analyzing the reflected signals, the system can determine the distance and approximate size of objects in the surrounding environment. This information is then translated into auditory or tactile feedback to provide the user with a comprehensive understanding of their surroundings. In addition to object detection, the system incorporates face recognition technology to enable blind individuals to recognize and identify people they encounter. Using a combination of image processing algorithms and machine learning techniques, facial features are analyzed, matched against a pre-existing database, and communicated to the user in a non-visual format.  © 2023 IEEE.","auditory; facial recognition; object detection; ultra sonic","Face recognition; Machine learning; Object recognition; Ultrasonic applications; Ultrasonic sensors; Auditory; Blind individuals; Face recognition technologies; Facial recognition; Innovative approaches; Objects detection; Real- time; Reflected signal; Ultra sonic; Visually impaired; Object detection","","","Institute of Electrical and Electronics Engineers Inc.","","979-835039458-0","","","English","Intell. Comput. Control Eng. Bus. Syst., ICCEBS","Conference paper","Final","","Scopus","2-s2.0-85189154709"
"Chen J.; Bai X.","Chen, Junzhang (57200305450); Bai, Xiangzhi (23466338600)","57200305450; 23466338600","Atmospheric Transmission and Thermal Inertia Induced Blind Road Segmentation with a Large-Scale Dataset TBRSD","2023","Proceedings of the IEEE International Conference on Computer Vision","","","","1053","1063","10","1","10.1109/ICCV51070.2023.00103","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85188233562&doi=10.1109%2fICCV51070.2023.00103&partnerID=40&md5=2b16cb18369d87ac987ca71d02e6b296","Beihang University, Image Processing Center, Beijing, China; Beihang University, State Key Laboratory of Virtual Reality Technology and Systems, China; Beihang University, Advanced Innovation Center for Biomedical Engineering, China","Chen J., Beihang University, Image Processing Center, Beijing, China; Bai X., Beihang University, Image Processing Center, Beijing, China, Beihang University, State Key Laboratory of Virtual Reality Technology and Systems, China, Beihang University, Advanced Innovation Center for Biomedical Engineering, China","Computer vision-based walking assistants are prominent tools for aiding visually impaired people in navigation. Blind road segmentation is a key element in these walking assistant systems. However, most walking assistant systems rely on visual light images, which is dangerous in weak illumination environments such as darkness or fog. To address this issue and enhance the safety of vision-based walking assistant systems, we developed a thermal infrared blind road segmentation neural network (TINN). In contrast to conventional segmentation techniques that primarily concentrate on enhancing feature extraction and perception, our approach is geared towards preserving the inherent radiation characteristics within the thermal imaging process. Initially, we modelled two critical factors in thermal infrared imaging - thermal light atmospheric transmission and thermal inertia effect. Subsequently, we use an encoder-decoder architecture to fuse the feathers extracted by the two modules. Additionally, to train the network and evaluate the effectiveness of the proposed method, we constructed a large-scale thermal infrared blind road segmentation dataset named TBRSD consists 5180 pixel-level manual annotations. The experimental results demonstrate that our method outperforms existing techniques and achieves state-of-the-art performance in thermal blind road segmentation, as validated on benchmark thermal infrared semantic segmentation datasets such as MFNet and SODA. The dataset and our code are both publicly available in https://github.com/chenjzBUAA/TBRSD or http://xzbai.buaa.edu.cn/datasets.html. © 2023 IEEE.","","Automobile drivers; Benchmarking; Large datasets; Semantic Segmentation; Semantics; Transmissions; Atmospheric transmissions; Key elements; Large-scale datasets; Neural-networks; Road segmentation; Segmentation techniques; Thermal inertia; Thermal-infrared; Vision based; Visually impaired people; Roads and streets","X. Bai; Beihang University, Image Processing Center, Beijing, China; email: jackybxz@buaa.edu.cn","","Institute of Electrical and Electronics Engineers Inc.","15505499","979-835030718-4","PICVE","","English","Proc IEEE Int Conf Comput Vision","Conference paper","Final","","Scopus","2-s2.0-85188233562"
"Gladis K.P.A.; Srinivasan R.; Sugashini T.; Ananda Raj S.P.","Gladis, K.P. Ajitha (59136754600); Srinivasan, R. (57192398597); Sugashini, T. (58784815200); Ananda Raj, S.P. (59136754700)","59136754600; 57192398597; 58784815200; 59136754700","Smart-YOLO glass: Real-time video based obstacle detection using paddling/paddling SAB YOLO network1","2024","Journal of Intelligent and Fuzzy Systems","46","4","","10243","10256","13","0","10.3233/JIFS-234453","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85193727307&doi=10.3233%2fJIFS-234453&partnerID=40&md5=8a6cf416fe7602d86455f230d04144e6","Department of Information Technology, CSI Institute of Technology, Tamilnadu, Thovalai, India; Department of Computing Technologies, SRM Institute of Science and Technology, Tamilnadu, Kattankulathur, India; Department of Computer Science Engineering, Indra Ganesan College of Engineering, Tamilnadu, Trichy, India; Department of Computer Science Engineering, Presidency University, Bangalore, India","Gladis K.P.A., Department of Information Technology, CSI Institute of Technology, Tamilnadu, Thovalai, India; Srinivasan R., Department of Computing Technologies, SRM Institute of Science and Technology, Tamilnadu, Kattankulathur, India; Sugashini T., Department of Computer Science Engineering, Indra Ganesan College of Engineering, Tamilnadu, Trichy, India; Ananda Raj S.P., Department of Computer Science Engineering, Presidency University, Bangalore, India","Visual impairment people have many difficulties in everyday life, including communicating and getting information, as well as navigating independently and safely. Using auditory alerts, our study hopes to improve the lives of visually impaired individuals by alerting them to items in their path. In this research, a Video-based Smart object detection model named Smart YOLO Glass has been proposed for visually impaired persons. A Paddling - Paddling Squeeze and Attention YOLO Network model is trained with multiple images to detect outdoor objects to assist visually impaired people. In order to calculate the distance between a blind person and obstacles when moving from one location to another, the proposed method additionally included a distance-measuring sensor. The visually impaired will benefit from this system’s information about around objects and assistance with independent navigation. Recall, accuracy, specificity, precision, and F-measure were among the metrics used to evaluate the proposed strategy. Because there is less time complexity, the user can see the surrounding environment in real time. When comparing the proposed technique to Med glasses, DL smart glass, and DL-FDS, the total accuracy is improved by 7.6%, 4.8%, and 3.1%, respectively. © 2024 – IOS Press. All rights reserved.","deep learning; outdoor object detection; Visual impairment; wearable system; YOLO network","Deep learning; Eye protection; Glass; Object recognition; Obstacle detectors; Ophthalmology; Vision; Wearable technology; Deep learning; Objects detection; Obstacles detection; Outdoor object detection; Real time videos; Smart objects; Visual impairment; Visually impaired; Wearable systems; YOLO network; Object detection","T. Sugashini; Department of Computer Science Engineering, Indra Ganesan College of Engineering, Trichy, Tamilnadu, 620012, India; email: sugashini.ss@outlook.com","","IOS Press BV","10641246","","","","English","J. Intelligent Fuzzy Syst.","Article","Final","","Scopus","2-s2.0-85193727307"
"Mohana R.S.; Thangaraj P.; Krishnakumar B.; Praveen M.; Rohinth T.; Sanjayan S.","Mohana, R.S. (57204253622); Thangaraj, P. (26432403100); Krishnakumar, B. (57215548537); Praveen, M. (57224699112); Rohinth, T. (58695870400); Sanjayan, S. (58696122400)","57204253622; 26432403100; 57215548537; 57224699112; 58695870400; 58696122400","ShopBot for Visually Impaired People Using Deep Learning","2023","AIP Conference Proceedings","2764","1","060004","","","","0","10.1063/5.0144972","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85176782591&doi=10.1063%2f5.0144972&partnerID=40&md5=376289efc68a4b57d94a5de438f92ad1","Department of Computer Science and Engineering, Kongu Engineering College, Tamil Nadu, 638060, India; Centre for Research and Development, KPR Institute of Engineering and Technology, TamilNadu, 641407, India","Mohana R.S., Department of Computer Science and Engineering, Kongu Engineering College, Tamil Nadu, 638060, India; Thangaraj P., Centre for Research and Development, KPR Institute of Engineering and Technology, TamilNadu, 641407, India; Krishnakumar B., Department of Computer Science and Engineering, Kongu Engineering College, Tamil Nadu, 638060, India; Praveen M., Department of Computer Science and Engineering, Kongu Engineering College, Tamil Nadu, 638060, India; Rohinth T., Department of Computer Science and Engineering, Kongu Engineering College, Tamil Nadu, 638060, India; Sanjayan S., Department of Computer Science and Engineering, Kongu Engineering College, Tamil Nadu, 638060, India","The advancements in computer vision and artificial intelligence are assisting in the improvement of the lives of persons with disabilities. Blindness or disabilities in the eyesight, is one of the top disabilities in both men and women, affects more than 285 million people around the world. Accessible visual information is one of the most important for improving the freedom and safety of blind and the visually challenged people. There is an immediate need to build an intelligent system to assist them in their navigation in their day to day activities. The proposed development is a shopping assistant for visually impaired people. The model will use the camera of the user's mobile phone to identify the packaged products that are available on the supermarkets. The image classification system is trained with the help of Convolutional neural network (CNN). Following the recognition of the item, the system will notify the user what product is in front of him in the form of voice note. With the accuracy rate of 97%, our model has successfully identified products with greater correctness than previous models. © 2023 American Institute of Physics Inc.. All rights reserved.","Accuracy; Assistant; Convolutional neural network; Image classification; TensorFlow Lite; Text-To-speech","","P. Thangaraj; Centre for Research and Development, KPR Institute of Engineering and Technology, TamilNadu, 641407, India; email: thangarajp59@gmail.com","L. R.; P. T.; H. S.","American Institute of Physics Inc.","0094243X","","","","English","AIP Conf. Proc.","Conference paper","Final","","Scopus","2-s2.0-85176782591"
"Kasabekar O.; Jeurkar S.; Kamble R.; Srivel R.; Kamble M.","Kasabekar, Om (58929421600); Jeurkar, Shardul (58534214400); Kamble, Riya (59172000600); Srivel, R. (57207013398); Kamble, Milind (56642553000)","58929421600; 58534214400; 59172000600; 57207013398; 56642553000","A Smart Device for Real-Time Assistance to the Visually Impaired People","2024","Proceedings of the 2024 10th International Conference on Communication and Signal Processing, ICCSP 2024","","","","24","29","5","0","10.1109/ICCSP60870.2024.10544056","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85195936776&doi=10.1109%2fICCSP60870.2024.10544056&partnerID=40&md5=530147733b0293d7b8402eaf2c3e73ea","Vishwakarma Institute of Technology, Department of Electronics and Telecommunication, Pune, India; Adhiparasakthi Engineering College, Department of Computer Science, Melmaruvathur, India","Kasabekar O., Vishwakarma Institute of Technology, Department of Electronics and Telecommunication, Pune, India; Jeurkar S., Vishwakarma Institute of Technology, Department of Electronics and Telecommunication, Pune, India; Kamble R., Vishwakarma Institute of Technology, Department of Electronics and Telecommunication, Pune, India; Srivel R., Adhiparasakthi Engineering College, Department of Computer Science, Melmaruvathur, India; Kamble M., Vishwakarma Institute of Technology, Department of Electronics and Telecommunication, Pune, India","This research proposes an advanced and cost-effective smart blind device equipped with Internet of Things (IoT) sensors to address significant navigational challenges for individuals with visual impairments in both indoor and outdoor environments. The system employs IoT technologies to detect obstacles, staircases, and wet terrain, enhancing the safety of walking experiences. To bolster users' security, the device features a panic button, triggering the transmission of alert messages about their location. A companion software application enables acquaintances to modify contact numbers for alert messages in the device configurations. Additionally, the study introduces an artificial navigation system with adjustable sensitivity, integrating a GPS module and ultrasonic proximity sensors. Through ultrasonic reflections, this device empowers visually impaired individuals to autonomously navigate diverse surroundings, including negotiating barriers and potholes. The device's versatility is evident in its adaptable attachment options, such as shoes, clothing, body parts, and walking sticks, contributing to its reliability and flexibility.  © 2024 IEEE.","Buzzer; ESP8266; GPS; Sensor; ultrasonic","Application programs; Cost effectiveness; Internet of things; Ultrasonic applications; Ultrasonic reflection; Buzzer; Cost effective; Esp8266; Indoor environment; Internet of things technologies; Outdoor environment; Real- time; Smart devices; Visual impairment; Visually impaired people; Global positioning system","O. Kasabekar; Vishwakarma Institute of Technology, Department of Electronics and Telecommunication, Pune, India; email: om.kasabekar21@vit.edu","","Institute of Electrical and Electronics Engineers Inc.","","979-835035306-8","","","English","Proc. Int. Conf. Commun. Signal Process., ICCSP","Conference paper","Final","","Scopus","2-s2.0-85195936776"
"Ge S.; Huang X.-T.; Lin Y.-N.; Li Y.-C.; Dong W.-T.; Dang W.-M.; Xu J.-J.; Yi M.; Xu S.-Y.","Ge, Song (57222866846); Huang, Xuan-Tuo (58891149100); Lin, Yan-Ni (57222864228); Li, Yan-Cheng (58026334600); Dong, Wen-Tian (27167908600); Dang, Wei-Min (8428765300); Xu, Jing-Jing (57189596265); Yi, Ming (35790814500); Xu, Sheng-Yong (57226226930)","57222866846; 58891149100; 57222864228; 58026334600; 27167908600; 8428765300; 57189596265; 35790814500; 57226226930","Remote Virtual Companion via Tactile Codes and Voices for The People With Visual Impairment*","2024","Progress in Biochemistry and Biophysics","51","1","","158","176","18","1","10.16476/j.pibb.2023.0053","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85185265294&doi=10.16476%2fj.pibb.2023.0053&partnerID=40&md5=fe3228eec921e7d1d99c072960eeffb1","Key Laboratory for the Physics & Chemistry of Nanodevices, School of Electronics, Peking University, Beijing, 100871, China; School of Electronics Engineering and Computer Science, Peking University, Beijing, 100871, China; Peking University Sixth Hospital, Peking University Institute of Mental Health, NHC Key Laboratory of Mental Health (Peking University), National Clinical Research Center for Mental Disorders, Peking University Sixth Hospital), Beijing, 100191, China; School of Microelectronics, Shandong University, Jinan, 250100, China; Key Laboratory for Neuroscience, School of Basic Medical Sciences, Neuroscience Research Institute, Department of Neurobiology, School of Public Health, Peking University, Beijing, 100191, China","Ge S., Key Laboratory for the Physics & Chemistry of Nanodevices, School of Electronics, Peking University, Beijing, 100871, China; Huang X.-T., School of Electronics Engineering and Computer Science, Peking University, Beijing, 100871, China; Lin Y.-N., Key Laboratory for the Physics & Chemistry of Nanodevices, School of Electronics, Peking University, Beijing, 100871, China; Li Y.-C., School of Electronics Engineering and Computer Science, Peking University, Beijing, 100871, China; Dong W.-T., Peking University Sixth Hospital, Peking University Institute of Mental Health, NHC Key Laboratory of Mental Health (Peking University), National Clinical Research Center for Mental Disorders, Peking University Sixth Hospital), Beijing, 100191, China; Dang W.-M., Peking University Sixth Hospital, Peking University Institute of Mental Health, NHC Key Laboratory of Mental Health (Peking University), National Clinical Research Center for Mental Disorders, Peking University Sixth Hospital), Beijing, 100191, China; Xu J.-J., School of Microelectronics, Shandong University, Jinan, 250100, China; Yi M., Key Laboratory for Neuroscience, School of Basic Medical Sciences, Neuroscience Research Institute, Department of Neurobiology, School of Public Health, Peking University, Beijing, 100191, China; Xu S.-Y., Key Laboratory for the Physics & Chemistry of Nanodevices, School of Electronics, Peking University, Beijing, 100871, China","Objective Existing artificial vision devices can be divided into two types: implanted devices and extracorporeal devices, both of which have some disadvantages. The former requires surgical implantation, which may lead to irreversible trauma, while the latter has some defects such as relatively simple instructions, limited application scenarios and relying too much on the judgment of artificial intelligence (AI) to provide enough security. Here we propose a system that has voice interaction and can convert surrounding environment information into tactile commands on head and neck. Compared with existing extracorporeal devices, our device can provide a larger capacity of information and has advantages such as lower cost, lower risk, suitable for a variety of life and work scenarios. Methods With the latest remote wireless communication and chip technologies, microelectronic devices, cameras and sensors worn by the user, as well as the huge database and computing power in the cloud, the backend staff can get a full insight into the scenario, environmental parameters and status of the user remotely (for example, across the city) in real time. In the meanwhile, by comparing the cloud database and in-memory database and with the help of AI-assisted recognition and manual analysis, they can quickly develop the most reasonable action plan and send instructions to the user. In addition, the backend staff can provide humanistic care and emotional sustenance through voice dialogs. Results This study originally proposes the concept of “remote virtual companion” and demonstrates the related hardware and software as well as test results. The system can not only achieve basic guide functions, for example, helping a person with visual impairment to shop in supermarkets, find seats at cafes, walk on the streets, construct complex puzzles, and play cards, but also can meet the demand for fast-paced daily tasks such as cycling. Conclusion Experimental results show that this “remote virtual companion” is applicable for various scenarios and demands. It can help blind people with their travels, shopping and entertainment, or accompany the elderlies with their trips, wilderness explorations, and travels. © 2024 Institute of Biophysics,Chinese Academy of Sciences. All rights reserved.","artificial visual aid; navigation; remote virtual companion; tactile code; visually impaired users","","M. Yi; Key Laboratory for Neuroscience, School of Basic Medical Sciences, Neuroscience Research Institute, Department of Neurobiology, School of Public Health, Peking University, Beijing, 100191, China; email: mingyi@hsc.pku.edu.cn; S.-Y. Xu; Key Laboratory for the Physics & Chemistry of Nanodevices, School of Electronics, Peking University, Beijing, 100871, China; email: xusy@pku.edu.cn","","Institute of Biophysics,Chinese Academy of Sciences","10003282","","","","English","Prog. Biochem. Biophys.","Article","Final","","Scopus","2-s2.0-85185265294"
"Venkat Ragavan S.; Tarun A.H.; Yogeeshwar S.; Vishwath Kumar B.S.; Sofana Reka S.","Venkat Ragavan, S. (58080756100); Tarun, A.H. (58080031800); Yogeeshwar, S. (57221872950); Vishwath Kumar, B.S. (57221872405); Sofana Reka, S. (56964480300)","58080756100; 58080031800; 57221872950; 57221872405; 56964480300","A realtime portable and accessible aiding system for the blind – a cloud based approach","2023","Multimedia Tools and Applications","82","13","","20641","20654","13","1","10.1007/s11042-023-14419-9","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85146965898&doi=10.1007%2fs11042-023-14419-9&partnerID=40&md5=cf4f10ee88bb9e49b59aa82a074a4a47","School of Computer Science Engineering, Vellore Institute of Technology, Chennai, India; School of Electronics Engineering, Vellore Institute of Technology, Chennai, India; Centre for Smart Grid Technologies, School of Electronics Engineering, Vellore Institute of Technology, Chennai, India","Venkat Ragavan S., School of Computer Science Engineering, Vellore Institute of Technology, Chennai, India; Tarun A.H., School of Computer Science Engineering, Vellore Institute of Technology, Chennai, India; Yogeeshwar S., School of Electronics Engineering, Vellore Institute of Technology, Chennai, India; Vishwath Kumar B.S., School of Electronics Engineering, Vellore Institute of Technology, Chennai, India; Sofana Reka S., Centre for Smart Grid Technologies, School of Electronics Engineering, Vellore Institute of Technology, Chennai, India","With the rise of AI and Deep Learning technologies, it is now possible to give the visually impaired a sense of sight. This work intends to propose a system, which helps the blind to perceive their surrounding without any extra hand. The system harnesses the power of revolutionary cloud technology, cutting-edge artificial intelligence systems and state of the art language translation technologies for the inevitable cause of assisting the blind. This work mainly focusses on developing a simple gesture-controlled cloud based mobile application, which would allow them to capture their surroundings and help them to navigate through their surroundings in real-time. In this work a real case system architecture is proposed which would analyse the spatial reference of objects in the image and also the custom trained NLP engine generates a description that is narrated in their own native language which stands the unique aspect of the work. The proposed application proves to be a one-stop solution for the visually impaired with real case analysis. To strengthen the analysis of the work, results pertaining to the system architecture emphasising its real-time performance and accessibility are done. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Image captioning; Language translation; Multilingual voice; Object spatial analysis; Optical Character Recognition (OCR); Text to Speech (TTS)","Computer architecture; Deep learning; Engineering education; Image analysis; Spatial variables measurement; Translation (languages); Cloud-based; Image captioning; Language translation; Multilingual voice; Object spatial analyse; Optical character recognition; Real- time; Spatial analysis; Text to speech; Optical character recognition","S. Sofana Reka; Centre for Smart Grid Technologies, School of Electronics Engineering, Vellore Institute of Technology, Chennai, India; email: chocos.sofana@gmail.com","","Springer","13807501","","MTAPF","","English","Multimedia Tools Appl","Article","Final","","Scopus","2-s2.0-85146965898"
"Bhorge S.; Kasodekar M.; Karmalkar A.; Kulkarni V.","Bhorge, Siddharth (57200145424); Kasodekar, Manas (59221460800); Karmalkar, Apoorv (59222302200); Kulkarni, Vedant (59221738200)","57200145424; 59221460800; 59222302200; 59221738200","Smart Blind Stick: Object Detection & GPS Integration for Enhanced Mobility","2024","Proceedings of the 3rd International Conference on Applied Artificial Intelligence and Computing, ICAAIC 2024","","","","1653","1658","5","0","10.1109/ICAAIC60222.2024.10575880","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85198746165&doi=10.1109%2fICAAIC60222.2024.10575880&partnerID=40&md5=1fd59db916ba066e7f9cb00e5b3ddecd","Vishwakarma Institute of Technology, Department of Electronics & Telecommunication (E&TC), Maharashtra, Pune, 411037, India","Bhorge S., Vishwakarma Institute of Technology, Department of Electronics & Telecommunication (E&TC), Maharashtra, Pune, 411037, India; Kasodekar M., Vishwakarma Institute of Technology, Department of Electronics & Telecommunication (E&TC), Maharashtra, Pune, 411037, India; Karmalkar A., Vishwakarma Institute of Technology, Department of Electronics & Telecommunication (E&TC), Maharashtra, Pune, 411037, India; Kulkarni V., Vishwakarma Institute of Technology, Department of Electronics & Telecommunication (E&TC), Maharashtra, Pune, 411037, India","This research study introduces the Object Detection Blind Stick (ODBS), a technologically advanced aid for visually impaired individuals. Integrating a GPS module, GSM module, ultrasonic sensor, and Arduino Uno microcontroller, the ODBS provides real-time obstacle detection, location tracking, and navigation assistance. Utilizing machine learning for object classification, the system enhances situational awareness and offers tailored feedback. The low-cost design and successful experimental results make the ODBS a practical and effective solution for improving the mobility and safety of individuals with visual impairments. © 2024 IEEE.","Arduino UNO; Assistive Learning; Blind Stick; Global Positioning System; Ultrasonic Sensor","Global system for mobile communications; Learning systems; Object detection; Object recognition; Obstacle detectors; Ultrasonic applications; Ultrasonic sensors; Arduino UNO; Assistive; Assistive learning; Blind stick; Enhanced mobility; Objects detection; Obstacles detection; Real- time; Research studies; Visually impaired; Global positioning system","S. Bhorge; Vishwakarma Institute of Technology, Department of Electronics & Telecommunication (E&TC), Pune, Maharashtra, 411037, India; email: siddharth.bhorge@vit.edu","","Institute of Electrical and Electronics Engineers Inc.","","979-835037519-0","","","English","Proc. Int. Conf. Appl. Artif. Intell. Comput., ICAAIC","Conference paper","Final","","Scopus","2-s2.0-85198746165"
"Farina M.; Lettieri E.; Filippi T.; Zoccarato F.; Perego P.; Di Francesco A.; Toletti G.","Farina, Marcello (35723824100); Lettieri, Emanuele (12143529800); Filippi, Tecla (59210084500); Zoccarato, Francesca (57202071537); Perego, Paolo (56819215600); Di Francesco, Andrea (58139286100); Toletti, Giovanni (6507253837)","35723824100; 12143529800; 59210084500; 57202071537; 56819215600; 58139286100; 6507253837","The freedom to run: developing an autonomous robot matching the needs of visually impaired citizens to technology opportunities","2024","Disability and Rehabilitation: Assistive Technology","","","","","","","0","10.1080/17483107.2024.2374454","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85197853377&doi=10.1080%2f17483107.2024.2374454&partnerID=40&md5=9302d241292e9a29a92cb2fe75957832","Department of Electronics, Information and Bioengineering, Politecnico di Milano, Milano, Italy; Department of Management, Politecnico di Milano, Milano, Italy; Department of Design, Politecnico di Milano, Milano, Italy; Reply, Milano, Italy","Farina M., Department of Electronics, Information and Bioengineering, Politecnico di Milano, Milano, Italy; Lettieri E., Department of Management, Politecnico di Milano, Milano, Italy; Filippi T., Department of Management, Politecnico di Milano, Milano, Italy; Zoccarato F., Department of Management, Politecnico di Milano, Milano, Italy; Perego P., Department of Design, Politecnico di Milano, Milano, Italy; Di Francesco A., Reply, Milano, Italy; Toletti G., Department of Management, Politecnico di Milano, Milano, Italy","Purpose: Visual impairment poses significant challenges in daily life, especially when navigating unfamiliar environments, resulting in inequalities and reduced quality of life. This study aimed to gain an in-depth understanding of the needs and perspectives of visually impaired people in sports-related contexts through surveys and focus groups, and to understand whether their needs are being met by current technological solutions. Materials and methods: To accomplish this, opinions gathered from focus groups and interviews were compared to the technological solutions found in the literature. Since many unmet needs were identified, participants from associations and organizations were asked to identify key characteristics for the development of a robot guide. The results underscored the paramount importance of an easy-to-use guide that offers accurate and personalized assistance. Participants expressed a strong desire for advanced features such as object recognition and navigation in complex environments, as well as adaptability to the user’s speed while providing the necessary safety features to ensure a high level of autonomy. Results: This research serves as a bridge between technological advances and the needs of the visually impaired, contributing to a more accessible and inclusive society. By addressing the unique challenges faced by the visually impaired individuals and tailoring technology to meet their needs, this study takes a significant step toward reducing disparities and improving the independence and quality of life for this community. Conclusions: As technology continues to advance, it has the potential to be a powerful tool in breaking down barriers and fostering a world where everyone, regardless of their visual ability, can navigate the world with confidence and ease. © 2024 Informa UK Limited, trading as Taylor & Francis Group.","Autonomous robotic guide; blind people; focus group; sports centers; survey; visually impaired people","","F. Zoccarato; Department of Management, Politecnico di Milano, Milano, Italy; email: francesca.zoccarato@polimi.it","","Taylor and Francis Ltd.","17483107","","","","English","Disabil. Rehabil. Assistive Technol.","Article","Article in press","","Scopus","2-s2.0-85197853377"
"Adarsh R.B.; Keeazhangote A.; Athulya K.K.; Harsha K.A.; Neethu Joseph C.","Adarsh, R.B. (59321360400); Keeazhangote, Akhin (59320349700); Athulya, K.K. (59320642000); Harsha, K.A. (59320642100); Neethu Joseph, C. (59320642200)","59321360400; 59320349700; 59320642000; 59320642100; 59320642200","Eye Controlled AI Assistive Device : Empowering Paralyzed Individuals","2024","2024 International Conference on Signal Processing, Computation, Electronics, Power and Telecommunication, IConSCEPT 2024 - Proceedings","","","","","","","0","10.1109/IConSCEPT61884.2024.10627806","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85203511369&doi=10.1109%2fIConSCEPT61884.2024.10627806&partnerID=40&md5=3a8aadfeb0769739175a8ca7073cc2b7","Jyothi Engineering College, Computer Science And Engineering, Kerala, Thrissur, India; Jyothi Engineering College, Kerala, Thrissur, India","Adarsh R.B., Jyothi Engineering College, Computer Science And Engineering, Kerala, Thrissur, India; Keeazhangote A., Jyothi Engineering College, Computer Science And Engineering, Kerala, Thrissur, India; Athulya K.K., Jyothi Engineering College, Computer Science And Engineering, Kerala, Thrissur, India; Harsha K.A., Jyothi Engineering College, Computer Science And Engineering, Kerala, Thrissur, India; Neethu Joseph C., Jyothi Engineering College, Kerala, Thrissur, India","This innovative system presents an eye-controlled wheelchair for people with mobility impairments and uses real-time eye tracking to interpret the user's gaze and translate it into a wheelchair. In this paper, hardware and software architecture of thas been described. The system offers seamless integration and scalability, and aims to improve the independence and quality of life of people with paralysis by providing intuitive and responsive mobility assistance in complex situations. This paper presents a compact but robust solution for wheelchair control using eye tracking for the paralysed. The system uses advanced computer vision techniques to accurately interpret eye movements in real time, providing smooth and efficient navigation for people with mobility impairments. This technology-based approach aims to improve accessibility and provide a reliable solution to improve mobility and independence. © 2024 IEEE.","Arduino; Eye Tracking; Wheelchair","Assistive technology; Disabled persons; Eye controlled devices; Eye movements; Eye protection; Mobility aids for blind persons; Arduino; Assistive devices; Eye-tracking; Hardware and software architectures; Innovative systems; Mobility assistance; Quality of life; Real-time eye tracking; Robust solutions; Seamless integration; Wheelchairs","","","Institute of Electrical and Electronics Engineers Inc.","","979-833154068-5","","","English","Int. Conf. Signal Process., Comput., Electron., Power Telecommun., IConSCEPT - Proc.","Conference paper","Final","","Scopus","2-s2.0-85203511369"
"Bakali El Mohamadi M.; Anouzla A.; Zrira N.; Ouazzani-Touhami K.","Bakali El Mohamadi, Mohamed (58162508600); Anouzla, Adnan (58163177200); Zrira, Nabila (57193328564); Ouazzani-Touhami, Khadija (57211442130)","58162508600; 58163177200; 57193328564; 57211442130","A Systematic Review on Blind and Visually Impaired Navigation Systems","2024","Lecture Notes in Networks and Systems","826","","","151","160","9","0","10.1007/978-3-031-47672-3_17","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85187803057&doi=10.1007%2f978-3-031-47672-3_17&partnerID=40&md5=44ad16ea7a8f1653f20aa4276278a80c","Ecole Nationale Supérieure des Mines de Rabat, ADOS Team, LISTD Laboratory, Rabat, Morocco","Bakali El Mohamadi M., Ecole Nationale Supérieure des Mines de Rabat, ADOS Team, LISTD Laboratory, Rabat, Morocco; Anouzla A., Ecole Nationale Supérieure des Mines de Rabat, ADOS Team, LISTD Laboratory, Rabat, Morocco; Zrira N., Ecole Nationale Supérieure des Mines de Rabat, ADOS Team, LISTD Laboratory, Rabat, Morocco; Ouazzani-Touhami K., Ecole Nationale Supérieure des Mines de Rabat, ADOS Team, LISTD Laboratory, Rabat, Morocco","Visually impaired people face particular navigation challenges in daily life. Indoor and outdoor areas have increasingly become complex environments due to the wide range of agents and entities that make up the ecosystem. Many assistive mobility tools have been developed over the years to overcome this problem. However, these solutions do not satisfy the needs of visually impaired individuals in terms of efficient guidance and real-time processing, as well as the difficulties encountered when handling and operating the entire system. The purpose of this paper is to provide a comprehensive overview of strategies and approaches for developing navigational devices based on 3D computer vision in order to evaluate the detection and recognition accuracy of each system’s software, and to examine the hardware components needed for easy and convenient navigation. With this purpose in mind, we attempted to present a broad synopsis of existing studies using the systematic literature review methodology. The results obtained shed light on CNN-based, cloud computing-based, smartphone-based, and R-GBD-based systems. This review gives helpful information on technological equipment for developing electronic navigation assistive devices in both indoor and outdoor environments. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.","3D computer vision; Deep learning; Hardware; Navigation; RGB-D; Visually impaired people","Computer hardware; Computer vision; Deep learning; Distributed computer systems; Indoor positioning systems; Navigation systems; 3D computer vision; Assistive mobilities; Blind and visually impaired; Complex environments; Daily lives; Deep learning; Hardware; RGB-D; Systematic Review; Visually impaired people; Smartphones","M. Bakali El Mohamadi; Ecole Nationale Supérieure des Mines de Rabat, ADOS Team, LISTD Laboratory, Rabat, Morocco; email: m.bakali-elmohamadi@enim.ac.ma","Gherabi N.; Awad A.I.; Nayyar A.; Bahaj M.","Springer Science and Business Media Deutschland GmbH","23673370","978-303147671-6","","","English","Lect. Notes Networks Syst.","Conference paper","Final","","Scopus","2-s2.0-85187803057"
"Hemavathy J.; Sabarika Shree A.; Priyanka S.; Subhashree K.","Hemavathy, J. (56483178500); Sabarika Shree, A. (58940205400); Priyanka, S. (57211315726); Subhashree, K. (58940101900)","56483178500; 58940205400; 57211315726; 58940101900","AI Based Voice Assisted Object Recognition for Visually Impaired Society","2023","2023 International Conference on Data Science, Agents and Artificial Intelligence, ICDSAAI 2023","","","","","","","0","10.1109/ICDSAAI59313.2023.10452456","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85187790872&doi=10.1109%2fICDSAAI59313.2023.10452456&partnerID=40&md5=02df39cfcc5986ba508f70c740aae79e","Panimalar Engineering College, Chennai, India; Panimalar Engineering College, Information Technology, Chennai, India","Hemavathy J., Panimalar Engineering College, Chennai, India; Sabarika Shree A., Panimalar Engineering College, Information Technology, Chennai, India; Priyanka S., Panimalar Engineering College, Information Technology, Chennai, India; Subhashree K., Panimalar Engineering College, Information Technology, Chennai, India","Blind individuals face the significant challenge of not being able to clearly identify objects in their environment, relying heavily on their other senses to navigate the world. The absence of sight limits their ability to independently experience and understand the physical world around them. To address this fundamental issue, we propose the development of a voice-assisted application that aims to provide technical support and assistance to blind individuals through their smartphones. Unlike existing methods that often require in-person assistance, our application seeks to bridge the gap in the visually impaired community's access to information and independence, leveraging current technological advancements. Our object detection model system combines multiple components, including optical character recognition and object and human recognition utilizing obstacle detection sensors. When the smartphone's camera captures an image, it is then processed through a dataset that we have curated. The processing involves a matching process using the K-nearest neighbor algorithm in machine learning to identify and provide information about the objects and people within the image. This comprehensive approach not only aids in object identification but also enhances the user's understanding of their surroundings. The primary goal of this application is to empower blind individuals to lead more independent lives. By providing real-time information about their environment, it equips them with the tools they need to make informed decisions and navigate the world with greater confidence. Ultimately, our voice-assisted application represents a significant step toward breaking down the barriers faced by the blind community, leveraging technology to enhance their quality of life and foster greater autonomy. © 2023 IEEE.","CNN; Images; Object recognition; Open-CV; Technology; Visually Impaired","Indoor positioning systems; Machine learning; Navigation; Nearest neighbor search; Object detection; Object recognition; Obstacle detectors; Optical character recognition; Blind individuals; Image; Objects recognition; Open-CV; Physical world; Smart phones; Technical assistance; Technical support; Technology; Visually impaired; Smartphones","J. Hemavathy; Panimalar Engineering College, Chennai, India; email: Hemaramya27@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","","979-835034891-0","","","English","Int. Conf. Data Sci., Agents Artif. Intell., ICDSAAI","Conference paper","Final","","Scopus","2-s2.0-85187790872"
"Theodorou P.; Meliones A.; Filios C.","Theodorou, Paraskevi (57194044913); Meliones, Apostolos (55949094700); Filios, Costas (56997582700)","57194044913; 55949094700; 56997582700","Smart traffic lights for people with visual impairments: A literature overview and a proposed implementation","2023","British Journal of Visual Impairment","41","4","","697","725","28","3","10.1177/02646196221099154","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85131565006&doi=10.1177%2f02646196221099154&partnerID=40&md5=c6d7945fd8318c0cf5b4e76aed4d85ef","University of Piraeus, Greece","Theodorou P., University of Piraeus, Greece; Meliones A., University of Piraeus, Greece; Filios C., University of Piraeus, Greece","Attempting to establish aids for individuals who are visually impaired has urged many cities to seek solutions for improving their quality of life. Namely, cities have installed sound-emitting devices into traffic lights as well as sidewalks that assist their navigation. Moreover, as cities are always striving to move forward and achieve innovations concerning navigation for disabled individuals, smart traffic lights, capable of synchronizing in real-time according to traffic and individual mobility conditions, are already being installed around the world. This is in line with the adoption of the smart city concept, which involves a set of methodologies and indicators that regulate how cities perform regarding the promotion of citizens’ quality of life. Another important principle is the techno-economic aspect indicating the need for low-cost careful planning to produce cost-efficient solutions, while additional important issues are maintenance, power efficiency, and the means to coordinate numerous devices to facilitate operation in a timely and reliable manner. In this article, we present an overview of the existing solutions for the navigation of people who are blind and visually impaired along with a requirement analysis performed on the feedback received from interviews with members of the Lighthouse for the Blind of Greece both of which lead to the proposal of a new implementation that pushes the state of the art. © The Author(s) 2022.","Blind outdoor navigation; cloud-based technology; computer vision; GPS positioning; machine learning algorithms; maps-based navigation; smart traffic lights; wearable devices","","P. Theodorou; University of Piraeus, Greece; email: theodoroup@unipi.gr","","SAGE Publications Ltd","02646196","","","","English","British Journal of Visual Impairment","Article","Final","","Scopus","2-s2.0-85131565006"
"Abdel-Rahman A.B.; Allam A.; Abdelwahab M.; Elsharkawy M.; Kobasy B.; Eltabakh H.; Essam O.","Abdel-Rahman, Adel B. (7102847221); Allam, Ahmed (36569991500); Abdelwahab, Moataz (16642214100); Elsharkawy, Mostafa (58991347400); Kobasy, Bassam (58990654500); Eltabakh, Habib (58990654600); Essam, Omar (58990793500)","7102847221; 36569991500; 16642214100; 58991347400; 58990654500; 58990654600; 58990793500","A Smart Blind Stick with Object Detection, Obstacle Avoidance, and IoT Monitoring for Enhanced Navigation and Safety","2023","Proceedings of the 11th International Japan-Africa Conference on Electronics, Communications, and Computations, JAC-ECC 2023","","","","21","24","3","0","10.1109/JAC-ECC61002.2023.10479623","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85190655919&doi=10.1109%2fJAC-ECC61002.2023.10479623&partnerID=40&md5=5436510e4da065f3e35e4209793ab42b","Egypt-Japan University of Science and Technology, Department of Electronics and Communications Engineering, New Borg El-Arab City, Alexandria, 21934, Egypt","Abdel-Rahman A.B., Egypt-Japan University of Science and Technology, Department of Electronics and Communications Engineering, New Borg El-Arab City, Alexandria, 21934, Egypt; Allam A., Egypt-Japan University of Science and Technology, Department of Electronics and Communications Engineering, New Borg El-Arab City, Alexandria, 21934, Egypt; Abdelwahab M., Egypt-Japan University of Science and Technology, Department of Electronics and Communications Engineering, New Borg El-Arab City, Alexandria, 21934, Egypt; Elsharkawy M., Egypt-Japan University of Science and Technology, Department of Electronics and Communications Engineering, New Borg El-Arab City, Alexandria, 21934, Egypt; Kobasy B., Egypt-Japan University of Science and Technology, Department of Electronics and Communications Engineering, New Borg El-Arab City, Alexandria, 21934, Egypt; Eltabakh H., Egypt-Japan University of Science and Technology, Department of Electronics and Communications Engineering, New Borg El-Arab City, Alexandria, 21934, Egypt; Essam O., Egypt-Japan University of Science and Technology, Department of Electronics and Communications Engineering, New Borg El-Arab City, Alexandria, 21934, Egypt","Visual impairment affects millions of people globally, presenting challenges in everyday tasks. This research paper introduces a Smart Blind Stick, equipped with deep learning-based object recognition, health, and location monitoring through IoT capabilities and ultrasonic sensors-based obstacle avoidance. The system enhances independence and safety for visually impaired individuals, providing real-time feedback and remote monitoring. Results demonstrate high accuracy and user satisfaction, paving the way for further advancements in assistive technology. Future work includes optimizing data transfer, enhancing sensors, and expanding features for a comprehensive solution. © 2023 IEEE.","Assistive technology; Deep learning; Embedded vision systems; Image processing; Internet of Things; Single-Shot detector","Data transfer; Deep learning; Embedded systems; Image enhancement; Object detection; Object recognition; Ultrasonic applications; Assistive technology; Deep learning; Embedded vision system; Embedded visions; Images processing; Objects detection; Obstacles avoidance; Single-shot; Single-shot detector; Vision systems; Internet of things","","Bedair A.; Kanaya H.; Yousef A.H.","Institute of Electrical and Electronics Engineers Inc.","","979-835036932-8","","","English","Proc. Int. Japan-Africa Conf. Electron., Commun., Comput., JAC-ECC","Conference paper","Final","","Scopus","2-s2.0-85190655919"
"Ghose S.; Roy K.; Prevost J.J.","Ghose, Sanchita (57188995808); Roy, Krishna (57217138056); Prevost, John J. (26040928800)","57188995808; 57217138056; 26040928800","SoundEYE: AI-Driven Sensory Augmentation System for Visually Impaired Individuals through Natural Sound","2024","2024 19th Annual System of Systems Engineering Conference, SoSE 2024","","","","147","152","5","0","10.1109/SOSE62659.2024.10620973","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85202175696&doi=10.1109%2fSOSE62659.2024.10620973&partnerID=40&md5=5bb24f80bc212c18006af9fcd3a2b2a6","San Francisco State University, Computer Engineering, San Francisco, United States; New Mexico Institute of Mining and Technology, Electrical Engineering, Socorro, United States; University of Texas at San Antonio, Electrical and Computer Engineering, San Antonio, United States","Ghose S., San Francisco State University, Computer Engineering, San Francisco, United States; Roy K., New Mexico Institute of Mining and Technology, Electrical Engineering, Socorro, United States; Prevost J.J., University of Texas at San Antonio, Electrical and Computer Engineering, San Antonio, United States","Sensory augmentation is required for the visually impaired person (VIP) to compensate for the loss of vision and enhance their perception of the environment. Existing solutions for navigation aids have limited flexibility to respond to real-time changes in actions happening in a visual scene, limiting the blind individual to perceive the surroundings comprehensively and spontaneously. In this paper, we introduce an AI-driven sensory augmentation system (SoundEYE) for VIPs that will produce sound from visuals utilizing the advantages of employing natural auditory guidance which is readily comprehensible to the user. The proposed SoundEYE system incorporates a novel sound generation AI model combining visual action recognition and adversarial audio synthesis networks that can automatically generate representable sound for critical situations. This leads to a significant impact on VIPs to comprehend and deal with real-world events. Unlike the existing sound aid solutions, SoundEYE is capable of analyzing a complex scene with the presence of multiple sound sources and generating representable synchronous sound through capturing audio-visual synchronization with moving objects. In addition, we propose a comprehensive evaluation methodology to assess the performance of the system including an empirical study plan to analyze the impact of natural sound over vocal auditory cues for the VIP. © 2024 IEEE.","deep learning; sensory augmentation; sound generation; video recognition; visually impaired person","Audio studios; Audition; Mobility aids for blind persons; Vision; Vision aids; Augmentation systems; Deep learning; Natural sounds; Navigation aids; Real-time changes; Sensory augmentations; Sound generation; Video recognition; Visually impaired; Visually impaired persons; Acoustic generators","S. Ghose; San Francisco State University, Computer Engineering, San Francisco, United States; email: sanchitaghose@sfsu.edu","","Institute of Electrical and Electronics Engineers Inc.","","979-835036591-7","","","English","Annu. Syst. Syst. Eng. Conf., SoSE","Conference paper","Final","","Scopus","2-s2.0-85202175696"
"","","","Proceedings of the 17th ACM International Conference on PErvasive Technologies Related to Assistive Environments, PETRA 2024","2024","ACM International Conference Proceeding Series","","","","","","702","0","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85198054927&partnerID=40&md5=252a74fc47064af2b002a8b71d4b9577","","","The proceedings contain 108 papers. The topics discussed include: SafeRoute: a safer outdoor navigation algorithm with smart routing for people with visual impairment; a method for presenting UML class diagrams with audio for blind and visually impaired students; proposal for a versatile smart white cane infrastructure using a 3D printer: as a R&D platform and a practical product design; querying knowledge graphs in Greek language; exploring connections between computer programming and empathy; human-robot interactive system for warehouses using speech, slam, and deep learning-based barcode recognition; towards a user-centered design approach for the development of social assistive robots in office environments; adaptive motion imitation for robot assisted physiotherapy using dynamic time warping and recurrent neural network; and examining diverse gender dynamics in human-robot interaction: trust, privacy, and safety perceptions.","","","","Karim E.; Nikanfar S.; Pavel H.R.","Association for Computing Machinery","","979-840071760-4","","","English","ACM Int. Conf. Proc. Ser.","Conference review","Final","","Scopus","2-s2.0-85198054927"
"Valipoor M.M.; de Antonio A.","Valipoor, Mohammad Moeen (57446611600); de Antonio, Angélica (35607774800)","57446611600; 35607774800","Recent trends in computer vision-driven scene understanding for VI/blind users: a systematic mapping","2023","Universal Access in the Information Society","22","3","","983","1005","22","10","10.1007/s10209-022-00868-w","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85124341060&doi=10.1007%2fs10209-022-00868-w&partnerID=40&md5=97d44520f69fb6a4c3269273c19e0c92","Madrid HCI Lab, Universidad Politecnica de Madrid, ETS Ingenieros Informáticos, Madrid, Spain","Valipoor M.M., Madrid HCI Lab, Universidad Politecnica de Madrid, ETS Ingenieros Informáticos, Madrid, Spain; de Antonio A., Madrid HCI Lab, Universidad Politecnica de Madrid, ETS Ingenieros Informáticos, Madrid, Spain","During the past years, the development of assistive technologies for visually impaired (VI)/blind people has helped address various challenges in their lives by providing services such as obstacle detection, indoor/outdoor navigation, scene description, text reading, facial recognition and so on. This systematic mapping review is mainly focused on the scene understanding aspect (e.g., object recognition and obstacle detection) of assistive solutions. It provides guidance for researchers in this field to understand the advances during the last four and a half years. This is because deep learning techniques together with computer vision have become more powerful and accurate than ever in tasks like object detection. These advancements can bring a radical change in the development of high-quality assistive technologies for VI/blind users. Additionally, an overview of the current challenges and a comparison between different solutions is provided to indicate the pros and cons of existing approaches. © 2022, The Author(s).","Assistive technology; Computer vision; Scene understanding; Systematic mapping; Visually impaired","Character recognition; Computer vision; Deep learning; Face recognition; Object detection; Object recognition; Obstacle detectors; Assistive technology; Assistive technology for visually impaired; Blind people; Blind users; Indoor/outdoor; Obstacles detection; Recent trends; Scene understanding; Systematic mapping; Visually impaired; Mapping","M.M. Valipoor; Madrid HCI Lab, Universidad Politecnica de Madrid, ETS Ingenieros Informáticos, Madrid, Spain; email: moeen.valipoor@alumnos.upm.es","","Springer Science and Business Media Deutschland GmbH","16155289","","","","English","Univers. Access Inf. Soc.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85124341060"
"Theodorou P.; Tsiligkos K.; Meliones A.","Theodorou, Paraskevi (57194044913); Tsiligkos, Kleomenis (56638552300); Meliones, Apostolos (55949094700)","57194044913; 56638552300; 55949094700","Multi-Sensor Data Fusion Solutions for Blind and Visually Impaired: Research and Commercial Navigation Applications for Indoor and Outdoor Spaces","2023","Sensors","23","12","5411","","","","4","10.3390/s23125411","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85163974408&doi=10.3390%2fs23125411&partnerID=40&md5=b8e1cf22ef290225cb7da3da53cb4ba4","Department of Digital Systems, School of Information and Communication Technologies, University of Piraeus, Piraeus, 18534, Greece","Theodorou P., Department of Digital Systems, School of Information and Communication Technologies, University of Piraeus, Piraeus, 18534, Greece; Tsiligkos K., Department of Digital Systems, School of Information and Communication Technologies, University of Piraeus, Piraeus, 18534, Greece; Meliones A., Department of Digital Systems, School of Information and Communication Technologies, University of Piraeus, Piraeus, 18534, Greece","Several assistive technology solutions, targeting the group of Blind and Visually Impaired (BVI), have been proposed in the literature utilizing multi-sensor data fusion techniques. Furthermore, several commercial systems are currently being used in real-life scenarios by BVI individuals. However, given the rate by which new publications are made, the available review studies become quickly outdated. Moreover, there is no comparative study regarding the multi-sensor data fusion techniques between those found in the research literature and those being used in the commercial applications that many BVI individuals trust to complete their everyday activities. The objective of this study is to classify the available multi-sensor data fusion solutions found in the research literature and the commercial applications, conduct a comparative study between the most popular commercial applications (Blindsquare, Lazarillo, Ariadne GPS, Nav by ViaOpta, Seeing Assistant Move) regarding the supported features as well as compare the two most popular ones (Blindsquare and Lazarillo) with the BlindRouteVision application, developed by the authors, from the standpoint of Usability and User Experience (UX) through field testing. The literature review of sensor-fusion solutions highlights the trends of utilizing computer vision and deep learning techniques, the comparison of the commercial applications reveals their features, strengths, and weaknesses while Usability and UX demonstrate that BVI individuals are willing to sacrifice a wealth of features for more reliable navigation. © 2023 by the authors.","assistive technologies; comparison analysis; computer vision; deep learning; remote sensing; sensor fusion techniques; usability and user experience","Blindness; Humans; Self-Help Devices; Sensory Aids; Visually Impaired Persons; Computer vision; Data visualization; Deep learning; Indoor positioning systems; Learning systems; Well testing; Assistive technology; Blind and visually impaired; Comparison analysis; Deep learning; Fusion techniques; Remote-sensing; Sensor fusion; Sensor fusion technique; Usability and user experience; Users' experiences; blindness; human; visually impaired person; Remote sensing","P. Theodorou; Department of Digital Systems, School of Information and Communication Technologies, University of Piraeus, Piraeus, 18534, Greece; email: theodoroup@unipi.gr","","MDPI","14248220","","","37420578","English","Sensors","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85163974408"
"Singh J.N.; Mishra P.; Nigam A.","Singh, J.N. (57925424500); Mishra, Palak (59169228000); Nigam, Abhishek (59169694900)","57925424500; 59169228000; 59169694900","Application Software for visually disabled, Object Detection using CNN","2023","Proceedings - IEEE 2023 5th International Conference on Advances in Computing, Communication Control and Networking, ICAC3N 2023","","","","1122","1126","4","0","10.1109/ICAC3N60023.2023.10541382","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85195785663&doi=10.1109%2fICAC3N60023.2023.10541382&partnerID=40&md5=a77b80200b182a185648a7185d7d54b5","Galgotias University, Gr Noida, India","Singh J.N., Galgotias University, Gr Noida, India; Mishra P., Galgotias University, Gr Noida, India; Nigam A., Galgotias University, Gr Noida, India","In order to provide non-visual access to the Android operating system, this article suggests Be My Eyes, a universal voice control solution. Nearly 60% of the world's total population of blind people are thought to reside in INDIA. There are several assistances for disabled people, who are considered second-class citizens in our expanding society. The blind mobility guarantee is one of many necessary supports. Even today, it is difficult for blind persons to move freely despite numerous efforts.Voice recognition algorithms are much more necessary now than they ever have been because of the explosive expansion of wireless communication. Users can concentrate on their current tasks without experiencing more hand or eye strain by using speech apps that are based on voice interfaces, voice recognition, and voice conversation management. The application listens to your voice instructions and then speaks back in response. Your voice will be converted to text by the programme.This study focuses on object identification using CNN to assist the blind in using only their smartphone cameras for navigation. The object detection app will be created for both the iPhone and the Android platform. The visually challenged will be able to better traverse the world with only their mobile phones thanks to this application software. For object detection, a convolutional neural network will be used. There are numerous algorithms at work, including support vector machines, background subtraction, optical flow, time resolution, Kalman filtering, and contour matching. In addition to the methods we discussed, Convolutional Neural Network (CNN) is the most recent technique for object detection. © 2023 IEEE.","CNN; control programme Android operating system; visual access.; worldwide voice","Application programs; Convolution; Convolutional neural networks; Object detection; Object recognition; Smartphones; Speech recognition; Support vector machines; Applications software; Control program; Control program android operating system; Convolutional neural network; Non visuals; Objects detection; Visual access; Visual access.; Voice control; Worldwide voice; Android (operating system)","J.N. Singh; Galgotias University, Gr Noida, India; email: singhjn2000@gmail.com","Sharma V.; Sinha J.; Chandraprabha M.; Kumar S.; Rana A.K.","Institute of Electrical and Electronics Engineers Inc.","","979-835033086-1","","","English","Proc. - IEEE Int. Conf. Adv. Comput., Commun. Control Netw., ICAC3N","Conference paper","Final","","Scopus","2-s2.0-85195785663"
"Chaple S.; Raut V.; Patni J.C.; Banode A.; Ninawe S.; Shelke N.","Chaple, Shreya (59129688100); Raut, Vedanti (59128927800); Patni, Jagdish Chandra (46161508100); Banode, Ayush (59129115000); Ninawe, Samiksha (59128736100); Shelke, Nilesh (57193567661)","59129688100; 59128927800; 46161508100; 59129115000; 59128736100; 57193567661","Artificial Intelligence on Visually Impaired People: A Comprehensive Review","2024","Proceedings - 2024 5th International Conference on Intelligent Communication Technologies and Virtual Mobile Networks, ICICV 2024","","","","304","308","4","0","10.1109/ICICV62344.2024.00052","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85193213903&doi=10.1109%2fICICV62344.2024.00052&partnerID=40&md5=6479eb4b4f961074b9b9058413fbe6f2","Symbiosis Institute of Technology Nagpur Campus, Symbiosis International (Deemed University), Pune, India","Chaple S., Symbiosis Institute of Technology Nagpur Campus, Symbiosis International (Deemed University), Pune, India; Raut V., Symbiosis Institute of Technology Nagpur Campus, Symbiosis International (Deemed University), Pune, India; Patni J.C., Symbiosis Institute of Technology Nagpur Campus, Symbiosis International (Deemed University), Pune, India; Banode A., Symbiosis Institute of Technology Nagpur Campus, Symbiosis International (Deemed University), Pune, India; Ninawe S., Symbiosis Institute of Technology Nagpur Campus, Symbiosis International (Deemed University), Pune, India; Shelke N., Symbiosis Institute of Technology Nagpur Campus, Symbiosis International (Deemed University), Pune, India","Artificial Intelligence has become a significant tool in modern technology, enabling people to interact with machines through various methods. Individuals with visual impairments have trouble doing tasks because they are either blind or have poor vision. BVI stands for Blind and Visually Impaired. Solutions must also advance with technology to ensure that individuals can effectively navigate their surroundings and assist them in real-time navigation. The study conducts surveys on visually challenged individuals in the community and aims to assist them by providing smart gadgets to identify faces, colours, and objects. Moreover, this study emphasizes more on different technologies and methods that are used earlier to help visually impaired people in their day-to-day life. © 2024 IEEE.","Artificial Intelligence; Convolutional Neural Networks (CNN); navigation; Object Recognition; Red Green Blue-Depth (RGB-D) camera; SKYE; smart stick; Voice Assistant","Convolutional neural networks; Convolutional neural network; Depth camera; Objects recognition; Red green blue-depth  camera; Red green blues; SKYE; Smart stick; Visually impaired people; Voice assistant; Object recognition","J.C. Patni; Symbiosis Institute of Technology Nagpur Campus, Symbiosis International (Deemed University), Pune, India; email: patnijack@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","","979-835038564-9","","","English","Proc. - Int. Conf. Intell. Commun. Technol. Virtual Mob. Networks, ICICV","Conference paper","Final","","Scopus","2-s2.0-85193213903"
"Rohith Kumar A.; Sanjay K.; Praveen M.","Rohith Kumar, A. (58893698400); Sanjay, K. (57211514200); Praveen, M. (57211777065)","58893698400; 57211514200; 57211777065","EchoGuide: Empowering the Visually Impaired with IoT-Enabled Smart Stick and Audio Navigation","2023","2nd International Conference on Automation, Computing and Renewable Systems, ICACRS 2023 - Proceedings","","","","1770","1774","4","0","10.1109/ICACRS58579.2023.10405085","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85185373174&doi=10.1109%2fICACRS58579.2023.10405085&partnerID=40&md5=cea65e523cf3e23f7f63a751510f455b","Dept. of Electronics and Communication Engineering, Saveetha Engineering College, Chennai, India","Rohith Kumar A., Dept. of Electronics and Communication Engineering, Saveetha Engineering College, Chennai, India; Sanjay K., Dept. of Electronics and Communication Engineering, Saveetha Engineering College, Chennai, India; Praveen M., Dept. of Electronics and Communication Engineering, Saveetha Engineering College, Chennai, India","Many blind and disabled people nowadays experience great hardship due to the numerous obstacles and the inherent dangers they must confront daily. The AIoT Blind Stick is a groundbreaking assistive device designed to empower visually impaired individuals with enhanced mobility and independence. This innovative solution integrates a Raspberry Pi Zero, high-resolution camera, MPU sensor, ultrasonic sensor, Buzzer, and speaker to create a comprehensive system that addresses the visually impaired community's unique challenges. The device's advanced features, including object recognition, obstacle detection, and orientation tracking, contribute to its effectiveness in providing real-time feedback to users. Through a combination of auditory alerts and voice prompts, the AIoT Blind Stick offers a multi-modal feedback system, catering to diverse user preferences. The detailed results and discussions on the device's performance, highlighting its potential to revolutionize the way visually impaired individuals navigate their environments. The AIoT Blind Stick represents a significant advancement in assistive technology, potentially greatly improving the quality of life for visually impaired individuals worldwide. Further research and refinement are recommended to optimize its functionality across various real-world scenarios. © 2023 IEEE.","Artificial intelligence of things; Blind Assistive Device; Navigation; Smart stick","Assistive technology; Internet of things; Object recognition; Obstacle detectors; Ultrasonic applications; Artificial intelligence of thing; Assistive devices; Audio navigation; Blind assistive device; Blind people; Disabled people; Enhanced mobility; Innovative solutions; Smart stick; Visually impaired; Feedback","","","Institute of Electrical and Electronics Engineers Inc.","","979-835034023-5","","","English","Int. Conf. Autom., Comput. Renew. Syst., ICACRS - Proc.","Conference paper","Final","","Scopus","2-s2.0-85185373174"
"Kim J.T.; Kothari Y.; Yu W.; Walker B.N.; Tan J.; Turk G.; Ha S.","Kim, J. Taery (59005660900); Kothari, Yash (59238413300); Yu, Wenhao (57189459079); Walker, Bruce N. (7402208804); Tan, Jie (42262817800); Turk, Greg (7005234161); Ha, Sehoon (51261111200)","59005660900; 59238413300; 57189459079; 7402208804; 42262817800; 7005234161; 51261111200","Transforming a Quadruped into a Guide Robot for the Visually Impaired: Formalizing Wayfinding, Interaction Modeling, and Safety Mechanism","2023","Proceedings of Machine Learning Research","229","","","","","","0","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85184345733&partnerID=40&md5=a4b3d4d90b8cfe69efcfd934ba1c03f4","Georgia Institute of Technology, United States; Google DeepMind, United Kingdom","Kim J.T., Georgia Institute of Technology, United States; Kothari Y., Georgia Institute of Technology, United States; Yu W., Google DeepMind, United Kingdom; Walker B.N., Georgia Institute of Technology, United States; Tan J., Google DeepMind, United Kingdom; Turk G., Georgia Institute of Technology, United States; Ha S., Georgia Institute of Technology, United States","This paper explores the principles for transforming a quadrupedal robot into a guide robot for individuals with visual impairments. A guide robot has great potential to resolve the limited availability of guide animals that are accessible to only two to three percent of the potential blind or visually impaired (BVI) users. To build a successful guide robot, our paper explores three key topics: (1) formalizing the navigation mechanism of a guide dog and a human, (2) developing a data-driven model of their interaction, and (3) improving user safety. First, we formalize the wayfinding task of the human-guide robot team using Markov Decision Processes based on the literature and interviews. Then we collect real human-robot interaction data from three visually impaired and six sighted people and develop an interaction model called the “Delayed Harness” to effectively simulate the navigation behaviors of the team. Additionally, we introduce an action shielding mechanism to enhance user safety by predicting and filtering out dangerous actions. We evaluate the developed interaction model and the safety mechanism in simulation, which greatly reduce the prediction errors and the number of collisions, respectively. We also demonstrate the integrated system on a quadrupedal robot with a rigid harness, by guiding users over 100+ m trajectories. © 2023 Proceedings of Machine Learning Research. All Rights Reserved.","Assistive Robot; Autonomous Navigation; Interaction Modeling","Data visualization; Human robot interaction; Machine learning; Navigation; Assistive robots; Autonomous navigation; Guide robots; Interaction modeling; Model mechanisms; Quadrupedal robot; Safety mechanisms; Visual impairment; Visually impaired; Way finding; Markov processes","","Tan J.; Toussaint M.; Darvish K.","ML Research Press","26403498","","","","English","Proc. Mach. Learn. Res.","Conference paper","Final","","Scopus","2-s2.0-85184345733"
"Gensytskyy O.; Nandi P.; Otis M.J.-D.; Tabi C.E.; Ayena J.C.","Gensytskyy, Oleksiy (58531293100); Nandi, Pratyush (58531591500); Otis, Martin J.-D. (57210200087); Tabi, Clinton Enow (58531441700); Ayena, Johannes C. (56786583800)","58531293100; 58531591500; 57210200087; 58531441700; 56786583800","Soil friction coefficient estimation using CNN included in an assistive system for walking in urban areas","2023","Journal of Ambient Intelligence and Humanized Computing","14","10","","14291","14307","16","1","10.1007/s12652-023-04667-w","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85167616766&doi=10.1007%2fs12652-023-04667-w&partnerID=40&md5=870d69134f30c28a5e54683868310039","Universite du Quebec a Chicoutimi, Chicoutimi, QC, Canada","Gensytskyy O., Universite du Quebec a Chicoutimi, Chicoutimi, QC, Canada; Nandi P., Universite du Quebec a Chicoutimi, Chicoutimi, QC, Canada; Otis M.J.-D., Universite du Quebec a Chicoutimi, Chicoutimi, QC, Canada; Tabi C.E., Universite du Quebec a Chicoutimi, Chicoutimi, QC, Canada; Ayena J.C., Universite du Quebec a Chicoutimi, Chicoutimi, QC, Canada","We present a smartphone-based solution to help visually impaired people autonomously navigate in urban environments. Contrary to previous works in the literature, the newly proposed system in this paper aims to determine the coefficient of friction (COF) of a soil for aiding in the safe movement of blind and visually impaired people (BVIP). Through convolutional neural networks (CNNs) trained to output this measure, our new investigation then offers the possibility of predicting a risk of falling in their next step by determining the maximum static friction force of the ground. Indeed, a commercial smartphone’s camera captures the video of the ground and sends frame as inputs to the CNN model for image segmentation and COF computation. We validated our proposed model in real experiments carried out on 8 types of soils, while also experimenting different CNN models and different optimizers. The proposed ResNet50 CNN-based system provides an accuracy of 96% in the classification of soil type, enabling to guide visually impaired persons. Combined with the associated COF of the soil type, it’s possible to estimate a risk of fall (stick or slip) for the next step in the front of the user from the past measured interaction force (using an instrumented insole) between the soil and the sole. Traditional navigation approaches do not consider the soil characteristics such as COF to guide blind and visually impaired person. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Computer vision; Deep learning; Risk of fall; Smartphone","Computer vision; Convolutional neural networks; Deep learning; Image segmentation; Neural network models; Risk assessment; Risk perception; Soils; Walking aids; Blind and visually impaired; Coefficient of frictions; Convolutional neural network; Deep learning; Neural network model; Risk of fall; Smart phones; Soil types; Visually impaired people; Visually impaired persons; Smartphones","O. Gensytskyy; Universite du Quebec a Chicoutimi, Chicoutimi, Canada; email: oleksiy.gensytskyy1@uqac.ca","","Springer Science and Business Media Deutschland GmbH","18685137","","","","English","J. Ambient Intell. Humanized Comput.","Article","Final","","Scopus","2-s2.0-85167616766"
"Faurina R.; Jelita A.; Vatresia A.; Agustian I.","Faurina, Ruvita (57222816441); Jelita, Anisa (58220761600); Vatresia, Arie (57191670280); Agustian, Indra (57208755262)","57222816441; 58220761600; 57191670280; 57208755262","Image captioning to aid blind and visually impaired outdoor navigation","2023","IAES International Journal of Artificial Intelligence","12","3","","1104","1117","13","1","10.11591/ijai.v12.i3.pp1104-1117","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85156165775&doi=10.11591%2fijai.v12.i3.pp1104-1117&partnerID=40&md5=097103d990600f22277740d0dc94855c","Department of Informatics, Faculty of Engineering, University of Bengkulu, Bengkulu, Indonesia; Department of Electrical Engineering, Faculty of Engineering, University of Bengkulu, Bengkulu, Indonesia","Faurina R., Department of Informatics, Faculty of Engineering, University of Bengkulu, Bengkulu, Indonesia; Jelita A., Department of Informatics, Faculty of Engineering, University of Bengkulu, Bengkulu, Indonesia; Vatresia A., Department of Informatics, Faculty of Engineering, University of Bengkulu, Bengkulu, Indonesia; Agustian I., Department of Electrical Engineering, Faculty of Engineering, University of Bengkulu, Bengkulu, Indonesia","Artificial intelligence technology has dramatically improved the quality of services for human needs, one of which is technology to improve the quality of services for the blind and visually impaired, particularly technology that can help them understand visual sights to facilitate navigation in their daily lives. This study developed an image captioning model to aid the blind and visually impaired in outdoor navigation. The image captioning model employs the encoder-decoder method, with the convolutional neural network (CNN) feature extraction and attention layer as encoders and the long short-term memory (LSTM) as decoders. ResNet101 and ResNet152 are used in the encoder to extract image features. The results of the extraction and caption are forwarded to the attention layer and the LSTM network. The attention layer uses the Bahdanau attention mechanism. The accuracy of the model is calculated using the bilingual evaluation understudy score (BLEU), metric for evaluation of translation with explicit ordering (METEOR) and recall-oriented understudy for gisting evaluation-longest common subsequence (ROUGE-L). ResNet101 performed the best on BLEU-4, scoring 91.811% and 94.0337% in the METEOR evaluation. The captioning results show that the model is quite successful in displaying a simple caption that is suitable for each image. © 2023, Institute of Advanced Engineering and Science. All rights reserved.","Attention mechanism; Convolutional neural network; Image captioning; Long short-term memory; Visually impaired","","R. Faurina; Department of Informatics, Faculty of Engineering, University of Bengkulu, Bengkulu, Jl. W.R Supratman, Kandang Limun, 38371, Indonesia; email: ruvita.faurina@unib.ac.id","","Institute of Advanced Engineering and Science","20894872","","","","English","IAES Int. J. Artif. Intell.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85156165775"
"Azzino T.; Mezzavilla M.; Rangan S.; Wang Y.; Rizzo J.-R.","Azzino, Tommy (57197734753); Mezzavilla, Marco (50262394700); Rangan, Sundeep (7006269762); Wang, Yao (35436329400); Rizzo, John-Ross (56527876700)","57197734753; 50262394700; 7006269762; 35436329400; 56527876700","5G Edge Vision: Wearable Assistive Technology for People with Blindness and Low Vision","2024","IEEE Wireless Communications and Networking Conference, WCNC","","","","","","","0","10.1109/WCNC57260.2024.10570607","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85198843196&doi=10.1109%2fWCNC57260.2024.10570607&partnerID=40&md5=1c3288e21e40eedd032b4c044028bc54","Nyu Tandon School Of Engineering, Brooklyn, NY, United States; Nyu Langone, Department Of Rehabilitation Medicine, New York, NY, United States","Azzino T., Nyu Tandon School Of Engineering, Brooklyn, NY, United States; Mezzavilla M., Nyu Tandon School Of Engineering, Brooklyn, NY, United States; Rangan S., Nyu Tandon School Of Engineering, Brooklyn, NY, United States; Wang Y., Nyu Tandon School Of Engineering, Brooklyn, NY, United States; Rizzo J.-R., Nyu Tandon School Of Engineering, Brooklyn, NY, United States, Nyu Langone, Department Of Rehabilitation Medicine, New York, NY, United States","In an increasingly visual world, people with blindness and low vision (pBLV) face substantial challenges in navigating their surroundings and interpreting visual information. From our previous work, VIS4ION is a smart wearable that helps pBLV in their daily challenges. It enables multiple microservices based on artificial intelligence (AI), such as visual scene processing, navigation, and vision-language inference. These microservices require powerful computational resources and, in some cases, stringent inference times, hence the need to offload computation to edge servers. This paper introduces a novel video streaming platform that improves the capabilities of VIS4ION by providing real-time support of the microservices at the network edge. When video is offloaded wirelessly to the edge, the time-varying nature of the wireless network requires adaptation strategies for a seamless video service. We demonstrate the performance of our adaptive real-time video streaming platform through experimentation with an open-source 5G deployment based on open air interface (OAI). The experiments demonstrate the ability to provide microservices robustly in time-varying network conditions.  © 2024 IEEE.","5G; AI; assistive technology; e-health; edge computing; testbed; video streaming; wearable","5G mobile communication systems; Delay-sensitive applications; Edge computing; Video streaming; Visual languages; Wearable technology; 5g; Assistive technology; E health; Edge computing; Ehealth; Low vision; Video-streaming; Visual information; Visual world; Wearable; Eye protection","T. Azzino; Nyu Tandon School Of Engineering, Brooklyn, United States; email: ta1731@nyu.edu; M. Mezzavilla; Nyu Tandon School Of Engineering, Brooklyn, United States; email: mezzavilla@nyu.edu; S. Rangan; Nyu Tandon School Of Engineering, Brooklyn, United States; email: srangan@nyu.edu; Y. Wang; Nyu Tandon School Of Engineering, Brooklyn, United States; email: yw523@nyu.edu; J.-R. Rizzo; Nyu Tandon School Of Engineering, Brooklyn, United States; email: johnrossrizzo@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","15253511","979-835030358-2","","","English","IEEE Wireless Commun. Networking Conf. WCNC","Conference paper","Final","","Scopus","2-s2.0-85198843196"
"Wong C.Y.; Ananto R.A.; Akiyama T.; Nemargut J.P.; Jung Moon A.","Wong, Christopher Yee (57037520400); Ananto, Rahatul Amin (57220030143); Akiyama, Tanaka (57568786900); Nemargut, Joseph Paul (23028244600); Jung Moon, A. (54969168600)","57037520400; 57220030143; 57568786900; 23028244600; 54969168600","Perspectives on Robotic Systems for the Visually Impaired","2024","ACM/IEEE International Conference on Human-Robot Interaction","","","","1134","1138","4","0","10.1145/3610978.3640698","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85188056573&doi=10.1145%2f3610978.3640698&partnerID=40&md5=bb492cd610d55c09bf03b4561c6a19d4","McGill University, Montreal, QC, Canada; Université de Montreal, Montreal, QC, Canada","Wong C.Y., McGill University, Montreal, QC, Canada; Ananto R.A., McGill University, Montreal, QC, Canada; Akiyama T., McGill University, Montreal, QC, Canada; Nemargut J.P., Université de Montreal, Montreal, QC, Canada; Jung Moon A., McGill University, Montreal, QC, Canada","Many roboticists hope to build robots and develop technologies that would one day help vulnerable populations to improve their quality of life. As there are over 2.2 billion people with visual impairments in the world, this vulnerable population is a prime target for robotic assistants to help. In a discussion with a Certified Orientation and Mobility Specialist, someone who helps individuals with visual impairments navigate and perform daily tasks effectively, some interesting and counter-intuitive questions were raised about technological developments, particularly robots. While these devices were meant to help the blind and visually impaired (BVI) population, many are, in reality, not practically beneficial. In this article, we highlight certain misconceptions about the BVI population and their needs. We emphasize the mismatch between robotics research and the needs of the individuals with visual impairments, especially from the lens of human-robot interaction (HRI) researchers. © 2024 Copyright held by the owner/author(s)","accessibility; assistive robots; design; human-robot interactions","Lenses; Machine design; Man machine systems; Accessibility; Assistive robots; Blind and visually impaired; Daily tasks; Humans-robot interactions; Quality of life; Robotic assistants; Robotic systems; Visual impairment; Visually impaired; Human robot interaction","","","IEEE Computer Society","21672148","979-840070323-2","","","English","ACM/IEEE Int. Conf. Hum.-Rob. Interact.","Conference paper","Final","","Scopus","2-s2.0-85188056573"
"Tang W.; Liu D.-E.; Zhao X.; Chen Z.; Zhao C.","Tang, Wu (57226739099); Liu, De-er (36554558600); Zhao, Xiaoli (57226738410); Chen, Zenghui (57226743525); Zhao, Chen (57226758025)","57226739099; 36554558600; 57226738410; 57226743525; 57226758025","A dataset for the recognition of obstacles on blind sidewalk","2023","Universal Access in the Information Society","22","1","","69","82","13","3","10.1007/s10209-021-00837-9","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85112498683&doi=10.1007%2fs10209-021-00837-9&partnerID=40&md5=25db04ca0f9e57496b0d06ee9ce88930","School of Civil and Surveying & Mapping Engineering, Jiangxi University of Science and Technology, Jiangxi, Ganzhou, 341000, China; School of Economics and Management, Jiangxi University of Science and Technology, Jiangxi, Ganzhou, 341000, China; Fujian Jingwei surveying and mapping information CO., Ltd, Fujian, Fuzhou, 350000, China","Tang W., School of Civil and Surveying & Mapping Engineering, Jiangxi University of Science and Technology, Jiangxi, Ganzhou, 341000, China; Liu D.-E., School of Civil and Surveying & Mapping Engineering, Jiangxi University of Science and Technology, Jiangxi, Ganzhou, 341000, China; Zhao X., School of Economics and Management, Jiangxi University of Science and Technology, Jiangxi, Ganzhou, 341000, China; Chen Z., School of Civil and Surveying & Mapping Engineering, Jiangxi University of Science and Technology, Jiangxi, Ganzhou, 341000, China; Zhao C., Fujian Jingwei surveying and mapping information CO., Ltd, Fujian, Fuzhou, 350000, China","Recently, the technology of assisting the navigation of visually impaired persons with computer vision has been greatly developed. A number of scholars have conducted related research, including indoor and outdoor object detection for blind people. However, there are still problems with some existing methods or datasets. Our work mainly proposes a dataset (OD) for assisting the detection and recognition of outdoor obstacles for blind people on blind sidewalk. We classify some common obstacles, train the dataset with state-of-the-art detectors to obtain detection models, and then analyze and compare these models in detail. The results show that our proposed dataset is very challenging. The OD and the detection model can be obtained at the following URL: https://github.com/TW0521/Obstacle-Dataset.git. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Blind sidewalk; Obstacle detection; OD; Visually impaired person (VIP)","Classification (of information); Obstacle detectors; Pavements; Blind people; Blind sidewalk; Detection models; Objects detection; Obstacles detection; OD; State of the art; Visually impaired person; Visually impaired persons; Object detection","D.-E. Liu; School of Civil and Surveying & Mapping Engineering, Jiangxi University of Science and Technology, Ganzhou, Jiangxi, 341000, China; email: landserver@163.com","","Springer Science and Business Media Deutschland GmbH","16155289","","","","English","Univers. Access Inf. Soc.","Article","Final","","Scopus","2-s2.0-85112498683"
"Almajdoub R.; Siddiqui M.A.","Almajdoub, Reima (59258608100); Siddiqui, Muhammad Ajmal (58183494800)","59258608100; 58183494800","A CNN -based Crosswalk Guidance for the Visually Impaired that Combines a Fuzzy Logic Controller and Genetic Algorithm","2024","2024 IEEE 4th International Maghreb Meeting of the Conference on Sciences and Techniques of Automatic Control and Computer Engineering, MI-STA 2024 - Proceeding","","","","746","754","8","0","10.1109/MI-STA61267.2024.10599717","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85201161108&doi=10.1109%2fMI-STA61267.2024.10599717&partnerID=40&md5=86d10bd56c645cc6d6a04c504c050607","Sebha University, Department of Computer Science, Faculty of Science, Libya; Smart Nations, Inception Institute of AI, Abu Dhabi, United Arab Emirates","Almajdoub R., Sebha University, Department of Computer Science, Faculty of Science, Libya; Siddiqui M.A., Smart Nations, Inception Institute of AI, Abu Dhabi, United Arab Emirates","This study focuses on the development of a support system for visual impairments that combines computer vision techniques, Convolutional Neural Networks (CNN), Fuzzy Logic Controller (FLC), and Genetic Algorithms (GA). The proposed system combines the advantages of three intelligent logic systems: CNN, FLC, and GA. This integrated system, named the NFG (Neural Fuzzy Genetic) system, aims to detect and track objects, measure the distance between obstacles and the blind or visually impaired (BVI) person, and provide movement guidance using three ultrasonic sensors with FLC. The combination of these technologies provides a novel approach to improving the mobility and safety of individuals who are Blind or Visually Impaired (BVI). In particular, the use of object detection and tracking through CNN is implemented, with the range for obstacle detection extending up to 40 meters. The obstacle recognition system is trained on the ResNet50 model, comprising 50 million trained images and over 1000 classifications of obstacles, resulting in high accuracy in identifying obstacles. This speed in identifying, avoiding, and guiding ensures efficient navigation towards the intended destination. FLC is then utilized to provide guidance that considers uncertainty in the information used for guidance such as near, far. It assists in making appropriate decisions in the event of obstacles, navigating around them, and determining directions for movement in obstacle-free paths. The system moves at a steering angle determined by fuzzy rules. The information from CNNs and sensors with FLC is integrated to optimize guidance information using GA. This optimization process fine-tunes the rules of FLC, aiming to find the best possible solutions for effective guidance.  © 2024 IEEE.","(NFG) System; Blind visually impaired persons (BVI); Collision Avoidance; Convolutional Neural Network (CNN); Fuzzy Logic Controller (FLC); Genetic Algorithm (GA); Objects detection; Smart glasses","Computer circuits; Controllers; Convolution; Convolutional neural networks; Fuzzy inference; Fuzzy neural networks; Genetic algorithms; Image segmentation; Obstacle detectors; Ultrasonic applications; (neural fuzzy genetic) system; Blind visually impaired person; Collisions avoidance; Convolutional neural network; Fuzzy logic controller; Fuzzy logic controllers; Genetic algorithm; Genetic systems; Neural fuzzy; Objects detection; Smart glass; Visually impaired persons; Object detection","","","Institute of Electrical and Electronics Engineers Inc.","","979-835037263-2","","","English","IEEE Int. Maghreb Meet. Conf. Sci. Techniques Autom. Control Comput. Eng., MI-STA - Proceeding","Conference paper","Final","","Scopus","2-s2.0-85201161108"
"Hwang H.; Jung H.-T.; Giudice N.A.; Biswas J.; Lee S.I.; Kim D.","Hwang, Hochul (57946718100); Jung, Hee-Tae (37057404800); Giudice, Nicholas A. (15724751900); Biswas, Joydeep (35774160500); Lee, Sunghoon Ivan (55049449300); Kim, Donghyun (57201849434)","57946718100; 37057404800; 15724751900; 35774160500; 55049449300; 57201849434","Towards Robotic Companions: Understanding Handler-Guide Dog Interactions for Informed Guide Dog Robot Design","2024","Conference on Human Factors in Computing Systems - Proceedings","","","596","","","","0","10.1145/3613904.3642181","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85194890410&doi=10.1145%2f3613904.3642181&partnerID=40&md5=f0e157389829ad0bfa80404f72b1098e","University of Massachusetts Amherst, Amherst, MA, United States; Indiana University Indianapolis, Indianapolis, IN, United States; University of Maine, Orono, ME, United States; University of Texas at Austin, Austin, TX, United States","Hwang H., University of Massachusetts Amherst, Amherst, MA, United States; Jung H.-T., Indiana University Indianapolis, Indianapolis, IN, United States; Giudice N.A., University of Maine, Orono, ME, United States; Biswas J., University of Texas at Austin, Austin, TX, United States; Lee S.I., University of Massachusetts Amherst, Amherst, MA, United States; Kim D., University of Massachusetts Amherst, Amherst, MA, United States","Dog guides are favored by blind and low-vision (BLV) individuals for their ability to enhance independence and confidence by reducing safety concerns and increasing navigation efficiency compared to traditional mobility aids. However, only a relatively small proportion of BLV individuals work with dog guides due to their limited availability and associated maintenance responsibilities. There is considerable recent interest in addressing this challenge by developing legged guide dog robots. This study was designed to determine critical aspects of the handler-guide dog interaction and better understand handler needs to inform guide dog robot development. We conducted semi-structured interviews and observation sessions with 23 dog guide handlers and 5 trainers. Thematic analysis revealed critical limitations in guide dog work, desired personalization in handler-guide dog interaction, and important perspectives on future guide dog robots. Grounded on these findings, we discuss pivotal design insights for guide dog robots aimed for adoption within the BLV community. © 2024 Copyright held by the owner/author(s)","Accessibility; Individuals with Disabilities & Assistive Technologies; Interview; Robot","Robots; Accessibility; Assistive technology; Guide dogs; Individual with disability & assistive technology; Interview; Low vision; Mobility aids; Robot designs; Robot development; Safety concerns; Machine design","S.I. Lee; University of Massachusetts Amherst, Amherst, United States; email: silee@cs.umass.edu; D. Kim; University of Massachusetts Amherst, Amherst, United States; email: donghyunkim@umass.edu","","Association for Computing Machinery","","979-840070330-0","","","English","Conf Hum Fact Comput Syst Proc","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85194890410"
"Fang X.; He W.; Yu H.; Wu S.; Zhang R.; Wu J.","Fang, Xiaoya (58810612400); He, Weisen (59122581200); Yu, Han (59122157400); Wu, Shuolei (59123011800); Zhang, Ruxing (59123011900); Wu, Jiajia (59122157500)","58810612400; 59122581200; 59122157400; 59123011800; 59123011900; 59122157500","Guided Blind Guidance APP Based on Path Planning and Obstacle Detection","2023","ACM International Conference Proceeding Series","","","","813","817","4","0","10.1145/3653081.3653218","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85192849466&doi=10.1145%2f3653081.3653218&partnerID=40&md5=ff8eeab81bcd54f52a51a6bca6632f5a","Tianjin University of Science and Technology, Tianjin, 300457, China","Fang X., Tianjin University of Science and Technology, Tianjin, 300457, China; He W., Tianjin University of Science and Technology, Tianjin, 300457, China; Yu H., Tianjin University of Science and Technology, Tianjin, 300457, China; Wu S., Tianjin University of Science and Technology, Tianjin, 300457, China; Zhang R., Tianjin University of Science and Technology, Tianjin, 300457, China; Wu J., Tianjin University of Science and Technology, Tianjin, 300457, China","This article proposes a new guiding software based on advanced route planning and object detection technology that aims to increase the mobility and convenience of visually impaired users dramatically. This APP use deep learning algorithms to efficiently recognize and locate surrounding impediments, allowing users to walk more confidently and safely. In addition to its strong obstacle recognition function, the guidance APP includes a huge data analysis remote navigation function. This program can not only connect to existing navigation systems, but also design the safest and most efficient trip paths for users by analyzing a significant quantity of multidimensional travel and map data. Furthermore, users can save frequently visited locations for future travel, improving navigation stability and accuracy. This innovation not only helps visually challenged individuals more independent, but it also boosts their self-esteem. This navigation APP is a great marriage of technology and human care, delivering a more intelligent, convenient, and dependable navigation solution for visually impaired people, allowing them to better integrate into society and live more independent and independent lives.  © 2023 ACM.","big data technology; interactive design; IoT; Multi scene detection; obstacle recognition; remote navigation","Big data; Internet of things; Motion planning; Navigation systems; Object detection; Obstacle detectors; Big data technology; Data technologies; Interactive design; IoT; Multi scene detection; Obstacle recognition; Obstacles detection; Remote navigation; Route planning; Scene detection; Deep learning","X. Fang; Tianjin University of Science and Technology, Tianjin, 300457, China; email: f924100376@gmail.com","","Association for Computing Machinery","","979-840071648-5","","","English","ACM Int. Conf. Proc. Ser.","Conference paper","Final","","Scopus","2-s2.0-85192849466"
"Caytuiro-Silva N.E.; Castro-Gutierrez E.G.; Peña-Alejandro J.M.","Caytuiro-Silva, Nicolás E. (57214230737); Castro-Gutierrez, Eveling G. (58152837700); Peña-Alejandro, Jackeline M. (58630679800)","57214230737; 58152837700; 58630679800","A Systematic Review of Assistive Tools for Individuals with Visual Impairments: Advancements in Assistive Technologies, Internet of Things and Computer Vision","2023","CEUR Workshop Proceedings","3693","","","258","270","12","0","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85195429428&partnerID=40&md5=d3f34a5cc2176b18dd383879c6892aa8","Universidad Católica de Santa María, Urb. San José s/n Umacollo, Arequipa, Peru","Caytuiro-Silva N.E., Universidad Católica de Santa María, Urb. San José s/n Umacollo, Arequipa, Peru; Castro-Gutierrez E.G., Universidad Católica de Santa María, Urb. San José s/n Umacollo, Arequipa, Peru; Peña-Alejandro J.M., Universidad Católica de Santa María, Urb. San José s/n Umacollo, Arequipa, Peru","Visual impairment significantly impacts the lives of millions globally, affecting daily activities and independence. Assistive technologies have emerged as promising tools to enhance autonomy and inclusion for individuals with visual disabilities. Despite numerous tools addressing mobility, navigation, orientation, and object recognition, many remain as proposals or prototypes, with limited impact on the visually impaired community. A comprehensive systematic review is crucial to assess the current state of assistive technology, IoT, and Computer Vision, identifying limitations, areas for improvement, and opportunities for new solutions. This review aims to analyze and synthesize theoretical and practical literature related to assistive tools for individuals with visual impairments. Conducting an exhaustive search on academic databases such as IEEE and Scopus, the review focuses on keywords like computer vision, deep learning, blind or visually impaired. Inclusion and exclusion criteria will guide study selection, with a focus on evaluating study quality. The systematic review analyzes recent technological advancements in assistive tools for the visually impaired, assessing limitations and contributions found in the literature. Key aspects, such as the accuracy and reliability of IoT and Computer Vision-based assistive technologies, are thoroughly evaluated. The University Isabel I systematic review method is employed, involving a manual search of 71 articles from journals, conference proceedings, and books. The findings provide valuable insights for future research, offering a current overview of existing assistive tools for visual impairment. Limitations and improvements identified guide and inspire future research in assistive technologies, IoT, and computer vision. Results reveal a higher publication rate in the Institute of Electrical and Electronics Engineers (IEEE) journal from the United States. The predominant limitation is technological dependence (16.46%), while the most significant contribution lies in the accuracy of detecting objects of interest (11.70%). This systematic review aims to broaden the understanding of existing assistive tools for visual impairment, focusing on technological advancements in Computer Vision and IoT. It anticipates guiding future research towards developing more effective assistive tools for visually impaired individuals. © 2023 Copyright for this paper by its authors.","Assistive Technologies; Computer Vision; IoT Technology; Visual Impairment","Computer vision; Deep learning; Internet of things; Object recognition; Ophthalmology; Reviews; 'current; Assistive technology; Assistive tool; Daily activity; Institute of Electricals and Electronics Engineers; IoT technology; Systematic Review; Technological advancement; Visual impairment; Visually impaired; Object detection","N.E. Caytuiro-Silva; Universidad Católica de Santa María, Arequipa, Urb. San José s/n Umacollo, Peru; email: nicolas.caytuiro@ucsm.edu.pe","Cardona-Reyes H.; Ortiz-Esparza M.A.","CEUR-WS","16130073","","","","English","CEUR Workshop Proc.","Conference paper","Final","","Scopus","2-s2.0-85195429428"
"Madake J.; Bhatlawande S.; Shilaskar S.; Deshpande P.; Deshpande V.","Madake, Jyoti (57222127908); Bhatlawande, Shripad (55212307900); Shilaskar, Swati (57189017229); Deshpande, Piyush (58695585200); Deshpande, Viraj (58696579600)","57222127908; 55212307900; 57189017229; 58695585200; 58696579600","Staircase Detection for Safe Mobility of Visually Impaired People","2023","AIP Conference Proceedings","2754","1","070018","","","","0","10.1063/5.0161151","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85176807350&doi=10.1063%2f5.0161151&partnerID=40&md5=e516182c1417bf8a53ad59b203ddcb51","Department of Electronics and Telecommunication Engineering, Vishwakarma Institute of Technology, Pune, 411037, India","Madake J., Department of Electronics and Telecommunication Engineering, Vishwakarma Institute of Technology, Pune, 411037, India; Bhatlawande S., Department of Electronics and Telecommunication Engineering, Vishwakarma Institute of Technology, Pune, 411037, India; Shilaskar S., Department of Electronics and Telecommunication Engineering, Vishwakarma Institute of Technology, Pune, 411037, India; Deshpande P., Department of Electronics and Telecommunication Engineering, Vishwakarma Institute of Technology, Pune, 411037, India; Deshpande V., Department of Electronics and Telecommunication Engineering, Vishwakarma Institute of Technology, Pune, 411037, India","Vision is extremely important to human beings, due to the fact that it perceives and interprets everything around by simply looking at it and its visual features. But some individuals have some kind of visual impairment and face many difficulties in their day-To-day life. They need assistance for navigation. This paper proposes a cane-based system to detect the presence of a staircase and alert the user. For the detection of stairs, a machine learning-based mode is developed. In this work SIFT is used for feature extraction. PCA is used for dimensionality reduction. 5 classifier algorithms are compared to classify the staircase. The classification accuracy for the classifiers is KNN 79.89%, Random Forest 95.54%, Logistic Regression 61.62%, SVM 72.29%, Decision Tree 82.99%. © 2023 American Institute of Physics Inc.. All rights reserved.","Assistive technology; human blindness.; Image Classification; object detection; SIFT","","J. Madake; Department of Electronics and Telecommunication Engineering, Vishwakarma Institute of Technology, Pune, 411037, India; email: jyoti.madake@vit.edu","Nikhare C.P.; Perveen A.; Haider J.; Purohit R.; Singh S.K.","American Institute of Physics Inc.","0094243X","","","","English","AIP Conf. Proc.","Conference paper","Final","","Scopus","2-s2.0-85176807350"
"Boussihmed A.; El Makkaoui K.; Ouahbi I.; Maleh Y.; Chetouani A.","Boussihmed, Ahmed (58882676600); El Makkaoui, Khalid (58753856500); Ouahbi, Ibrahim (57189640113); Maleh, Yassine (56780201500); Chetouani, Abdelaziz (57191631195)","58882676600; 58753856500; 57189640113; 56780201500; 57191631195","A TinyML model for sidewalk obstacle detection: aiding the blind and visually impaired people","2024","Multimedia Tools and Applications","","","","","","","0","10.1007/s11042-024-20070-9","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85202940434&doi=10.1007%2fs11042-024-20070-9&partnerID=40&md5=9b5f64aead0280456a6589dff08f0bf8","LaMAO, ORAS team, ENCG, Mohammed Premier University, Oujda, Morocco; Multidisciplinary Faculty of Nador, Mohammed Premier University, Oujda, Morocco; Laboratory LaSTI, ENSAK, Sultan Moulay Slimane University, Beni Mellal, Morocco","Boussihmed A., LaMAO, ORAS team, ENCG, Mohammed Premier University, Oujda, Morocco; El Makkaoui K., Multidisciplinary Faculty of Nador, Mohammed Premier University, Oujda, Morocco; Ouahbi I., Multidisciplinary Faculty of Nador, Mohammed Premier University, Oujda, Morocco; Maleh Y., Laboratory LaSTI, ENSAK, Sultan Moulay Slimane University, Beni Mellal, Morocco; Chetouani A., LaMAO, ORAS team, ENCG, Mohammed Premier University, Oujda, Morocco","This paper presents a pioneering study on the feasibility of implementing deep learning on resource-restricted IoT devices for real-world applications. We introduce a TinyML model configured for sidewalk obstacle detection tailored explicitly to assist those with visual impairments-a demographic often hindered by urban navigation challenges. Our investigation primarily focuses on adapting traditionally computationally intensive deep learning models to the stringent confines of IoT systems, where both memory and processing power are markedly limited. With a remarkably small footprint of just 1.93 MB and a robust mean average precision (mAP) of 50%, the proposed model achieves breakthrough outcomes, making it particularly well-suited for lightweight IoT devices. We demonstrate an exceptional inference speed of 96.2 milliseconds on a standard CPU, signifying a substantial step toward real-time processing in assistive technologies. The implications of this research are profound, emphasizing TinyML’s potential to bridge the gap between advanced machine learning capabilities and the accessibility demands of assistive devices for visually impaired individuals. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.","Blind and visually impaired people (BVIP); Computer vision (CV); Deep learning (DL); Internet of Things (IoT); Object detection; Tiny machine learning (TinyML)","Adversarial machine learning; Contrastive Learning; Deep learning; Machine vision; Pedestrian safety; Vision; Blind and visually impaired; Blind and visually impaired people; Computer vision; Deep learning; Internet of thing; Machine learning models; Machine-learning; Objects detection; Tiny machine learning; Visually impaired people; Obstacle detectors","Y. Maleh; Laboratory LaSTI, ENSAK, Sultan Moulay Slimane University, Beni Mellal, Morocco; email: y.maleh@usms.ma","","Springer","13807501","","MTAPF","","English","Multimedia Tools Appl","Article","Article in press","","Scopus","2-s2.0-85202940434"
"Bhatlawande S.; Katkalambekar V.; Singh K.; Kulkarni C.; Madake J.; Shilaskar S.","Bhatlawande, Shripad (55212307900); Katkalambekar, Varad (58695089100); Singh, Khushi (58731692600); Kulkarni, Chinmay (58395957500); Madake, Jyoti (57222127908); Shilaskar, Swati (57189017229)","55212307900; 58695089100; 58731692600; 58395957500; 57222127908; 57189017229","An Expert System for Detection of Household Furniture for Safe Mobility of Blind People","2023","AIP Conference Proceedings","2754","1","160004","","","","0","10.1063/5.0161156","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85176730124&doi=10.1063%2f5.0161156&partnerID=40&md5=54d8e3302ad8785848f56579d54d9eac","Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Pune, 411037, India","Bhatlawande S., Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Pune, 411037, India; Katkalambekar V., Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Pune, 411037, India; Singh K., Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Pune, 411037, India; Kulkarni C., Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Pune, 411037, India; Madake J., Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Pune, 411037, India; Shilaskar S., Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Pune, 411037, India","The goal of this paper is to develop an assistive device for the visually impaired that will allow them to move more freely in unfamiliar indoor situations by recognizing household objects that may hinder their route. This solution focuses on safe navigation in unfamiliar interior environments. Human-Assisted mobility and assistive aids are expensive, especially for people residing in developing countries like India. These are not affordable for the population which has a per capita GDP of 1500. This solution aims to provide cues for movement in real-Time which can be initiated by a simple obstacle detection. The basic feature of obstacle detection is to give a better experience and also be pocket-friendly. This system implements KNN, Random Forest, Decision Trees, and SVM classifiers. The proposed system analyzed that the Random Forest classifier gave the best performance accuracy for indoor obstacle detection. This system has implemented a simple guidance system that gives a tactile impulse to the user for moving in the right direction. The real-Time assistive solution demonstrated 98.3% accuracy for object detection and reliable execution on a low-power device. © 2023 American Institute of Physics Inc.. All rights reserved.","Assistive technology; Computer Vision; Image Classification; Indoor Obstacle; Vision Impairment","","S. Bhatlawande; Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Pune, 411037, India; email: Shripad.bhatlawande@vit.edu","Nikhare C.P.; Perveen A.; Haider J.; Purohit R.; Singh S.K.","American Institute of Physics Inc.","0094243X","","","","English","AIP Conf. Proc.","Conference paper","Final","","Scopus","2-s2.0-85176730124"
"Ye W.; He N.; Wang J.; Yuan X.","Ye, Weijia (57211686970); He, Ning (57211685971); Wang, Jin (57431311200); Yuan, Xiaozhongling (57221528284)","57211686970; 57211685971; 57431311200; 57221528284","A Navigation System for Guiding Blind People in Indoor and Outdoor","2023","Proceedings - 2023 International Conference on Computer Science and Automation Technology, CSAT 2023","","","","20","24","4","0","10.1109/CSAT61646.2023.00015","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85190420953&doi=10.1109%2fCSAT61646.2023.00015&partnerID=40&md5=9a63093c5127ad6ea3449d7c964cd8f3","R and D Center, The Second Research Institute of Caac, Chengdu, China","Ye W., R and D Center, The Second Research Institute of Caac, Chengdu, China; He N., R and D Center, The Second Research Institute of Caac, Chengdu, China; Wang J., R and D Center, The Second Research Institute of Caac, Chengdu, China; Yuan X., R and D Center, The Second Research Institute of Caac, Chengdu, China","the development of navigation tools for blind people has become an important focus in the field of assistive technology research. This paper presents a navigation system for guiding blind people both in indoor and outdoor circumstance, the system consists of three parts, two vibration bracelets mounted on the user's hand wrist, a wearable intelligent device wearing in user's chest, the video cameras will realize the circumstance information and design an app be loaded in user's mobile phone, with synthetic speech output guiding blind people to the determine. We design a navigation system with positioning system, location based and image processing algorithms for orienting and tracking the position of blind people in complex environment.  © 2023 IEEE.","Accessibility; Blind and visually impaired; localization; Navigation","Assistive technology; Image processing; Navigation systems; Speech synthesis; Video cameras; Accessibility; Assistive technology; Blind and visually impaired; Blind people; Intelligent devices; Localisation; Navigation tools; Speech output; Synthetic speech; Technology research; Indoor positioning systems","N. He; R and D Center, The Second Research Institute of Caac, Chengdu, China; email: hening@caacsri.com","","Institute of Electrical and Electronics Engineers Inc.","","979-835037143-7","","","English","Proc. - Int. Conf. Comput. Sci. Autom. Technol., CSAT","Conference paper","Final","","Scopus","2-s2.0-85190420953"
"Nayak A.S.; Avantika P.H.; Kiran Kumar M.S.; Sheth N.; Srinivas K.S.","Nayak, Adarsh S. (58940027500); Avantika, P.H. (58939813400); Kiran Kumar, M.S. (58940240800); Sheth, Nidhi (58939813500); Srinivas, K.S. (57219795186)","58940027500; 58939813400; 58940240800; 58939813500; 57219795186","NETRA: A Revolutionary Navigation Aid for the Visually Impaired","2024","Lecture Notes in Networks and Systems","812","","","361","372","11","0","10.1007/978-981-99-8031-4_32","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85187780281&doi=10.1007%2f978-981-99-8031-4_32&partnerID=40&md5=5356047ba8ea751e13aeae864365e6c7","PES University, Bangalore, India","Nayak A.S., PES University, Bangalore, India; Avantika P.H., PES University, Bangalore, India; Kiran Kumar M.S., PES University, Bangalore, India; Sheth N., PES University, Bangalore, India; Srinivas K.S., PES University, Bangalore, India","In India, approximately 0.36% of the total population is visually impaired or blind. These visually impaired people constantly need a guardian to help them, and use of a normal white cane restricts their mobility in unfamiliar environments. Even though there are smart canes available in the market, they are not trained to work in Indian conditions. In this research paper, we explore the use of object detection, image segmentation, and captioning in order to generate reliable audio instructions, which will help the blind person in navigation. This paper proposes a design for a smart cane that includes image sensors, ultrasonic sensors, and SOS button mounted on it, which will be working along with the smartphone camera in order to provide the live feed video of the surroundings the user is in. The YOLOv7 model, which works on the custom dataset built on the COCO dataset, helps in object detection. The paper emphasizes the way we can use deep learning algorithms to solve the problem of difficulty in mobility of the visually impaired. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.","Google API navigation assistance; Image segmentation and captioning; Image sensors; Object detection; SOS button; Ultrasonic sensors; Visual prosthetics assistance","Deep learning; Image segmentation; Object detection; Object recognition; Obstacle detectors; Ultrasonic applications; Ultrasonic sensors; Google API navigation assistance; Google+; Image captioning; Images segmentations; Navigation aids; Objects detection; Smart canes; SOS button; Visual prosthetic assistance; Visually impaired; Navigation","A.S. Nayak; PES University, Bangalore, India; email: pes1ug20cs620@pesu.pes.edu","Nagar A.K.; Jat D.S.; Mishra D.; Joshi A.","Springer Science and Business Media Deutschland GmbH","23673370","978-981998030-7","","","English","Lect. Notes Networks Syst.","Conference paper","Final","","Scopus","2-s2.0-85187780281"
"Ben Atitallah A.; Said Y.; Ben Atitallah M.A.; Albekairi M.; Kaaniche K.; Boubaker S.","Ben Atitallah, Ahmed (8576049700); Said, Yahia (53867137900); Ben Atitallah, Mohamed Amin (57217948619); Albekairi, Mohammed (57763190200); Kaaniche, Khaled (57217928562); Boubaker, Sahbi (36545771900)","8576049700; 53867137900; 57217948619; 57763190200; 57217928562; 36545771900","An effective obstacle detection system using deep learning advantages to aid blind and visually impaired navigation","2024","Ain Shams Engineering Journal","15","2","102387","","","","5","10.1016/j.asej.2023.102387","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85165285912&doi=10.1016%2fj.asej.2023.102387&partnerID=40&md5=9238179f373b6bd485297bea3c8bf1bc","Department of Electrical Engineering, College of Engineering, Jouf University, Sakaka, Saudi Arabia; Remote Sensing Unit, College of Engineering, Northern Border University, Arar, Saudi Arabia; Laboratory of Electronics and Microelectronics (LR99ES30), University of Monastir, Tunisia; Laboratory of informatics, Gaspard-Monge, A3SI, ESIEE Paris, CNRS, Gustave Eiffel University, France; LETI, ENIS, University of Sfax, Sfax, Tunisia; College of Computer Science and Engineering, University of Jeddah, Jeddah, Saudi Arabia","Ben Atitallah A., Department of Electrical Engineering, College of Engineering, Jouf University, Sakaka, Saudi Arabia; Said Y., Remote Sensing Unit, College of Engineering, Northern Border University, Arar, Saudi Arabia, Laboratory of Electronics and Microelectronics (LR99ES30), University of Monastir, Tunisia; Ben Atitallah M.A., Laboratory of informatics, Gaspard-Monge, A3SI, ESIEE Paris, CNRS, Gustave Eiffel University, France, LETI, ENIS, University of Sfax, Sfax, Tunisia; Albekairi M., Department of Electrical Engineering, College of Engineering, Jouf University, Sakaka, Saudi Arabia; Kaaniche K., Department of Electrical Engineering, College of Engineering, Jouf University, Sakaka, Saudi Arabia; Boubaker S., College of Computer Science and Engineering, University of Jeddah, Jeddah, Saudi Arabia","Blind and visually impaired people face different challenges when navigating indoors and outdoors. In this context, we suggest developing an obstacle detection system based on a modified YOLO v5 neural network architecture. The suggested system is capable of recognizing and locating a set of landmark indoor and outdoor objects that are extremely useful for Blind and Visually Impaired (BVI) navigation aids. Training and evaluation experiments were conducted using two datasets: the IODR dataset for indoor object detection and the MS COCO dataset for outdoor object detection. We used several optimization strategies, such as model width scaling, quantization, and channel pruning, to guarantee that the suggested work is implemented in embedded devices in a lightweight manner. The proposed system was successful in achieving results that were extremely competitive in terms of processing time as well as the precision of obstacle detection. © 2023 THE AUTHORS","Deep Learning; Lightweight implementation; Navigation Assistance; Obstacle detection; Visually Impaired","Deep learning; Navigation; Network architecture; Object recognition; Obstacle detectors; Blind and visually impaired; Deep learning; Lightweight implementation; Navigation assistance; Neural network architecture; Objects detection; Obstacles detection; Obstacles detection systems; Visually impaired; Visually impaired people; Object detection","A. Ben Atitallah; Department of Electrical Engineering, College of Engineering, Jouf University, Sakaka, Saudi Arabia; email: abenatitallah@ju.edu.sa","","Ain Shams University","20904479","","","","English","Ain Shams Eng. J.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85165285912"
"Abraham A.; Issac B.M.; Roy A.; Thoppil A.J.; Raj A.; Jacob D.; Mathews C.K.","Abraham, Anishamol (58909508800); Issac, Bini M. (58755225400); Roy, Allen (58909569900); Thoppil, Abhijith J (58909508900); Raj, Aswin (58909415900); Jacob, Daan (58909416000); Mathews, Christy K. (59074849500)","58909508800; 58755225400; 58909569900; 58909508900; 58909415900; 58909416000; 59074849500","Seeing Through Sound: Object Detection and Distance Analyzer for the Visually Impaired Using Audio Feedback","2023","2023 Annual International Conference on Emerging Research Areas: International Conference on Intelligent Systems, AICERA/ICIS 2023","","","","","","","0","10.1109/AICERA/ICIS59538.2023.10419988","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85186138049&doi=10.1109%2fAICERA%2fICIS59538.2023.10419988&partnerID=40&md5=dc99636f220bf588d61f0e0b0d7f3087","Amal Jyothi College of Engineering, Dept of Cse, Kottayam, India","Abraham A., Amal Jyothi College of Engineering, Dept of Cse, Kottayam, India; Issac B.M., Amal Jyothi College of Engineering, Dept of Cse, Kottayam, India; Roy A., Amal Jyothi College of Engineering, Dept of Cse, Kottayam, India; Thoppil A.J., Amal Jyothi College of Engineering, Dept of Cse, Kottayam, India; Raj A., Amal Jyothi College of Engineering, Dept of Cse, Kottayam, India; Jacob D., Amal Jyothi College of Engineering, Dept of Cse, Kottayam, India; Mathews C.K., Amal Jyothi College of Engineering, Dept of Cse, Kottayam, India","Blind Aid is a technology designed specifically for visually impaired individuals, aiming to assist them in navigating their environment. Through a combination of hardware and software, users are provided with real-time auditory information about their surroundings, including details about nearby obstacles. The hardware component of the system consists of a small and portable device equipped with sensors that track and assess the user's surroundings, along with a speaker for delivering audio output. The software component utilizes machine learning algorithms to interpret and process the sensor data, enabling users to receive concise and accurate audio descriptions of their environment. By enabling simpler and more confident navigation, Blind Aid significantly contributes to enhancing the independence and overall quality of life for people with visual impairments.  © 2023 IEEE.","electronic navigation aid; Raspberry Pi; visual aid; Visual Impairment; Visually impaired people; wearable system","Audio acoustics; Feedback; Learning algorithms; Machine learning; Wearable technology; Electronic navigation; Electronic navigation aid; Navigation aids; Objects detection; Raspberry pi; Visual aids; Visual impairment; Visually impaired; Visually impaired people; Wearable systems; Object detection","","","Institute of Electrical and Electronics Engineers Inc.","","979-835030345-2","","","English","Annu. Int. Conf. Emerg. Res. Areas: Int. Conf. Intell. Syst., AICERA/ICIS","Conference paper","Final","","Scopus","2-s2.0-85186138049"
"Kim J.T.; Yu W.; Tan J.; Turk G.; Ha S.","Kim, J. Taery (59005660900); Yu, Wenhao (57189459079); Tan, Jie (42262817800); Turk, Greg (7005234161); Ha, Sehoon (51261111200)","59005660900; 57189459079; 42262817800; 7005234161; 51261111200","How to Train Your Guide Dog:Wayfinding and Safe Navigation with Human-Robot Modeling","2023","ACM/IEEE International Conference on Human-Robot Interaction","","","","221","225","4","5","10.1145/3568294.3580076","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85150464894&doi=10.1145%2f3568294.3580076&partnerID=40&md5=3fd559a37a48d8da92e16e71e58c86a7","Georgia Institute of Technology, Atlanta, GA, United States; Google, Mountain View, CA, United States; Google, Atlanta, GA, United States","Kim J.T., Georgia Institute of Technology, Atlanta, GA, United States; Yu W., Google, Mountain View, CA, United States; Tan J., Google, Mountain View, CA, United States; Turk G., Georgia Institute of Technology, Atlanta, GA, United States; Ha S., Georgia Institute of Technology, Atlanta, GA, United States, Google, Atlanta, GA, United States","A robot guide dog has the potential to enhance the independence and quality of life of individuals who are blind or visually impaired by providing accessible, automated, and intelligent guidance. However, developing effective robot guide dogs requires researchers not only to solve robotic perception and planning problems but also to understand complicated two-way interactions of the human-robot team. This work presents the formal definition of the wayfinding task of the robotic guide dog that is grounded by common practices in the real world. Given such a task, we train an effective policy for the robot guide dog while investigating two different human models, a rotating rod model and a rigid harness model. We show that our robot can safely guide a human user to avoid several obstacles in the real world. We also demonstrate that a proper human model is necessary to achieve collision-free navigation for both the human and the robot. © 2023 IEEE Computer Society. All rights reserved.","accessibility; deep reinforcement learning; Guide dog robot; human-robot modeling; navigation; quadrupedal robot","Deep learning; Human robot interaction; Intelligent robots; Reinforcement learning; Robot programming; Accessibility; Deep reinforcement learning; Guide dog robot; Guide dogs; Human robots; Human-robot modeling; Quadrupedal robot; Reinforcement learnings; Robot guides; Robot modeling; Navigation","","","IEEE Computer Society","21672148","978-145039970-8","","","English","ACM/IEEE Int. Conf. Hum.-Rob. Interact.","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85150464894"
"Kuriakose B.; Shrestha R.; Sandnes F.E.","Kuriakose, Bineeth (57189239916); Shrestha, Raju (37016669000); Sandnes, Frode Eika (35594004500)","57189239916; 37016669000; 35594004500","DeepNAVI: A deep learning based smartphone navigation assistant for people with visual impairments","2023","Expert Systems with Applications","212","","118720","","","","20","10.1016/j.eswa.2022.118720","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85138478150&doi=10.1016%2fj.eswa.2022.118720&partnerID=40&md5=fb74afee9a9c2c5fa71c61174b9f3787","Department of Computer Science, Oslo Metropolitan University, Oslo, Norway","Kuriakose B., Department of Computer Science, Oslo Metropolitan University, Oslo, Norway; Shrestha R., Department of Computer Science, Oslo Metropolitan University, Oslo, Norway; Sandnes F.E., Department of Computer Science, Oslo Metropolitan University, Oslo, Norway","Navigation assistance is an active research area, where one aim is to foster independent living for people with vision impairments. Despite the fact that many navigation assistants use advanced technologies and methods, we found that they did not explicitly address two essential requirements in a navigation assistant - portability and convenience. It is equally imperative in designing a navigation assistant for the visually impaired that the device is portable and convenient to use without much training. Some navigation assistants do not provide users with detailed information about the obstacle types that can be detected, which is essential to make informed decisions when navigating in real-time. To address these gaps, we propose DeepNAVI, a smartphone-based navigation assistant that leverages deep learning competence. Besides providing information about the type of obstacles present, our system can also provide information about their position, distance from the user, motion status, and scene information. All this information is offered to users through audio mode without compromising portability and convenience. With a small model size and rapid inference time, our navigation assistant can be deployed on a portable device such as a smartphone and work seamlessly in a real-time environment. We conducted a pilot test with a user to assess the usefulness and practicality of the system. Our testing results indicate that our system has the potential to be a practical and useful navigation assistant for the visually impaired. © 2022 The Author(s)","Blind; Deep learning; Navigation assistant; Portable; Smartphone; Visual impairment","Deep learning; Navigation; Blind; Deep learning; Independent living; Navigation assistant; Portable; Research areas; Smart phones; Vision impairments; Visual impairment; Visually impaired; Smartphones","B. Kuriakose; Department of Computer Science, Oslo Metropolitan University, Oslo, Norway; email: bineethk@oslomet.no","","Elsevier Ltd","09574174","","ESAPE","","English","Expert Sys Appl","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85138478150"
"Lian Y.; Liu D.-E.; Ji W.-Z.","Lian, Yue (57966016800); Liu, De-er (36554558600); Ji, Wei-zhen (59285020600)","57966016800; 36554558600; 59285020600","Survey and analysis of the current status of research in the field of outdoor navigation for the blind","2024","Disability and Rehabilitation: Assistive Technology","19","4","","1657","1675","18","0","10.1080/17483107.2023.2227224","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85164469357&doi=10.1080%2f17483107.2023.2227224&partnerID=40&md5=6ce6ce61b58ceec793054668d2ceae46","School of Civil Engineering and Mapping and Engineering, Jiangxi University of Technology, Jiangxi, Ganzhou, China; State Key Laboratory of Remote Sensing Science, Beijing Normal University, Beijing, China","Lian Y., School of Civil Engineering and Mapping and Engineering, Jiangxi University of Technology, Jiangxi, Ganzhou, China; Liu D.-E., School of Civil Engineering and Mapping and Engineering, Jiangxi University of Technology, Jiangxi, Ganzhou, China; Ji W.-Z., State Key Laboratory of Remote Sensing Science, Beijing Normal University, Beijing, China","Purpose: In this article, we comprehensively review the current situation and research on technology related to outdoor travel for blind and visually impaired people (BVIP), given the diverse types and incomplete functionality of navigation aids for the blind. This aims to provide a reference for related research in the fields of outdoor travel for BVIP and blind navigation. Materials and methods: We compiled articles related to blind navigation, of which a total of 227 of them are included in the search criteria. One hundred and seventy-nine articles are selected from the initial set, from a technical point of view, to elaborate on five aspects of blind navigation: system equipment, data sources, guidance algorithms, optimization of related methods, and navigation maps. Results: The wearable form of assistive devices for the blind has the most research, followed by the handheld type of aids. The RGB data class based on vision sensor is the most common source of navigation environment information data. Object detection based on picture data is also particularly rich among navigation algorithms and associated methods, indicating that computer vision technology has become an important study content in the field of blind navigation. However, research on navigation maps is relatively less. Conclusions: In the study and development of assistive equipment for BVIP, there will be an emphasis on prioritizing attributes, such as lightness, portability, and efficiency. In light of the upcoming driverless era, the research focus will be on the development of visual sensors and computer vision technologies that can aid in navigation for the blind. © 2023 Informa UK Limited, trading as Taylor & Francis Group.","assistive devices; Blind; blind navigation; computer vision; outdoor","Algorithms; Blindness; Humans; Self-Help Devices; Spatial Navigation; Visually Impaired Persons; Wearable Electronic Devices; algorithm; blindness; human; spatial orientation; visually impaired person","Y. Lian; School of Civil Engineering and Mapping and Engineering, Jiangxi University of Technology, Ganzhou, Jiangxi, 341000, China; email: leeyelyly@qq.com","","Taylor and Francis Ltd.","17483107","","","37402242","English","Disabil. Rehabil. Assistive Technol.","Review","Final","","Scopus","2-s2.0-85164469357"
"Neo J.R.E.; Visperas C.A.; Tan M.P.H.; Tay S.S.","Neo, Jin Rui Edmund (58499241800); Visperas, Christine Alejandro (57214098905); Tan, Melvin Peng Hwee (58500559100); Tay, San San (7102586535)","58499241800; 57214098905; 58500559100; 7102586535","Novel use of robot-assisted gait rehabilitation in a patient with stroke and blindness","2023","BMJ Case Reports","16","7","e255457","","","","0","10.1136/bcr-2023-255457","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85165471912&doi=10.1136%2fbcr-2023-255457&partnerID=40&md5=192eab90ea51446450031f03a0335603","Rehabilitation Medicine, Changi General Hospital, Singapore; Rehabilitative Services, Changi General Hospital, Singapore","Neo J.R.E., Rehabilitation Medicine, Changi General Hospital, Singapore; Visperas C.A., Rehabilitation Medicine, Changi General Hospital, Singapore; Tan M.P.H., Rehabilitative Services, Changi General Hospital, Singapore; Tay S.S., Rehabilitation Medicine, Changi General Hospital, Singapore","Robot-assisted gait training (RAGT) is an effective adjunctive treatment for patients with stroke that helps to regain functional mobility and is applied in many rehabilitation units for poststroke neurorecovery. We discuss our successful attempt to apply RAGT in a patient with blindness that impeded his ability to maintain balance during gait training. He initially required two assistants to walk, but after undergoing conventional therapy with adjunctive RAGT, he improved to standby assistance for ambulation. There were also improvements in balance, activity tolerance and quality of life. Low-or-no vision states can affect the pace of recovery poststroke, but RAGT and conventional physiotherapy can possibly be combined in such patients to improve balance and motor outcomes. The Andago robot's safety features of weight support, harnessed suspension and walking mode selection supported our decision and enabled us to apply it safely for this patient.  © 2023 BMJ Publishing Group. All rights reserved.","Physiotherapy (rehabilitation); Rehabilitation medicine; Stroke","Blindness; Gait; Humans; Male; Quality of Life; Robotics; Stroke; adult; Article; balance disorder; blindness; body equilibrium; body weight; brain infarction; case report; cerebrovascular accident; clinical article; decision making; device safety; human; male; middle aged; mobilization; motor dysfunction; muscle strength; muscle weakness; neurorehabilitation; nuclear magnetic resonance imaging; paresthesia; patient safety; physical activity; physiotherapy; proprioception; quality of life; robot-assisted gait training; robotics; slurred speech; stroke patient; suspension; treatment duration; visual disorder; walking; blindness; complication; gait","S.S. Tay; Rehabilitation Medicine, Changi General Hospital, Singapore; email: taysansan@yahoo.com","","BMJ Publishing Group","1757790X","","","37479488","English","BMJ Case Rep.","Article","Final","","Scopus","2-s2.0-85165471912"
"Yu L.; Wu M.-W.; Cen G.; Xi Y.-X.; Yan X.; Qiu T.-H.","Yu, Li (58940101300); Wu, Ming-Wei (55764368700); Cen, Gang (57197618467); Xi, Yong-Xin (58939884200); Yan, Xin (58940101400); Qiu, Tian-Hao (58939884300)","58940101300; 55764368700; 57197618467; 58939884200; 58940101400; 58939884300","AI-Assisted Project-Based Learning and Exploration in the Field of Electronic Information Engineering Majors","2024","Communications in Computer and Information Science","2024 CCIS","","","278","292","14","0","10.1007/978-981-97-0791-1_24","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85187798256&doi=10.1007%2f978-981-97-0791-1_24&partnerID=40&md5=f1ab8434e95870f1ba8e9d1b506c6cbb","School of Information and Electronic Engineering, Zhejiang University of Science and Technology, Hangzhou, 310023, China","Yu L., School of Information and Electronic Engineering, Zhejiang University of Science and Technology, Hangzhou, 310023, China; Wu M.-W., School of Information and Electronic Engineering, Zhejiang University of Science and Technology, Hangzhou, 310023, China; Cen G., School of Information and Electronic Engineering, Zhejiang University of Science and Technology, Hangzhou, 310023, China; Xi Y.-X., School of Information and Electronic Engineering, Zhejiang University of Science and Technology, Hangzhou, 310023, China; Yan X., School of Information and Electronic Engineering, Zhejiang University of Science and Technology, Hangzhou, 310023, China; Qiu T.-H., School of Information and Electronic Engineering, Zhejiang University of Science and Technology, Hangzhou, 310023, China","Artificial intelligence (AI) is advancing rapidly and finding its way into various industries. Universities are placing a high priority on incorporating AI into professional education. However, for students who are not majoring in AI, applying AI technology to projects can be challenging. To foster the development of application-oriented talents in the AI era, this article presents an illustrative example of project-based learning that focuses on the use of AI tools for assisted education in the context of blind navigation vehicles. The project specifically concentrates on core AI applications, such as pedestrian detection using Raspberry Pi and line tracking for blind navigation. Voice broadcasting is employed to facilitate interaction with visually impaired individuals. The project is carried out in teams, with an emphasis on utilizing user-friendly AI tools for project-based education and practical experimentation. By integrating theory and practice, students have the opportunity to enhance their practical skills, deepen their understanding of AI, strengthen teamwork, improve communication skills, and foster innovative thinking. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.","Artificial Intelligence; Project-Based Education; Team Collaboration","E-learning; Students; Artificial intelligence technologies; Artificial intelligence tools; Blind navigation; Electronic information; Information engineerings; Professional education; Project based learning; Project-based; Project-based education; Team collaboration; Artificial intelligence","M.-W. Wu; School of Information and Electronic Engineering, Zhejiang University of Science and Technology, Hangzhou, 310023, China; email: wumingwei2004@aliyun.com","Hong W.; Kanaparan G.","Springer Science and Business Media Deutschland GmbH","18650929","978-981970790-4","","","English","Commun. Comput. Info. Sci.","Conference paper","Final","","Scopus","2-s2.0-85187798256"
"Shanmukhi K.D.; Kumar P.N.P.; Anand M.V.; Ranjith Kumar B.; Reddy V.S.","Shanmukhi, K. Durga (57781021900); Kumar, P. Naga Pavan (57782337600); Anand, M. Viswanath (57782074800); Ranjith Kumar, B. (57781022000); Reddy, V. Siva (57216142071)","57781021900; 57782337600; 57782074800; 57781022000; 57216142071","A study of assistive technologies for visually impaired","2023","AIP Conference Proceedings","2796","1","120002","","","","0","10.1063/5.0148915","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85176814973&doi=10.1063%2f5.0148915&partnerID=40&md5=3ffdf9e6528b45c8ea5e42eae9b01331","V R Siddhartha Engineering College, JNTUK, Electronics and Communication Engineering, Vijayawada, India","Shanmukhi K.D., V R Siddhartha Engineering College, JNTUK, Electronics and Communication Engineering, Vijayawada, India; Kumar P.N.P., V R Siddhartha Engineering College, JNTUK, Electronics and Communication Engineering, Vijayawada, India; Anand M.V., V R Siddhartha Engineering College, JNTUK, Electronics and Communication Engineering, Vijayawada, India; Ranjith Kumar B., V R Siddhartha Engineering College, JNTUK, Electronics and Communication Engineering, Vijayawada, India; Reddy V.S., V R Siddhartha Engineering College, JNTUK, Electronics and Communication Engineering, Vijayawada, India","From the past few years, The navigation systems are advancing with every possible technology. Researchers are do-ing there best to improve the navigation system and guidance systems. These advancement in the navigation systems with some modification can be helpful to the visually impaired people to navigate in there day to day life. This paper gives a review of the better technologies present in the society to help the visually impaired. This paper covers the use of different technologies like AI(artificial intelligence),Deep learning, IOT etc in creating useful gadgets for the visually impaired people. Traditional approaches such as: cane or guide dog, And there are gadgets such as infrared based cane or ultrasonic blind stick with are limited by the distance of identifying the objects. And there are some gadgets which are microcontroller based and microprocessor based. Some of the proposed systems uses the computer vision for object detection, character recognition, face detection etc. There are wide range of gadgets involving various technologies to help the visually impaired. The review covers various technologies used in different proposed systems for the visually impaired people and help to develop a better navigating device for visually impaired.  © 2023 Author(s).","","","K.D. Shanmukhi; V R Siddhartha Engineering College, JNTUK, Electronics and Communication Engineering, Vijayawada, India; email: shannukunapareddy@gmail.com","Rao K.V.S.; Rao P.R.K.","American Institute of Physics Inc.","0094243X","","","","English","AIP Conf. Proc.","Conference paper","Final","","Scopus","2-s2.0-85176814973"
"Krishna A.N.; Chaitra Y.L.; Bharadwaj A.M.; Abbas K.T.; Abraham A.; Prasad A.S.","Krishna, A.N. (54915169800); Chaitra, Y.L. (57345137700); Bharadwaj, Atul M. (57223142093); Abbas, K.T. (59083298800); Abraham, Allen (59004325400); Prasad, Anirudh S. (59004949700)","54915169800; 57345137700; 57223142093; 59083298800; 59004325400; 59004949700","Real-time Machine Vision System for the Visually Impaired","2024","SN Computer Science","5","4","399","","","","0","10.1007/s42979-024-02741-4","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85191438450&doi=10.1007%2fs42979-024-02741-4&partnerID=40&md5=9a49ba17557b49c328baf64a4f4c9d29","Department of CSE, SJB Institute of Technology, Affiliated to VTU, Karnataka, Bengaluru, 560060, India; Department of CSE, BNM Institute of Technology, Affiliated to VTU, Karnataka, Bengaluru, 560070, India","Krishna A.N., Department of CSE, SJB Institute of Technology, Affiliated to VTU, Karnataka, Bengaluru, 560060, India; Chaitra Y.L., Department of CSE, BNM Institute of Technology, Affiliated to VTU, Karnataka, Bengaluru, 560070, India; Bharadwaj A.M., Department of CSE, SJB Institute of Technology, Affiliated to VTU, Karnataka, Bengaluru, 560060, India; Abbas K.T., Department of CSE, SJB Institute of Technology, Affiliated to VTU, Karnataka, Bengaluru, 560060, India; Abraham A., Department of CSE, SJB Institute of Technology, Affiliated to VTU, Karnataka, Bengaluru, 560060, India; Prasad A.S., Department of CSE, SJB Institute of Technology, Affiliated to VTU, Karnataka, Bengaluru, 560060, India","This work aims to provide a robust assistive system that uses the interface of a conversational chatbot to guide people who are blind in outdoor navigation. The proposed system will also be aware of situations where people close to the user are not wearing masks. We propose and analyze the effectiveness of a sophisticated system consisting of a mask detector, common objects detector and an obstacle avoidance system. The proposed system can be entirely hosted on a secure remote server and used on any smartphone with stable internet connectivity, with all the processing handled by the server. The responses each sub-system generates are transmitted back to the client, where the user listens to the response. The proposed system outperforms to detect masks with an accuracy of 99.51% and object detection with an 80.36% success rate, respectively. © The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd 2024. corrected publication 2024.","Chatbot; Object detection; Personal assistant; Yolov4-tiny","","A.N. Krishna; Department of CSE, SJB Institute of Technology, Bengaluru, Affiliated to VTU, Karnataka, 560060, India; email: drkrishnaan@gmail.com","","Springer","2662995X","","","","English","SN COMPUT. SCI.","Article","Final","","Scopus","2-s2.0-85191438450"
"Triyono L.; Gernowo R.; Prayitno; Cholil S.R.; Hestiningsih I.; Wiktasari; Fahriah S.","Triyono, Liliek (57201666445); Gernowo, Rahmat (56433461600); Prayitno (57219541919); Cholil, Saifur Rohman (57222507550); Hestiningsih, Idhawati (58590934400); Wiktasari (58760975800); Fahriah, Sirli (57201742545)","57201666445; 56433461600; 57219541919; 57222507550; 58590934400; 58760975800; 57201742545","Advancing Accessibility: An Artificial Intelligence Framework for Obstacle Detection and Navigation Assistance for the Visually Impaired","2023","E3S Web of Conferences","448","","02042","","","","0","10.1051/e3sconf/202344802042","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85179608658&doi=10.1051%2fe3sconf%2f202344802042&partnerID=40&md5=8e46528e42c4657199680a430f1ca457","Doctoral Program of Information System School of Postgraduate Studies, Diponegoro University, Semarang, Indonesia; Department of Electrical Engineering, Politeknik Negeri Semarang, Semarang, 50275, Indonesia","Triyono L., Doctoral Program of Information System School of Postgraduate Studies, Diponegoro University, Semarang, Indonesia, Department of Electrical Engineering, Politeknik Negeri Semarang, Semarang, 50275, Indonesia; Gernowo R., Doctoral Program of Information System School of Postgraduate Studies, Diponegoro University, Semarang, Indonesia; Prayitno, Department of Electrical Engineering, Politeknik Negeri Semarang, Semarang, 50275, Indonesia; Cholil S.R., Doctoral Program of Information System School of Postgraduate Studies, Diponegoro University, Semarang, Indonesia; Hestiningsih I., Department of Electrical Engineering, Politeknik Negeri Semarang, Semarang, 50275, Indonesia; Wiktasari, Department of Electrical Engineering, Politeknik Negeri Semarang, Semarang, 50275, Indonesia; Fahriah S., Department of Electrical Engineering, Politeknik Negeri Semarang, Semarang, 50275, Indonesia","The white cane has long been a fundamental tool for individuals with visual impairments, aiding in surface detection and obstacle identification. However, its limitations in detecting moving objects and distant obstacles pose significant safety risks, particularly in congested areas and busy streets. While service animals offer an alternative, they come with training challenges and high costs. To address these limitations and enhance safety, this paper proposes a comprehensive collision detection and prevention system. The proposed system integrates cutting-edge technologies, including image processing, deep learning, Internet of Things (IoT), cloud computing, and audio production devices. By combining these technologies with the white cane, the system offers a sophisticated navigation option for the visually impaired, effectively detecting and preventing potential collisions. In busy environtment scenarios, the system proves its effectiveness by complementing the white cane's use, overcoming its inherent limitations, and significantly improving navigation capabilities. Through this innovative approach, blind individuals gain enhanced situational awareness, empowering them to navigate diverse environments with increased confidence and safety. By mitigating the drawbacks of the white cane, the proposed system provides a comprehensive and cost-effective solution to enhance the mobility and safety of the visually impaired. This research contributes to the advancement of assistive technologies, offering a valuable resource for researchers, policymakers, and practitioners in the field of accessibility and inclusive design. © The Authors, published by EDP Sciences, 2023.","","","","Isnanto R.R.; Hadiyanto null; Warsito B.","EDP Sciences","25550403","","","","English","E3S Web Conf.","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85179608658"
"Dongre S.; Tripathi A.; Tiwadi S.; Gholap D.; Tekade S.; Zanzane S.","Dongre, Shital (57215095931); Tripathi, Ankur (58778865900); Tiwadi, Shivam (58779528900); Gholap, Dnyanesh (58779200800); Tekade, Shreyash (58778692600); Zanzane, Sayee (58779369000)","57215095931; 58778865900; 58779528900; 58779200800; 58778692600; 58779369000","Smart Visual Assistance for Blind Using Deep Learning Approach","2023","AIP Conference Proceedings","2981","1","020018","","","","0","10.1063/5.0182616","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85180559237&doi=10.1063%2f5.0182616&partnerID=40&md5=8d0857e47486d161a60550ef18dfec24","Department of Information Technology Vishwakarma Institute of Technology, Maharashtra, Pune, 411037, India","Dongre S., Department of Information Technology Vishwakarma Institute of Technology, Maharashtra, Pune, 411037, India; Tripathi A., Department of Information Technology Vishwakarma Institute of Technology, Maharashtra, Pune, 411037, India; Tiwadi S., Department of Information Technology Vishwakarma Institute of Technology, Maharashtra, Pune, 411037, India; Gholap D., Department of Information Technology Vishwakarma Institute of Technology, Maharashtra, Pune, 411037, India; Tekade S., Department of Information Technology Vishwakarma Institute of Technology, Maharashtra, Pune, 411037, India; Zanzane S., Department of Information Technology Vishwakarma Institute of Technology, Maharashtra, Pune, 411037, India","Globally, there are over 253 million people who are blind. These people may find it difficult to move about without running into barriers. Because they must rely on others for navigation, blind people may feel vulnerable and dependent. Hence, there is a need for some system which will assist impaired people and will make lives easier. Numerous technologies have been applied in various fields across the globe. But none of these technologies have helped visually impaired people yet. Hence, we came up with a system for blind people using deep learning. This system includes voice assistance for blind people for understanding objects in their surroundings. This deep learning model uses the YOLO (You Only Look Once) algorithm as well as voice announcements using TTS (Text to Speech). Therefore, using this model visually impaired people could understand their surroundings without any human intervention. © 2023 American Institute of Physics Inc.. All rights reserved.","","","S. Dongre; Department of Information Technology Vishwakarma Institute of Technology, Pune, Maharashtra, 411037, India; email: shital.dongre@vit.edu","Swain D.; Hu Y-C.; Swain D.; Roy S.","American Institute of Physics Inc.","0094243X","978-073544757-8","","","English","AIP Conf. Proc.","Conference paper","Final","","Scopus","2-s2.0-85180559237"
"Peng C.; Yang D.; Zhao D.; Cheng M.; Dai J.; Jiang L.","Peng, Chunhao (58075053600); Yang, Dapeng (55717974500); Zhao, Deyu (58546776300); Cheng, Ming (57191522136); Dai, Jinghui (57282860800); Jiang, Li (59294351000)","58075053600; 55717974500; 58546776300; 57191522136; 57282860800; 59294351000","Viiat-hand: a Reach-and-grasp Restoration System Integrating Voice interaction, Computer vision, Auditory and Tactile feedback for Blind Amputees","2024","IEEE Robotics and Automation Letters","","","","1","8","7","0","10.1109/LRA.2024.3448218","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85201782412&doi=10.1109%2fLRA.2024.3448218&partnerID=40&md5=8d95d80874ef8f5b8267bb28cf20f764","State Key Laboratory of Robotics and System, Harbin Institute of Technology, Harbin, China; Artificial Intelligence Laboratory, Harbin Institute of Technology, Harbin, China","Peng C., State Key Laboratory of Robotics and System, Harbin Institute of Technology, Harbin, China; Yang D., Artificial Intelligence Laboratory, Harbin Institute of Technology, Harbin, China; Zhao D., State Key Laboratory of Robotics and System, Harbin Institute of Technology, Harbin, China; Cheng M., State Key Laboratory of Robotics and System, Harbin Institute of Technology, Harbin, China; Dai J., State Key Laboratory of Robotics and System, Harbin Institute of Technology, Harbin, China; Jiang L., State Key Laboratory of Robotics and System, Harbin Institute of Technology, Harbin, China","For blind and visually impaired (BVI) amputees, the combined loss of vision and grasping abilities turns the seemingly simple task of reaching and grasping into a significant challenge. This paper introduces a novel multi-sensory prosthesis system designed for BVI amputees to assist in perception, navigation, and grasping tasks. The system integrates voice interaction, environmental perception, grasp guidance, collaborative control, and auditory&#x002F;tactile feedback. Specifically, it processes user commands, provides environmental data via auditory&#x002F;tactile channels, and manages collaborative control of grasp gestures and wrist angles for stable object handling. The prototype, viiathand, was experimentally tested with eight able-bodied and four blind subjects performing reach-and-grasp tasks, showing that users could accurately reach (average time:15.24s) and securely grasp objects (average time:17.23s) in an indoor setting. The system also proved to be user-friendly, requiring minimal training for users to become adept IEEE","auditory aid; Cameras; Collaboration; computer vision; human-machine interaction; Navigation; prosthetic hand; Prosthetics; Task analysis; Vibrations; visual impairment; Wrist","Artificial limbs; Hearing aids; Human rehabilitation equipment; Human robot interaction; Job analysis; Sensory feedback; Vision aids; Auditory aid; Collaboration; Human machine interaction; Prosthetic hands; Reach and grasp; Task analysis; Vibration; Visual impairment; Voice interaction; Wrist; Audition","","","Institute of Electrical and Electronics Engineers Inc.","23773766","","","","English","IEEE Robot. Autom.","Article","Article in press","","Scopus","2-s2.0-85201782412"
"Jain G.; Hindi B.; Xie M.; Zhang Z.; Srinivasula K.; Ghasemi M.; Weiner D.; Xu X.Y.T.; Paris S.A.; Tedjo C.; Bassin J.; Malcolm M.; Turkcan M.; Ghaderi J.; Kostic Z.; Zussman G.; Smith B.A.","Jain, Gaurav (57193405037); Hindi, Basel (58238040800); Xie, Mingyu (58664566900); Zhang, Zihao (58554911300); Srinivasula, Koushik (58664426300); Ghasemi, Mahshid (57422831700); Weiner, Daniel (58664335200); Xu, Xin Yi Therese (58237799900); Paris, Sophie Ana (58664522400); Tedjo, Chloe (58723343900); Bassin, Josh (58722815700); Malcolm, Michael (58237800000); Turkcan, Mehmet (57210882705); Ghaderi, Javad (24773339600); Kostic, Zoran (57207510598); Zussman, Gil (14421965000); Smith, Brian A. (18038896900)","57193405037; 58238040800; 58664566900; 58554911300; 58664426300; 57422831700; 58664335200; 58237799900; 58664522400; 58723343900; 58722815700; 58237800000; 57210882705; 24773339600; 57207510598; 14421965000; 18038896900","Towards Street Camera-based Outdoor Navigation for Blind Pedestrians","2023","ASSETS 2023 - Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility","","","77","","","","0","10.1145/3597638.3614498","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85177827680&doi=10.1145%2f3597638.3614498&partnerID=40&md5=b58a134bcc92e3c2809a750b51ea9988","Columbia University, New York, NY, United States; Lehman College, New York, NY, United States; Pomona College, Claremont, CA, United States; Texas A&M University, College Station, TX, United States; The Pennsylvania State University, State College, PA, United States; SUNY At Albany, Albany, NY, United States","Jain G., Columbia University, New York, NY, United States; Hindi B., Columbia University, New York, NY, United States; Xie M., Columbia University, New York, NY, United States; Zhang Z., Columbia University, New York, NY, United States; Srinivasula K., Columbia University, New York, NY, United States; Ghasemi M., Columbia University, New York, NY, United States; Weiner D., Lehman College, New York, NY, United States; Xu X.Y.T., Pomona College, Claremont, CA, United States; Paris S.A., Columbia University, New York, NY, United States; Tedjo C., Texas A&M University, College Station, TX, United States; Bassin J., The Pennsylvania State University, State College, PA, United States; Malcolm M., SUNY At Albany, Albany, NY, United States; Turkcan M., Columbia University, New York, NY, United States; Ghaderi J., Columbia University, New York, NY, United States; Kostic Z., Columbia University, New York, NY, United States; Zussman G., Columbia University, New York, NY, United States; Smith B.A., Columbia University, New York, NY, United States","Blind and low-vision (BLV) people use GPS-based systems for outdoor navigation assistance, which provide instructions to get from one place to another. However, such systems do not provide users with real-time, precise information about their location and surroundings which is crucial for safe navigation. In this work, we investigate whether street cameras can be used to address aspects of navigation that BLV people still find challenging with existing GPS-based assistive technologies. We conducted formative interviews with six BLV participants to identify specific challenges they face in outdoor navigation. We discovered three main challenges: anticipating environment layouts, avoiding obstacles while following directions, and crossing noisy street intersections. To address these challenges, we are currently developing a street camera-based navigation system that provides real-time auditory feedback to help BLV users avoid obstacles, know exactly when to cross the street, and understand the overall layout of the environment. We close by discussing our evaluation plan.  © 2023 Owner/Author.","computer vision; outdoor navigation; street camera; testbed evaluation; Visual impairments","Cameras; Global positioning system; Real time systems; Assistive technology; Avoiding obstacle; Camera-based; Low vision; Outdoor navigation; Real- time; Safe navigations; Street camera; Testbed evaluation; Visual impairment; Computer vision","","","Association for Computing Machinery, Inc","","979-840070220-4","","","English","ASSETS - Proc. Int. ACM SIGACCESS Conf. Comput. Access.","Conference paper","Final","","Scopus","2-s2.0-85177827680"
"Bastola A.; Enam M.A.; Bastola A.; Gluck A.; Brinkley J.","Bastola, Ashish (58040351600); Enam, Md Atik (57208404225); Bastola, Ananta (58242989900); Gluck, Aaron (57219486743); Brinkley, Julian (55604856000)","58040351600; 57208404225; 58242989900; 57219486743; 55604856000","Multi-Functional Glasses for the Blind and Visually Impaired: Design and Development","2023","Proceedings of the Human Factors and Ergonomics Society","67","1","","995","1001","6","3","10.1177/21695067231192450","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85183809497&doi=10.1177%2f21695067231192450&partnerID=40&md5=728006ff7e5eac3b24ef0dc9e001b736","School of Computing, Clemson University, Clemson, SC, United States","Bastola A., School of Computing, Clemson University, Clemson, SC, United States; Enam M.A., School of Computing, Clemson University, Clemson, SC, United States; Bastola A., School of Computing, Clemson University, Clemson, SC, United States; Gluck A., School of Computing, Clemson University, Clemson, SC, United States; Brinkley J., School of Computing, Clemson University, Clemson, SC, United States","This paper presents the design of a glasses system for the blind and visually impaired that incorporates various technologies to enhance their daily living experience. The glasses are equipped with a speaker and haptic feedback, allowing users to receive audible and tactile notifications about their surroundings. We also implemented image understanding for identifying multiple unique objects, people, and environmental obstacles. Additionally, the glasses can read signs and printed materials via text recognition technology. Finally, the glasses incorporate navigation tools, helping users to find their way in unfamiliar environments. The system has the potential to significantly improve the independence and quality of life of those with visual impairments, as it could be a valuable assistive technology tool for a wide range of applications. © 2023 Human Factors and Ergonomics Society.","Accessibility; Artificial Intelligence; Autonomous Vehicle; Blind and Visually Impaired; Deep Learning; Glasses; Motion Planning; Navigation for Blind; Object Detection; Voice Recognition","Character recognition; Deep learning; Glass; Motion planning; Object detection; Object recognition; Speech recognition; Accessibility; Autonomous Vehicles; Blind and visually impaired; Deep learning; Design and Development; Glass systems; Motion-planning; Multi-functional; Navigation for blind; Objects detection; Autonomous vehicles","A. Bastola; School of Computing, Clemson University, Clemson, 810 college ave, 29634-0001, United States; email: abastol@clemson.edu","","SAGE Publications Inc.","10711813","","PHFSD","","English","Proc Hum Factors Ergon Soc","Conference paper","Final","","Scopus","2-s2.0-85183809497"
"Sugashini T.; Balakrishnan G.","Sugashini, T. (58784815200); Balakrishnan, G. (59157782500)","58784815200; 59157782500","YOLO glass: video-based smart object detection using squeeze and attention YOLO network","2024","Signal, Image and Video Processing","18","3","","2105","2115","10","3","10.1007/s11760-023-02855-x","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85180889451&doi=10.1007%2fs11760-023-02855-x&partnerID=40&md5=39c31a8f76052dcf232f4173fcc0f3e9","Department of Computer Science Engineering, Indra Ganesan College of Engineering, Trichy, 620012, India","Sugashini T., Department of Computer Science Engineering, Indra Ganesan College of Engineering, Trichy, 620012, India; Balakrishnan G., Department of Computer Science Engineering, Indra Ganesan College of Engineering, Trichy, 620012, India","Visually impairments or blindness people need guidance in order to avoid collision risks with outdoor obstacles. Recently, technology has been proving its presence in all aspects of human life, and new devices provide assistance to humans on a daily basis. However, due to real-time dynamics or a lack of specialized knowledge, object detection confronts a reliability difficulty. To overcome the challenge, YOLO Glass a Video-based Smart object detection model has been proposed for visually impaired person to navigate effectively in indoor and outdoor environments. Initially the captured video is converted into key frames and pre-processed using Correlation Fusion-based disparity approach. The pre-processed images were augmented to prevent overfitting of the trained model. The proposed method uses an obstacle detection system based on a Squeeze and Attendant Block YOLO Network model (SAB-YOLO). A proposed system assists visually impaired users in detecting multiple objects and their locations relative to their line of sight, and alerts them by providing audio messages via headphones. The system assists blind and visually impaired people in managing their daily tasks and navigating their surroundings. The experimental results show that the proposed system improves accuracy by 98.99%, proving that it can accurately identify objects. The detection accuracy of the proposed method is 5.15%, 7.15% and 9.7% better that existing YOLO v6, YOLO v5 and YOLO v3, respectively. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2023.","Deep learning; Outdoor object detection; Visually impairment; Wearable system","Deep learning; Glass; Object recognition; Obstacle detectors; Wearable technology; Collision risks; Deep learning; Human lives; New devices; Objects detection; Outdoor object detection; Real-time dynamics; Smart objects; Visually impairments; Wearable systems; Object detection","T. Sugashini; Department of Computer Science Engineering, Indra Ganesan College of Engineering, Trichy, 620012, India; email: sugashinit@gmail.com","","Springer Science and Business Media Deutschland GmbH","18631703","","","","English","Signal Image Video Process.","Article","Final","","Scopus","2-s2.0-85180889451"
"Zhang H.; Zhuang P.","Zhang, Haoyang (59034238400); Zhuang, Peidong (23986742500)","59034238400; 23986742500","A New Generation of Intelligent Aid Cane for Blindness Based on STM32","2024","Lecture Notes in Electrical Engineering","1033","","","389","397","8","0","10.1007/978-981-99-7502-0_43","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85192178312&doi=10.1007%2f978-981-99-7502-0_43&partnerID=40&md5=8880381972805095c10d7e60f98c3d9d","College of Electronics Engineering, Heilongjiang University, Heilongjiang, Harbin, 150080, China","Zhang H., College of Electronics Engineering, Heilongjiang University, Heilongjiang, Harbin, 150080, China; Zhuang P., College of Electronics Engineering, Heilongjiang University, Heilongjiang, Harbin, 150080, China","With the rapid development of urban traffic, the travel of the visually impaired is facing great challenges, and the traditional blind rod can no longer meet their needs for safe travel. At present, the development of front-end guide products is in a bottleneck period, and the application problems of traditional guide products are frequent, and there is a lack of innovation and ways to solve the problems. Learn from the popular intelligent devices in the last five years, thereby liberating the hands of the visually impaired, providing more accurate navigation services, improving the living standards of the blind, and ensuring the safety of the blind. In addition to visual impairment, blind people have the same intelligence as normal people. Normal travel is part of the independent life of disabled people, and they enjoy the same right to freedom as normal people. Providing travel freedom for the blind is conducive to equal participation in society and improving living standards. In order to solve the problem of difficult travel for the blind, we will design an intelligent guide cane, which is based on STM32 microcontroller and supported by GPS positioning module, ultrasonic ranging module, GSM module, voice module, etc., which can realize multiple functions such as real-time positioning, obstacle alarm, sending SMS, calling emergency contacts and so on. Walking stick adopts voice broadcasting and key operation to carry out man–machine interaction. The intelligent blind rod has complete functions, small size and low cost, and is easy to scale production. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.","Guide cane; STM32; The blind; Travel","Application problems; Front end; Guide cane; Learn+; Living standards; STM32; The blind; Travel; Urban traffic; Visually impaired; Global system for mobile communications","P. Zhuang; College of Electronics Engineering, Heilongjiang University, Harbin, Heilongjiang, 150080, China; email: zhuangpeidong@163.com","Wang W.; Liu X.; Na Z.; Zhang B.","Springer Science and Business Media Deutschland GmbH","18761100","978-981997555-6","","","English","Lect. Notes Electr. Eng.","Conference paper","Final","","Scopus","2-s2.0-85192178312"
"Singh S.S.; Agrawal M.; Eliazer M.","Singh, Shivang Sunil (58093990100); Agrawal, Mayank (59038859000); Eliazer, M. (57216789624)","58093990100; 59038859000; 57216789624","Collision detection and prevention for the visually impaired using computer vision and machine learning","2023","Advances in Engineering Software","179","","103424","","","","1","10.1016/j.advengsoft.2023.103424","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85147541190&doi=10.1016%2fj.advengsoft.2023.103424&partnerID=40&md5=293c07ba64ac461501f6d3046d827d20","Computer science and Engineering, SRM Institute of Science and Technology, Chennai, India","Singh S.S., Computer science and Engineering, SRM Institute of Science and Technology, Chennai, India; Agrawal M., Computer science and Engineering, SRM Institute of Science and Technology, Chennai, India; Eliazer M., Computer science and Engineering, SRM Institute of Science and Technology, Chennai, India","The white cane is the most common and widely used travel aid for the blind. Its cost is not too high and many government hospitals provide it for free to needy and poor people. It is a great tool that helps blind people detect uneven surfaces, obstacles on the ground, steps, holes, and other hazards. However, it has a major drawback in detecting obstacles that are beyond the range of the white cane, especially when the object is in motion. For example, when a blind person crosses the road, they may not be able to detect a car or person coming from either side (left and right), and it is not much help to the user when navigating through crowded places, which can lead to accidents. Service animals (such as dogs, birds, and miniature horses) are good at handling these types of situations, but they are not easy to train and are very expensive. This paper takes references from many papers and proposes a collision detection and prevention system made of integration of many technologies including image processing, cloud computing, machine learning, IoT and audio production devices, with better option for the blind people for navigating through roads and crowded places and using it along with white cane will make it more effective and will be able to cover some drawbacks of the device. © 2023","Artificial intelligence; Cloud computing; Cloud services; Computer vision; Crowd detection; IoT; Machine learning; Mobile application; Range finding","Accidents; Cloud computing; Computer vision; Internet of things; Web services; Blind people; Cloud services; Cloud-computing; Collision detection; Collisions prevention; Crowd detection; IoT; Machine-learning; Mobile applications; White cane; Machine learning","S.S. Singh; Computer science and Engineering, SRM Institute of Science and Technology, Chennai, India; email: shivangsunil.singh66@gmail.com","","Elsevier Ltd","09659978","","AESOD","","English","Adv Eng Software","Article","Final","","Scopus","2-s2.0-85147541190"
"Balakrishnan A.; Ramana K.; Ashok G.; Viriyasitavat W.; Ahmad S.; Gadekallu T.R.","Balakrishnan, Amutha (26639006000); Ramana, Kadiyala (58474933000); Ashok, Gokul (57396096800); Viriyasitavat, Wattana (24479013900); Ahmad, Sultan (57194429140); Gadekallu, Thippa Reddy (57217062630)","26639006000; 58474933000; 57396096800; 24479013900; 57194429140; 57217062630","Sonar glass—Artificial vision: Comprehensive design aspects of a synchronization protocol for vision based sensors","2023","Measurement: Journal of the International Measurement Confederation","211","","112636","","","","4","10.1016/j.measurement.2023.112636","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85150766843&doi=10.1016%2fj.measurement.2023.112636&partnerID=40&md5=070efaee30dd7d3d9eaa92724cc1e835","School of Computing, SRM Institute of Science and Technology, Tamilnadu, Chennai, 603203, India; Department of Artificial Intelligence & Data Science, Chaitanya Bharathi Institute of Technology, Telangana, Hyderabad, 500075, India; Business Information Technology Division, Department of Statistics, Faculty of Commerce and Accountancy, Chulalongkorn University, Bangkok, Thailand; Department of Computer Science, College of Computer Engineering and Sciences, Prince Sattam Bin Abdulaziz University, P.O.Box. 151, Alkharj, 11942, Saudi Arabia; Department of Information Technology, Vellore Institute of Technology, Tamilnadu, Vellore, 632014, India; Department of Electrical and Computer Engineering, Lebanese American University, Byblos, 13-5053, Lebanon","Balakrishnan A., School of Computing, SRM Institute of Science and Technology, Tamilnadu, Chennai, 603203, India; Ramana K., Department of Artificial Intelligence & Data Science, Chaitanya Bharathi Institute of Technology, Telangana, Hyderabad, 500075, India; Ashok G., School of Computing, SRM Institute of Science and Technology, Tamilnadu, Chennai, 603203, India; Viriyasitavat W., Business Information Technology Division, Department of Statistics, Faculty of Commerce and Accountancy, Chulalongkorn University, Bangkok, Thailand; Ahmad S., Department of Computer Science, College of Computer Engineering and Sciences, Prince Sattam Bin Abdulaziz University, P.O.Box. 151, Alkharj, 11942, Saudi Arabia; Gadekallu T.R., Department of Information Technology, Vellore Institute of Technology, Tamilnadu, Vellore, 632014, India, Department of Electrical and Computer Engineering, Lebanese American University, Byblos, 13-5053, Lebanon","Supporting visually impaired people during their navigation is a challenging task that involves localization, tracking, navigation, obstacle avoidance, and path guidance. Many researchers have experimented with Sonar, RFID, GPS, NFC, walking sticks, waist-based devices, and even computer vision modules for blind navigation. Using five sonar sensors for obstacle detection with direction and timestamp, we developed Sonar Glass. As humans see in left, right, front, top, and bottom directions based on eye angle and the head pose, the sonar glass is designed to provide obstacle information at the same angle. The head movement of the visually impaired person activates a pair of sensors for each module. Understanding human eye movement mechanisms and developing a synchronization protocol for each pair of visual sensors on sonar glass sitting on both eyes is the main goal of this paper. The major challenge lies in understanding and simulating the human vision mechanism and to realize how the field of Artificial Intelligence can be a contributor in producing technologies for the visually impaired. We are using log-polar transform to simulate human retinal image mapping. The Scale Invariant Feature Transform (SIFT) algorithm has also been implemented. It is the first time that both human eyes can be replaced by vision sensors. After comparing the estimated obstacle information from one sensor pair with that of the other sensors, the voice track is activated. The blind person uses the nearest obstacle information to avoid the obstacle and to extract spatial information about obstacles ahead of the user and provide an early warning. Unique in its design, the sonar Glass's synchronization protocol for a pair of related sensors provides possible object information in that direction. The executions were simulated in MATLAB and the results obtained in real time are found to be promising as the tests were carried out both indoor and outdoor. The sonar Glass design is unique of its kind and the synchronization protocol for the pair of related sensors provides the possible vision about the object information at that particular direction. © 2023 Elsevier Ltd","Artificial vision; Human eye; Object detection; Sonar glass; Synchronization protocol; Vision based sensors","Eye movements; Glass; MATLAB; Navigation; Obstacle detectors; Sonar; Synchronization; Comprehensive designs; Human eye; Localisation; Object information; Objects detection; Obstacles avoidance; Sonar glass; Synchronization protocols; Vision-based sensors; Visually impaired people; Object detection","K. Ramana; Department of Artificial Intelligence & Data Science, Chaitanya Bharathi Institute of Technology, Hyderabad, Telangana, 500075, India; email: ramana.it01@gmail.com","","Elsevier B.V.","02632241","","MSRMD","","English","Meas J Int Meas Confed","Article","Final","","Scopus","2-s2.0-85150766843"
"Vamsi T.M.N.; Shankar K.R.; Karthik G.; Puruvi M.; Chanti B.","Vamsi, T. Mohana Naga (57215412072); Shankar, K. Ravi (59170294100); Karthik, G. (59172108400); Puruvi, M. (59171144200); Chanti, B. (59170294200)","57215412072; 59170294100; 59172108400; 59171144200; 59170294200","Deep Learning–Based Surrounding Descriptor for the Visually Challenged","2024","Springer Proceedings in Mathematics and Statistics","421","","","155","163","8","0","10.1007/978-3-031-51167-7_15","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85195868254&doi=10.1007%2f978-3-031-51167-7_15&partnerID=40&md5=e338750eb085eeef3505dd675423ac0a","Department of Computer Science and Engineering, Gitam University, Andhra Pradesh, Visakhapatnam, India","Vamsi T.M.N., Department of Computer Science and Engineering, Gitam University, Andhra Pradesh, Visakhapatnam, India; Shankar K.R., Department of Computer Science and Engineering, Gitam University, Andhra Pradesh, Visakhapatnam, India; Karthik G., Department of Computer Science and Engineering, Gitam University, Andhra Pradesh, Visakhapatnam, India; Puruvi M., Department of Computer Science and Engineering, Gitam University, Andhra Pradesh, Visakhapatnam, India; Chanti B., Department of Computer Science and Engineering, Gitam University, Andhra Pradesh, Visakhapatnam, India","The proposed work presented in this chapter, “Deep learning–based surroundings descriptor for the visually challenged,” aims to make a system software that helps visually impaired or blind individuals in perceiving and understanding their surroundings. Visually challenged persons are not able to enjoy the surroundings and nature as we normally do, and this project aims at improving their quality of life. The system will capture images to detect environmental features and then provide real-world auditory feedback to the user. The goal of this project is to create a reliable, user-friendly, and assistive technology and an affordable device software that improves the visually challenged person’s life and makes them independent, allowing them to move around more safely and confidently. This project uses several deep learning and NLP technologies such as CNN and LSTM. The image inputs are taken and the features are extracted using the VGG16 model, which is an advanced version of CNN that is great at object identification and localization. In addition, we used LSTM for training the model with the extracted features and the corresponding captions. Finally, when a user gives an image input to the trained model, it predicts the caption, and then it converts the text-to-speech for the user. The Surroundings Descriptor technology aims to solve the difficulties that visually impaired people face when navigating their surroundings and to improve their experience of life. The project includes technology design and development, system testing and evaluation, and improving the model based on user feedback. Finally, the Surroundings Descriptor has the potential to significantly improve visually impaired individuals’ mobility and independence, allowing them to participate better in public life and live more fulfilling lives. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.","Assistive technology; Deep learning; LSTM; Natural language processing; Text-to-speech; VGG16","Long short-term memory; Natural language processing systems; Assistive technology; Deep learning; Descriptors; Language processing; LSTM; Natural language processing; Natural languages; Text to speech; VGG16; Visually impaired; Assistive technology","","Lin F.M.; Patel A.; Kesswani N.; Sambana B.","Springer","21941009","978-303151166-0","","","English","Springer Proc. Math. Stat.","Conference paper","Final","","Scopus","2-s2.0-85195868254"
"Moustafa A.; Konsowa A.A.; Waseem G.; Almaz N.M.; Magdy O.; Hany Y.; Fahmy O.","Moustafa, Abdulrahman (59301821400); Konsowa, Ahmed Amr (59301906400); Waseem, George (59301992600); Almaz, Nada Mohamed (59302033200); Magdy, Omar (57490877800); Hany, Youssef (59301774000); Fahmy, Omar (16315847600)","59301821400; 59301906400; 59301992600; 59302033200; 57490877800; 59301774000; 16315847600","Empowering Accessibility: A Braille Translator App with Enhanced User Experience and Integration of Machine Learning Technologies","2024","2024 International Telecommunications Conference, ITC-Egypt 2024","","","","294","299","5","0","10.1109/ITC-Egypt61547.2024.10620578","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85202342925&doi=10.1109%2fITC-Egypt61547.2024.10620578&partnerID=40&md5=5285f43e9d84155b7cdca0c6bd7483d3","Arab Academy for Science, Technology and Maritime Transport, Cairo, Egypt; Badr University in Cairo, Electrical Engineering Dept., Cairo, Egypt","Moustafa A., Arab Academy for Science, Technology and Maritime Transport, Cairo, Egypt; Konsowa A.A., Arab Academy for Science, Technology and Maritime Transport, Cairo, Egypt; Waseem G., Arab Academy for Science, Technology and Maritime Transport, Cairo, Egypt; Almaz N.M., Arab Academy for Science, Technology and Maritime Transport, Cairo, Egypt; Magdy O., Arab Academy for Science, Technology and Maritime Transport, Cairo, Egypt; Hany Y., Arab Academy for Science, Technology and Maritime Transport, Cairo, Egypt; Fahmy O., Badr University in Cairo, Electrical Engineering Dept., Cairo, Egypt","This paper proposes a Braille Translation Mobile Application. It will be designed to seamlessly translate Braille content into several languages while offering speech output functionality. It aims to enable visually impaired individuals who have not yet had the chance to learn braille to read it, as well as aid them to learn it by testing their skills using the app. Moreover, it will allow any sighted individuals in their circle to be able to read the same thing they are reading. For the backend of the application, the YOLO-v8 model will identify and locate the objects in the image using the datasets. Datasets used are 'Double-Sided Braille Image Dataset' by Li et al., 2018, 'Angelina Reader', made by Ilya Ovodov for training and testing AI models. OCR will be used to convert the images involved into data. For braille translation, GPT-4 API has been used, and Open AI's TTS API was used for speed output functionality. As for the frontend, Flutter will be used so that the application can be accessible for both Android and iOS users all around the world. The application will have a user-friendly interface that presents both the Braille image that has been sent and detected, and its translation. Lastly, the application will provide many customization options based on the preferences of the users and accessibility needs. © 2024 IEEE.","Braille-translator; Flutter app; GPT-4 API; Machine-learning; Visually-impaired; YOLO-v8","Mobility aids for blind persons; Braille translation; Braille-translator; Flutter app; GPT-4 API; Learn+; Machine-learning; User integration; Users' experiences; Visually impaired; YOLO-v8; Computer aided language translation","","","Institute of Electrical and Electronics Engineers Inc.","","979-835035140-8","","","English","Int. Telecommun. Conf., ITC-Egypt","Conference paper","Final","","Scopus","2-s2.0-85202342925"
"Wang J.; Zhao S.","Wang, Junjie (58991579600); Zhao, Shanshan (56088799400)","58991579600; 56088799400","See-Shop System: An Assistive Outdoor Navigation System for the Visual Impaired Based on Deep Learning Methods","2023","Proceedings - 2023 International Conference on Advanced Enterprise Information System, AEIS 2023","","","","16","20","4","0","10.1109/AEIS61544.2023.00010","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85190695049&doi=10.1109%2fAEIS61544.2023.00010&partnerID=40&md5=b5a12c19e51f8be0010a19826a1e4141","Xi'an Jiaotong-Liverpool University, Xjtlu, Jiangsu Province, Suzhou city, China","Wang J., Xi'an Jiaotong-Liverpool University, Xjtlu, Jiangsu Province, Suzhou city, China; Zhao S., Xi'an Jiaotong-Liverpool University, Xjtlu, Jiangsu Province, Suzhou city, China","Visually impaired and blind individuals often encounter challenges in locating stores on the street, requiring guidance to ensure their daily activities are safe and effective. The objective of this study is to develop an assisted navigation system based on real-time semantic segmentation and Chinese text recognition. The proposed system aims to assist visually impaired individuals in obtaining information about the relative location of stores. It is highly user-friendly and can provide coordinate references in unfamiliar environments, aiding them in various activities like navigation and purchasing. In this work, we utilize the BiseNet v2 network, a real-time semantic segmentation model, to identify and detect shop sign objects. Additionally, we employ PP-OCR v3, an ultra-lightweight Optical Character Recognition (OCR) network, for Chinese text recognition. Furthermore, the system incorporates a speech broadcast function to convert visual information into auditory feedback. By providing real -time support for visually impaired individuals, this assistive navigation application demonstrates great potential in promoting public welfare.  © 2023 IEEE.","Assistive Navigation; BiseNet v2; Computer Vision; morphological operations; PP-OCR v3; Segmantation; Shop sign recognition; Visually impaired and blind people","Deep learning; Learning systems; Mathematical morphology; Navigation systems; Object detection; Optical character recognition; Semantic Segmentation; Semantics; Assistive navigations; Bisenet v2; Blind people; Morphological operations; PP-optical character recognition v3; Segmantation; Shop sign recognition; Sign recognition; Visually impaired; Visually impaired people; Computer vision","S. Zhao; Xi'an Jiaotong-Liverpool University, Xjtlu, Suzhou city, Jiangsu Province, China; email: Shanshan.Zhao@xjtlu.edu.cn","","Institute of Electrical and Electronics Engineers Inc.","","979-835035926-8","","","English","Proc. - Int. Conf. Adv. Enterp. Inf. Syst., AEIS","Conference paper","Final","","Scopus","2-s2.0-85190695049"
"Harsha Vardhan Reddy P.; Kashyap B.; Naik B.C.; Pal P.; Singh S.K.; Bansod S.; Kumar Y.","Harsha Vardhan Reddy, Palakolanu (59251016700); Kashyap, Bhumireddy (59251119600); Naik, Bhukhya Charan (58345665300); Pal, Prashant (58093449500); Singh, Shashank Kumar (57209533315); Bansod, Saurabh (58093240100); Kumar, Yogesh (59277386800)","59251016700; 59251119600; 58345665300; 58093449500; 57209533315; 58093240100; 59277386800","Virtual Assistant and Navigation for the Visually Impaired Using Deep Neural Network and Image Processing","2024","Lecture Notes in Networks and Systems","1023 LNNS","","","425","440","15","0","10.1007/978-981-97-3604-1_29","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85200660648&doi=10.1007%2f978-981-97-3604-1_29&partnerID=40&md5=e3959f7a4a7d820ac6caad1b21182d7c","NIELIT Aurangabad, Aurangabad, India","Harsha Vardhan Reddy P., NIELIT Aurangabad, Aurangabad, India; Kashyap B., NIELIT Aurangabad, Aurangabad, India; Naik B.C., NIELIT Aurangabad, Aurangabad, India; Pal P., NIELIT Aurangabad, Aurangabad, India; Singh S.K., NIELIT Aurangabad, Aurangabad, India; Bansod S., NIELIT Aurangabad, Aurangabad, India; Kumar Y., NIELIT Aurangabad, Aurangabad, India","The rapid advancement of leading-edge encourages people to employ available resources to simplify everyday duties and enhance the norm and caliber of living for individuals who are unsighted. This module gives real-time aural advice to enable safe and independent navigation and advises developing a product to assist blind in navigating their environment. The gadget features an inbuilt detection of barriers module utilizing artificial intelligence to recognize impediments and give the user portable, hands-free haptic feedback. Additionally, the device has a text identification mechanism which can transform any written content it recognizes to voice which the wearer can perceive through headphones incorporated into their glasses. This module uses image processing techniques to improve accuracy and speed. Users could not have accessed printed elements like menus and signs in this manner in the past. Additionally, the system features a module for recognizing sign boards that speak text on signs. Following a comprehensive evaluation that considered factors such as speed, accuracy, and user experience, the Virtual Assistant have the capacity to greatly improve the standard of life for individuals who are unsighted. This study advances assistive technology while demonstrating the amazing power of combining state-of-the-art technologies to solve practical problems. By integrating these qualities into wearable technology, the proposed method could enhance the quality of life and freedom for individuals who are blind. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.","Haptic feedback; Navigation; Object detection; Text recognition module; Wearable","Assistive technology; Character recognition; Deep neural networks; Feedback; Image enhancement; Navigation; Wearable technology; Haptic feedbacks; Images processing; Neural-network processing; Objects detection; Text recognition; Text recognition module; Virtual assistants; Virtual navigation; Visually impaired; Wearable; Object detection","S. Bansod; NIELIT Aurangabad, Aurangabad, India; email: saurabhbansod@nielit.gov.in","Shivakumara P.; Mahanta S.; Singh Y.J.","Springer Science and Business Media Deutschland GmbH","23673370","978-981973603-4","","","English","Lect. Notes Networks Syst.","Conference paper","Final","","Scopus","2-s2.0-85200660648"
"Yu Z.; Hu M.","Yu, Zhezhou (8938987700); Hu, Minhu (59173749200)","8938987700; 59173749200","Real Environment Warning Model for Visually Impaired People in Trouble on the Blind Roads Based on Wavelet Scattering Network","2024","IEEE Access","12","","","82156","82167","11","0","10.1109/ACCESS.2024.3412328","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85196088441&doi=10.1109%2fACCESS.2024.3412328&partnerID=40&md5=7d30026de2751d0723adc0bf67421451","Guangdong Peizheng College, Guangzhou, 510830, China; Jilin University, College of Computer Science and Technology, Changchun, 130012, China","Yu Z., Guangdong Peizheng College, Guangzhou, 510830, China, Jilin University, College of Computer Science and Technology, Changchun, 130012, China; Hu M., Guangdong Peizheng College, Guangzhou, 510830, China","When the visually impaired walk on the blind road, once they encounter obstacles to block the road, it will bring panic and even safety risks to the visually impaired. To solve this problem, based on the wavelet scattering network (WSN), this study proposes a real environment warning mode for visually impaired people when they are in trouble on the blind road. When the blind road is interrupted or obstructed, visually impaired people will experience tension or anxiety. In this study, Based on electroencephalogram (EEG) signals, this study uses the WSN method to identify the emotional state of visually impaired people, and then determine whether they need assistance. The wavelet scattering coefficients of EEG signals are extracted using the WSN method, resulting in the formation of a feature matrix. Subsequently, a support vector machine is employed for the purposes of classification and recognition. The results show that the recognition accuracy of this method reaches 95.11% in the three states of normal emotional state, general nervous state, and very nervous state of the created datasets. The classification accuracy on the SEED-IV dataset is 86.14%. The WSN method suggested in this study exhibits superior recognition accuracy and the quickest algorithm execution time when compared to previous emotion identification methods. In addition, compared with the no-warning model, the warning model proposed in this study can greatly reduce the time it takes for visually impaired people to wait for help. The WSN-based warning mode provides more reliable travel assistance for visually impaired people and reduces the risk of accidental injury.  © 2013 IEEE.","blind road; early warning mode; EEG; Visually impaired people; WSN","Biomedical signal processing; Brain; Classification (of information); Electrophysiology; Emotion Recognition; Roads and streets; Wireless sensor networks; Blind road; Brain modeling; Dog; Early warning; Early warning mode; Emotion recognition; Road; Scattering networks; Visually impaired people; Warning modes; Wavelet scattering network; Electroencephalography","Z. Yu; Guangdong Peizheng College, Guangzhou, 510830, China; email: 2604163@peizheng.edu.cn","","Institute of Electrical and Electronics Engineers Inc.","21693536","","","","English","IEEE Access","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85196088441"
"Surekha P.; Pavani M.G.; Jahanavi B.; Siri B.; Singh T.","Surekha, P. (57226527671); Pavani, M. Ganga (58675630900); Jahanavi, B. (58675631000); Siri, B. (58676183900); Singh, Tilottama (57369957700)","57226527671; 58675630900; 58675631000; 58676183900; 57369957700","Voice-based e-mail for Information systems: An aid to visually impaired","2023","E3S Web of Conferences","430","","01062","","","","0","10.1051/e3sconf/202343001062","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85175457512&doi=10.1051%2fe3sconf%2f202343001062&partnerID=40&md5=b41c4aeb02d9e5c8384766b36ef25b6c","Department of Computer Science and Engineering, Gokaraju Rangaraju Institute of Engineering and Technology, Hyderabad, India; Uttaranchal Institute of Management, Uttaranchal University, Dehradun, 248007, India","Surekha P., Department of Computer Science and Engineering, Gokaraju Rangaraju Institute of Engineering and Technology, Hyderabad, India; Pavani M.G., Department of Computer Science and Engineering, Gokaraju Rangaraju Institute of Engineering and Technology, Hyderabad, India; Jahanavi B., Department of Computer Science and Engineering, Gokaraju Rangaraju Institute of Engineering and Technology, Hyderabad, India; Siri B., Department of Computer Science and Engineering, Gokaraju Rangaraju Institute of Engineering and Technology, Hyderabad, India; Singh T., Uttaranchal Institute of Management, Uttaranchal University, Dehradun, 248007, India","Artificial intelligence has the potential to revolutionize the way that visually impaired individuals interact with technology. With this in mind, a voice-based email system for visually impaired individuals has been proposed as a solution to provide a convenient and accessible way of managing email. The system leverages advanced Natural language processing (NLP)and Automatic Speech Recognition(ASR) technologies to convert spoken commands into email actions. The system supports voice commands for dictating emails, text-to-speech functionality for reading emails, and speech-to-text for writing emails, navigating the inbox, giving the count of unseen emails, and managing emails. This system is designed to be user-friendly, intuitive, and accessible, ensuring that blind individuals can easily navigate and use it with ease. Implementation of this system has the potential to greatly improve the daily lives of visually impaired individuals by providing a more convenient and accessible way to manage emails. This project presents an automation system for AI-powered voice-based email designed specifically information passing system to help blind individuals. © 2023 EDP Sciences. All rights reserved.","","","P. Surekha; Department of Computer Science and Engineering, Gokaraju Rangaraju Institute of Engineering and Technology, Hyderabad, India; email: prekha.572@gmail.com","Swadesh Kumar S.","EDP Sciences","25550403","","","","English","E3S Web Conf.","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85175457512"
"Yauri R.; Alvarez K.; Cotaquispe J.; Ynquilla J.; Llerena O.","Yauri, Ricardo (57202855362); Alvarez, Kevin (59232015500); Cotaquispe, Junior (59232232900); Ynquilla, Jordy (59231688400); Llerena, Oscar (58612228800)","57202855362; 59232015500; 59232232900; 59231688400; 58612228800","Guidance device for visually impaired people based on ultrasonic signals and open hardware","2024","International Journal of Reconfigurable and Embedded Systems","13","3","","520","527","7","0","10.11591/ijres.v13.i3.pp520-527","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85199442665&doi=10.11591%2fijres.v13.i3.pp520-527&partnerID=40&md5=1a82b985a7a1e8a032d9f5e2bf14ced4","Facultad de Ingeniería, Universidad Tecnológica del Perú, Lima, Peru; Facultad de Ingeniería Electrónica y Eléctrica, Universidad Nacional Mayor de San Marcos, Lima, Peru; Department of Computer Science and Engineering, Seoul National University of Science and Technology, Seoul, South Korea; Facultad de Ingeniería, Universidad Tecnológica del Perú, 125 Natalio Sanchez Road, Santa Beatriz, Lima, Peru","Yauri R., Facultad de Ingeniería, Universidad Tecnológica del Perú, Lima, Peru, Facultad de Ingeniería Electrónica y Eléctrica, Universidad Nacional Mayor de San Marcos, Lima, Peru, Facultad de Ingeniería, Universidad Tecnológica del Perú, 125 Natalio Sanchez Road, Santa Beatriz, Lima, Peru; Alvarez K., Facultad de Ingeniería Electrónica y Eléctrica, Universidad Nacional Mayor de San Marcos, Lima, Peru; Cotaquispe J., Facultad de Ingeniería Electrónica y Eléctrica, Universidad Nacional Mayor de San Marcos, Lima, Peru; Ynquilla J., Facultad de Ingeniería Electrónica y Eléctrica, Universidad Nacional Mayor de San Marcos, Lima, Peru; Llerena O., Department of Computer Science and Engineering, Seoul National University of Science and Technology, Seoul, South Korea","Visual impairment is a complex challenge that affects people of all ages, and it is estimated that around 2.2 billion people worldwide lack adequate access to medical treatment and support. In Latin America, there is a lack of attention to people with visual disabilities, evidenced by poor urban infrastructure and lack of compliance with inclusion laws. Some projects stand out for the use of prototypes with artificial vision technology, global positioning system (GPS) and smart canes. Therefore, the objective of the project is to use ultrasonic sensors and a low-cost electronic device coupled to canes, for obstacle detection and mobility using an open hardware embedded system. The results confirmed the efficiency in the detection and operation of the ultrasonic sensor by activating the light emitting diode (LED), the buzzer and the vibrating motor according to the programmed distances. Challenges were identified, such as adapting the sensor to the tilt of the cane and the importance of accurate calibration of the ultrasonic sensor. The system met its objectives by detecting objects in a range of 2 to 50 cm and providing sound alerts to improve the perception of blind people. © 2024, Institute of Advanced Engineering and Science. All rights reserved.","Arduino; Open hardware; Ultrasonic signals; Visual impairment; Walking stick","","R. Yauri; Facultad de Ingeniería, Universidad Tecnológica del Perú, Lima, Peru; email: ryaurir@unmsm.edu.pe","","Institute of Advanced Engineering and Science","20894864","","","","English","Int. J. Reconfigurable. Embedded. Syst.","Article","Final","","Scopus","2-s2.0-85199442665"
"Afif M.; Ayachi R.; Said Y.; Atri M.","Afif, Mouna (57194068439); Ayachi, Riadh (57210106980); Said, Yahia (53867137900); Atri, Mohamed (23017853700)","57194068439; 57210106980; 53867137900; 23017853700","An indoor scene recognition system based on deep learning evolutionary algorithms","2023","Soft Computing","27","21","","15581","15594","13","1","10.1007/s00500-023-09177-7","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85169818982&doi=10.1007%2fs00500-023-09177-7&partnerID=40&md5=56fbe2ae717fe0855ecdda50cccfae3f","Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia; Electrical Engineering Department, College of Engineering, Northern Border University, Arar, Saudi Arabia; College of Computer Science, King Khalid University, Abha, Saudi Arabia","Afif M., Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia; Ayachi R., Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia; Said Y., Electrical Engineering Department, College of Engineering, Northern Border University, Arar, Saudi Arabia; Atri M., College of Computer Science, King Khalid University, Abha, Saudi Arabia","Blind and visually impaired (BVI) face various problems during their navigation. Being unable to rely on sight greatly restricts their capacity to learn information about their surroundings. Scene recognition is crucial in improving life quality for BVI. Scene recognition systems can recognize and characterize the visual world using powerful artificial intelligence algorithms, allowing users to receive a critical overview. Indoor scene recognition systems are crucial for BVI to explore their environment. These systems are essential for increasing their independence, safety, and overall quality of life. Developing technology that allows BVI to perceive their environment and enable them to navigate and interact with the world on their own is extremely important. We propose in this paper a scene recognition system to assist BVI in their daily activities. The proposed work was developed on top of an efficient set of deep learning techniques called “Deep Evolutionary Algorithms (DEAs)”. DEAs are a type of algorithm that solves complicated search problems by combining the principles of evolutionary computing with deep learning. DEAs provide optimization techniques inspired by the process of natural selection. They iteratively develop a population of potential networks to identify optimum or near-optimal networks to solve complicated problems through genetic methods including mutation, crossover, and selection. To ensure the efficiency of the proposed work, extensive experiments have been conducted using two benchmark datasets the MIT 67 dataset and the Scene 15 dataset. New state-of-the-art results have been ensured by the proposed work in terms of recognition accuracy. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Blinds and visually impaired; Deep learning; Evolutionary algorithms; Indoor scene recognition","Deep learning; Iterative methods; Learning algorithms; Learning systems; Artificial intelligence algorithms; Blind and visually impaired; Deep learning; Indoor scene recognition; Learn+; Life qualities; Recognition systems; Safety qualities; Scene recognition; Visual world; Evolutionary algorithms","M. Afif; Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia; email: mouna.afif@outlook.fr","","Springer Science and Business Media Deutschland GmbH","14327643","","","","English","Soft Comput.","Article","Final","","Scopus","2-s2.0-85169818982"
"Saraf P.S.; Watve S.A.; Kulkarni A.R.","Saraf, Prajakta S. (58184281900); Watve, Sanika A. (58184423000); Kulkarni, Anagha R. (59258420600)","58184281900; 58184423000; 59258420600","Novel Approach for Easy Navigation based on Acoustics and Relative Senses for Visually Impaired People","2023","International Journal of Computer Information Systems and Industrial Management Applications","15","2023","","641","650","9","0","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85199785539&partnerID=40&md5=71f0aa65a8fd83c2850790ad5504090e","MKSSS’s Cummins College of Engineering for Women Pune, India","Saraf P.S., MKSSS’s Cummins College of Engineering for Women Pune, India; Watve S.A., MKSSS’s Cummins College of Engineering for Women Pune, India; Kulkarni A.R., MKSSS’s Cummins College of Engineering for Women Pune, India","People with blindness or partial-sightedness have a decreased or absolutely no ability to visualize the outside world. These people experience limitations in their mobility, productivity, and independence which increases the risk of injuries and accidents. Visually impaired people can navigate in their houses without assistance since they are familiar with their surroundings. However, navigating around new locations is the most difficult task for visually impaired people. This paper focuses on designing a novel system for the easy navigation of visually impaired people in familiar and unfamiliar surroundings. This paper enhances the features of the system by using novel techniques that in turn employ a blend of Deep Learning algorithms with simple coordinate geometry. The aim of this paper is to provide a generic, multipurpose system for the visually impaired or partially sighted that would help them locate objects in their surroundings. The object detection algorithm used in this piece of work, renders an accuracy of 99.5%. © MIR Labs, www.mirlabs.net/ijcisim/index.html","Coordinate Geometry; Deep Learning; Navigation based on Acoustics and Relative Senses; Object Detection; Visually impaired (VI)","","","","Machine Intelligence Research (MIR) Labs","21507988","","","","English","Int.  J.  Comput.  Inf.  Sys. Ind.  Manage.  Appl.","Article","Final","","Scopus","2-s2.0-85199785539"
"Abishek Kumar G.; Surya G.; Sathyadurga V.","Abishek Kumar, G. (59156919600); Surya, G. (59156386500); Sathyadurga, V. (59156128800)","59156919600; 59156386500; 59156128800","Echo Guidance: Voice-Activated Application for Blind with Smart Assistive Stick Using Machine Learning and IoT","2024","2024 International Conference on Advances in Data Engineering and Intelligent Computing Systems, ADICS 2024","","","","","","","1","10.1109/ADICS58448.2024.10533517","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85195219190&doi=10.1109%2fADICS58448.2024.10533517&partnerID=40&md5=b09ab22058695dd3f6d91c3c82c0e6cd","Hindustan Institute Of Technology And Science, Department Of Computer Science & Engineering, Chennai, India","Abishek Kumar G., Hindustan Institute Of Technology And Science, Department Of Computer Science & Engineering, Chennai, India; Surya G., Hindustan Institute Of Technology And Science, Department Of Computer Science & Engineering, Chennai, India; Sathyadurga V., Hindustan Institute Of Technology And Science, Department Of Computer Science & Engineering, Chennai, India","Navigating the world without sight presents profound challenges for millions of visually impaired individuals. Echo Guidance includes a pioneering application, addresses these obstacles by facilitating independence and mobility through voice-driven interaction and an IoT smart assistive device for obstacle detection by utilizing Ultrasonic, GPS and GSM modules powered by Arduino. This study explores the design and evaluation of Echo Guidance, highlighting its utilization of advanced algorithms like RGB888 conversion, matrix transformation, and Optical Character Recognition (OCR) for real-time object recognition. The application boasts a voice-centric interface tailored to the needs of the visually impaired, seamlessly integrating day-to-day functionalities such as weather forecasting, reminders, and a calculator accessible through voice commands. Central to Echo Guidance is its innovative object detection and recognition approach, employing the MobileNetObjDetector class powered by TensorFlow Lite for swift and accurate object detection. The OverlayView class visually represents detected objects, enhancing spatial awareness. Additionally, the integration of IoT devices, such as the smart stick with ultrasonic sensors for obstacle detection and head-level obstacle detection, coupled with SOS functionality in case of any emergency using GPS and GSM modules, further enhances Echo Guidance's capabilities. This research underscores Echo Guidance's transformative potential in improving the quality of life for visually impaired individuals, representing a significant advancement in assistive technology. Prioritizing accessibility and user experience, this paper empowers individuals worldwide with newfound independence and confidence in navigating their surroundings.  © 2024 IEEE.","GPS; GSM module; MobileNet; navigation; object recognition; TensorFlow Lite; visual impairment; voice-driven interaction","Air navigation; Global positioning system; Global system for mobile communications; Indoor positioning systems; Machine learning; Object recognition; Optical character recognition; Ultrasonic applications; Weather forecasting; Assistive; GSM module; Machine-learning; Mobilenet; Objects recognition; Obstacles detection; Tensorflow lite; Visual impairment; Visually impaired; Voice-driven interaction; Linear transformations","","","Institute of Electrical and Electronics Engineers Inc.","","979-835036482-8","","","English","Int. Conf. Adv. Data Eng. Intell. Comput. Syst., ADICS","Conference paper","Final","","Scopus","2-s2.0-85195219190"
"Ajitha Gladis K.P.; Madavarapu J.B.; Raja Kumar R.; Sugashini T.","Ajitha Gladis, K.P. (23984076600); Madavarapu, Jhansi Bharathi (58550015100); Raja Kumar, R. (55835425100); Sugashini, T. (58784815200)","23984076600; 58550015100; 55835425100; 58784815200","In-out YOLO glass: Indoor-outdoor object detection using adaptive spatial pooling squeeze and attention YOLO network","2024","Biomedical Signal Processing and Control","91","","105925","","","","3","10.1016/j.bspc.2023.105925","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85183313072&doi=10.1016%2fj.bspc.2023.105925&partnerID=40&md5=7b41bc9996f7ef3e122ef2ed98051187","Information Technology, CSI Institute of Technology, Tamil Nadu, Thovalai, 629302, India; Department of Information Technology, University of Cumberlands, 6178 College Station Drive, Williamsburg, 40769, KY, United States; Department of Mathematics, Sathyabama Institute of Science and Technology, Chennai, 119, India; Department of Computer Science Engineering, Indra Ganesan College of Engineering, Trichy, 620012, India","Ajitha Gladis K.P., Information Technology, CSI Institute of Technology, Tamil Nadu, Thovalai, 629302, India; Madavarapu J.B., Department of Information Technology, University of Cumberlands, 6178 College Station Drive, Williamsburg, 40769, KY, United States; Raja Kumar R., Department of Mathematics, Sathyabama Institute of Science and Technology, Chennai, 119, India; Sugashini T., Department of Computer Science Engineering, Indra Ganesan College of Engineering, Trichy, 620012, India","Visually impaired people encounter numerous obstacles in their daily lives, including difficulty navigating and discriminating between different environments. Therefore, smart gadgets able to ease some of these individuals' challenges are urgently needed. There are three main difficulties found in the lives of visually impaired people: identifying people, adapting to both indoor and outdoor environments, and receiving alerts about possible dangers. To overcome these challenges, the Indoor-Outdoor YOLO Glass network (In-Out YOLO) an object detection model-based video has been proposed to assist visually impaired people. The proposed method uses a novel Adaptive Spatial pyramid-based Squeeze Excitation and Attention YOLO network for detecting the object. By providing information about nearby items and enabling independent navigation, the proposed approach will benefit those who are visually impaired. For visually impaired people, the proposed approach facilitates the identification and avoidance of objects that impact their daily activities and ability to function at work, both indoors and outdoors. The tested indoor object detection system performs well, as seen by its 98.95 % accuracy rate, according to the testing data. © 2023","Audio output; Deep learning; Indoor and outdoor navigation; Object detection; Visually impaired people","Air navigation; Deep learning; Glass; Indoor positioning systems; Object recognition; Well testing; glass; Audio-output; Daily lives; Deep learning; Individual challenges; Indoor environment; Indoor navigation; Indoor/outdoor; Objects detection; Outdoor navigation; Visually impaired people; Article; assistive technology; attention; daily life activity; deep learning; diabetic retinopathy; diagnosis; functional status; glaucoma; human; image enhancement; indoor environment; partial blindness; performance indicator; signal processing; videorecording; vision; visual stimulation; visually impaired person; Object detection","T. Sugashini; Department of Computer Science Engineering, Indra Ganesan College of Engineering, Trichy, 620012, India; email: sugashini.ss123@gmail.com","","Elsevier Ltd","17468094","","","","English","Biomed. Signal Process. Control","Article","Final","","Scopus","2-s2.0-85183313072"
"Lima R.; Barreto L.; Amaral A.; Paiva S.","Lima, Rui (57543851100); Barreto, Luis (24723589900); Amaral, Antonio (56207302800); Paiva, Sara (36447905300)","57543851100; 24723589900; 56207302800; 36447905300","Visually Impaired People Positioning Assistance System Using Artificial Intelligence","2023","IEEE Sensors Journal","23","7","","7758","7765","7","1","10.1109/JSEN.2023.3244128","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85149386919&doi=10.1109%2fJSEN.2023.3244128&partnerID=40&md5=6999d384561d95471215ef7838387f49","ADiT-LAB, Instituto Politécnico de Viana Do Castelo, Viana do Castelo, 4900-347, Portugal; Instituto Superior de Engenharia Do Porto, Porto, 4249-015, Portugal","Lima R., ADiT-LAB, Instituto Politécnico de Viana Do Castelo, Viana do Castelo, 4900-347, Portugal; Barreto L., ADiT-LAB, Instituto Politécnico de Viana Do Castelo, Viana do Castelo, 4900-347, Portugal; Amaral A., Instituto Superior de Engenharia Do Porto, Porto, 4249-015, Portugal; Paiva S., ADiT-LAB, Instituto Politécnico de Viana Do Castelo, Viana do Castelo, 4900-347, Portugal","Blindness and visual impairment are commonly associated with social and functional limitations, almost 45 million people in the world have blindness, and 135 million have any visual impairment. This condition has a significant impact on the quality of life and brings many challenges to the individual, one of which is the navigation and positioning tasks. Although there are already apps capable of helping visually impaired people (VIP) for mobility purposes, most of them focus on detecting obstacles and, therefore, on avoiding dangerous situations. However, mobility of VIP involves many more tasks, such as knowing their exact position and staying informed along an entire route. For this purpose, a standalone and customizable solution is proposed that uses traditional visual recognition of landmarks to process the surroundings of the current location of the visually impaired person using a smartphone and informing about the nearby places assuring the user a sense of the site. For feature detection, it used the oriented features from accelerated segment test (FAST) and rotated binary robust-independent elementary feature (BRIEF) (ORB) algorithm, and for feature matching, it used the brute-force method with the k-nearest neighbor (KNN) algorithm. Results show that the proposed solution can analyze pictures in fractions of a second with satisfactory accuracy. © 2001-2012 IEEE.","Blindness; feature detection; feature matching; landmark detection; oriented features from accelerated segment test (FAST) and rotated binary robust-independent elementary feature (BRIEF) (ORB) algorithm; visual impairment","Eye protection; Feature extraction; Nearest neighbor search; Assistance system; Blindness; Condition; Features detections; Features matching; Landmark detection; Oriented FAST and rotated BRIEF algorithm; Quality of life; Visual impairment; Visually impaired people; Ophthalmology","R. Lima; ADiT-LAB, Instituto Politécnico de Viana Do Castelo, Viana do Castelo, 4900-347, Portugal; email: ruirochalima1@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","1530437X","","","","English","IEEE Sensors J.","Article","Final","","Scopus","2-s2.0-85149386919"
"Saleem T.; Sivakumar V.","Saleem, Talal (59207275000); Sivakumar, V. (59207083800)","59207275000; 59207083800","A Mobile Lens: Voice-Assisted Smartphone Solutions for the Sightless to Assist Indoor Object Identification","2024","EAI Endorsed Transactions on Internet of Things","10","","","","","","0","10.4108/eetiot.6450","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85197673078&doi=10.4108%2feetiot.6450&partnerID=40&md5=4454a9ab0d4678229ce197ea1e32ff02","1st Faculty of Computing and Engineering Technology, Asia Pacific University of Technology and Innovation (APU), Kuala Lumpur, Malaysia; 2nd Faculty of Computing and Engineering Technology, Asia Pacific University of Technology and Innovation (APU), Kuala Lumpur, Malaysia","Saleem T., 1st Faculty of Computing and Engineering Technology, Asia Pacific University of Technology and Innovation (APU), Kuala Lumpur, Malaysia; Sivakumar V., 2nd Faculty of Computing and Engineering Technology, Asia Pacific University of Technology and Innovation (APU), Kuala Lumpur, Malaysia","Every aspect of life is organized around sight. For visually impaired individuals, accidents often occur while walking due to collisions with people or walls. To navigate and perform daily tasks, visually impaired people typically rely on white cane sticks, assistive trained guide dogs, or volunteer individuals. However, guide dogs are expensive, making them unaffordable for many, especially since 90% of fully blind individuals live in low-income countries. Vision is crucial for participating in school, reading, walking, and working. Without it, people struggle with independent mobility and quality of life. While numerous applications are developed for the general public, there is a significant gap in mobile on-device intelligent assistance for visually challenged people. Our custom mobile deep learning model shows object classification accuracy of 99.63%. This study explores voice-assisted smartphone solutions as a cost-effective and efficient approach to enhance the independent mobility, navigation, and overall quality of life for visually impaired or blind individuals. © 2024 Saleem et al., licensed to EAI.","Artificial intelligence; Deep learning; Indoor object identification; Mobile applications for the blind; Visual impairment","","T. Saleem; 1st Faculty of Computing and Engineering Technology, Asia Pacific University of Technology and Innovation (APU), Kuala Lumpur, Malaysia; email: TP053459@mail.apu.edu.my","","European Alliance for Innovation","24141399","","","","English","EAI Endorsed Trans. Internet Things","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85197673078"
"Norena-Acosta N.; Ewulum C.; Pekala M.; Kobilarov M.; Billings S.","Norena-Acosta, Nicolas (57724620600); Ewulum, Chigozie (57546665300); Pekala, Michael (7005118704); Kobilarov, Marin (15725462500); Billings, Seth (37761152200)","57724620600; 57546665300; 7005118704; 15725462500; 37761152200","Contextually-Aware Autonomous Navigation Framework for Human Guidance","2024","Proceedings of SPIE - The International Society for Optical Engineering","13057","","130570P","","","","0","10.1117/12.3013337","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85197813635&doi=10.1117%2f12.3013337&partnerID=40&md5=08d170f427e62861b43f5a4cc58b170b","Johns Hopkins Applied Physics Laboratory, Laurel, MD, United States; Johns Hopkins University, Baltimore, MD, United States","Norena-Acosta N., Johns Hopkins Applied Physics Laboratory, Laurel, MD, United States, Johns Hopkins University, Baltimore, MD, United States; Ewulum C., Johns Hopkins Applied Physics Laboratory, Laurel, MD, United States; Pekala M., Johns Hopkins Applied Physics Laboratory, Laurel, MD, United States; Kobilarov M., Johns Hopkins University, Baltimore, MD, United States; Billings S., Johns Hopkins Applied Physics Laboratory, Laurel, MD, United States","A human-centric navigation system has been developed with a focus on supporting blind users of prosthetic vision devices by providing these users the ability to navigate their environment independently. The system maps the environment and localizes the user while incorporating context-enhanced information about the scene generated by AI-based methods. A deep learning semantic segmentation engine is utilized to process information from RGB and incorporates depth imaging sensors to produce semantic mappings of the scene. The heightened level of environmental interpretability provided by semantic mapping enables high-level human-computer interactions with the user, such as queries for guidance to specific objects or features within the environment. Unlike traditional sensor-based mapping frameworks that represent the environment as simple occupied / unoccupied space, our semantic mapping approach interprets the identity of occupied space as specific types of objects and their regional association to region types (e.g., static, movable, dynamic). The semantic segmentation also enables contextually-aware scene processing, which our framework leverages for robust ground estimation and tracking with fused depth data to distinguish above-ground obstacles. To help address the highly limited vision performance of current prosthetic vision technology, the processed depth information is used to generate augmented vision feedback for the prosthetic vision user by filtering out ground and background scene elements and highlighting near-field obstacles to aid in visual identification and avoidance of obstacles while navigating. Supplemental user feedback is provided via a directional haptic headband and voice-based notifications paired with spatial sound for path following along autonomously computed trajectories towards desired destinations. An optimized architecture enables real-time performance on a wearable embedded processing platform, which provides high-fidelity update rates for time-critical tasks such as localization and user feedback while decoupling tasks with heavy computational loads. Substantial speed-up is thereby achieved compared to the conventional baseline implementation. © 2024 SPIE.","autonomous navigation; human-computer interface for low-vision users; multimodal semantic segmentation; prosthetic vision; real-time architecture optimization; semantic SLAM","Deep learning; Feedback; Human computer interaction; Information filtering; Mapping; Navigation systems; Prosthetics; Robotics; Semantic Segmentation; Architecture optimization; Autonomous navigation; Human computer interfaces; Human-computer interface for low-vision user; Low vision; Multi-modal; Multimodal semantic segmentation; Prosthetic vision; Real-time architecture; Real-time architecture optimization; Semantic segmentation; Semantic SLAM; Semantics","N. Norena-Acosta; Johns Hopkins Applied Physics Laboratory, Laurel, United States; email: nnorena2@jh.edu","Kadar I.; Blasch E.P.; Grewe L.L.; Balaji B.; Kirubarajan T.","SPIE","0277786X","978-151067432-5","PSISD","","English","Proc SPIE Int Soc Opt Eng","Conference paper","Final","","Scopus","2-s2.0-85197813635"
"Bazhenov A.; Berman V.; Satsevich S.; Shalopanova O.; Cabrera M.A.; Lykov A.; Tsetserukou D.","Bazhenov, Artem (58664649500); Berman, Vladimir (58784522800); Satsevich, Sergei (58785044900); Shalopanova, Olga (58898118700); Cabrera, Miguel Altamirano (57202532442); Lykov, Artem (57973286300); Tsetserukou, Dzmitry (15726820800)","58664649500; 58784522800; 58785044900; 58898118700; 57202532442; 57973286300; 15726820800","DogSurf: Qadruped Robot Capable of GRU-based Surface Recognition for Blind Person Navigation","2024","ACM/IEEE International Conference on Human-Robot Interaction","","","","238","242","4","0","10.1145/3610978.3640606","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85188085716&doi=10.1145%2f3610978.3640606&partnerID=40&md5=3a0de090c6bce44d5eca8399050645c7","Skolkovo Institute of Science and Technology, Moscow, Russian Federation","Bazhenov A., Skolkovo Institute of Science and Technology, Moscow, Russian Federation; Berman V., Skolkovo Institute of Science and Technology, Moscow, Russian Federation; Satsevich S., Skolkovo Institute of Science and Technology, Moscow, Russian Federation; Shalopanova O., Skolkovo Institute of Science and Technology, Moscow, Russian Federation; Cabrera M.A., Skolkovo Institute of Science and Technology, Moscow, Russian Federation; Lykov A., Skolkovo Institute of Science and Technology, Moscow, Russian Federation; Tsetserukou D., Skolkovo Institute of Science and Technology, Moscow, Russian Federation","This paper introduces DogSurf - a new approach of using quadruped robots to help visually impaired people navigate in real world. The presented method allows the quadruped robot to detect slippery surfaces, and to use audio and haptic feedback to inform the user when to stop. A state-of-the-art GRU-based neural network architecture with mean accuracy of 99.925% was proposed for the task of multiclass surface classification for quadruped robots. A dataset was collected on a Unitree Go1 Edu robot. The dataset and code have been posted to the public domain. © 2024 Copyright held by the owner/author(s)","GRU; Guidance Robot; IMU; Quadruped Robot; Robotics; Surface Recognition; Terrain classification; Visually Impaired","Air navigation; Multipurpose robots; Blind person; GRU; Guidance robot; IMU; New approaches; Quadruped Robots; Surface recognition; Terrain classification; Visually impaired; Visually impaired people; Network architecture","","","IEEE Computer Society","21672148","979-840070323-2","","","English","ACM/IEEE Int. Conf. Hum.-Rob. Interact.","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85188085716"
"Bhatlawande S.; Hegde A.; Jain A.; Jain N.; Shilaskar S.; Madake J.","Bhatlawande, Shripad (55212307900); Hegde, Ashish (57490381800); Jain, Aryan (57562168900); Jain, Naman (57646072200); Shilaskar, Swati (57189017229); Madake, Jyoti (57222127908)","55212307900; 57490381800; 57562168900; 57646072200; 57189017229; 57222127908","Mobility aid for identification of bus and auto-rikshaw for visually challenged people","2023","AIP Conference Proceedings","2755","1","020004","","","","0","10.1063/5.0148314","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85177080937&doi=10.1063%2f5.0148314&partnerID=40&md5=d5687333fa5d67addeef519e809b9009","Department of Electronics and Telecommunication Engineering, Vishwakarma Institute of Technology, Maharashtra, Pune, 411037, India","Bhatlawande S., Department of Electronics and Telecommunication Engineering, Vishwakarma Institute of Technology, Maharashtra, Pune, 411037, India; Hegde A., Department of Electronics and Telecommunication Engineering, Vishwakarma Institute of Technology, Maharashtra, Pune, 411037, India; Jain A., Department of Electronics and Telecommunication Engineering, Vishwakarma Institute of Technology, Maharashtra, Pune, 411037, India; Jain N., Department of Electronics and Telecommunication Engineering, Vishwakarma Institute of Technology, Maharashtra, Pune, 411037, India; Shilaskar S., Department of Electronics and Telecommunication Engineering, Vishwakarma Institute of Technology, Maharashtra, Pune, 411037, India; Madake J., Department of Electronics and Telecommunication Engineering, Vishwakarma Institute of Technology, Maharashtra, Pune, 411037, India","In this paper a machine learning based solution for detection of public transport the visually impaired people have been proposed. The proposed solution helps visually challenged people to detect cabs and buses. One of the major challenges faced by individuals with complete loss of vision is to navigate around places. Lack of awareness about the public transport facilities available for them in their vicinity is a challenge for the visually impaired. Existing assistive aids are not directed towards solving this problem. The solution proposed in this paper employs five machine learning classifiers namely Decision tree, Random Forest, SVM, Gaussian Naïve Bayes and KNN which are trained on a custom dataset. SIFT is employed for feature detection and PCA is used for dimensionality reduction. The trained model precisely detects cabs and buses, and then provides audio and haptic feedback to the user. The distinct points to note about the solution are the low power requirements and reduced latency both of which are essential for its purpose. In this approach maximum accuracy achieved is 98.1% with Random Forest classifier model.  © 2023 Author(s).","Artificial Intelligence; blind navigation; Computer Vision; Machine Learning; object detection; outdoor scene detection; public transport.; SIFT; visually impaired","","S. Bhatlawande; Department of Electronics and Telecommunication Engineering, Vishwakarma Institute of Technology, Pune, Maharashtra, 411037, India; email: shripad.bhatlawande@vit.edu","Mishra S.S.; Parasar D.; Agarwala R.","American Institute of Physics Inc.","0094243X","","","","English","AIP Conf. Proc.","Conference paper","Final","","Scopus","2-s2.0-85177080937"
"García-Lázaro H.G.; Teng S.","García-Lázaro, Haydée G. (55552974100); Teng, Santani (36810972700)","55552974100; 36810972700","Sensory and Perceptual Decisional Processes Underlying the Perception of Reverberant Auditory Environments","2024","eNeuro","11","8","","","","","0","10.1523/ENEURO.0122-24.2024","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85201681201&doi=10.1523%2fENEURO.0122-24.2024&partnerID=40&md5=f441c53d92881140cb69e5cec85a94cf","Smith-Kettlewell Eye Research Institute, San Francisco, 94115, CA, United States","García-Lázaro H.G., Smith-Kettlewell Eye Research Institute, San Francisco, 94115, CA, United States; Teng S., Smith-Kettlewell Eye Research Institute, San Francisco, 94115, CA, United States","Reverberation, a ubiquitous feature of real-world acoustic environments, exhibits statistical regularities that human listeners leverage to self-orient, facilitate auditory perception, and understand their environment. Despite the extensive research on sound source representation in the auditory system, it remains unclear how the brain represents real-world reverberant environments. Here, we characterized the neural response to reverberation of varying realism by applying multivariate pattern analysis to electroencephalographic (EEG) brain signals. Human listeners (12 males and 8 females) heard speech samples convolved with real-world and synthetic reverberant impulse responses and judged whether the speech samples were in a “real” or “fake” environment, focusing on the reverberant background rather than the properties of speech itself. Participants distinguished real from synthetic reverberation with ∼75% accuracy; EEG decoding reveals a multistage decoding time course, with dissociable components early in the stimulus presentation and later in the perioffset stage. The early component predominantly occurred in temporal electrode clusters, while the later component was prominent in centroparietal clusters. These findings suggest distinct neural stages in perceiving natural acoustic environments, likely reflecting sensory encoding and higher-level perceptual decision-making processes. Overall, our findings provide evidence that reverberation, rather than being largely suppressed as a noise-like signal, carries relevant environmental information and gains representation along the auditory system. This understanding also offers various applications; it provides insights for including reverberation as a cue to aid navigation for blind and visually impaired people. It also helps to enhance realism perception in immersive virtual reality settings, gaming, music, and film production. © 2024 García-Lázaro and Teng.","auditory perception; EEG; MVPA; natural acoustic environments; reverberation","Acoustic Stimulation; Adult; Auditory Perception; Brain; Decision Making; Electroencephalography; Environment; Female; Humans; Male; Speech Perception; Young Adult; accuracy; adult; Article; auditory evoked potential; auditory stimulation; clinical article; computer model; controlled study; cross validation; decision making; electroencephalography; environmental noise; female; hearing; human; human experiment; male; motor evoked potential; nerve potential; normal human; signal noise ratio; spatiotemporal analysis; speech perception; stimulus response; support vector machine; task performance; brain; decision making; environment; physiology; speech perception; young adult","H.G. García-Lázaro; Smith-Kettlewell Eye Research Institute, San Francisco, 94115, United States; email: haydee@ski.org; S. Teng; Smith-Kettlewell Eye Research Institute, San Francisco, 94115, United States; email: santani@ski.org","","Society for Neuroscience","23732822","","","39122554","English","eNeuro","Article","Final","","Scopus","2-s2.0-85201681201"
"Pundlik S.; Shivshanker P.; Traut-Savino T.; Luo G.","Pundlik, Shrinivas (8728311600); Shivshanker, Prerana (57218676759); Traut-Savino, Tim (58631949600); Luo, Gang (36499782000)","8728311600; 57218676759; 58631949600; 36499782000","Field Evaluation of a Mobile App for Assisting Blind and Visually Impaired Travelers to Find Bus Stops","2024","Translational Vision Science and Technology","13","1","11","","","","0","10.1167/tvst.13.1.11","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85182547778&doi=10.1167%2ftvst.13.1.11&partnerID=40&md5=dee6d9ca37de999a71b4ed1663c0a691","Schepens Eye Research Institute of Mass Eye & Ear, Harvard Medical School, Boston, MA, United States; The Carroll Center for the Blind, Newton, MA, United States","Pundlik S., Schepens Eye Research Institute of Mass Eye & Ear, Harvard Medical School, Boston, MA, United States; Shivshanker P., Schepens Eye Research Institute of Mass Eye & Ear, Harvard Medical School, Boston, MA, United States; Traut-Savino T., The Carroll Center for the Blind, Newton, MA, United States; Luo G., Schepens Eye Research Institute of Mass Eye & Ear, Harvard Medical School, Boston, MA, United States","Purpose: GPS location-based navigation apps are insufficient to aid blind and visually impaired (BVI) travelers for micro-navigation tasks, such as finding the exact location of bus stops. The resulting large gaps could lead to BVI travelers missing their bus. We evaluated the ability of a signage detection mobile app, All_Aboard, to guide BVI travelers precisely to the bus stops compared to Google Maps alone. Methods: The All_Aboard app detected bus stop signs in real-time via smartphone camera using a deep neural network model, and provided distance coded audio feedback to help localize the detected sign. BVI individuals used the All_Aboard and Google Maps app to localize 10 bus stops each in downtown and suburban Boston, Massachusetts. For each bus stop, the subjects used both apps to navigate as close as possible to the physical bus stop sign, starting from 30 to 50 meters away. The outcome measures were success rate and gap distance between the app-indicated location and the actual physical location of the bus stop. Results: The study involved 24 legally blind participants (mean age [SD] = 51 [14] years; 11 [46%] women). The success rate of the All_Aboard app (91%) was significantly higher than the Google Maps (52%, P < 0.001). The gap distance when using the All_Aboard app was significantly lower (mean = 1.8, 95% confidence interval [CI] = 1.2–2.3 meters) compared to the Google Maps alone (mean = 7, 95% CI = 6.5–7.5 meters, P < 0.001). Conclusions: All_Aboard micro-navigation app guided BVI travelers to bus stops more accurately and reliably than a location-based macro-navigation app alone. Translational Relevance: The All_Aboard app together with a macro-navigation app can potentially help BVI individuals independently access public transportation. © 2024 The Authors.","","Adolescent; Blindness; Female; Humans; Male; Mobile Applications; Neural Networks, Computer; adult; Article; blindness; controlled study; deep neural network; female; health care personnel; hospitalization; human; male; middle aged; outcome assessment; sensory feedback; visual field; visual impairment; visually impaired person; adolescent; artificial neural network; blindness","S. Pundlik; Schepens Eye Research Institute of Mass Eye & Ear, Boston, 20 Staniford St., 02114, United States; email: shrinivas_pundlik@meei.harvard.edu","","Association for Research in Vision and Ophthalmology Inc.","21642591","","","38224330","English","Translational Vis. Sci. Technol.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85182547778"
"Hong B.; Guo Y.; Chen M.; Nie Y.; Feng C.; Li F.","Hong, Bin (56688092700); Guo, Yihang (58927224600); Chen, Meimei (58926792900); Nie, Yahui (54684636100); Feng, Changyuan (58946538700); Li, Fugeng (58946292300)","56688092700; 58927224600; 58926792900; 54684636100; 58946538700; 58946292300","Collaborative route map and navigation of the guide dog robot based on optimum energy consumption","2024","AI and Society","","","","","","","0","10.1007/s00146-024-01879-2","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85188184834&doi=10.1007%2fs00146-024-01879-2&partnerID=40&md5=13a0cc36d3ec0f2675ceefc81a39a60c","Tianjin University, Tianjin, China; Tianjin University of Technology and Education, Tianjin, China; Tianjin Tianbo Science & amp; Technology Co., Ltd., Tianjin, China; Internal Combustion Engine Research Institute, Tianjin University, Tianjin, China","Hong B., Tianjin University, Tianjin, China, Tianjin Tianbo Science & amp; Technology Co., Ltd., Tianjin, China, Internal Combustion Engine Research Institute, Tianjin University, Tianjin, China; Guo Y., Tianjin University, Tianjin, China, Internal Combustion Engine Research Institute, Tianjin University, Tianjin, China; Chen M., Tianjin University, Tianjin, China, Internal Combustion Engine Research Institute, Tianjin University, Tianjin, China; Nie Y., Tianjin University of Technology and Education, Tianjin, China, Internal Combustion Engine Research Institute, Tianjin University, Tianjin, China; Feng C., Tianjin University, Tianjin, China, Internal Combustion Engine Research Institute, Tianjin University, Tianjin, China; Li F., Tianjin University, Tianjin, China, Internal Combustion Engine Research Institute, Tianjin University, Tianjin, China","The guide dog robot (GDR) is a low-speed companion robot that serves visually impaired people and is used to guide blind people to walk steadily, carrying a variety of intelligent technologies and needing to have the ability to guide with optimal energy consumption in specific scenarios. This paper proposes an innovative technique for virtual-real collaborative path planning and navigation of the GDR specific indoor scenarios, and designs an experimental method for virtual-real collaborative path planning of the GDR specific scenarios. The energy consumption integral equation is used to solve for the energy consumption of the GDR with virtual-real synergy, and the difference in energy consumption is compared for three different navigation directions: horizontal, vertical and oblique obstacle avoidance. The results show that the optimized GDR saves 6.91% in rectilinear movement and 10.60% in curved movement. The efficiency of planning and navigating the GDR in specific domestic scenarios is verified by a virtual-real cooperative. The realization of optimal path planning for energy consumption is instrumental in exploring many of the most significant thought in the path planning and navigation of mobile robots in indoor specific scenarios. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2024.","Energy optimal; Guide Dog Robot (GDR); Path planning; Virtual-real collaboration","Indoor positioning systems; Integral equations; Intelligent robots; Mobile robots; Motion planning; Navigation; Robot programming; Energy; Energy optimal; Energy-consumption; Guide dog robot; Guide dogs; Low speed; Optimum energy; Route map; Route navigation; Virtual-real collaboration; Energy utilization","B. Hong; Tianjin University, Tianjin, China; email: hongbin@tju.edu.cn","","Springer Science and Business Media Deutschland GmbH","09515666","","","","English","AI Soc.","Article","Article in press","","Scopus","2-s2.0-85188184834"
"Figueroa-Hernandez A.G.; Perdomo-Vasquez C.; Gomez-Escobar D.; Galvis-Pedraza H.; Medina-Castañeda J.F.; González-Vargas A.M.","Figueroa-Hernandez, Angie Giovanna (58799979100); Perdomo-Vasquez, Camila (58799906300); Gomez-Escobar, Dayani (58799681300); Galvis-Pedraza, Hardy (58800050200); Medina-Castañeda, Juan Fernando (58799757400); González-Vargas, Andrés Mauricio (57151254000)","58799979100; 58799906300; 58799681300; 58800050200; 58799757400; 57151254000","Haptic Interface for Remote Guidance of People with Visual Disabilities","2024","IFMBE Proceedings","100","","","681","689","8","0","10.1007/978-3-031-49407-9_67","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85181776301&doi=10.1007%2f978-3-031-49407-9_67&partnerID=40&md5=944d7432448600a2b864ca2c23a5cc0a","Universidad Autónoma de Occidente, Cali, Colombia","Figueroa-Hernandez A.G., Universidad Autónoma de Occidente, Cali, Colombia; Perdomo-Vasquez C., Universidad Autónoma de Occidente, Cali, Colombia; Gomez-Escobar D., Universidad Autónoma de Occidente, Cali, Colombia; Galvis-Pedraza H., Universidad Autónoma de Occidente, Cali, Colombia; Medina-Castañeda J.F., Universidad Autónoma de Occidente, Cali, Colombia; González-Vargas A.M., Universidad Autónoma de Occidente, Cali, Colombia","Visually impaired people experience difficulties in many daily life activities. One of them is navigation through unknown environments. Assistive alternatives such as white canes, guide dogs or guide persons are commonly available. There are also new developments on high-end automated technologies that make use of computer vision, GPS and different sensors in order to provide navigation indications for blind users. These automated technologies, however, are often expensive, and the access to guide dogs or persons is not always easy. Therefore, in this work we present the design and prototyping of a haptic interface for remote guidance of visually impaired people. With this device, any person (such as a relative or a guidance service) can help the user to move around unfamiliar environments by using the camera on the user’s smartphone and transmitting tactile indications via a wearable vibration device, in order to not interfere with the users’ hearing and provide a safe and comfortable experience. © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Assistive device; Blind; Haptic; Remote guidance; Visual impairments","Assistive technology; Audition; Assistive devices; Automated technology; Blind; Guide dogs; Haptics; Haptics interfaces; Remote guidance; Visual disability; Visual impairment; Visually impaired people; Haptic interfaces","A.G. Figueroa-Hernandez; Universidad Autónoma de Occidente, Cali, Colombia; email: amgonzalezv@uao.edu.co","Marques J.L.B.; Rodrigues C.R.; Suzuki D.O.H.; García Ojeda R.; Marino Neto J.","Springer Science and Business Media Deutschland GmbH","16800737","978-303149406-2","","","English","IFMBE Proc.","Conference paper","Final","","Scopus","2-s2.0-85181776301"
"Hamilton-Fletcher G.; Liu M.; Sheng D.; Feng C.; Hudson T.E.; Rizzo J.-R.; Chan K.C.","Hamilton-Fletcher, Giles (56039687500); Liu, Mingxin (58854022000); Sheng, Diwei (57318227500); Feng, Chen (51461205400); Hudson, Todd E. (7203069715); Rizzo, John-Ross (56527876700); Chan, Kevin C. (34968940300)","56039687500; 58854022000; 57318227500; 51461205400; 7203069715; 56527876700; 34968940300","Accuracy and Usability of Smartphone-Based Distance Estimation Approaches for Visual Assistive Technology Development","2024","IEEE Open Journal of Engineering in Medicine and Biology","5","","","54","58","4","0","10.1109/OJEMB.2024.3358562","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85183662391&doi=10.1109%2fOJEMB.2024.3358562&partnerID=40&md5=093ad8218469f52dcd134595b574c5ef","New York University, Department of Rehabilitative Medicine, NYU Grossman School of Medicine, NYU Langone Health, New York, 10017, NY, United States; New York University Tandon School of Engineering, Department of Civil and Urban Engineering, Department of Mechanical and Aerospace Engineering, Brooklyn, 11201, NY, United States; New York University, Department of Ophthalmology, NYU Grossman School of Medicine, NYU Langone Health, New York, 10017, NY, United States; New York University, Department of Biomedical Engineering, Tandon School of Engineering, New York, 11201, NY, United States; New York University, Department of Radiology, NYU Grossman School of Medicine, NYU Langone Health, New York, 10017, NY, United States","Hamilton-Fletcher G., New York University, Department of Rehabilitative Medicine, NYU Grossman School of Medicine, NYU Langone Health, New York, 10017, NY, United States, New York University, Department of Ophthalmology, NYU Grossman School of Medicine, NYU Langone Health, New York, 10017, NY, United States; Liu M., New York University, Department of Ophthalmology, NYU Grossman School of Medicine, NYU Langone Health, New York, 10017, NY, United States; Sheng D., New York University Tandon School of Engineering, Department of Civil and Urban Engineering, Department of Mechanical and Aerospace Engineering, Brooklyn, 11201, NY, United States; Feng C., New York University Tandon School of Engineering, Department of Civil and Urban Engineering, Department of Mechanical and Aerospace Engineering, Brooklyn, 11201, NY, United States; Hudson T.E., New York University, Department of Rehabilitative Medicine, NYU Grossman School of Medicine, NYU Langone Health, New York, 10017, NY, United States; Rizzo J.-R., New York University, Department of Rehabilitative Medicine, NYU Grossman School of Medicine, NYU Langone Health, New York, 10017, NY, United States, New York University, Department of Biomedical Engineering, Tandon School of Engineering, New York, 11201, NY, United States; Chan K.C., New York University, Department of Ophthalmology, NYU Grossman School of Medicine, NYU Langone Health, New York, 10017, NY, United States, New York University, Department of Biomedical Engineering, Tandon School of Engineering, New York, 11201, NY, United States, New York University, Department of Radiology, NYU Grossman School of Medicine, NYU Langone Health, New York, 10017, NY, United States","Goal: Distance information is highly requested in assistive smartphone Apps by people who are blind or low vision (PBLV). However, current techniques have not been evaluated systematically for accuracy and usability. Methods: We tested five smartphone-based distance-estimation approaches in the image center and periphery at 1-3 meters, including machine learning (CoreML), infrared grid distortion (IR_self), light detection and ranging (LiDAR_back), and augmented reality room-tracking on the front (ARKit_self) and back-facing cameras (ARKit_back). Results: For accuracy in the image center, all approaches had <±2.5 cm average error, except CoreML which had ±5.2-6.2 cm average error at 2-3 meters. In the periphery, all approaches were more inaccurate, with CoreML and IR_self having the highest average errors at ±41 cm and ±32 cm respectively. For usability, CoreML fared favorably with the lowest central processing unit usage, second lowest battery usage, highest field-of-view, and no specialized sensor requirements. Conclusions: We provide key information that helps design reliable smartphone-based visual assistive technologies to enhance the functionality of PBLV.  © 2020 IEEE.","Assistive technology; blindness; low vision; navigation; sensory substitution","Augmented reality; Errors; Learning systems; Navigation; Optical radar; Program processors; Smartphones; Usability engineering; Assistive technology; Average errors; Blindness; Distance estimation; Estimation approaches; Low vision; Sensory substitution; Smart phones; Usability; Cameras","K.C. Chan; New York University, Department of Ophthalmology, NYU Grossman School of Medicine, NYU Langone Health, New York, 10017, United States; email: chuenwing.chan@fulbrightmail.org","","Institute of Electrical and Electronics Engineers Inc.","26441276","","","","English","IEEE open J. Eng. Med. Biol.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85183662391"
"Schieber H.; Kleinbeck C.; Theelke L.; Kraft M.; Kreimeier J.; Roth D.","Schieber, Hannah (57670292300); Kleinbeck, Constantin (57200214229); Theelke, Luisa (57211004720); Kraft, Miriam (58928422600); Kreimeier, Julian (57202949210); Roth, Daniel (56385013000)","57670292300; 57200214229; 57211004720; 58928422600; 57202949210; 56385013000","MR-Sense: A Mixed Reality Environment Search Assistant for Blind and Visually Impaired People","2024","Proceedings - 2024 IEEE International Conference on Artificial Intelligence and eXtended and Virtual Reality, AIxVR 2024","","","","166","175","9","0","10.1109/AIxVR59861.2024.00029","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85187220404&doi=10.1109%2fAIxVR59861.2024.00029&partnerID=40&md5=46dc40577cf9b8d42b764e952706a3c6","Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany; Technical University of Munich, School of Medicine and Health Klinikum Rechts der Isar, Munich, Germany","Schieber H., Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany, Technical University of Munich, School of Medicine and Health Klinikum Rechts der Isar, Munich, Germany; Kleinbeck C., Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany, Technical University of Munich, School of Medicine and Health Klinikum Rechts der Isar, Munich, Germany; Theelke L., Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany, Technical University of Munich, School of Medicine and Health Klinikum Rechts der Isar, Munich, Germany; Kraft M., Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany; Kreimeier J., Technical University of Munich, School of Medicine and Health Klinikum Rechts der Isar, Munich, Germany; Roth D., Technical University of Munich, School of Medicine and Health Klinikum Rechts der Isar, Munich, Germany","Search tasks can be challenging for blind or visually impaired people. To determine an object's location and to navigate there, they often rely on the limited sensory capabilities of a white cane, search haptically, or ask for help. We introduce MR-Sense, a mixed reality assistant to support search and navigation tasks. The system is designed in a participatory fashion and utilizes sensory data of a standalone mixed reality head-mounted display to perform deep learning-driven object recognition and environment mapping. The user is supported in object search tasks via spatially mapped audio and vibrotactile feedback. We conducted a preliminary user study including ten blind or visually impaired participants and a final user evaluation with thirteen blind or visually impaired participants. The final study reveals that MR-Sense alone cannot replace the cane but provides a valuable addition in terms of usability and task load. We further propose a standardized evaluation setup for replicable studies and highlight relevant potentials and challenges fostering future work towards employing technology in accessibility. © 2024 IEEE.","accessibility; blind and visually impaired people; mixed reality","Deep learning; Helmet mounted displays; Object recognition; Accessibility; Blind and visually impaired; Blind and visually impaired people; Mixed reality; Mixed-reality environment; Object location; Search tasks; Visually impaired; Visually impaired people; White cane; Mixed reality","","","Institute of Electrical and Electronics Engineers Inc.","","979-835037202-1","","","English","Proc. - IEEE Int. Conf. Artif. Intell. Ext. Virtual Real., AIxVR","Conference paper","Final","","Scopus","2-s2.0-85187220404"
"Rani T.P.; Sakthy S.S.; Kalaichelvi P.; Vignesh T.; Priyadharshan M.","Rani, T.P. (55940979900); Sakthy, S. Susila (57923623100); Kalaichelvi, P. (57213919361); Vignesh, T. (36464321600); Priyadharshan, M. (58127000200)","55940979900; 57923623100; 57213919361; 36464321600; 58127000200","Visual Information Translator Using Smart Glasses for Blind","2023","2023 Intelligent Computing and Control for Engineering and Business Systems, ICCEBS 2023","","","","","","","1","10.1109/ICCEBS58601.2023.10449230","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85189153263&doi=10.1109%2fICCEBS58601.2023.10449230&partnerID=40&md5=763952db07d78843be0d6e040d56f27f","Sri Sairam Engineering College, Dept of Information Technology, Chennai, India","Rani T.P., Sri Sairam Engineering College, Dept of Information Technology, Chennai, India; Sakthy S.S., Sri Sairam Engineering College, Dept of Information Technology, Chennai, India; Kalaichelvi P., Sri Sairam Engineering College, Dept of Information Technology, Chennai, India; Vignesh T., Sri Sairam Engineering College, Dept of Information Technology, Chennai, India; Priyadharshan M., Sri Sairam Engineering College, Dept of Information Technology, Chennai, India","In an increasingly interconnected and technologically driven world, the need for innovative solutions to address the challenges faced by individuals with visual impairments has never been more pressing. A person with visual or vision impairment generally implies that their vision is so weakened that it cannot be corrected to a normal level. The main objective is to develop a smart-glasses that utilize the Internet of Things and Machine Learning methodologies to aid individuals who are visually impaired or experiencing vision-related issues, such as Glaucoma, Diabetes-related retinopathy, and other related conditions. The system's main components are a Pi camera, a Bluetooth or WiFi module and a microcontroller. The system processes visual information and translates it into audio. Using the input from the camera the system can translate the text into speech so that a blind person can read like an average human without the need for a braille writing system, A facial recognition system utilizes a photograph or video to create a mapping of facial features. It then compares this information with a database of known faces to identify a match. Additionally, The system can recognise and identify objects that are directly in front of the user. With audio output, the system can guide the user, enabling visually impaired individuals to navigate their environment autonomously, without needing assistance from another person. A mobile application is developed to interface between the user and the smart glasses.  © 2023 IEEE.","Facial recognition; Microcontroller; obstacle detection; Smart-Glasses; Text-to-speech; vision impairment","Cameras; Face recognition; Glass; Obstacle detectors; Speech recognition; Vision; Facial recognition; Innovative solutions; Obstacles detection; Pressung; Smart glass; Text to speech; Vision impairments; Visual impairment; Visual information; Visually impaired; Microcontrollers","","","Institute of Electrical and Electronics Engineers Inc.","","979-835039458-0","","","English","Intell. Comput. Control Eng. Bus. Syst., ICCEBS","Conference paper","Final","","Scopus","2-s2.0-85189153263"
"Machado F.; Marroquín A.; Fuentes J.A.","Machado, Federico (57193830668); Marroquín, Alberto (57884420400); Fuentes, José Antonio (58072550500)","57193830668; 57884420400; 58072550500","Computer Vision Extended Perception System for Blind People","2022","Proceedings of the 2022 IEEE 40th Central America and Panama Convention, CONCAPAN 2022","","","","","","","0","10.1109/CONCAPAN48024.2022.9997641","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85146543109&doi=10.1109%2fCONCAPAN48024.2022.9997641&partnerID=40&md5=b4ae245ffeb458d88bacc502a3e6e0be","IIIE, Universidad Don Bosco, Soyapango, El Salvador; Universidad de Oriente, Facultad de Ingeniería y Arquitectura, San Miguel, El Salvador","Machado F., IIIE, Universidad Don Bosco, Soyapango, El Salvador; Marroquín A., IIIE, Universidad Don Bosco, Soyapango, El Salvador; Fuentes J.A., Universidad de Oriente, Facultad de Ingeniería y Arquitectura, San Miguel, El Salvador","There is a considerable number of visually impaired people in the world, who live their daily lives with limitations in their mobility due to the little information they have about their environment. This project proposes to use embedded systems to develop an assistance system for the blind, which, through machine learning models, describes the objects closest to the user and if they are known or unknown. To do this, two trained Machine Learning models will be used, the first to detect common objects and the second to identify particular objects. The intention of using both networks is to improve the accuracy of the system, by first detecting objects or people in an image and then classifying them with known labels (names or some identifier). The results obtained show the benefit of using these neural networks for the recognition of objects in the environment from a general database and then with a personalized one. Finally, to indicate the identified objects to blind people, the text of their labels is translated into speech.  © 2022 IEEE.","Computer Vision; Embedded System; Machine Learning; Neural Network","Embedded systems; Image enhancement; Machine learning; Object detection; Assistance system; Blind people; Daily lives; Embedded-system; Machine learning models; Machine-learning; Neural-networks; Perception systems; Systems for the blinds; Visually impaired people; Computer vision","","","Institute of Electrical and Electronics Engineers Inc.","","978-172816715-2","","","English","Proc. IEEE Central America Panama Conv., CONCAPAN","Conference paper","Final","","Scopus","2-s2.0-85146543109"
"Sae-Jia B.; Paderon R.L.; Srimuninnimit T.","Sae-Jia, Boonthicha (58560147700); Paderon, Rodolfo Lian (58559664500); Srimuninnimit, Thatchai (58559829800)","58560147700; 58559664500; 58559829800","A Head-Mounted Assistive Device for Visually Impaired People with Warning System from Object Detection and Depth Estimation","2023","Journal of Physics: Conference Series","2550","1","012034","","","","1","10.1088/1742-6596/2550/1/012034","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85169596835&doi=10.1088%2f1742-6596%2f2550%2f1%2f012034&partnerID=40&md5=8a234c6d64642643eeb3707eee1da2e3","Dept. of Robotics and Artificial Intelligence Engineering, Faculty of Engineering, Chulalongkorn University, Thailand; Faculty of Medicine, Chulalongkorn University, Thailand","Sae-Jia B., Dept. of Robotics and Artificial Intelligence Engineering, Faculty of Engineering, Chulalongkorn University, Thailand; Paderon R.L., Dept. of Robotics and Artificial Intelligence Engineering, Faculty of Engineering, Chulalongkorn University, Thailand; Srimuninnimit T., Faculty of Medicine, Chulalongkorn University, Thailand","People with visual impairment use white cane as their traditional method for perceiving the surroundings. However, the utilization of a cane is limited by its length and orientation. In Thailand, the obstacles on paveway in daily life are not located only on the floor but also above knee level which sometimes could be harmful to pedestrians, especially blind people. A head-mounted assistive device is developed to be an enhancement used with a cane for the visually impaired to comprehend their environment both lower and higher the knee level. The assistive device is designed to be compact and light-weight. It could also send the tactile feedback as a warning from vibration motors mounted on the device. To generate a warning signal, YOLOv4 is used to detect the location of obstacles and depth map from the stereo camera is used to estimate the distance mapping into 4 defined ranges: dangerous, very close, close and fine. The results indicate that the head-mounted assistive device has the ability to perceive obstacles locating farther than 0.9 m. The prediction returned 9.23%, 14.63% and 7.86% error when estimating the depth of obstacles at 1.3 m., 2.8 m. and 4.2 m. respectively. The average execution time for the device to return the command controlling vibration motors is 0.13 second and the maximum estimated time for the motor to send the haptic feedback is 1.05 second.  © Published under licence by IOP Publishing Ltd.","","Feedback; Stereo image processing; Assistive devices; Depth Estimation; Detection estimation; Objects detection; Paveways; Thailand; Vibration motor; Visual impairment; Visually impaired people; White cane; Object detection","","","Institute of Physics","17426588","","","","English","J. Phys. Conf. Ser.","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85169596835"
"Scheidig A.; Hartramph R.; Schuetz B.; Mueller S.; Kunert K.S.; Lahne J.; Oelschlegel U.; Scheidig R.; Gross H.-M.","Scheidig, Andrea (13105198000); Hartramph, Robert (58690952600); Schuetz, Benjamin (57210646636); Mueller, Steffen (57199400995); Kunert, Kathleen S. (58690124700); Lahne, Johanna (58745559300); Oelschlegel, Ute (58690675600); Scheidig, Ruediger (58691513300); Gross, Horst-Michael (7202668837)","13105198000; 58690952600; 57210646636; 57199400995; 58690124700; 58745559300; 58690675600; 58691513300; 7202668837","Feasibility Study: Towards a Robot-Assisted Gait Training in Ophthalmological Rehabilitation","2023","IEEE International Conference on Rehabilitation Robotics","","","","","","","0","10.1109/ICORR58425.2023.10304760","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85176420978&doi=10.1109%2fICORR58425.2023.10304760&partnerID=40&md5=17da5e40182e7d9657b2791d7470ed8d","Ilmenau University of Technology, Neuroinformatics and Cognitive Robotics Lab, Ilmenau, 98684, Germany; Regiomed Rehabilitation Clinic GmbH, Masserberg, Germany; MetraLabs GmbH, Ilmenau, Germany","Scheidig A., Ilmenau University of Technology, Neuroinformatics and Cognitive Robotics Lab, Ilmenau, 98684, Germany; Hartramph R., Ilmenau University of Technology, Neuroinformatics and Cognitive Robotics Lab, Ilmenau, 98684, Germany; Schuetz B., Ilmenau University of Technology, Neuroinformatics and Cognitive Robotics Lab, Ilmenau, 98684, Germany; Mueller S., Ilmenau University of Technology, Neuroinformatics and Cognitive Robotics Lab, Ilmenau, 98684, Germany; Kunert K.S., Regiomed Rehabilitation Clinic GmbH, Masserberg, Germany; Lahne J., Regiomed Rehabilitation Clinic GmbH, Masserberg, Germany; Oelschlegel U., MetraLabs GmbH, Ilmenau, Germany; Scheidig R., MetraLabs GmbH, Ilmenau, Germany; Gross H.-M., Ilmenau University of Technology, Neuroinformatics and Cognitive Robotics Lab, Ilmenau, 98684, Germany","The idea of using mobile assistance robots for gait training in rehabilitation has been increasingly explored in recent years due to the associated benefits. This paper describes how the previous results of research and praxis on gait training with a mobile assistance robot in orthopedic rehabilitation can be transferred to ophthalmic-related orientation and mobility training for blind and visually impaired people. To this end, the specific requirements for such orientation and mobility training are presented from a therapeutic perspective. Using sensory data, it is investigated how the analysis of training errors can be automated and transferred back to the training person. These pre-examinations are the prerequisite for any form of robot-assisted mobile gait training in ophthamological rehabilitation, which does not exist so far and which is expected to be of great benefit to these patients. © 2023 IEEE.","","Exercise Therapy; Feasibility Studies; Gait; Gait Disorders, Neurologic; Humans; Robotics; Patient rehabilitation; Sensory analysis; Blind and visually impaired; Feasibility studies; Gait training; Orthopedic rehabilitation; Sensory data; Training errors; Visually impaired people; feasibility study; gait; human; kinesiotherapy; neurologic gait disorder; procedures; robotics; Robots","","","IEEE Computer Society","19457898","979-835034275-8","","37941232","English","IEEE Int. Conf. Rehabil. Rob.","Conference paper","Final","","Scopus","2-s2.0-85176420978"
"Karamchandani H.; Kumari S.; Dutta K.K.; Kehkeshan Jalall S.; Saurav S.; Kumar S.","Karamchandani, Harsha (58478854500); Kumari, Sarita (56603726200); Dutta, Kusumika Krori (37055339300); Kehkeshan Jalall, S. (57346742000); Saurav, Sahil (58549397600); Kumar, Subham (58593466600)","58478854500; 56603726200; 37055339300; 57346742000; 58549397600; 58593466600","Development of Machine Learning Model for Assistance of Visually Impaired","2023","International Conference on Applied Intelligence and Sustainable Computing, ICAISC 2023","","","","","","","0","10.1109/ICAISC58445.2023.10199677","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85168757823&doi=10.1109%2fICAISC58445.2023.10199677&partnerID=40&md5=1f9208eda0a426dedde0a857af5a3385","Nitte Meenakshi Institute of Technology, Department of ECE, Bangalore, India; Amity University, Department of ECE, Jharkhand, India; M S Ramaiah Institute of Technology, Department of EEE, Bangalore, India; Presidency University, Department of ECE, Bangalore, India","Karamchandani H., Nitte Meenakshi Institute of Technology, Department of ECE, Bangalore, India; Kumari S., Amity University, Department of ECE, Jharkhand, India; Dutta K.K., M S Ramaiah Institute of Technology, Department of EEE, Bangalore, India; Kehkeshan Jalall S., Presidency University, Department of ECE, Bangalore, India; Saurav S., Nitte Meenakshi Institute of Technology, Department of ECE, Bangalore, India; Kumar S., Nitte Meenakshi Institute of Technology, Department of ECE, Bangalore, India","People with reduced vision or total blindness frequently have trouble navigating new environments on their own. It can be difficult to travel or even just stroll down a busy street. As a result, many people with poor eyesight frequently travel with a sighted friend or relative. It is harder for them to recall where they usually are. The goal of this initiative is to completely liberate the blind individual. The project is a virtual eye that communicates with the outside environment using a camera. The system continuously gets data from the camera. The information is gathered using cameras that are meant to be in the Centre of the eyes. Input signals were processed using APIs and algorithms, followed by speech processing units are utilized to communicate with the blind individual. By employing the audio output to communicate the processed information about their environment, blind people can move and finish their task easily on their own. The accuracy of the suggested method is 98%, 95%, and 90%, respectively, for object detection, face recognition, and Optical Character Recognition (OCR) methodology. The recognized object, text and face are then processed to obtain audio instructions. © 2023 IEEE.","COCO Datasets; Haar Cascade Algorithm; Machine learning; SSD Detection; Tensor-Flow API; Visually Impaired Assistance","Cameras; Face recognition; Learning algorithms; Object detection; Object recognition; Optical character recognition; Speech processing; Blind individuals; Cascade algorithm; COCO dataset; Haar cascade algorithm; Machine learning models; Machine-learning; SSD detection; Tensor-flow API; Visually impaired; Visually impaired assistance; Machine learning","H. Karamchandani; Nitte Meenakshi Institute of Technology, Department of ECE, Bangalore, India; email: harsha.mkaramchandani@nmit.ac.in","","Institute of Electrical and Electronics Engineers Inc.","","979-835032379-5","","","English","Int. Conf. Appl. Intell. Sustain. Comput., ICAISC","Conference paper","Final","","Scopus","2-s2.0-85168757823"
"Joshi M.; Shukla A.; Srivastava J.; Rastogi M.; Mujumdar S.; Tripathi H.","Joshi, Malay (58151405200); Shukla, Aditi (58150794000); Srivastava, Jayesh (58151097200); Rastogi, Manya (58150794100); Mujumdar, Suvarna (57449129700); Tripathi, Himanshu (58613545500)","58151405200; 58150794000; 58151097200; 58150794100; 57449129700; 58613545500","DRISHTI: Visual Navigation Assistant for Visually Impaired","2023","Journal of Physics: Conference Series","2570","1","012032","","","","1","10.1088/1742-6596/2570/1/012032","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85172019124&doi=10.1088%2f1742-6596%2f2570%2f1%2f012032&partnerID=40&md5=ae5cd36b53020a485a7a1afd60057d9a","Ajay Kumar Garg Engineering College, Uttar Pradesh, Ghaziabad, 201009, India","Joshi M., Ajay Kumar Garg Engineering College, Uttar Pradesh, Ghaziabad, 201009, India; Shukla A., Ajay Kumar Garg Engineering College, Uttar Pradesh, Ghaziabad, 201009, India; Srivastava J., Ajay Kumar Garg Engineering College, Uttar Pradesh, Ghaziabad, 201009, India; Rastogi M., Ajay Kumar Garg Engineering College, Uttar Pradesh, Ghaziabad, 201009, India; Mujumdar S., Ajay Kumar Garg Engineering College, Uttar Pradesh, Ghaziabad, 201009, India; Tripathi H., Ajay Kumar Garg Engineering College, Uttar Pradesh, Ghaziabad, 201009, India","In today's society, where independent living is becoming increasingly important, it can be extremely constricting for those who are blind. Blind and visually impaired (BVI) people face challenges because they need manual support to prompt information about their environment. In this work, we took our first step towards developing an affordable and high-performing eye wearable assistive device, DRISHTI, to provide visual navigation assistance for BVI people. This system comprises a camera module, ESP32 processor, Bluetooth module, smartphone and speakers. Using artificial intelligence, this system is proposed to detect and understand the nature of the users' path and obstacles ahead of the user in that path and then inform BVI users about it via audio output to enable them to acquire directions by themselves on their journey. This first step discussed in this paper involves establishing a proof-of-concept of achieving the right balance of affordability and performance by testing an initial software integration of a currency detection algorithm on a low-cost embedded arrangement. This work will lay the foundation for our upcoming works toward achieving the goal of assisting the maximum of BVI people around the globe in moving independently. © 2023 Institute of Physics Publishing. All rights reserved.","Deep Learning (Resnet-50); gTTS; Tesseract OCR; YOLOv7","Integration testing; Blind and visually impaired; Deep learning (resnet-50); GTTS; Independent living; Tesseract; Tesseract OCR; Visual Navigation; Visually impaired; Visually impaired people; YOLOv7; Deep learning","M. Joshi; Ajay Kumar Garg Engineering College, Ghaziabad, Uttar Pradesh, 201009, India; email: malay1931025@akgec.ac.in","Ahuja H.; Srivastava A.; Bhuvaneshwari G.","Institute of Physics","17426588","","","","English","J. Phys. Conf. Ser.","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85172019124"
"Tripathi S.; Singh S.; Tanya T.; Kapoor S.; Kirti; Saini A.S.","Tripathi, Subrat (58195812200); Singh, Saurabh (58262509600); Tanya, T. (58195670400); Kapoor, Shivani (58195368700); Kirti (58196111300); Saini, Amanpreet Singh (26664970500)","58195812200; 58262509600; 58195670400; 58195368700; 58196111300; 26664970500","Analysis of Obstruction Avoidance Assistants to Enhance the Mobility of Visually Impaired Person: A Systematic Review","2023","2023 International Conference on Artificial Intelligence and Smart Communication, AISC 2023","","","","134","142","8","3","10.1109/AISC56616.2023.10085416","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85153524304&doi=10.1109%2fAISC56616.2023.10085416&partnerID=40&md5=1dd4001bc1909a1d7c3762ea5e3ae9a5","Galgotias College of Engineering and Technology, Uttar Pradesh, Greater Noida, India; Indian Instiute of Technology, Himachal Pradesh, Mandi, India","Tripathi S., Galgotias College of Engineering and Technology, Uttar Pradesh, Greater Noida, India; Singh S., Galgotias College of Engineering and Technology, Uttar Pradesh, Greater Noida, India; Tanya T., Galgotias College of Engineering and Technology, Uttar Pradesh, Greater Noida, India; Kapoor S., Galgotias College of Engineering and Technology, Uttar Pradesh, Greater Noida, India; Kirti, Indian Instiute of Technology, Himachal Pradesh, Mandi, India; Saini A.S., Galgotias College of Engineering and Technology, Uttar Pradesh, Greater Noida, India","In the field of research for assistive technology, the advancement of navigational tools for those who are blind had emerged as a major concern. This article offers valuable insights for researchers working on assistive technologies for people who are blind or visually impaired, specifically focusing on indoor and outdoor navigation systems. To create such systems, it is important to study past research in this area, starting from early investigations on electronic travel aids to the more recent utilization of advanced artificial vision models. This paper aims to provide an effective vision for researchers to use in developing technological instruments for the visually impaired. This process involves a comprehensive evaluation of various technological approaches in order to achieve the challenging goal of creating an effective navigation system. We carried out a separate systematic review because papers in this field tend to focus more on software-based computer vision solutions than on hardware. We have analyzed 191 relevant studies published between 2011 and 2022 to create a comprehensive understanding of the topic at hand. This systematic mapping process will assist researchers, engineers, and practitioners in making informed decisions by identifying shortcomings in current navigation assistance technology and proposing new and improved smart assistant applications that can ensure the safety and accuracy of direction for people who are blind or visually impaired.  © 2023 IEEE.","Blind; navigation assistant; travelling aid; vision and non-vision","Computer hardware; Navigation systems; Vision; Assistive technology; Blind; Indoor navigation system; Navigation assistant; Outdoor navigation systems; Systematic Review; Traveling aids; Vision and non-vision; Visually impaired; Visually impaired persons; Indoor positioning systems","S. Tripathi; Galgotias College of Engineering and Technology, Greater Noida, Uttar Pradesh, India; email: subrattripath.542@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","","979-835032230-9","","","English","Int. Conf. Artif. Intell. Smart Commun., AISC","Conference paper","Final","","Scopus","2-s2.0-85153524304"
"Shete V.; Shaikh A.; Sawant R.; Singh C.; Sridevi P.P.; Harikrishnan R.","Shete, Vikram (25638658200); Shaikh, Ariz (57225197532); Sawant, Rhea (57225194539); Singh, Chetna (57225200496); Sridevi, Ponmalar P. (56587901000); Harikrishnan, R. (56587668400)","25638658200; 57225197532; 57225194539; 57225200496; 56587901000; 56587668400","SMART SHOPPING CART FOR VISUALLY IMPAIRED INDIVIDUALS","2022","Review of Computer Engineering Research","9","2","","122","134","12","6","10.18488/76.v9i2.3083","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85135588314&doi=10.18488%2f76.v9i2.3083&partnerID=40&md5=2168c6d7bcd5d91d0d325bd71610d2d9","Symbiosis Institute of Technology, Symbiosis International Deemed University, Pune, India; Department of Computational Intelligence, School of Computing, SRM Institute of Science and Technology, Chennai, India","Shete V., Symbiosis Institute of Technology, Symbiosis International Deemed University, Pune, India; Shaikh A., Symbiosis Institute of Technology, Symbiosis International Deemed University, Pune, India; Sawant R., Symbiosis Institute of Technology, Symbiosis International Deemed University, Pune, India; Singh C., Symbiosis Institute of Technology, Symbiosis International Deemed University, Pune, India; Sridevi P.P., Department of Computational Intelligence, School of Computing, SRM Institute of Science and Technology, Chennai, India; Harikrishnan R., Symbiosis Institute of Technology, Symbiosis International Deemed University, Pune, India","Vision is one of our most important tools to help us survive in the world. Not being able to see well or at all is only the tip of the iceberg that people who are visually impaired have to deal with. Along with that, comes a whole new set of societal challenges, stigma, fears, and beliefs making these groups a minority and not shedding enough light on issues that they face is more common than we realize. While everyone in today’s world chases the buzzwords artificial intelligence and robotics to create complex technologies to solve problems like the mundaneness of a task, it would be incorrect to forget about the bigger picture that hints at helping mankind save the rest of it. The visually impaired face a variety of issues right from not being able to navigate along a path to the very basic grocery shopping at a shopping mall. This paper proposes an idea of a smart shopping cart that assists the visually impaired to navigate through various isles of a shopping mart and help them purchase the items they wish to buy with the help of braille assisted buttons and haptic feedback control without feeling the need for any human assistance. © 2022 Conscientia Beam. All Rights Reserved.","Artificial intelligence; Blind; Braille; Internet of things; Smart shopping cart; Smart trolley; Visually impaired","","R. Sawant; Symbiosis Institute of Technology, Symbiosis International Deemed University, Pune, India; email: rheasawant123@gmail.com","","Conscientia Beam","24109142","","","","English","Rev. Comput. Eng. Res.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85135588314"
"Shinghal D.; Shinghal K.; Saxena S.; Saxena A.; Saxena N.; Sharma A.","Shinghal, Deepti (57803237400); Shinghal, Kshitij (57191899272); Saxena, Shuchita (57198613378); Saxena, Amit (57210423740); Saxena, Nishant (57188737704); Sharma, Amit (56669080800)","57803237400; 57191899272; 57198613378; 57210423740; 57188737704; 56669080800","Artificial Intelligence Based Visually Impaired Assist System","2022","2022 International Conference on Advances in Computing, Communication and Materials, ICACCM 2022","","","","","","","0","10.1109/ICACCM56405.2022.10009591","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85146873977&doi=10.1109%2fICACCM56405.2022.10009591&partnerID=40&md5=65fe837a63efeb82a9617bdf3ffaa784","International Association for Engineerig & Technology, Singapore; Moradabad Institute of Technology, Dept. of E&c Engg., UP, Moradabad, India; Tula's Institute, Uttrakhand, Dehradun, India","Shinghal D., International Association for Engineerig & Technology, Singapore; Shinghal K., Moradabad Institute of Technology, Dept. of E&c Engg., UP, Moradabad, India; Saxena S., Moradabad Institute of Technology, Dept. of E&c Engg., UP, Moradabad, India; Saxena A., Moradabad Institute of Technology, Dept. of E&c Engg., UP, Moradabad, India; Saxena N., Tula's Institute, Uttrakhand, Dehradun, India; Sharma A., Moradabad Institute of Technology, Dept. of E&c Engg., UP, Moradabad, India","In present work, a system is proposed which is unique in a way that there is a requirement of visually impaired friendly buildings. In current scenario when a visually impaired person enters a building which is Visually Impaired (VI) friendly, an attendant hands him over braille based navigation chart or electronic guide system The proposed system automatically detects a visually impaired person makes an announcement, generates an alert message from the basket where VI person enabled braille based guide maps are kept. The system was tested and it is able to detect blind persons with good accuracy.  © 2022 IEEE.","Artificial intelligence. Machine learning; Raspberry pi-4; Visually Impaired","'current; Artificial intelligence.; Blind person; Guide maps; Guide system; Machine-learning; Raspberry pi-4; Visually impaired; Visually impaired persons; Machine learning","D. Shinghal; International Association for Engineerig & Technology, Singapore; email: shinghaldeepti0@gmail.com","Semwal S.; Joshi T.","Institute of Electrical and Electronics Engineers Inc.","","978-166547439-9","","","English","Int. Conf. Adv. Comput., Commun. Mater., ICACCM","Conference paper","Final","","Scopus","2-s2.0-85146873977"
"Khan D.A.; Zamir M.A.; Umar M.S.; Haider Z.","Khan, Danish Asad (57686771800); Zamir, Mohammed Asif (57687027400); Umar, Mohammad Sarosh (55262920800); Haider, Zafaryab (58003254700)","57686771800; 57687027400; 55262920800; 58003254700","Assistive Stick for Visually Impaired People","2022","2022 5th International Conference on Multimedia, Signal Processing and Communication Technologies, IMPACT 2022","","","","","","","1","10.1109/IMPACT55510.2022.10029287","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85148023722&doi=10.1109%2fIMPACT55510.2022.10029287&partnerID=40&md5=012801190460c9798a9b0b780cbab8e0","Jamia Millia Islamia, D/O Computer Engineering, New Delhi, India; Indian Institute of Technology Bombay, Sjmsom, Mumbai, India; Aligarh Muslim University, D/O Computer Engineering, Aligarh, India; Aligarh Muslim University, Iiru, Aligarh, India","Khan D.A., Jamia Millia Islamia, D/O Computer Engineering, New Delhi, India; Zamir M.A., Indian Institute of Technology Bombay, Sjmsom, Mumbai, India; Umar M.S., Aligarh Muslim University, D/O Computer Engineering, Aligarh, India; Haider Z., Aligarh Muslim University, Iiru, Aligarh, India","Blind or visually impaired people have difficulty navigating the street and are also unaware of nearby landmarks. They need an assistant (e.g. a cane, human, dog, e.t.c.) to go from one place to another place. In this modern era, it is possible to develop smart solutions with the help of modern technologies like deep learning (DL), natural language processing (NLP), internet of things (IoT), etc. that can assist them without any human intervention. A smart stick is one of the solutions that can help them with navigation and get to know the surrounding places. This paper proposed a smart solution and developed a smart stick to help visually impaired people with the use of DL and NLP and devices like the Raspberry Pi, an ultrasonic sensor, and a camera. This smart stick can detect obstacles using an ultrasonic sensor within the range of 1.5 metres and alert the person by generating audio as a warning message. The stick is also able to capture images from its surroundings using the camera and can read the text present in the captured image. The text in the image is detected using a deep learning model called the EAST text detector and recognised using an OCR Engine tesseract. Furthermore, the recognised text is converted into audio so that the person can hear about their surroundings. All calculations and processing are done on the Raspberry Pi, and the generated audio can be heard by using a speaker(earphone or headphone). The proposed solution is successfully implemented and demonstrated, and it is found to be very efficient and cost-effective.  © 2022 IEEE.","Deep Learning; EAST; Natural Language Processing; OCR; Raspberry Pi; Tesseract; Text Recognition; Ultrasonic Sensor","Character recognition; Cost effectiveness; Deep learning; Internet of things; Natural language processing systems; Ultrasonic applications; Ultrasonic sensors; Deep learning; EAST; Language processing; Natural language processing; Natural languages; Raspberry pi; Smart solutions; Tesseract; Text recognition; Visually impaired people; Cameras","D.A. Khan; Jamia Millia Islamia, D/O Computer Engineering, New Delhi, India; email: dasadkhan05@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","","978-166547647-8","","","English","Int. Conf. Multimed., Signal Process. Commun. Technol., IMPACT","Conference paper","Final","","Scopus","2-s2.0-85148023722"
"Shilaskar S.; Kadam A.; Kadam R.; Jadhav T.; Bhatlawande S.; Madake J.","Shilaskar, Swati (57189017229); Kadam, Amey (58075009400); Kadam, Rupesh (58074204800); Jadhav, Tanuja (58073686500); Bhatlawande, Shripad (55212307900); Madake, Jyoti (57222127908)","57189017229; 58075009400; 58074204800; 58073686500; 55212307900; 57222127908","Detection of Outdoor Traffic and Kids Playing Scene for Visually Impaired People","2023","Lecture Notes in Electrical Engineering","959","","","219","236","17","0","10.1007/978-981-19-6581-4_18","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85146694011&doi=10.1007%2f978-981-19-6581-4_18&partnerID=40&md5=83d94687708ccef525999eb6cb0f7cba","Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Pune, 411037, India","Shilaskar S., Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Pune, 411037, India; Kadam A., Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Pune, 411037, India; Kadam R., Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Pune, 411037, India; Jadhav T., Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Pune, 411037, India; Bhatlawande S., Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Pune, 411037, India; Madake J., Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Pune, 411037, India","In this paper, a novel outdoor scene detection technique for visually impaired people has been proposed. This system focuses on the detection of traffic jam scenes and playing kids scenes. Visually impaired people (VIP) face more difficulties walking alone outdoor. As they cannot prevent themselves from unexpectedly hitting objects, avoiding stray animals, and bumping into people. It inspired us to work on a solution for the mobility of blinds. This paper proposes an effective outdoor scene detection system for improving their lifestyle. Traditional solutions are available for visually impaired people like white cane and guide dogs. Traditional solutions address obstacle detection and path guidance, but scene-cognition is somehow a neglected domain. The proposed system can detect two scenes. SIFT and K-means clustering machine learning algorithms have been used in this system along with the following classifiers with accuracy-decision tree (84%), KNN (84.75%), random forest (88.58%), SVC linear kernel (85.08%), SVC polynomial kernel (85.33%), SVC radial basis function kernel (87.08%), and linear regression (85.08%). © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Assistive aid; Computer vision; Kids playing scene detection; Machine learning; Traffic scene detection; Visually impaired","Adaptive boosting; Decision trees; K-means clustering; Learning systems; Machine learning; Obstacle detectors; Radial basis function networks; Traffic congestion; Assistive; Assistive aid; Kid playing scene detection; Machine-learning; Scene detection; Traffic scene; Traffic scene detection; Visually impaired; Visually impaired people; Computer vision","S. Shilaskar; Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Pune, 411037, India; email: swati.shilaskar@vit.edu","Kulkarni A.J.; Mirjalili S.; Udgata S.K.","Springer Science and Business Media Deutschland GmbH","18761100","978-981196580-7","","","English","Lect. Notes Electr. Eng.","Conference paper","Final","","Scopus","2-s2.0-85146694011"
"Shawki F.A.; Mahfouz M.; Abdelrazek M.A.; Sayed G.I.","Shawki, Fatema A. (58620629200); Mahfouz, Mariem (58620569700); Abdelrazek, Mohamed A. (56080446200); Sayed, Gehad Ismail (57023745800)","58620629200; 58620569700; 56080446200; 57023745800","Empowering Individuals with Visual Impairments: A Deep Learning-Based Smartphone Navigation Assistant","2023","Lecture Notes on Data Engineering and Communications Technologies","184","","","19","30","11","0","10.1007/978-3-031-43247-7_2","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85172416167&doi=10.1007%2f978-3-031-43247-7_2&partnerID=40&md5=fec53f980c3edbd2aa839074b20e6a90","School of Computer Science, Canadian International College (CIC), New Cairo, Cairo, Egypt","Shawki F.A., School of Computer Science, Canadian International College (CIC), New Cairo, Cairo, Egypt; Mahfouz M., School of Computer Science, Canadian International College (CIC), New Cairo, Cairo, Egypt; Abdelrazek M.A., School of Computer Science, Canadian International College (CIC), New Cairo, Cairo, Egypt; Sayed G.I., School of Computer Science, Canadian International College (CIC), New Cairo, Cairo, Egypt","Long white canes and other technological aids are frequently used by visually impaired people to identify and avoid obstacles ahead and hazards. Due to their general knowledge of everything’s location, visually impaired individuals can move freely in their homes without much assistance. However, they encounter more challenges and risk harm when they roam the streets. To help visually impaired individuals navigate the streets independently and safely, this paper proposes a deep learning-based smartphone navigation assistant system. The backend and frontend are the two main components. On the front end, the images are captured by utilizing the mobile camera. The backend is fed with these captured images. A You Only Look Once (YOLOv8) deep learning architecture is used in the backend, followed by a rule-based model. Finally, a set of pre-recorded audio messages that contain navigational guidance is returned to the user. The deep-learning architecture is trained and fine-tuned on a dataset gathered from five different sources. The experimental results showed that the proposed model can be effectively used to help people who are blind. Additionally, the outcomes demonstrated that YOLOv8 achieved the best outcomes when compared to other deep-learning architectures. The proposed system achieved a 97% overall accuracy. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Deep learning; Mobile application; Rule-based modeling; Visual impairment; YOLOv8","Architecture; Deep learning; Learning systems; Navigation; Deep learning; Learning architectures; Mobile applications; Rule-based models; Smart phones; Visual impairment; Visually impaired; Visually impaired people; White cane; YOLOv8; Smartphones","G.I. Sayed; School of Computer Science, Canadian International College (CIC), New Cairo Cairo, Egypt; email: Gehad_Sayed@cic-cairo.com","","Springer Science and Business Media Deutschland GmbH","23674512","","","","English","Lecture. Notes. Data Eng. Commun. Tech.","Conference paper","Final","","Scopus","2-s2.0-85172416167"
"Zare F.; Sedighi P.; Delrobaei M.","Zare, Fateme (58584371300); Sedighi, Paniz (57222708284); Delrobaei, Mehdi (24824009300)","58584371300; 57222708284; 24824009300","A Wearable RFID-Based Navigation System for the Visually Impaired","2022","10th RSI International Conference on Robotics and Mechatronics, ICRoM 2022","","","","572","577","5","3","10.1109/ICRoM57054.2022.10025351","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85148065011&doi=10.1109%2fICRoM57054.2022.10025351&partnerID=40&md5=818942400102ad5d089cb305ffcb846a","K. N. Toosi University of Technology, Faculty of Electrical Engineering, Tehran, Iran; University of Alberta, Faculty of Electrical and Computer Engineering, Alberta, Canada","Zare F., K. N. Toosi University of Technology, Faculty of Electrical Engineering, Tehran, Iran; Sedighi P., University of Alberta, Faculty of Electrical and Computer Engineering, Alberta, Canada; Delrobaei M., K. N. Toosi University of Technology, Faculty of Electrical Engineering, Tehran, Iran","Recent studies have focused on developing advanced assistive devices to help blind or visually impaired people. Navigation is challenging for this community; however, developing a simple yet reliable navigation system is still an unmet need. This study targets the navigation problem and proposes a wearable assistive system. We developed a smart glove and shoe set based on radio-frequency identification technology to assist visually impaired people with navigation and orientation in indoor environments. The system enables the user to find the directions through audio feedback. To evaluate the device's performance, we designed a simple experimental setup. The proposed system has a simple structure and can be personalized according to the user's requirements. The results identified that the platform is reliable, power efficient, and accurate enough for indoor navigation. © 2022 IEEE.","assistive technology; human-machine interaction; Radio Frequency Identification; wearable devices","Indoor positioning systems; Navigation systems; Radio frequency identification (RFID); Radio waves; Wearable technology; Assistive devices; Assistive system; Assistive technology; Human machine interaction; Navigation problem; Radio-frequency-identification; Simple++; Visually impaired; Visually impaired people; Wearable devices; Assistive technology","F. Zare; K. N. Toosi University of Technology, Faculty of Electrical Engineering, Tehran, Iran; email: fatemezare79@email.kntu.ac.ir","","Institute of Electrical and Electronics Engineers Inc.","","978-166545452-0","","","English","RSI Int. Conf. Robot. Mechatronics, ICRoM","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85148065011"
"Samuel P.; Dhanaraj R.K.; Balusamy B.; Narmatha C.","Samuel, Prithi (57211325231); Dhanaraj, Rajesh Kumar (56884774100); Balusamy, Balamurugan (55050821600); Narmatha, C. (56418673200)","57211325231; 56884774100; 55050821600; 56418673200","Dark Vision Assistive Application for Deaf-Blind People","2023","2023 3rd International Conference on Computing and Information Technology, ICCIT 2023","","","","1","6","5","0","10.1109/ICCIT58132.2023.10273935","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85175433436&doi=10.1109%2fICCIT58132.2023.10273935&partnerID=40&md5=5d5841e77d0f5b5753afae8484d92e3f","Srm Institute of Science and Technology, Department of Computational Intelligence, Chennai, India; Symbiosis International, Deemed University, Symbiosis Institute of Computer Studies and Research (SICSR), Pune, India; Shiv Nadar University, Noida, India; University of Tabuk, Industrial Innovation & Robotics Center, Faculty of Computers and Information Technology, Tabuk City, Saudi Arabia","Samuel P., Srm Institute of Science and Technology, Department of Computational Intelligence, Chennai, India; Dhanaraj R.K., Symbiosis International, Deemed University, Symbiosis Institute of Computer Studies and Research (SICSR), Pune, India; Balusamy B., Shiv Nadar University, Noida, India; Narmatha C., University of Tabuk, Industrial Innovation & Robotics Center, Faculty of Computers and Information Technology, Tabuk City, Saudi Arabia","For visually impaired, deaf-blind people, both communication and navigation are difficult activities and indoor navigation is becoming increasingly harder for them. It is even more difficult for deaf-blind people than it is for non-visually impaired people. People with visual impairments or deaf- blindness frequently rely on external support systems to make judgments, such as trained canines, humans, or specialized equipment. As a result, deaf-blind people are in desperate need of an assistive application that allows blind people to navigate and communicate freely without relying on others. The goal of the application is to create a more adaptable and cost-effective system for deaf-blind and visually impaired people. Through both outdoor and interior applications, as well as walking in even unfamiliar locations without relying on others. © 2023 IEEE.","assistive technology; Braille; deaf-blind; image classification; machine learning","Assistive technology; Cost effectiveness; Machine learning; Assistive applications; Assistive technology; Braille; Deaf-blind; Deaf-blind people; Images classification; Indoor navigation; Machine-learning; Visually impaired; Visually impaired people; Image classification","P. Samuel; Srm Institute of Science and Technology, Department of Computational Intelligence, Chennai, India; email: drprithisamuel@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","","979-835032148-7","","","English","Int. Conf. Comput. Inf. Technol., ICCIT","Conference paper","Final","","Scopus","2-s2.0-85175433436"
"","","","Nano-, Bio-, Info-Tech Sensors, and Wearable Systems 2023","2023","Proceedings of SPIE - The International Society for Optical Engineering","12485","","","","","97","0","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85160252680&partnerID=40&md5=e66ecf85df7467f599bdad88f6cee724","","","The proceedings contain 14 papers. The topics discussed include: stretchable piezoelectric polymer blend for dynamic stress and strain sensing; a multiplayer virtual reality platform to evaluate electronic travel aid performance for persons with blindness and low vision; cochlea-inspired sound visualization method; high accurate and efficient electrical impedance tomography for fast brain imaging; a cost-effective, accessible reduced graphene oxide (rGO) multifunctional wearable sensor via transfer printing; aramid nanofibers-based multidimensional structure; strong, functional and hydro-stable straws for plastic replacement; object classification robot hand system using thermal conductivity; skin-printable and self-adhesive hydrogel electrodes for functional electrical stimulation therapy; hybrid piezoelectric-magnetic, self-sensing actuator for vibration damping; and developing a novel electro-optic scanner for potential micro display and head-mounted display application.","","","","Kim J.; Oh I.; Porfiri M.; Yoon H.","SPIE","0277786X","978-151066077-9","PSISD","","English","Proc SPIE Int Soc Opt Eng","Conference review","Final","","Scopus","2-s2.0-85160252680"
"Deepa J.; Maria Adeline P.; Madhumita S.S.S.; Pavalaselvi N.","Deepa, J. (56622944900); Maria Adeline, P. (58264994100); Madhumita, Sai S. S. (58965208600); Pavalaselvi, N. (58264994200)","56622944900; 58264994100; 58965208600; 58264994200","Obstacle Detection and Navigation for The Visually Impaired","2023","2023 9th International Conference on Advanced Computing and Communication Systems, ICACCS 2023","","","","905","909","4","1","10.1109/ICACCS57279.2023.10112994","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85159760302&doi=10.1109%2fICACCS57279.2023.10112994&partnerID=40&md5=6a5ad4cea724f7bcc4aad9960f16d959","Easwari Engineering College, Computer Science and Engineering, Chennai, India","Deepa J., Easwari Engineering College, Computer Science and Engineering, Chennai, India; Maria Adeline P., Easwari Engineering College, Computer Science and Engineering, Chennai, India; Madhumita S.S.S., Easwari Engineering College, Computer Science and Engineering, Chennai, India; Pavalaselvi N., Easwari Engineering College, Computer Science and Engineering, Chennai, India","Because restrictiveness poses a serious problem for those who are visually impaired, independent living is becoming more and more crucial today. People who are visually impaired have difficulties because they require manual aid to learn about their surroundings. People with visual impairments find it difficult to carry out most of their everyday activities, such as eating, taking a walk, conversing with others, etc., because visual information provides the foundation for many actions. Thanks to technological breakthroughs, it is now possible to assist those who are blind or visually impaired. This study may be a survey of a device that helps the blind by using sensors and voice commands. Finding the distance, we wish to measure is difficult these days. In any event, the tape is a straightforward option, but this type of technology is only susceptible to human error. A rangefinder module had already been created by engineers. The module has a lot of problems, including a distance limit and varying outcomes with different colored barriers, which means we must constantly measure it before using it, they ultimately found. Distances are frequently measured with human inaccuracy. This project's main goal is to precisely calculate the minimum width's distance. This project uses ultrasonic sensors to measure distance measurement. The transferors will reply if you talk. The project value determines the precise distance from any obstacle we want to measure. The device can be used for a variety of purposes, such as robotics, car sensors for avoiding obstructions, and distance estimates in construction camps. The process of creating the gadget was based on as many uses as feasible from university courses, including lab equipment and programs for Microprocessor, Basic Electrical Engineering, Multimedia, and Electronics. The results show that the proposed device offers visually impaired people with higher accessibility, comfort, and simplicity of navigation when compared to the white cane. By using voice commands, this recently developed application can lead the blind. © 2023 IEEE.","Arduino; Arduino Uno; Distance Measurement; IoT; Obstacle Detection; Power Supply; Ultrasonic Sensors; Voice Commands","Distance measurement; Electric power systems; Multimedia systems; Obstacle detectors; Robots; Ultrasonic sensors; Arduino; Arduino uno; Independent living; IoT; Learn+; Obstacles detection; Power supply; Visual impairment; Visually impaired; Voice command; Ultrasonic applications","J. Deepa; Easwari Engineering College, Computer Science and Engineering, Chennai, India; email: deepa.j@eec.srmrmp.edu.in","","Institute of Electrical and Electronics Engineers Inc.","","979-835039737-6","","","English","Int. Conf. Adv. Comput. Commun. Syst., ICACCS","Conference paper","Final","","Scopus","2-s2.0-85159760302"
"Mohamed E.; Sirlantzis K.; Howells G.","Mohamed, Elhassan (57211158506); Sirlantzis, Konstantinos (9239813700); Howells, Gareth (35565062100)","57211158506; 9239813700; 35565062100","Real-time Powered Wheelchair Assistive Navigation System Based on Intelligent Semantic Segmentation for Visually Impaired Users","2022","5th IEEE International Image Processing, Applications and Systems Conference, IPAS 2022","","","","","","","1","10.1109/IPAS55744.2022.10053051","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85149793066&doi=10.1109%2fIPAS55744.2022.10053051&partnerID=40&md5=c436c8fde9fbec0e8141d68c9e66b929","School of Engineering, University of Kent, Canterbury, Kent, United Kingdom; School of Computing, University of Kent, Canterbury, Kent, United Kingdom","Mohamed E., School of Engineering, University of Kent, Canterbury, Kent, United Kingdom; Sirlantzis K., School of Engineering, University of Kent, Canterbury, Kent, United Kingdom; Howells G., School of Computing, University of Kent, Canterbury, Kent, United Kingdom","People with movement disabilities may find powered wheelchair driving a challenging task due to their comorbidities. Certain visually impaired persons with mobility disabilities are not prescribed a powered wheelchair because of their sight condition. However, powered wheelchairs are essential to the majority of these disabled users for commuting and social interaction. It is vital for their independence and wellbeing. In this paper, we propose to use a semantic segmentation (SS) system based on deep learning algorithms to provide environmental cues and information to visually impaired wheelchair users to aid with the navigation process. The system classifies the objects of the indoor environment and presents the annotated output on a display customised to the user's condition. The user can select a target object, for which the system can display the estimated distance from the current position of the wheelchair. The system runs in real-time, using a depth camera installed on the wheelchair, and it displays the scene in front of the wheelchair with every pixel annotated with distinguishable colour to represent the different components of the environment along with the distance to the target object. Our system has been designed, implemented and deployed on a real powered wheelchair for practical evaluation. The proposed system helped the users to estimate more accurately the distance to the target objects with a relative error of 19.8% and 18.4% for the conditions of a) semi-neglect and b) short-sightedness, respectively, compared to errors of 47.8% and 5.6% without the SS system. In our experiments, healthy participants were put in simulated conditions representing the above visual impairments using instruments commonly used in medical research for this purpose. Finally, our system helps to visualise, on the display, hidden areas of the environment and blind spots that visually impaired users would not be able to see without it. © 2022 IEEE.","Convolutional neural networks; Practical implementation; Semantic segmentation systems; Visually impaired disabled users","Convolutional neural networks; Deep learning; Navigation systems; Real time systems; Semantic Web; Semantics; Wheelchairs; Condition; Convolutional neural network; Powered wheelchair; Practical implementation; Segmentation system; Semantic segmentation; Semantic segmentation system; Target object; Visually impaired; Visually impaired disabled user; Semantic Segmentation","","","Institute of Electrical and Electronics Engineers Inc.","","978-166546219-8","","","English","IEEE Int. Image Process., Appl. Syst. Conf., IPAS","Conference paper","Final","","Scopus","2-s2.0-85149793066"
"Huang C.; Wang J.; Ma J.","Huang, Chenggang (57956314400); Wang, Jing (58577097700); Ma, Jianwei (57219147530)","57956314400; 58577097700; 57219147530","Design exploration of a tactile path defect detection vehicle","2023","Proceedings of SPIE - The International Society for Optical Engineering","12754","","127541O","","","","0","10.1117/12.2684219","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85171270653&doi=10.1117%2f12.2684219&partnerID=40&md5=734c19bdbdb41a106ad3249693124d57","Qingdao University, 308 Ningxia Road, Laoshan District, Shandong, Qingdao City, China","Huang C., Qingdao University, 308 Ningxia Road, Laoshan District, Shandong, Qingdao City, China; Wang J., Qingdao University, 308 Ningxia Road, Laoshan District, Shandong, Qingdao City, China; Ma J., Qingdao University, 308 Ningxia Road, Laoshan District, Shandong, Qingdao City, China","In order to solve the problems of poor paving effect of the existing tactile path, late maintenance is not timely, resulting in inconvenient travel for the blind, the shortcomings of the existing tactile path management are analyzed, so as to guide the design. In the design process, a defect detection method based on the YOLO-V5 target detection algorithm was adopted to realize the processing of tactile path samples, training model and classification of tactile path defect types, and a complete set of tactile path defect detection equipment was designed. This equipment can detect the tactile path in real time, judge and classify the defects of the tactile path, find out the problematic tactile path and send the geographical location information back to the background staff to assist the municipal administration department and the tactile path rectification construction unit to grasp the overall construction situation of the tactile path. Compared with the existing artificial tactile path defect detection methods, the proposed tactile path defect detection method based on OLO-V5 has better recognition performance and higher detection efficiency, and can accurately detect and identify various tactile path defects. At the same time, this study focuses on the safety of travel for the visually impaired, which helps to improve the equal rights of the visually impaired to participate in social activities and enjoy public resources, and helps to optimize barrier-free facilities, which plays a crucial role in the harmonious and orderly development of the society. © 2023 SPIE.","defect detecting; tactile path; YOLO-V5","Classification (of information); Construction equipment; Design; Defect detecting; Defect detection; Defect detection method; Design Exploration; Design-process; Management IS; Path management; Tactile path; Visually impaired; YOLO-v5; Defects","J. Ma; Qingdao University, Qingdao City, 308 Ningxia Road, Laoshan District, Shandong, China; email: h610667827@gmail.com","Shen L.; Zhong G.","SPIE","0277786X","978-151066756-3","PSISD","","English","Proc SPIE Int Soc Opt Eng","Conference paper","Final","","Scopus","2-s2.0-85171270653"
"Dasu S.; Ranade M.M.; Sarkar S.; Anala M.R.","Dasu, Shravya (58093838400); Ranade, Manali M. (58093875000); Sarkar, Shravasti (57704846500); Anala, M.R. (55734811700)","58093838400; 58093875000; 57704846500; 55734811700","Computer Vision based Tools and Technologies for Navigational Assistance for the Visually Impaired","2022","Proceedings - International Conference on Augmented Intelligence and Sustainable Systems, ICAISS 2022","","","","802","808","6","0","10.1109/ICAISS55157.2022.10010946","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85147551867&doi=10.1109%2fICAISS55157.2022.10010946&partnerID=40&md5=9086f2f3e4d56145773353c033bd3878","RV College of Engineering, Department of Computer Science and Engineering, Bengaluru, India; Department of Information Science and Engineering, RV College of Engineering, Bengaluru, India","Dasu S., RV College of Engineering, Department of Computer Science and Engineering, Bengaluru, India; Ranade M.M., RV College of Engineering, Department of Computer Science and Engineering, Bengaluru, India; Sarkar S., RV College of Engineering, Department of Computer Science and Engineering, Bengaluru, India; Anala M.R., Department of Information Science and Engineering, RV College of Engineering, Bengaluru, India","One of the most pressing challenges concerning visual impairment is navigation in complex real-life environments. Blind people often use guide dogs which are expensive, canes, or other people to find their way around. Over the years, various tools have been developed to make the process of way-finding easier for the visually impaired, based on numerous technologies, including computer vision, SLAM algorithms and embedded systems. In the following paper, a number of the most effective tools and their corresponding mechanisms are discussed. These have been divided into software and hardware-oriented tools to provide the reader with better clarity. Furthermore, an analysis of the effectiveness of these alternative methods has been done, along with possible solutions to any shortcomings posed by these tools. The review concludes with a discussion about the different aspects to keep in mind while building navigational tools to assist the visually impaired.  © 2022 IEEE.","Computer Vision; Embedded Systems; Image Segmentation; Navigation; Visual Impairment; Wearable Technology","Computer vision; Embedded systems; Image segmentation; Ophthalmology; Vision; Wearable technology; Blind people; Embedded-system; Guide dogs; Images segmentations; Pressung; Tools and technologies; Vision based; Visual impairment; Visually impaired; Way finding; Navigation","","","Institute of Electrical and Electronics Engineers Inc.","","978-166548962-1","","","English","Proc. - Int. Conf. Augment. Intell. Sustain. Syst., ICAISS","Conference paper","Final","","Scopus","2-s2.0-85147551867"
"Bolla D.R.; Trisheela S.; Nagarathana P.; Sarojadevi H.","Bolla, Dileep Reddy (57193058845); Trisheela, S. (58038901900); Nagarathana, P. (57189247231); Sarojadevi, H. (56126209900)","57193058845; 58038901900; 57189247231; 56126209900","Object Detection in Computer Vision Using Machine Learning Algorithm For Visually Impaired People","2022","MysuruCon 2022 - 2022 IEEE 2nd Mysore Sub Section International Conference","","","","","","","2","10.1109/MysuruCon55714.2022.9972494","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85145356151&doi=10.1109%2fMysuruCon55714.2022.9972494&partnerID=40&md5=2b14033abaed1f28a27354d819f6aab5","Nitte Meenakshi Institute of Technology, Department of Computer Science and Engineering, Bengaluru, India","Bolla D.R., Nitte Meenakshi Institute of Technology, Department of Computer Science and Engineering, Bengaluru, India; Trisheela S., Nitte Meenakshi Institute of Technology, Department of Computer Science and Engineering, Bengaluru, India; Nagarathana P., Nitte Meenakshi Institute of Technology, Department of Computer Science and Engineering, Bengaluru, India; Sarojadevi H., Nitte Meenakshi Institute of Technology, Department of Computer Science and Engineering, Bengaluru, India","Traditionally, blind individuals go to their destinations with a white cane or a guide dog. They were unable to quickly recognize their surroundings, though. Consequently, The development of a navigation system for visually impaired people in an interior environment is the topic of this study. In order to provide an efficient and user-friendly navigation tool, passive radio frequency identification (RFID) transponders are mounted on the floor, such as on tactile pavement, to establish such RFID networks. the navigational system created includes a digital compass to help persons who are blind or visually handicapped walk correctly and in the right direction, especially while turning. This system makes use of the ideas of localization and placement using a digital compass as well as vocal prompts for directions. Others looked into how to get blind people back onto the right path when they go off course and calibrate the digital compass for accuracy. Additionally, in order to determine the practicality of the developed navigation system, a comparison between two subjects - a mobile robot and a human - is done. The results of the experiment show that a mobile robot and a human move at different speeds. People who are blind or visually impaired will benefit from this project since the voice-activated navigation system will improve their travel experience and make it safer and more comfortable. © 2022 IEEE.","Machine learning; object detection; visually impaired","Learning algorithms; Machine learning; Mobile robots; Navigation systems; Object recognition; Radio frequency identification (RFID); Radio navigation; Robot vision; Blind individuals; Digital compass; Guide dogs; Interior environments; Machine learning algorithms; Machine-learning; Objects detection; Visually impaired; Visually impaired people; White cane; Object detection","","","Institute of Electrical and Electronics Engineers Inc.","","978-166549790-9","","","English","MysuruCon - IEEE Mysore Sub Sect. Int. Conf.","Conference paper","Final","","Scopus","2-s2.0-85145356151"
"Afif M.; Ayachi R.; Said Y.; Pissaloux E.; Atri M.","Afif, Mouna (57194068439); Ayachi, Riadh (57210106980); Said, Yahia (53867137900); Pissaloux, Edwige (7003850978); Atri, Mohamed (23017853700)","57194068439; 57210106980; 53867137900; 7003850978; 23017853700","An efficient object detection system for indoor assistance navigation using deep learning techniques","2022","Multimedia Tools and Applications","81","12","","16601","16618","17","8","10.1007/s11042-022-12577-w","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85125596639&doi=10.1007%2fs11042-022-12577-w&partnerID=40&md5=2425e3d66c7358e3bc9464bdf50c12b1","Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia; Electrical Engineering Department, College of Engineering, Northern Border University, Arar, Saudi Arabia; LITIS Laboratory & CNRS FR 3638 University of Rouen Normandy, Rouen, France; College of Computer Science, King Khalid University, Abha, Saudi Arabia","Afif M., Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia; Ayachi R., Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia; Said Y., Electrical Engineering Department, College of Engineering, Northern Border University, Arar, Saudi Arabia; Pissaloux E., LITIS Laboratory & CNRS FR 3638 University of Rouen Normandy, Rouen, France; Atri M., College of Computer Science, King Khalid University, Abha, Saudi Arabia","Building new systems used for indoor objects detection and indoor assistance navigation presents a very crucial task especially in artificial intelligence and computer science fields. The number of blind and visually impaired persons (VIP) is increasing day by day. In order to help this category of persons, we propose to develop a new indoor object-detection system based on deep convolutional neural networks (DCNNs). The proposed system is developed based on the one-stage neural network RetinaNet. In order to train and evaluate the developed system, we propose to build a new indoor objects dataset which also presents 11,000 images containing 24 indoor landmark objects highly valuable for indoor assistance navigation. The proposed dataset provides a high intra and inter-class variation and various challenging conditions which aim to build a robust detection system for blind and visually impaired people (VIP) mobility. Experimental results prove the high detection performances of the developed indoor objects detection and recognition system. We obtained a detection accuracy reaching up to 98.75% mAP and 62 FPS as a detection speed. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Blind and visually impaired persons; Deep convolutional neural networks (DCNN); Deep learning; Indoor navigation; Indoor object detection","Convolution; Convolutional neural networks; Deep neural networks; Indoor positioning systems; Navigation; Object detection; Blind and visually impaired; Blind and visually impaired person; Deep convolutional neural network; Deep learning; Efficient object detections; Indoor navigation; Indoor object detection; Learning techniques; Object detection systems; Visually impaired persons; Object recognition","M. Afif; Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia; email: mouna.afif@outlook.fr","","Springer","13807501","","MTAPF","","English","Multimedia Tools Appl","Article","Final","","Scopus","2-s2.0-85125596639"
"Bhatlawande S.; Gokhale N.; Mehta D.V.; Gaikwad P.; Shilaskar S.; Madake J.","Bhatlawande, Shripad (55212307900); Gokhale, Neel (58075266600); Mehta, Dewang V. (58074477000); Gaikwad, Parag (58073948600); Shilaskar, Swati (57189017229); Madake, Jyoti (57222127908)","55212307900; 58075266600; 58074477000; 58073948600; 57189017229; 57222127908","Electronic Travel Aid for Crosswalk Detection for Visually Challenged People","2023","Lecture Notes in Electrical Engineering","959","","","249","259","10","0","10.1007/978-981-19-6581-4_20","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85146682600&doi=10.1007%2f978-981-19-6581-4_20&partnerID=40&md5=b6b6ac3efcfe4c2fcbc43a48c79decaf","Electronics and Telecommunication Engineering Department, Vishwakarma Institute of Technology, Pune, 411037, India","Bhatlawande S., Electronics and Telecommunication Engineering Department, Vishwakarma Institute of Technology, Pune, 411037, India; Gokhale N., Electronics and Telecommunication Engineering Department, Vishwakarma Institute of Technology, Pune, 411037, India; Mehta D.V., Electronics and Telecommunication Engineering Department, Vishwakarma Institute of Technology, Pune, 411037, India; Gaikwad P., Electronics and Telecommunication Engineering Department, Vishwakarma Institute of Technology, Pune, 411037, India; Shilaskar S., Electronics and Telecommunication Engineering Department, Vishwakarma Institute of Technology, Pune, 411037, India; Madake J., Electronics and Telecommunication Engineering Department, Vishwakarma Institute of Technology, Pune, 411037, India","Vision is one of the most important sensory systems. It helps us to sense and gauge the surroundings around us. It also helps us to see colors as well as to assess any dangers in our path. Therefore, the proposed system is aimed to aid people who are visually impaired or blind. These people face the most difficulties when they are commuting alone. Independent movement in an unknown environment is strenuous for these people. They need to be always accompanied by someone, especially when crossing roads. The proposed system can be used when crossing the roads to detect a zebra crossing. Although there are a few solutions available today, those are not easy to use. Achieving ease of use with a cheaper cost is the author’s aim in building this system. Computer vision and machine learning models have been used to find a solution to this problem. The authors have prepared a dataset of 5510 images. The system uses SIFT descriptors for feature extraction. K-means and PCA for dimensionality reduction are used. Five ML classifiers are trained, and the performance is assessed. The highest accuracy was achieved by the Random Forest Classifier, which was 89%. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Computer vision; Crosswalk detection; Machine learning; PCA; Python; Random forest classifier; SIFT descriptors; Visually challenged","Classification (of information); Computer vision; Crosswalks; E-learning; K-means clustering; Machine learning; Crosswalk detection; Electronic travel aidss; Machine-learning; PCA; Random forest classifier; Sensory system; SIFT descriptors; Unknown environments; Visually challenged; Visually impaired; Python","S. Bhatlawande; Electronics and Telecommunication Engineering Department, Vishwakarma Institute of Technology, Pune, 411037, India; email: shripad.bhatlawande@vit.edu","Kulkarni A.J.; Mirjalili S.; Udgata S.K.","Springer Science and Business Media Deutschland GmbH","18761100","978-981196580-7","","","English","Lect. Notes Electr. Eng.","Conference paper","Final","","Scopus","2-s2.0-85146682600"
"Rajeswari R.P.; Aradhana B.","Rajeswari, R.P. (57208898915); Aradhana, B. (58102639600)","57208898915; 58102639600","Character Recognition in Scene Images Using MSER and CNN","2022","Communications in Computer and Information Science","1697 CCIS","","","99","107","8","0","10.1007/978-3-031-22405-8_8","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85148001368&doi=10.1007%2f978-3-031-22405-8_8&partnerID=40&md5=b015dbacb837e0fc65cbe4d48ef497be","Rao Bahadur Y Mahabaleswarappa Engineering College, Karnataka, Ballari, 583104, India; Ballari Institute of Technology and Management, Karnataka, Ballari, 583104, India","Rajeswari R.P., Rao Bahadur Y Mahabaleswarappa Engineering College, Karnataka, Ballari, 583104, India; Aradhana B., Ballari Institute of Technology and Management, Karnataka, Ballari, 583104, India","Text detection in Scene images has procured significance in recent decade. Due to its diversified applications in blind navigation assistance for Visually impaired, traffic monitoring, Automatic driving assistance systems etc., Text detection has stimulated new research avenues in area of computer vision Text detection is a trivial task because of varying color, font face and size, orientation of text against complex background. A diversity of deep learning techniques are introduced by researchers for graphical text detection in images. The article proposed method consisting of 3 stages. First, we use Otsu’s method for text separation from background. Secondly Text ROI’s are extracted using Maximally stable Ensemble method (MSER). Finally, each extracted text ROI is classified using ConvNets. CNN classifier have been trained to recognize Scene Text Characters. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Classification; CNN; MSER; Text detection","Automobile drivers; Deep learning; Image classification; Learning systems; Text processing; Automatic driving; Blind navigation; Complex background; Driving assistance systems; Learning techniques; MSER; Navigation assistance for visually impaired; Scene image; Text detection; Traffic monitoring; Character recognition","R.P. Rajeswari; Rao Bahadur Y Mahabaleswarappa Engineering College, Ballari, Karnataka, 583104, India; email: rajeswarirp@rymec.in","Guru D.S.; Y. H. S.K.; K. B.; Agrawal R.K.; Ichino M.","Springer Science and Business Media Deutschland GmbH","18650929","978-303122404-1","","","English","Commun. Comput. Info. Sci.","Conference paper","Final","","Scopus","2-s2.0-85148001368"
"Mendis G.L.M.M.; Deshan W.M.Y.; Bandara H.M.G.M.; Gunethilake K.C.; Wijendra D.; Krishara J.","Mendis, G.L.M.M. (58560734200); Deshan, W.M.Y. (58559625800); Bandara, H.M.G.M. (58560422100); Gunethilake, K.C. (58560734300); Wijendra, Dinuka (57193736232); Krishara, Jenny (57353919200)","58560734200; 58559625800; 58560422100; 58560734300; 57193736232; 57353919200","Look AI - An Intelligent System for Socialization of Visually Impaired","2023","2023 6th International Conference on Artificial Intelligence and Big Data, ICAIBD 2023","","","","351","356","5","0","10.1109/ICAIBD57115.2023.10206191","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85169610523&doi=10.1109%2fICAIBD57115.2023.10206191&partnerID=40&md5=52494cad7d4bc6fcfa45442b96261933","Sri Lanka Institute of Information Technology, Department of Software Engineering, Colombo, Sri Lanka; Sri Lanka Institute of Information Technology, Department of Information Technology, Colombo, Sri Lanka","Mendis G.L.M.M., Sri Lanka Institute of Information Technology, Department of Software Engineering, Colombo, Sri Lanka; Deshan W.M.Y., Sri Lanka Institute of Information Technology, Department of Software Engineering, Colombo, Sri Lanka; Bandara H.M.G.M., Sri Lanka Institute of Information Technology, Department of Information Technology, Colombo, Sri Lanka; Gunethilake K.C., Sri Lanka Institute of Information Technology, Department of Information Technology, Colombo, Sri Lanka; Wijendra D., Sri Lanka Institute of Information Technology, Department of Information Technology, Colombo, Sri Lanka; Krishara J., Sri Lanka Institute of Information Technology, Department of Information Technology, Colombo, Sri Lanka","It has found that 30% of the people among the visually impaired people are having a significant number of depressive symptoms whereas, the prevalence of depression in blind people was reported to be 33%, or nearly double the rate of the general population. Therefore, social activities among visually impaired people should increase. This research paper provides a unique system for assisting visually impaired individuals in navigating indoor environments heavily relies on the use of artificial intelligence, particularly deep learning and computer vision techniques. By leveraging these advanced technologies, the system can analyze and process large amounts of visual data in real-time, accurately identifying and locating various objects in the environment, including faces, doors, stairs, cross walks, traffic lights, potholes and other obstacles. The system then provides real-time feedback to the user via an audio interface, enabling them to navigate indoor and outdoor environments safely and independently. The use of artificial intelligence in this system is particularly significant, as it enables the system to adapt and improve over time based on user feedback and additional data. That means, the system can continuously improve its performance and accuracy, ultimately resulting in a more effective and reliable tool for visually impaired individuals. The authors have also evaluated the system in a real-world setting, demonstrating its effectiveness in assisting visually impaired individuals with indoor navigation. Overall, the proposed system has the potential to significantly enhance the quality of life for visually impaired individuals, showcasing the transformative power of artificial intelligence in addressing real-world challenges. © 2023 IEEE.","artificial intelligence; convolutional neural networks; machine learning; recurrent neural network","Convolutional neural networks; Intelligent systems; Learning systems; Blind people; Convolutional neural network; Depressive symptom; General population; Indoor environment; Machine-learning; Research papers; Social activities; Visually impaired; Visually impaired people; Recurrent neural networks","G.L.M.M. Mendis; Sri Lanka Institute of Information Technology, Department of Software Engineering, Colombo, Sri Lanka; email: it19986654@my.sliit.lk","","Institute of Electrical and Electronics Engineers Inc.","","978-166549125-9","","","English","Int. Conf. Artif. Intell. Big Data, ICAIBD","Conference paper","Final","","Scopus","2-s2.0-85169610523"
"Shidore M.; Kakurle S.; Shetkar M.; Kapshe M.; Kamble A.","Shidore, Mrunal (57218177745); Kakurle, Sahil (58783927100); Shetkar, Manish (58783321000); Kapshe, Malhar (58783927200); Kamble, Abhijeet (58784082100)","57218177745; 58783927100; 58783321000; 58783927200; 58784082100","Eye for Blind: A Deep Learning-Based Sensory Navigation System for the Blind","2023","Lecture Notes in Networks and Systems","782 LNNS","","","229","240","11","0","10.1007/978-981-99-6568-7_21","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85180745411&doi=10.1007%2f978-981-99-6568-7_21&partnerID=40&md5=4522415eab8d43a217bb1cea7bd81e91","Vishwakarma Institute of Technology, Bibwewadi, Pune, India","Shidore M., Vishwakarma Institute of Technology, Bibwewadi, Pune, India; Kakurle S., Vishwakarma Institute of Technology, Bibwewadi, Pune, India; Shetkar M., Vishwakarma Institute of Technology, Bibwewadi, Pune, India; Kapshe M., Vishwakarma Institute of Technology, Bibwewadi, Pune, India; Kamble A., Vishwakarma Institute of Technology, Bibwewadi, Pune, India","The research proposes a new system to assist visually impaired individuals in navigating their surroundings by introducing an “Eye for Blind” system that employs Convolutional Neural Networks (CNN). Our approach utilizes a CNN model to recognize images and generates audio feedback with descriptive labels. This innovative system has the potential to help visually impaired individuals detect and recognize photos, enhancing their independence and quality of life. We evaluated our proposed system using publicly available datasets, and the outcomes suggest that our approach has a high level of accuracy in recognizing social media posts, demonstrating its potential as a solution for the visually impaired community. By using the Eye for Blind system, visually impaired individuals will be able to navigate their surroundings more effectively and efficiently, eliminating their dependence on assistance from others. This technology has enormous potential to improve the quality of life for visually impaired individuals and can contribute to the development of other similar systems that can enhance their independence and mobility. In conclusion, our proposed system has significant potential to make a positive impact on the lives of visually impaired individuals, and we hope that it can be further developed and implemented in the future. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","CNN; LSTM; Neural network; Object","Convolutional neural networks; Image enhancement; Long short-term memory; Neural network models; Audio feedbacks; Convolutional neural network; Innovative systems; LSTM; Neural network model; Neural-networks; Object; Quality of life; Systems for the blinds; Visually impaired; Navigation systems","M. Shidore; Vishwakarma Institute of Technology, Pune, Bibwewadi, India; email: mrunal.shidore@vit.edu","Fong S.; Dey N.; Joshi A.","Springer Science and Business Media Deutschland GmbH","23673370","978-981996567-0","","","English","Lect. Notes Networks Syst.","Conference paper","Final","","Scopus","2-s2.0-85180745411"
"Athulya N.K.; Ramachandran S.; George N.; Ambily N.; Shine L.","Athulya, N.K. (58631570700); Ramachandran, Sivakumar (57190345683); George, Neetha (57209531409); Ambily, N. (56513960800); Shine, Linu (57214071974)","58631570700; 57190345683; 57209531409; 56513960800; 57214071974","Enhancing Outdoor Mobility and Environment Perception for Visually Impaired Individuals Through a Customized CNN-based System","2023","International Journal of Advanced Computer Science and Applications","14","9","","1051","1058","7","0","10.14569/IJACSA.2023.01409109","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85173161452&doi=10.14569%2fIJACSA.2023.01409109&partnerID=40&md5=0975d42154ed357875b8fca3e37d6772","Department of Electronics and Communication, College of Engineering Trivandrum, India; Department of Electronics and Communication, Government Engineering College Wayanad, India; Department of Electronics and Communication, Rajeev Gandhi Institute of Technology Kottayam, India; Department of Electronics and Communication, Government Engineering College Idukki, India","Athulya N.K., Department of Electronics and Communication, College of Engineering Trivandrum, India; Ramachandran S., Department of Electronics and Communication, Government Engineering College Wayanad, India; George N., Department of Electronics and Communication, Rajeev Gandhi Institute of Technology Kottayam, India; Ambily N., Department of Electronics and Communication, Government Engineering College Idukki, India; Shine L., Department of Electronics and Communication, Rajeev Gandhi Institute of Technology Kottayam, India","Visual impairment indicates any kind of vision loss including blindness. Individuals with visual impairments face significant challenges when trying to perceive their surroundings from a global perspective and navigating unfamiliar environments. Existing assistive technologies predominantly focus on obstacle avoidance, neglecting to provide comprehensive information about the overall environment. To address this gap, the proposed system employs a customized Convolutional Neural Network (CNN) model tailored to accurately predict the type of outdoor ground terrain the user is traversing. This information is then conveyed to the user audibly. It can also detect the presence of puddles on the road and let the user know whether the outside floor is wet (slippery). The proposed deep-learning architecture is trained on images collected from sources including the Stagnant Water dataset, the GTOS-Mobile dataset and a custom dataset. The trained model is then integrated into an Android app, providing visually impaired (VI) people with effective surrounding perception capabilities, leading to better travel and, ultimately, better living. © (2023), (Science and Information Organization). All Rights Reserved.","deep learning; puddle detection; terrain identification; Visually impaired","Deep learning; Transfer learning; Vision; Convolutional neural network; Deep learning; Environment perceptions; Global perspective; Network based systems; Puddle detection; Terrain identification; Vision loss; Visual impairment; Visually impaired; Convolutional neural networks","","","Science and Information Organization","2158107X","","","","English","Intl. J. Adv.  Comput. Sci. Appl.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85173161452"
"Agrawal S.; West M.E.; Hayes B.","Agrawal, Shivendra (57208160774); West, Mary Etta (57222250836); Hayes, Bradley (55638866400)","57208160774; 57222250836; 55638866400","A Novel Perceptive Robotic Cane with Haptic Navigation for Enabling Vision-Independent Participation in the Social Dynamics of Seat Choice","2022","IEEE International Conference on Intelligent Robots and Systems","2022-October","","","9156","9163","7","7","10.1109/IROS47612.2022.9981219","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85146354490&doi=10.1109%2fIROS47612.2022.9981219&partnerID=40&md5=e423a57f893929aa9da500858cf71bc7","University of Colorado Boulder, Department of Computer Science, United States","Agrawal S., University of Colorado Boulder, Department of Computer Science, United States; West M.E., University of Colorado Boulder, Department of Computer Science, United States; Hayes B., University of Colorado Boulder, Department of Computer Science, United States","Goal-based navigation in public places is critical for independent mobility and for breaking barriers that exist for blind or visually impaired (BVI) people in a sight-centric society. Through this work we present a proof-of-concept system that autonomously leverages goal-based navigation assistance and perception to identify socially preferred seats and safely guide its user towards them in unknown indoor environments. The robotic system includes a camera, an IMU, vibrational motors, and a white cane, powered via a backpack-mounted laptop. The system combines techniques from computer vision, robotics, and motion planning with insights from psychology to perform 1) SLAM and object localization, 2) goal disambiguation and scoring, and 3) path planning and guidance. We introduce a novel 2-motor haptic feedback system on the cane's grip for navigation assistance. Through a pilot user study we show that the system is successful in classifying and providing haptic navigation guidance to socially preferred seats, while optimizing for users' convenience, privacy, and intimacy in addition to increasing their confidence in independent navigation. The implications are encouraging as this technology, with careful design guided by the BVI community, can be adopted and further developed to be used with medical devices enabling the BVI population to better independently engage in socially dynamic situations like seat choice. © 2022 IEEE.","","Indoor positioning systems; Motion planning; Robot programming; Robot vision; Visual servoing; Breakings; Concept Systems; Goal-based navigation; Haptics; Indoor environment; Proof of concept; Public places; Social dynamics; Visually impaired; Visually impaired people; Navigation","","","Institute of Electrical and Electronics Engineers Inc.","21530858","978-166547927-1","85RBA","","English","IEEE Int Conf Intell Rob Syst","Conference paper","Final","","Scopus","2-s2.0-85146354490"
"Bhatlawande S.; Agrawal H.; Jagdale A.; Agrawal A.; Shilaskar S.","Bhatlawande, Shripad (55212307900); Agrawal, Harshita (58529383400); Jagdale, Adhiraj (57852842700); Agrawal, Anusha (57852628700); Shilaskar, Swati (57189017229)","55212307900; 58529383400; 57852842700; 57852628700; 57189017229","Electronic Travel Aid for Bus Detection and Bus-Route Number Recognition for Blind People","2023","Lecture Notes in Electrical Engineering","1056 LNEE","","","71","78","7","0","10.1007/978-981-99-3656-4_8","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85174513766&doi=10.1007%2f978-981-99-3656-4_8&partnerID=40&md5=1aafae257c2e98027d28045f8ed881c4","Department of Electronics and Telecommunication Engineering, Vishwakarma Institute of Technology, Pune, 411038, India","Bhatlawande S., Department of Electronics and Telecommunication Engineering, Vishwakarma Institute of Technology, Pune, 411038, India; Agrawal H., Department of Electronics and Telecommunication Engineering, Vishwakarma Institute of Technology, Pune, 411038, India; Jagdale A., Department of Electronics and Telecommunication Engineering, Vishwakarma Institute of Technology, Pune, 411038, India; Agrawal A., Department of Electronics and Telecommunication Engineering, Vishwakarma Institute of Technology, Pune, 411038, India; Shilaskar S., Department of Electronics and Telecommunication Engineering, Vishwakarma Institute of Technology, Pune, 411038, India","This paper proposes a novel camera-based bus detection and route number recognition system for visually impaired people. Visually impaired people face several challenges in their daily life of which mobility is a major issue. A major challenge is safe and independent commuting by making use of public transportation systems like buses. Visually impaired people are unable to recognize a particular public bus and read the route numbers. The aids currently available have limited functionalities and are based on technologies that have a steep learning curve. The main feature of the proposed system is that it is effectively able to detect the bus and recognize the bus number in a cluttered and noisy scene using SIFT features, OCR, and various normalization processes. It then tells the destination of the bus as an audio output to the visually impaired person. Steps were also taken to ensure feature scaling is done, so that no feature gets overlooked. The five algorithms discussed in the paper are decision tree which gave an accuracy of 87.63%, random forest that provided an accuracy of 83.84%, support vector machine that gave an accuracy of 83.75%, K-nearest neighbor that provided an accuracy of 82.68%, and logistic regression that gave an accuracy of 83.59%. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Aid for visually impairment; Classification; Computer vision; Machine learning; Number recognition; Object detection; OCR; SIFT","Bus transportation; Buses; Decision trees; E-learning; Learning systems; Nearest neighbor search; Object detection; Support vector machines; Aid for visually impairment; Blind people; Bus route; Electronic travel aidss; Machine-learning; Number recognition; Objects detection; SIFT; Visually impaired people; Visually impairments; Computer vision","S. Bhatlawande; Department of Electronics and Telecommunication Engineering, Vishwakarma Institute of Technology, Pune, 411038, India; email: shripad.bhatlawande@vit.edu","Chakraborty B.; Chakraborty B.; Biswas A.; Chakrabarti A.","Springer Science and Business Media Deutschland GmbH","18761100","978-981993655-7","","","English","Lect. Notes Electr. Eng.","Conference paper","Final","","Scopus","2-s2.0-85174513766"
"Moon H.-S.; Seo J.","Moon, Hee-Seung (57192929320); Seo, Jiwon (56092498500)","57192929320; 56092498500","Sample-Efficient Training of Robotic Guide Using Human Path Prediction Network","2022","IEEE Access","10","","","104996","105007","11","1","10.1109/ACCESS.2022.3210932","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85139389749&doi=10.1109%2fACCESS.2022.3210932&partnerID=40&md5=5455526c01083b162861acb58267e588","Yonsei University, School of Integrated Technology, Incheon, 21983, South Korea","Moon H.-S., Yonsei University, School of Integrated Technology, Incheon, 21983, South Korea; Seo J., Yonsei University, School of Integrated Technology, Incheon, 21983, South Korea","Training a robot that engages with people is challenging; it is expensive to directly involve people in the training process, which requires numerous data samples. This paper presents an alternative approach for resolving this problem. We propose a human path prediction network (HPPN) that generates a user's future trajectory based on sequential robot actions and human responses using a recurrent-neural-network structure. Subsequently, an evolution-strategy-based robot training method using only the virtual human movements generated using the HPPN is presented. It is demonstrated that our proposed method permits sample-efficient training of a robotic guide for visually impaired people. By collecting only 1.5 K episodes from real users, we were able to train the HPPN and generate more than 100 K virtual episodes required for training the robot. The trained robot precisely guided blindfolded participants along a target path. Furthermore, using virtual episodes, we investigated a new reward design that prioritizes human comfort during the robot's guidance without incurring additional costs. This sample-efficient training method is expected to be widely applicable to future robots that interact physically with humans.  © 2022 IEEE.","Blind navigation; evolution strategy; humana-robot interaction; recurrent neural network; robotic guide","Data visualization; Human computer interaction; Human robot interaction; Job analysis; Machine design; Machinery; Personnel training; Robot vision; Virtual reality; Blind navigation; Evolution strategies; Human response; Humans-robot interactions; Path prediction; Robot training; Robot vision systems; Robotic guides; Task analysis; Training process; Recurrent neural networks","J. Seo; Yonsei University, School of Integrated Technology, Incheon, 21983, South Korea; email: jiwon.seo@yonsei.ac.kr","","Institute of Electrical and Electronics Engineers Inc.","21693536","","","","English","IEEE Access","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85139389749"
"Ananth C.; Thenmozhi P.; Vadakkan D.J.; Kavitha S.; Kumar T.A.","Ananth, Christo (57198803549); Thenmozhi, P. (57223007139); Vadakkan, Densy John (58067762900); Kavitha, S. (59012876600); Kumar, T. Ananth (57191486735)","57198803549; 57223007139; 58067762900; 59012876600; 57191486735","Artificial Intelligence based Visual Aid with Live Tracking of Visually Impaired People","2022","Proceedings of International Conference on Technological Advancements in Computational Sciences, ICTACS 2022","","","","573","577","4","3","10.1109/ICTACS56270.2022.9988614","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85146368366&doi=10.1109%2fICTACS56270.2022.9988614&partnerID=40&md5=ba2eb7b364427dfbed3072d28d39cfb3","Samarkand State University, Department of Natural and Exact Sciences, Uzbekistan; St. Joseph's College of Engineering, Department of Electronics and Communication Engineering, Chennai, India; College of Computer Studies, University of Technology, Bahrain; Kalasalingam Academy of Research and Education, Department of Mechanical Engineering, Krishnankoil, India; Computer Science and Engineering, Ifet College of Engineering, Tamilnadu, Villupuram, India","Ananth C., Samarkand State University, Department of Natural and Exact Sciences, Uzbekistan; Thenmozhi P., St. Joseph's College of Engineering, Department of Electronics and Communication Engineering, Chennai, India; Vadakkan D.J., College of Computer Studies, University of Technology, Bahrain; Kavitha S., Kalasalingam Academy of Research and Education, Department of Mechanical Engineering, Krishnankoil, India; Kumar T.A., Computer Science and Engineering, Ifet College of Engineering, Tamilnadu, Villupuram, India","Blind people cannot do things like navigate on their own, recognize objects, or avoid obstacles because they cannot see where they are going or what is around them. In the future, there will be an article about a new visual aid for people who are completely deaf or blind. As a result, the Raspberry Pi 3 Model B+ was chosen because it is cheap, small, and easy to work with other systems. This model was made to show how the prototype would work and how it would look. The system's obstacle avoidance configuration includes a camera and other sensors and object detection and tracking algorithms to help it avoid things. People can use ultrasonic technology and a camera to figure out how far away they are from a specific object or person, so they can figure out how far away they are. As an extra layer of protection, the GPS system tracks people's movements who cannot see. The products will be lightweight and easy to move, making them easy to move. Regular eyeglasses can be used as a base for these glasses so that you do not have to spend extra money or make things more complicated. Using this idea to make eyeglasses would be easy, and it would not make them more expensive or more complicated. The price or complexity of eyeglasses would not change if we made one of these products, so it would be easy to put one in.  © 2022 IEEE.","Artificial Intelligence; Global Positioning System; Internet of Things (IoT); Raspberry Pi-3B","Artificial intelligence; Cameras; Internet of things; Object detection; Tracking (position); Ultrasonic applications; Avoid obstacles; Blind people; Internet of thing; Object detection algorithms; Object detection and tracking; Object tracking algorithm; Obstacles avoidance; Raspberry pi-3b; Visual aids; Visually impaired people; Global positioning system","","","Institute of Electrical and Electronics Engineers Inc.","","978-166547657-7","","","English","Proc. Int. Conf. Technol. Adv. Comput. Sci., ICTACS","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85146368366"
"Tanya T.; Singh S.; Tripathi S.; Kapoor S.; Saini A.S.","Tanya, Tanya (58195670400); Singh, Saurabh (58262509600); Tripathi, Subrat (58195812200); Kapoor, Shivani (58195368700); Saini, Amanpreet Singh (26664970500)","58195670400; 58262509600; 58195812200; 58195368700; 26664970500","Obstruction Avoidance Framework for Indoor and Outdoor Navigation to Enhance Mobility of Visually Impaired Person","2023","2023 International Conference on Computational Intelligence, Communication Technology and Networking, CICTN 2023","","","","548","552","4","1","10.1109/CICTN57981.2023.10140455","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85163206455&doi=10.1109%2fCICTN57981.2023.10140455&partnerID=40&md5=0d3ef4bd84ec16d94415ff4bb2d11d1f","Galgotias College of Engineering and Technology, U.P., Greater Noida, India","Tanya T., Galgotias College of Engineering and Technology, U.P., Greater Noida, India; Singh S., Galgotias College of Engineering and Technology, U.P., Greater Noida, India; Tripathi S., Galgotias College of Engineering and Technology, U.P., Greater Noida, India; Kapoor S., Galgotias College of Engineering and Technology, U.P., Greater Noida, India; Saini A.S., Galgotias College of Engineering and Technology, U.P., Greater Noida, India","Blindness and visual impairments in computer vision are among the top ten disabilities affecting people, with India having the largest percentage of visually impaired people worldwide. An object detection and recognition framework is being created to aid in helping these people navigate and remain safe in their surroundings. This system uses a camera to capture an image and feed it as input to the software. The Single Shot MultiBox Detector (SSD) architecture is employed for object detection, utilizing deep neural networks to achieve accurate results. The input image is processed using the COCO datasets, which act as the system's training data and are predefined in the Tensor Flow framework. About 90% of objects in the actual world have features in these datasets, and depth estimation is used to calculate distance. The software then produces an audio output with the assistance of voice packages. Python is used to build the system because it has many built-in packages and libraries that make complicated code easier to understand. As a result, writing programmes with fewer lines of code is simpler. © 2023 IEEE.","blind; COCO Dataset; deep learning; real-time object; Tensor Flow","Codes (symbols); Deep neural networks; Indoor positioning systems; Object detection; Object recognition; Python; Blind; COCO dataset; Deep learning; Indoor navigation; Outdoor navigation; Real- time; Real-time object; Tensor flow; Visual impairment; Visually impaired persons; Tensors","T. Tanya; Galgotias College of Engineering and Technology, Greater Noida, U.P., India; email: tanya080999@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","","979-835033802-7","","","English","Int. Conf. Comput. Intell., Commun. Technol. Netw., CICTN","Conference paper","Final","","Scopus","2-s2.0-85163206455"
"Srikanth N.V.; Sai N.P.; Pandurangan R.","Srikanth, N Venkata (57201657163); Sai, Narra Prem (58838868300); Pandurangan, Raji (55324530600)","57201657163; 58838868300; 55324530600","Embedded Computer Vision for Object Recognition in Smart Devices for the Blind","2023","International Conference on Sustainable Communication Networks and Application, ICSCNA 2023 - Proceedings","","","","1635","1641","6","0","10.1109/ICSCNA58489.2023.10370696","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85182980223&doi=10.1109%2fICSCNA58489.2023.10370696&partnerID=40&md5=dad9d1fba2a240840bd99cc2dd0767f6","Saveetha Engineering College, Department of Electronics and Communication Engineering, Tamilnadu, India","Srikanth N.V., Saveetha Engineering College, Department of Electronics and Communication Engineering, Tamilnadu, India; Sai N.P., Saveetha Engineering College, Department of Electronics and Communication Engineering, Tamilnadu, India; Pandurangan R., Saveetha Engineering College, Department of Electronics and Communication Engineering, Tamilnadu, India","This study has proposed a novel method for assisting the visually impaired people by combining computer vision with cutting-edge technological instruments. Convolutional Neural Networks (CNNs) are primarily utilized for performing realtime object recognition. To address the object identification challenge, this study collects and preprocesses a large dataset, construct an efficient CNN architecture, optimize inference speed, and connect this system to smart devices. The results indicate significant gains in precision and throughput for practical applications. This innovation may significantly enhance the quality of life for the visually impaired by increasing their mobility. The research study has far-reaching implications for assistive technology and sheds light on the possibility of making the world more accessible for those with visual impairments. Long-term, intends to enhance the consumer product by enlarging the dataset, refining the model's precision, and investigating additional features. This research is a crucial first step in utilizing technology to provide equal opportunities to individuals with visual impairments. © 2023 IEEE.","Computer Vision; Convolutional Neural Network (CNN); Object Recognition; Smart Devices; Visual Impairment","Assistive technology; Consumer products; Convolution; Convolutional neural networks; Large datasets; Object recognition; Convolutional neural network; Cutting edges; Embedded computer vision; Novel methods; Objects recognition; Real-time object recognition; Smart devices; Visual impairment; Visually impaired people; Computer vision","","","Institute of Electrical and Electronics Engineers Inc.","","979-835031398-7","","","English","Int. Conf. Sustain. Commun. Networks Appl., ICSCNA - Proc.","Conference paper","Final","","Scopus","2-s2.0-85182980223"
"Yang Z.; Yang L.; Kong L.; Wei A.; Leaman J.; Brooks J.; Li B.","Yang, Zongming (57324089000); Yang, Liang (57192695601); Kong, Liren (57982956200); Wei, Ailin (57216151894); Leaman, Jesse (37067358900); Brooks, Johnell (7402788618); Li, Bing (57199786158)","57324089000; 57192695601; 57982956200; 57216151894; 37067358900; 7402788618; 57199786158","SeeWay: Vision-Language Assistive Navigation for the Visually Impaired","2022","Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics","2022-October","","","52","58","6","5","10.1109/SMC53654.2022.9945087","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85142718385&doi=10.1109%2fSMC53654.2022.9945087&partnerID=40&md5=6ac041a850874dc247d5dc8cf809a127","Clemson University International Center for Automotive Research (CU-ICAR), Automotive Engineering Department, Greenville, SC, United States; The City College of New York, New York, NY, United States; Clemson University, Computer Science Department, Clemson, SC, United States; Clemson University, Bioengineering Department, Clemson, SC, United States","Yang Z., Clemson University International Center for Automotive Research (CU-ICAR), Automotive Engineering Department, Greenville, SC, United States; Yang L., The City College of New York, New York, NY, United States; Kong L., Clemson University, Computer Science Department, Clemson, SC, United States; Wei A., Clemson University, Bioengineering Department, Clemson, SC, United States; Leaman J., Clemson University International Center for Automotive Research (CU-ICAR), Automotive Engineering Department, Greenville, SC, United States; Brooks J., Clemson University International Center for Automotive Research (CU-ICAR), Automotive Engineering Department, Greenville, SC, United States; Li B., Clemson University International Center for Automotive Research (CU-ICAR), Automotive Engineering Department, Greenville, SC, United States","Assistive navigation for blind or visually impaired (BVI) individuals is of significance to extend their mobility and safety in traveling, enhancing their employment opportunities and fostering personal fulfillment. Conventional research is mainly based on robotic navigation approaches through localization, mapping, and path planning frameworks. They require heavy manual annotation of semantic information in maps and its alignment with sensor mapping. Inspired by the fact that we human beings naturally rely on language instruction inquiry and visual scene understanding to navigate in an unfamiliar environment, this paper proposes a novel vision-language model-based approach for BVI navigation. It does not need heavy-labeled indoor maps and provides a Safe and Efficient E-Wayfinding (SeeWay) assistive solution for BVI individuals. The system consists of a scene-graph map construction module, a navigation path generation module for global path inference by vision-language navigation (VLN), and a navigation with obstacle avoidance module for real-time local navigation. The SeeWay system was deployed on portable iPhone devices with cloud computing assistance for the VLN model inference. The field tests show the effectiveness of the VLN global path finding and local path re-planning. Experiments and quantitative results reveal that heuristic-style instruction outperforms direction/detailed-style instructions for VLN success rate (SR), and the SR decreases as the navigation length increases.  © 2022 IEEE.","Assistive Devices; Scene-Graph Map; The Blind or Visually Impaired; Vision-Language Navigation","Air navigation; Motion planning; Robot programming; Semantics; Visual languages; Visual servoing; Assistive devices; Assistive navigations; Employment opportunities; Robotic navigation; Scene-graph map; Scene-graphs; The blind or visually impaired; Vision-language navigation; Visually impaired; Way finding; Mapping","B. Li; Clemson University International Center for Automotive Research (CU-ICAR), Automotive Engineering Department, Greenville, United States; email: bli4@clemson.edu","","Institute of Electrical and Electronics Engineers Inc.","1062922X","978-166545258-8","PICYE","","English","Conf. Proc. IEEE Int. Conf. Syst. Man Cybern.","Conference paper","Final","","Scopus","2-s2.0-85142718385"
"Joseph S.; Dhananjaya V.","Joseph, Shalini (58674386400); Dhananjaya, V. (57195314330)","58674386400; 57195314330","Object Detection and Localization for Visually Impaired People","2023","2023 International Conference on Network, Multimedia and Information Technology, NMITCON 2023","","","","","","","1","10.1109/NMITCON58196.2023.10276255","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85175400012&doi=10.1109%2fNMITCON58196.2023.10276255&partnerID=40&md5=2463baf396d276c10e6d7a3358b6c4f6","Nmit, Computer Science & Engineering, Bangalore, India; Impact College of Engineering, Computer Science & Engineering, Bangalore, India","Joseph S., Nmit, Computer Science & Engineering, Bangalore, India; Dhananjaya V., Impact College of Engineering, Computer Science & Engineering, Bangalore, India","Blind and visually impaired (VI) people have limited mobility because they cannot sense their surroundings or the terrain. They require constant help and walking aids to get through their everyday activities. Solutions that were put forth decades ago are now advancing quickly as a result of technical development and integration. While some notions have remained as research ideas, many assistance tools have been used in practical settings. Our goal is to enable blind people to travel independently by giving them the capacity to recognize nearby items and navigate to the object of interest with the use of an object recognition approach. A vital function of the human visual system, recovering the identification of adjacent objects within a restricted range is the goal of this. The feature extraction and matching algorithm is coupled with a voice-activated navigation system and a visual substitution system to identify and locate the object in photos. The proposed technology effectively locates and allows for the movement of blind people.  © 2023 IEEE.","CNN; Machine Learning Deep Learning; Object detection; Text to Speech; Yolo Algorithm","Air navigation; Deep learning; Navigation systems; Object detection; Blind and visually impaired; Blind people; Machine learning deep learning; Machine-learning; Object detection and localizations; Objects detection; Text to speech; Visually impaired people; Walking aid; Yolo algorithm; Object recognition","","","Institute of Electrical and Electronics Engineers Inc.","","979-835030082-6","","","English","Int. Conf. Netw., Multimed. Inf. Technol., NMITCON","Conference paper","Final","","Scopus","2-s2.0-85175400012"
"Bhatlawande S.; Aney Y.; Gaikwad A.; Anantwar V.; Shilaskar S.; Madake J.","Bhatlawande, Shripad (55212307900); Aney, Yash (58305522400); Gaikwad, Aatreya (58304912000); Anantwar, Vedant (58304305400); Shilaskar, Swati (57189017229); Madake, Jyoti (57222127908)","55212307900; 58305522400; 58304912000; 58304305400; 57189017229; 57222127908","Road Surface Classification and Obstacle Detection for Visually Impaired People","2023","Springer Proceedings in Mathematics and Statistics","403","","","57","68","11","0","10.1007/978-3-031-16178-0_6","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85161221842&doi=10.1007%2f978-3-031-16178-0_6&partnerID=40&md5=725c3a6416e59bf6477b88498bb47207","Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Pune, India","Bhatlawande S., Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Pune, India; Aney Y., Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Pune, India; Gaikwad A., Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Pune, India; Anantwar V., Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Pune, India; Shilaskar S., Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Pune, India; Madake J., Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Pune, India","This paper proposes an aid for the visually impaired which would guide or navigate them through the streets of India by helping them differentiate between the different kinds of roads, namely, tar and cement. Navigation for the visually impaired could turn out to be very dangerous due to obvious reasons. The solution presented would not just help them navigate through the roads but also make them independent to a certain extent. The system would also detect any objects in front of them and apprise the user making them a lot more aware of their surroundings. The current aids available for the visually impaired are costly. The proposed system is a lightweight, low-power embedded system that can be carried in a bag. It detects obstacles up to 4 m to avoid cognitive overload. The solution proposed in this work is detecting the different kinds of roads to guide the blind person to walk on the correct path while constantly informing them about their surroundings. For the proposed system, SIFT is used as a feature descriptor. Feature reduction is carried out using principal component analysis on seven clusters which were created using a k-means clustering algorithm. The system used four classifiers and compared their performance. The classifier used in this work is decision tree, KNN, random forest, and support vector machine (OVR) with classification accuracy coming out to be 83.03%, 83.93%, 88.54%, and 85.14%, respectively. The random forest classifier is the best performer of all. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Decision tree; KNN; Obstacle detection; Random forest; Road surface detection; SVM; Vision; Visually impaired","K-means clustering; Navigation; Nearest neighbor search; Object detection; Obstacle detectors; Principal component analysis; Roads and streets; Support vector machines; KNN; Obstacles detection; Random forests; Road surface detection; Road surfaces; Surface Classification; Surface detection; Surface obstacles; SVM; Visually impaired; Decision trees","S. Shilaskar; Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Pune, India; email: swati.shilaskar@vit.edu","Misra R.; Kesswani N.; Rajarajan M.; Veeravalli B.; Brigui I.; Patel A.; Singh T.N.","Springer","21941009","978-303116177-3","","","English","Springer Proc. Math. Stat.","Conference paper","Final","","Scopus","2-s2.0-85161221842"
"Kayalvizhi S.; Roshni S.; Ponraj R.; Priya Dharshini S.","Kayalvizhi, S. (58423374700); Roshni, S. (58020380300); Ponraj, Riya (58266441600); Priya Dharshini, S. (58725618400)","58423374700; 58020380300; 58266441600; 58725618400","A Comprehensive Study on Supermarket Indoor Navigation for Visually Impaired using Computer Vision Techniques","2023","2022 OPJU International Technology Conference on Emerging Technologies for Sustainable Development, OTCON 2022","","","","","","","1","10.1109/OTCON56053.2023.10114030","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85159779975&doi=10.1109%2fOTCON56053.2023.10114030&partnerID=40&md5=22ea95fcefefe42680fde4e6cb3e917b","Computer Science and Engineering, Easwari Engineering College, Chennai, India","Kayalvizhi S., Computer Science and Engineering, Easwari Engineering College, Chennai, India; Roshni S., Computer Science and Engineering, Easwari Engineering College, Chennai, India; Ponraj R., Computer Science and Engineering, Easwari Engineering College, Chennai, India; Priya Dharshini S., Computer Science and Engineering, Easwari Engineering College, Chennai, India","The ability to navigate is a fundamental skill that every person must have. People who are visually challenged need regular support when travelling from one place to another. It is difficult for them to have an autonomous shopping experience in a supermarket. The project's goal is to make one of the daily life's task easier for visually impaired persons. The project entails aiding visually impaired customers by guiding them to the destination of the corresponding product sections using an indoor navigation technique. The program aids in providing the visually impaired with shopping environment as well as assistance on how to buy their preferred goods. When logging into the Android app, the user or someone who is blind will speak about their shopping list preferences (voice input). The camera is turned on to provide the live-streaming video. The user can then begin navigating indoors to reach the section where the product is found. This incorporates the technology that is used in automated self-driving cars. Modeled highdefinition simulated maps of the supermarkets are used to navigate inside employing the shortest distance. The first item from the list is selected, and the user's location is identified by matching the features in the image frame with the map and used to compute the path to the product section from there. It provides speech output for navigation. Someone who assists the people to get the products from shelves will help them to get the exact item. The buying process continues until the last item on the list is bought. As a result, this will make tasks comparatively easier for visually impaired persons and make indoor navigation possible.  © 2023 IEEE.","CNN; deep learning; image recognition; indoor navigation; machine learning; neural networks; SLAM; voice to text","Character recognition; Computer vision; Deep learning; Indoor positioning systems; Navigation; Retail stores; Computer vision techniques; Deep learning; Indoor navigation; Machine-learning; Neural-networks; Project goals; SLAM; Visually impaired; Visually impaired persons; Voice to text; Image recognition","","","Institute of Electrical and Electronics Engineers Inc.","","978-166549294-2","","","English","OPJU Int. Technol. Conf. Emerg. Technol. Sustain. Dev., OTCON","Conference paper","Final","","Scopus","2-s2.0-85159779975"
"Liu T.; Hernandez J.; Gonzalez-Franco M.; Maselli A.; Kneisel M.; Glass A.; Chudge J.; Miller A.","Liu, Tiffany (57673061700); Hernandez, Javier (7403026064); Gonzalez-Franco, Mar (36080251200); Maselli, Antonella (6701337146); Kneisel, Melanie (57215331495); Glass, Adam (57215354010); Chudge, Jarnail (57672510100); Miller, Amos (57221318994)","57673061700; 7403026064; 36080251200; 6701337146; 57215331495; 57215354010; 57672510100; 57221318994","Characterizing and Predicting Engagement of Blind and Low-Vision People with an Audio-Based Navigation App","2022","Conference on Human Factors in Computing Systems - Proceedings","","","411","","","","3","10.1145/3491101.3519862","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85129712592&doi=10.1145%2f3491101.3519862&partnerID=40&md5=575074b69af9435038d44250518dabb3","Stanford University, United States; Microsoft Research, United States; Institute of Cognitive Sciences and Technologies, Italy","Liu T., Stanford University, United States; Hernandez J., Microsoft Research, United States; Gonzalez-Franco M., Microsoft Research, United States; Maselli A., Institute of Cognitive Sciences and Technologies, Italy; Kneisel M., Microsoft Research, United States; Glass A., Microsoft Research, United States; Chudge J., Microsoft Research, United States; Miller A., Microsoft Research, United States","Audio-based navigation technologies can help people who are blind or have low vision (BLV) with more independent navigation, mobility, and orientation. We explore how such technologies are incorporated into their daily lives using machine learning models trained on the engagement patterns of over 4,700 BLV people. For mobile navigation apps, we identify user engagement features like the duration of first-time use and engagement with spatial audio callouts that are greatly relevant to predicting user retention. This work contributes a more holistic understanding of important features associated with user retention and strong app usage, as well as insight into the exploration of ambient surroundings as a compelling use case for assistive navigation apps. Finally, we provide design implications to improve the accessibility and usability of audio-based navigation technology. © 2022 ACM.","app engagement prediction; behavior modeling; blind navigation; machine learning; mobile apps; spatial audio; user engagement; user modeling","E-learning; Machine learning; Navigation; App engagement prediction; Audio-based navigation; Behaviour models; Blind navigation; Low vision; Mobile app; Navigation technology; Spatial audio; User engagement; User Modelling; Forecasting","","","Association for Computing Machinery","","978-145039156-6","","","English","Conf Hum Fact Comput Syst Proc","Conference paper","Final","","Scopus","2-s2.0-85129712592"
"Bouteraa Y.","Bouteraa, Yassine (57202361442)","57202361442","Smart real time wearable navigation support system for BVIP","2023","Alexandria Engineering Journal","62","","","223","235","12","7","10.1016/j.aej.2022.06.060","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85135160613&doi=10.1016%2fj.aej.2022.06.060&partnerID=40&md5=caf27e8e54ba064ef9880b7219743b79","College of Computer Engineering and Sciences, Prince Sattam bin Abdulaziz University, Al-Kharj, 11942, Saudi Arabia; Control and Energy Management Laboratory (CEM Lab.), Ecole Nationale d Ingenieurs de Sfax (ENIS) & Institut Superieur de Biotechnologie de Sfax (ISBS), University of Sfax, Sfax, 3038, Tunisia","Bouteraa Y., College of Computer Engineering and Sciences, Prince Sattam bin Abdulaziz University, Al-Kharj, 11942, Saudi Arabia, Control and Energy Management Laboratory (CEM Lab.), Ecole Nationale d Ingenieurs de Sfax (ENIS) & Institut Superieur de Biotechnologie de Sfax (ISBS), University of Sfax, Sfax, 3038, Tunisia","In this paper, we develop a smart navigation aid system for blind and visually impaired people (BVIP). The proposed design revolves around a decision support system based on fuzzy logic, a Raspberry Pi4 board for real-time processing, a set of high-performance sensors, and a haptic voice interface to guide the user. The control architecture is based on the Robot Operating System (ROS) which takes care of connecting all the different nodes of the system and generates the decision in the form of a voice haptic message. A security assessment system is implemented using sensor data fusion and a fuzzy classifier to determine the human security path. Experimental tests carried out by BVIP in different environments have shown the effectiveness of the developed solution. © 2022 THE AUTHOR","Assistive technology; Blind and visually impaired people; Fuzzy classifier; Navigation aid; Obstacles detection, sensors data fusion; Wearable devices","Assistive technology; Data visualization; Fuzzy logic; Fuzzy sets; Haptic interfaces; Navigation; Robot Operating System; Robots; Sensor data fusion; Wearable sensors; Assistive technology; Blind and visually impaired; Blind and visually impaired people; Fuzzy classifiers; Navigation aids; Obstacle detection sensors; Obstacle detection, sensor data fusion; Sensors data fusion; Visually impaired people; Wearable devices; Decision support systems","","","Elsevier B.V.","11100168","","","","English","Alexandria Engineering Journal","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85135160613"
"Paswan V.K.; Choudhary A.","Paswan, Vivek Kumar (58736064700); Choudhary, Ayesha (23388226500)","58736064700; 23388226500","Camera Based Indoor Object Detection and Distance Estimation Framework for Assistive Mobility","2022","2022 IEEE International Conference on Service Operations and Logistics, and Informatics, SOLI 2022","","","","","","","1","10.1109/SOLI57430.2022.10294458","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85178373990&doi=10.1109%2fSOLI57430.2022.10294458&partnerID=40&md5=a27cd0f8186f9d408773074a8c094b3e","School of Computer & Systems Sciences, Jawaharlal Nehru University, New Delhi, India","Paswan V.K., School of Computer & Systems Sciences, Jawaharlal Nehru University, New Delhi, India; Choudhary A., School of Computer & Systems Sciences, Jawaharlal Nehru University, New Delhi, India","In this paper, we propose a novel, real-time, deep learning, and computer vision-based indoor object detection and distance estimation framework for assistive mobility. Blind and visually impaired people find it difficult to deal with indoor objects and their location in their daily life. The emerging deep learning technologies can help them to do this task efficiently and conveniently by providing them with information about indoor objects present in their surroundings. In our proposed framework, we have trained the recent YOLOv7 model for indoor object detection. We have used the bounding box parameters to estimate the distance of the detected object from the user. The information on detected objects and estimated distance have been provided to visually impaired users through audio feedback. Our proposed framework will assist visually impaired people in making informed decisions and make them more confident and better prepared during their navigation in an indoor environment.  © 2022 IEEE.","assistive mobility; computer vision; deep learning; Object detection; smart home technology; visually impaired","Automation; Deep learning; Object detection; Object recognition; Assistive mobilities; Camera-based; Deep learning; Detection estimation; Distance estimation; Object distance; Objects detection; Smart Home Technology; Visually impaired; Visually impaired people; Computer vision","","","Institute of Electrical and Electronics Engineers Inc.","","979-835033224-7","","","English","IEEE Int. Conf. Serv. Oper. Logist., Informatics, SOLI","Conference paper","Final","","Scopus","2-s2.0-85178373990"
"Madake J.; Jha M.; Meshram N.; Muhal N.; Naik U.; Bhatlawande S.","Madake, Jyoti (57222127908); Jha, Mayank (58683668200); Meshram, Nirmay (58683433400); Muhal, Narendra (58683668300); Naik, Unmesh (58683668400); Bhatlawande, Shripad (55212307900)","57222127908; 58683668200; 58683433400; 58683668300; 58683668400; 55212307900","Integrating Scene and Text Recognition for Improved Scene Caption to Assist Visually Impaired","2023","Communications in Computer and Information Science","1921 CCIS","","","32","44","12","0","10.1007/978-3-031-45124-9_4","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85175849775&doi=10.1007%2f978-3-031-45124-9_4&partnerID=40&md5=2f8d9648fc5f9189a2f9ea41a5f1c8ff","Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Maharashtra, Pune, India","Madake J., Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Maharashtra, Pune, India; Jha M., Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Maharashtra, Pune, India; Meshram N., Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Maharashtra, Pune, India; Muhal N., Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Maharashtra, Pune, India; Naik U., Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Maharashtra, Pune, India; Bhatlawande S., Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Maharashtra, Pune, India","The paper introduces an innovative real-time system designed to enhance the scene perception of visually impaired individuals. The proposed model leverages a deep learning approach, employing a CNN-LSTM encoder-decoder framework for efficient image captioning. This novel system integrates scene text information using optical character recognition (OCR) to generate meaningful scene captions as audible feedback for blind or visually impaired users. The proposed model is implemented on embedded processing unit, Jetson Nano equipped with monocular camera and mono-earphone. The model extracts scene features and text information to provide detailed descriptions of the environment. The LSTM-CNN model was trained on a Flickr 8K dataset and a custom dataset, specifically tailored to the Indian environment. Eight different CNN architectures, including VGG-16, EfficientNet-V2L, MobileNet-V2, Inception Resnet-V2, Resnet152-V2, EfficientNet-B7, Xception, and NASNet-Large, were tested to compare their scene caption performance with sigmoid and Tanh activation. Extensive evaluations were conducted using metrics such as BLUE, GLUE, METEOR, and RIBES, with VGG-16 and EfficientNet-V2L achieving the highest BLUE scores of 0.491 and 0.489, respectively. The proposed system presents a promising solution to provide visually impaired individuals with essential information about their surroundings through auditory perception for independent mobility and reduced risk for accident. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023.","Blind assistance; CNN; LSTM; OCR; scene caption","Interactive computer systems; Long short-term memory; Real time systems; Risk perception; Blind assistance; Encoder-decoder; Learning approach; LSTM; Real - Time system; Scene caption; Scene recognition; Text information; Text recognition; Visually impaired; Optical character recognition","J. Madake; Department of Electronics and Telecommunication, Vishwakarma Institute of Technology, Pune, Maharashtra, India; email: jyoti.madake@vit.edu","Shaw R.N.; Ghosh A.; Paprzycki M.","Springer Science and Business Media Deutschland GmbH","18650929","978-303145123-2","","","English","Commun. Comput. Info. Sci.","Conference paper","Final","","Scopus","2-s2.0-85175849775"
"Tarik H.; Hassan S.; Naqvi R.A.; Rubab S.; Tariq U.; Hamdi M.; Elmannai H.; Kim Y.J.; Cha J.-H.","Tarik, Hania (58310354100); Hassan, Shahzad (57195333723); Naqvi, Rizwan Ali (55975847900); Rubab, Saddaf (56594904900); Tariq, Usman (14827558600); Hamdi, Monia (55763124900); Elmannai, Hela (55366208300); Kim, Ye Jin (57204799604); Cha, Jae-Hyuk (24472530200)","58310354100; 57195333723; 55975847900; 56594904900; 14827558600; 55763124900; 55366208300; 57204799604; 24472530200","Empowering and conquering infirmity of visually impaired using AI-technology equipped with object detection and real-time voice feedback system in healthcare application","2023","CAAI Transactions on Intelligence Technology","","","","","","","0","10.1049/cit2.12243","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85161514369&doi=10.1049%2fcit2.12243&partnerID=40&md5=df067cc63e1c3715c5422ab85db2b7eb","Ericsson Pakistan, Islamabad, Pakistan; Computer Engineering Department, Bahria University Islamabad, Islamabad, Pakistan; Department of Unmanned Vehicle Engineering, Sejong University, Seoul, South Korea; Department of Computer Engineering, College of Computing and Informatics, University of Sharjah, Sharjah, United Arab Emirates; Department of Management Information Systems, CoBA, Prince Sattam Bin Abdulaziz University, Al Kharj, Saudi Arabia; Department of Information Technology, College of Computer and Information Sciences, Princess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia; Department of Computer Science, Hanyang University, Seoul, South Korea","Tarik H., Ericsson Pakistan, Islamabad, Pakistan; Hassan S., Computer Engineering Department, Bahria University Islamabad, Islamabad, Pakistan; Naqvi R.A., Department of Unmanned Vehicle Engineering, Sejong University, Seoul, South Korea; Rubab S., Department of Computer Engineering, College of Computing and Informatics, University of Sharjah, Sharjah, United Arab Emirates; Tariq U., Department of Management Information Systems, CoBA, Prince Sattam Bin Abdulaziz University, Al Kharj, Saudi Arabia; Hamdi M., Department of Information Technology, College of Computer and Information Sciences, Princess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia; Elmannai H., Department of Information Technology, College of Computer and Information Sciences, Princess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia; Kim Y.J., Department of Computer Science, Hanyang University, Seoul, South Korea; Cha J.-H., Department of Computer Science, Hanyang University, Seoul, South Korea","The Internet of Things is emerging as a crucial technology in aiding humans and making their lives easier. Among the human population, a large percentage of people suffer from disabilities resulting in challenges in everyday life particularly people with visual disabilities. While several inventions exist to aid people with blindness in their everyday lives, the tools are not adequate in terms of accessibility and efficiency. Smart cane is a useful technology that is being researched and developed to enhance navigation of the visually impaired through smart obstacle detection. Moreover, artificial intelligence is being incorporated in these devices to provide users with a sense of vision and allow them greater independence. However, these devices are expensive and often do not contain the desired functionality in a compact single device. The research by I-CANe aims to create a low-cost, single device system that is equipped with obstacle detection and identification features to enhance navigation for users without using multiple detection devices. The cane will also incorporate wireless communications for safety features such as Global Positioning System tracking and Global System for Mobile communication to allow maximum independence of the user. © 2023 The Authors. CAAI Transactions on Intelligence Technology published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology and Chongqing University of Technology.","deep learning; medical applications","Deep learning; Medical applications; Obstacle detectors; AI Technologies; Crucial technology; Deep learning; Detection time; Feedback systems; Health care application; Objects detection; Obstacles detection; Real-time voice; Visually impaired; Object detection","S. Rubab; Department of Computer Engineering, College of Computing and Informatics, University of Sharjah, Sharjah, United Arab Emirates; email: srubab@sharjah.ac.ae; J.-H. Cha; Department of Computer Science, Hanyang University, Seoul, South Korea; email: chajh@hanyang.ac.kr","","John Wiley and Sons Inc","24686557","","","","English","CAAI Trans. Intell. Technol.","Article","Article in press","All Open Access; Gold Open Access","Scopus","2-s2.0-85161514369"
"Afif M.; Ayachi R.; Said Y.; Atri M.","Afif, Mouna (57194068439); Ayachi, Riadh (57210106980); Said, Yahia (53867137900); Atri, Mohamed (23017853700)","57194068439; 57210106980; 53867137900; 23017853700","An evaluation of EfficientDet for object detection used for indoor robots assistance navigation","2022","Journal of Real-Time Image Processing","19","3","","651","661","10","9","10.1007/s11554-022-01212-4","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85126798336&doi=10.1007%2fs11554-022-01212-4&partnerID=40&md5=6237c73018c1c900a3998a1453585739","Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia; Electrical Engineering Department, College of Engineering, Northern Border University, Arar, Saudi Arabia; College of Computer Science, King Khalid University, Abha, Saudi Arabia","Afif M., Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia; Ayachi R., Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia; Said Y., Electrical Engineering Department, College of Engineering, Northern Border University, Arar, Saudi Arabia; Atri M., College of Computer Science, King Khalid University, Abha, Saudi Arabia","Indoor object detection and recognition present one of the most crucial tasks for computer vision and robotic systems. Developing new intelligent autonomous robots is required in various applications including blind and visually impaired people assistance navigation and smart healthcare. Intelligent robots navigation is still a very challenging problem as it involves various aspects including indoor objects detection, recognition and scene understanding. We propose in this work to develop an indoor object detection system that can be used for intelligent vision of robotics applications. We ensure in this paper a lightweight implementation of the system using EfficientDet neural network. The proposed work presents a vision-based detection system able to work on real mobile robots by studying and considering their limited resources implementations. To ensure a lightweight implementation of the proposed indoor objects detection system and to design a deployable system in mobile robots application, we applied the weights pruning technique. To contribute for an embedded implementation of the proposed system, we used a pruning method which successfully reduced the network size, complexity and computation resources. Experimental results have demonstrated the robustness of the proposed indoor object detection system that can be deployed for indoor robotics assistance navigation systems. Based on the obtained results, we note that the proposed system achieved very competitive results in terms of detection precision as well as processing time. The proposed system can runs in low-end devices as we succeeded to reduce the parameters and FLOPs number, we achieved 89% on the testing set of the proposed indoor data set for EfficientDet D2. We achieved 31 FPS for the basic EfficientDet model and 38 FPS for the pruned model. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Artificial intelligence; Blind and visually impaired; Deep learning; Indoor object detection; Indoor robot navigation; Neural networks; Pruning","Computer vision; Deep learning; Indoor positioning systems; Machine design; Mobile robots; Navigation systems; Object detection; Object recognition; Robotics; Statistical tests; Blind and visually impaired; Deep learning; Indoor object detection; Indoor robot navigation; Indoor robots; Neural-networks; Object detection systems; Pruning; Robot assistance; Robot navigation; Intelligent robots","M. Afif; Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia; email: mouna.afif@outlook.fr","","Springer Science and Business Media Deutschland GmbH","18618200","","","","English","J. Real-Time Image Process.","Article","Final","","Scopus","2-s2.0-85126798336"
"Pereira K.; Patel R.; Almeida J.; Sawant R.","Pereira, Kristen (57821576600); Patel, Rushil (58277493400); Almeida, Joy (58124332800); Sawant, Rupali (36053801200)","57821576600; 58277493400; 58124332800; 36053801200","Voice Assisted Image Captioning and VQA for Visually Challenged Individuals","2022","INDICON 2022 - 2022 IEEE 19th India Council International Conference","","","","","","","0","10.1109/INDICON56171.2022.10040196","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85149230219&doi=10.1109%2fINDICON56171.2022.10040196&partnerID=40&md5=7f075703fba17e48cd5855a965ad8768","Sardar Patel Institute of Technology, Department of Information Technology, Mumbai, India","Pereira K., Sardar Patel Institute of Technology, Department of Information Technology, Mumbai, India; Patel R., Sardar Patel Institute of Technology, Department of Information Technology, Mumbai, India; Almeida J., Sardar Patel Institute of Technology, Department of Information Technology, Mumbai, India; Sawant R., Sardar Patel Institute of Technology, Department of Information Technology, Mumbai, India","Vision is one of the most vital senses for a person's well being. More than 285 million people suffer from the problem of visual impairment. This count is predicted to be three fold in coming 30 years. Independent navigation and safe travel are difficult for these individuals, as they have difficulty perceiving information from their surrounding and communicating. Rather than relying on visual cues to guide blind people, the proposed project will help them navigate the world through the use of audio means. It will empower visually impaired individuals to explore independently by using the system to detect objects in their vicinity and without any outside assistance. Within our proposed research, we have built a mobile application which leverages the power of image processing, and deep learning techniques to identify and describe the current scene through the camera and inform it to the user by audio cues. As a result of not being able to distinguish between different objects, the already existing approaches have been limited, resulting in low performance and accuracy. With this we attempt to provide enhanced performance, better accuracy and hence a more practicable and reliable alternative. © 2022 IEEE.","Computer Vision; Flutter Application; Image Captioning; Natural Language Processing; Visual Question Answering","Deep learning; Learning systems; Natural language processing systems; Object detection; Vision; Visual languages; Flutter application; Image captioning; Language processing; Natural language processing; Natural languages; Performance; Question Answering; Visual impairment; Visual question answering; Well being; Computer vision","K. Pereira; Sardar Patel Institute of Technology, Department of Information Technology, Mumbai, India; email: kristen.pereira@spit.ac.in","","Institute of Electrical and Electronics Engineers Inc.","","978-166547350-7","","","English","INDICON - IEEE India Counc. Int. Conf.","Conference paper","Final","","Scopus","2-s2.0-85149230219"
"Ghatkamble R.; Ratish Kumar K.; John Hrithik S.; Harshith Kumar J.; Sujan P.S.","Ghatkamble, Rajlakshmi (57221101155); Ratish Kumar, K. (58515708000); John Hrithik, S. (58515708100); Harshith Kumar, J. (58515826500); Sujan, P.S. (57222122391)","57221101155; 58515708000; 58515708100; 58515826500; 57222122391","Computer Vision and IoT-Based Smart System for Visually Impaired People","2023","2023 International Conference on Artificial Intelligence and Applications, ICAIA 2023 and Alliance Technology Conference, ATCON-1 2023 - Proceeding","","","","","","","0","10.1109/ICAIA57370.2023.10169589","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85166368298&doi=10.1109%2fICAIA57370.2023.10169589&partnerID=40&md5=39e87a5b9f8adbe37f460dcfae091a64","Information Science and Engineering New Horizon, College of Engineering, Bangalore, India","Ghatkamble R., Information Science and Engineering New Horizon, College of Engineering, Bangalore, India; Ratish Kumar K., Information Science and Engineering New Horizon, College of Engineering, Bangalore, India; John Hrithik S., Information Science and Engineering New Horizon, College of Engineering, Bangalore, India; Harshith Kumar J., Information Science and Engineering New Horizon, College of Engineering, Bangalore, India; Sujan P.S., Information Science and Engineering New Horizon, College of Engineering, Bangalore, India","Moving from one location to another is one of the biggest issues that visually impaired individuals have. For these folks, walking canes that are readily accessible solely act as obstacle sensors. Long overdue is the requirement for an affordable guiding and navigation system for the blind. This paper's major goal is to use ultrasonic technology to broaden the electronic mobility aid for blind and visually impaired walkers. The research described in this article involves designing and implementing an ultrasonic navigation system to offer blind pedestrians completely autonomous obstacle avoidance as well as auditory and tactile feedback. A camera will be used to detect objects higher than knee height. This blind steering method is risk-free, accurate, and efficient. © 2023 IEEE.","Computer Vision; IoT; Machine Learning; Navigation; Object Detection","Internet of things; Machine learning; Navigation systems; Object detection; Ultrasonic applications; Electronic mobility; IoT; Machine-learning; Objects detection; Obstacle sensor; Smart System; Systems for the blinds; Ultrasonic technology; Visually impaired; Visually impaired people; Computer vision","R. Ghatkamble; Information Science and Engineering New Horizon, College of Engineering, Bangalore, India; email: dr.rajlakshmig.nhce@newhorizonindia.edu","","Institute of Electrical and Electronics Engineers Inc.","","978-166545627-2","","","English","Int. Conf. Artif. Intell. Appl., ICAIA Alliance Technol. Conf., ATCON-1 - Proceeding","Conference paper","Final","","Scopus","2-s2.0-85166368298"
"Saber H.M.; Al-Salihi N.K.; Omer R.M.D.","Saber, Hakar Mohsin (57200724388); Al-Salihi, Nawzad Kameran (24576701300); Omer, Rebaz Mohammed Dler (57195507693)","57200724388; 24576701300; 57195507693","Visually Impaired People Navigation System using Sensors and Neural Network","2022","Proceedings of the 2022 IEEE International Conference on Human-Machine Systems, ICHMS 2022","","","","","","","0","10.1109/ICHMS56717.2022.9980818","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85146279326&doi=10.1109%2fICHMS56717.2022.9980818&partnerID=40&md5=d3c6bb636b0e01c95cd3aa02b988baf2","University of Kurdistan Hewlêr, Computer Engineering, Kurdistan Region, Erbil, Iraq","Saber H.M., University of Kurdistan Hewlêr, Computer Engineering, Kurdistan Region, Erbil, Iraq; Al-Salihi N.K., University of Kurdistan Hewlêr, Computer Engineering, Kurdistan Region, Erbil, Iraq; Omer R.M.D., University of Kurdistan Hewlêr, Computer Engineering, Kurdistan Region, Erbil, Iraq","Without a personal assistant or a trained dog, it is difficult and somehow risky for visually impaired individuals to navigate indoors and outdoors. Navigation for individuals with visual impairment is now an active area of research, and numerous systems have been created. Developing a reliable and robust system will give the visually impaired more awareness about their surroundings, provides more independence in their lives, and reduces the risks they might face. This paper proposes the design and implementation of a prototype for a wearable system that enables blind and visually impaired individuals to navigate easily and safely by using computer vision and deep learning to identify objects and sensors to detect obstacles. Using an attached camera, the system uses image processing to recognize objects around the user. It identifies the objects based on a trained deep learning dataset and audibly communicates the object names to the user. Ultrasonic sensors detect nearby obstructions and alert the user by vibrating. Upon request, a Global Positioning System (GPS) receiver transmits the user's location via Short Message Service (SMS) to a caregiver. All the sensors and devices are connected to and controlled by an Arduino Mega microcontroller that has been programmed with efficient and dependable algorithms to perform tasks precisely and rapidly. Object recognition and reading are performed using a computer equipped with the necessary MATLAB software and libraries.  © 2022 IEEE.","Arduino; deep learning; GPS; navigation; neural network; sensors; visually impaired","Deep learning; Learning systems; MATLAB; Navigation; Ultrasonic applications; Active area; Arduino; Deep learning; Neural-networks; Personal assistants; Sensors network; Trained dogs; Visual impairment; Visually impaired; Visually impaired people; Global positioning system","","Kaber D.; Guerrieri A.; Fortino G.; Nurnberger A.","Institute of Electrical and Electronics Engineers Inc.","","978-166545238-0","","","English","Proc. IEEE Int. Conf. Hum.-Mach. Syst., ICHMS","Conference paper","Final","","Scopus","2-s2.0-85146279326"
"Madake J.; Badade M.; Barve M.; Bhatlawande S.; Shilaskar S.","Madake, Jyoti (57222127908); Badade, Mahesh (57874784900); Barve, Mrunal (58073946000); Bhatlawande, Shripad (55212307900); Shilaskar, Swati (57189017229)","57222127908; 57874784900; 58073946000; 55212307900; 57189017229","A Real-Time Detection of Indian Traffic Signs for Visually Impaired People","2023","Lecture Notes in Electrical Engineering","959","","","237","247","10","1","10.1007/978-981-19-6581-4_19","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85146701480&doi=10.1007%2f978-981-19-6581-4_19&partnerID=40&md5=5db1df1ebd91bd78f82094c1adb8f4b5","Vishwakarma Institute of Technology, Pune, India","Madake J., Vishwakarma Institute of Technology, Pune, India; Badade M., Vishwakarma Institute of Technology, Pune, India; Barve M., Vishwakarma Institute of Technology, Pune, India; Bhatlawande S., Vishwakarma Institute of Technology, Pune, India; Shilaskar S., Vishwakarma Institute of Technology, Pune, India","Outdoor navigation on crowded roads is the biggest challenge for visually impaired people. Safe and independent travel for blind people is possible if they are able to recognize the traffic signs. Active and real-time traffic sign recognition is a very crucial requirement for the safety of visually impaired people. The existing blind assistive aids do not support this problem. The aim of this paper is to recognize the Indian traffic signs with the help of SIFT as a feature descriptor. The most frequently used 7 traffic signs were used for identification in the scope of this paper. PCA is used for the optimization of the feature vector. Three classifiers were used for performance analysis and classification which were random forest, K-nearest neighbor, and decision tree. The trained model precisely detects the road traffic signs. The proposed system is an electronic travel aid for blind people with low power requirements and reduced latency. The results state that the recognition using random forest, KNN, and decision tree gives 73%, 71%, and 65% accuracy. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Computer vision; Indian traffic signs; Random forest; SIFT; Traffic sign detection; Visually impaired people","Computer vision; Decision trees; Nearest neighbor search; Signal detection; Blind people; Indian traffic sign; Outdoor navigation; Random forests; Real-time detection; Realtime traffic; SIFT; Traffic sign detection; Traffic sign recognition; Visually impaired people; Traffic signs","J. Madake; Vishwakarma Institute of Technology, Pune, India; email: jyoti.madake@vit.edu","Kulkarni A.J.; Mirjalili S.; Udgata S.K.","Springer Science and Business Media Deutschland GmbH","18761100","978-981196580-7","","","English","Lect. Notes Electr. Eng.","Conference paper","Final","","Scopus","2-s2.0-85146701480"
"Sivaganesan D.; Venkateshwaran M.; Dhinesh S.P.","Sivaganesan, D. (55164679900); Venkateshwaran, M. (58541245600); Dhinesh, S.P. (58541245700)","55164679900; 58541245600; 58541245700","Image to Audio Conversion to Aid Visually Impaired People by CNN","2023","2023 4th International Conference on Electronics and Sustainable Communication Systems, ICESC 2023 - Proceedings","","","","1707","1713","6","2","10.1109/ICESC57686.2023.10193308","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85168308396&doi=10.1109%2fICESC57686.2023.10193308&partnerID=40&md5=9f0d5d146576d823b8b5cf4beb9025a8","Psg Institute of Technology and Applied Research, Department of Computer Science and Engineering, Coimbatore, India; Sri Krishna Arts and Science College, Department of Computer Science, Coimbatore, India","Sivaganesan D., Psg Institute of Technology and Applied Research, Department of Computer Science and Engineering, Coimbatore, India; Venkateshwaran M., Psg Institute of Technology and Applied Research, Department of Computer Science and Engineering, Coimbatore, India; Dhinesh S.P., Sri Krishna Arts and Science College, Department of Computer Science, Coimbatore, India","This study suggests an innovative method for helping people who are blind or visually handicapped by turning visuals into sounds. In the proposed system, audio descriptions are produced in real-time together with significant features that are extracted from photos using deep learning algorithms. The proposed work is developed to be user-friendly, which includes a simple interface that aids blind individuals to easily capture and process images using a mobile device. A user research was undertaken to assess the efficiency of the suggested method, and the results were encouraging in terms of precision and usability. This initiative offers a promising technique to give people who are blind or visually impaired an alternate means of perceiving and interacting with their environment, therefore improving their quality of life. The suggested picture to audio converter system aims to overcome the drawbacks of current assistive devices that rely on braille or textual descriptions. Blind people can more easily interpret visual information that is necessary for daily life, such as recognising items, interpreting signs, or navigate unfamiliar situations, through offering audio descriptions of images. The system makes use of recent deep learning developments that have significantly improved picture identification as natural language processing. As a result, the suggested technique has the ability to offer audio descriptions that are more precise and comprehensive than current methods. This technology has the potential to be implemented into a variety of products, from cellphones to intelligent glasses, and could significantly improve the lives of people who are blind or visually impaired.  © 2023 IEEE.","Artificial Intelligence; Convolutional Neural Networks; Deep learning; Smartphones; Visually impaired","Deep learning; Learning algorithms; Natural language processing systems; Smartphones; Transfer learning; 'current; Audio description; Convolutional neural network; Deep learning; Innovative method; Real- time; Smart phones; Visually handicapped; Visually impaired; Visually impaired people; Convolutional neural networks","D. Sivaganesan; Psg Institute of Technology and Applied Research, Department of Computer Science and Engineering, Coimbatore, India; email: sivaganesan@psgitech.ac.in","","Institute of Electrical and Electronics Engineers Inc.","","979-835030009-3","","","English","Int. Conf. Electron. Sustain. Commun. Syst., ICESC - Proc.","Conference paper","Final","","Scopus","2-s2.0-85168308396"
"Busaeed S.; Katib I.; Albeshri A.; Corchado J.M.; Yigitcanlar T.; Mehmood R.","Busaeed, Sahar (57556223200); Katib, Iyad (26534538800); Albeshri, Aiiad (36617092600); Corchado, Juan M. (7006360842); Yigitcanlar, Tan (6505536041); Mehmood, Rashid (25643246000)","57556223200; 26534538800; 36617092600; 7006360842; 6505536041; 25643246000","LidSonic V2.0: A LiDAR and Deep-Learning-Based Green Assistive Edge Device to Enhance Mobility for the Visually Impaired","2022","Sensors","22","19","7435","","","","13","10.3390/s22197435","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85139940455&doi=10.3390%2fs22197435&partnerID=40&md5=a01e17e7909f0ff2743a16d02fd5f432","Faculty of Computer and Information Sciences, Imam Mohammad Ibn Saud Islamic University, Riyadh, 11564, Saudi Arabia; Department of Computer Science, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, 21589, Saudi Arabia; Bisite Research Group, University of Salamanca, Salamanca, 37007, Spain; Air Institute, IoT Digital Innovation Hub, Salamanca, 37188, Spain; Department of Electronics, Information and Communication, Faculty of Engineering, Osaka Institute of Technology, Osaka, 535-8585, Japan; School of Architecture and Built Environment, Queensland University of Technology, 2 George Street, Brisbane, 4000, QLD, Australia; High Performance Computing Center, King Abdulaziz University, Jeddah, 21589, Saudi Arabia","Busaeed S., Faculty of Computer and Information Sciences, Imam Mohammad Ibn Saud Islamic University, Riyadh, 11564, Saudi Arabia; Katib I., Department of Computer Science, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, 21589, Saudi Arabia; Albeshri A., Department of Computer Science, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, 21589, Saudi Arabia; Corchado J.M., Bisite Research Group, University of Salamanca, Salamanca, 37007, Spain, Air Institute, IoT Digital Innovation Hub, Salamanca, 37188, Spain, Department of Electronics, Information and Communication, Faculty of Engineering, Osaka Institute of Technology, Osaka, 535-8585, Japan; Yigitcanlar T., School of Architecture and Built Environment, Queensland University of Technology, 2 George Street, Brisbane, 4000, QLD, Australia; Mehmood R., High Performance Computing Center, King Abdulaziz University, Jeddah, 21589, Saudi Arabia","Over a billion people around the world are disabled, among whom 253 million are visually impaired or blind, and this number is greatly increasing due to ageing, chronic diseases, and poor environments and health. Despite many proposals, the current devices and systems lack maturity and do not completely fulfill user requirements and satisfaction. Increased research activity in this field is required in order to encourage the development, commercialization, and widespread acceptance of low-cost and affordable assistive technologies for visual impairment and other disabilities. This paper proposes a novel approach using a LiDAR with a servo motor and an ultrasonic sensor to collect data and predict objects using deep learning for environment perception and navigation. We adopted this approach using a pair of smart glasses, called LidSonic V2.0, to enable the identification of obstacles for the visually impaired. The LidSonic system consists of an Arduino Uno edge computing device integrated into the smart glasses and a smartphone app that transmits data via Bluetooth. Arduino gathers data, operates the sensors on the smart glasses, detects obstacles using simple data processing, and provides buzzer feedback to visually impaired users. The smartphone application collects data from Arduino, detects and classifies items in the spatial environment, and gives spoken feedback to the user on the detected objects. In comparison to image-processing-based glasses, LidSonic uses far less processing time and energy to classify obstacles using simple LiDAR data, according to several integer measurements. We comprehensively describe the proposed system’s hardware and software design, having constructed their prototype implementations and tested them in real-world environments. Using the open platforms, WEKA and TensorFlow, the entire LidSonic system is built with affordable off-the-shelf sensors and a microcontroller board costing less than USD 80. Essentially, we provide designs of an inexpensive, miniature green device that can be built into, or mounted on, any pair of glasses or even a wheelchair to help the visually impaired. Our approach enables faster inference and decision-making using relatively low energy with smaller data sizes, as well as faster communications for edge, fog, and cloud computing. © 2022 by the authors.","Arduino Uno; assistive tools; deep learning; edge computing; green computing; LiDAR; obstacle detection; obstacle recognition; sensors; smart app; smart mobility; sustainability; ultrasonic; visually impaired","Deep Learning; Disabled Persons; Humans; Self-Help Devices; Visually Impaired Persons; Wheelchairs; Augmented reality; Behavioral research; Costs; Decision making; Deep learning; Digital storage; Edge computing; Feedback; mHealth; Object detection; Obstacle detectors; Optical radar; Smartphones; Software design; Software testing; Sustainable development; Arduino uno; Assistive tool; Deep learning; Edge computing; LiDAR; Obstacle recognition; Obstacles detection; Smart app; Smart mobility; Visually impaired; disabled person; human; self help device; visually impaired person; wheelchair; Data visualization","R. Mehmood; High Performance Computing Center, King Abdulaziz University, Jeddah, 21589, Saudi Arabia; email: rmehmood@kau.edu.sa","","MDPI","14248220","","","36236546","English","Sensors","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85139940455"
"Dawre J.; Kheria I.; Jani J.; Doshi J.; Mangrulkar R.","Dawre, Jazib (58521402000); Kheria, Ishita (58365356800); Jani, Jay (57224439332); Doshi, Jay (58370207500); Mangrulkar, Ramchandra (55401536500)","58521402000; 58365356800; 57224439332; 58370207500; 55401536500","Smart Walking Stick : A Comprehensive Approach Towards IoT Enabled Stick for Visually Impaired","2022","2022 Sardar Patel International Conference on Industry 4.0 - Nascent Technologies and Sustainability for 'Make in India' Initiative, SPICON 2022","","","","","","","0","10.1109/SPICON56577.2022.10180466","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85166677209&doi=10.1109%2fSPICON56577.2022.10180466&partnerID=40&md5=0deae0cb948edfb0f69dbe5553d2fb42","DJ Sanghvi College of Engineering, Department of Computer Engineering, Mumbai, India","Dawre J., DJ Sanghvi College of Engineering, Department of Computer Engineering, Mumbai, India; Kheria I., DJ Sanghvi College of Engineering, Department of Computer Engineering, Mumbai, India; Jani J., DJ Sanghvi College of Engineering, Department of Computer Engineering, Mumbai, India; Doshi J., DJ Sanghvi College of Engineering, Department of Computer Engineering, Mumbai, India; Mangrulkar R., DJ Sanghvi College of Engineering, Department of Computer Engineering, Mumbai, India","According to the world health center, there are an estimated 36 million visually impaired people, most of them belonging to developing and underdeveloped countries. Many blind people in these countries rely on other people or guide dogs for assistance in their daily routine travel. This paper proposes a smart walking stick based on Raspberry Pi, Arduino, Infrared sensors and Ultrasonic sensors. Various electronic devices have been developed for blind people, but most of them cannot function due to the lack of infrastructure in these countries, as well as the prohibitive cost. Therefore, the paper sought to develop a cheap and workable alternative for these developing and underdeveloped countries. The paper proposes a light walking stick that integrates sensors and micro-controllers. Moreover, the proposed stick is also equipped with a built-in GPS system and includes an alert button that, when pressed, sends a SOS message along with the user's location to saved contacts.  © 2022 IEEE.","Arduino; Computer Vision; IoT; Raspberry Pi; Visually Impaired","Infrared detectors; Internet of things; Arduino; Blind people; Daily routines; Guide dogs; Health centers; Infrared sensor; IoT; Raspberry pi; Visually impaired; Visually impaired people; Computer vision","J. Dawre; DJ Sanghvi College of Engineering, Department of Computer Engineering, Mumbai, India; email: jazib980@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","","978-166546539-7","","","English","Sardar Patel Int. Conf. Indu. 4.0 - Nascent Technol. Sustain. 'Make India' Initiat., SPICON","Conference paper","Final","","Scopus","2-s2.0-85166677209"
"Kayukawa S.; Sato D.; Murata M.; Ishihara T.; Kosugi A.; Takagi H.; Morishima S.; Asakawa C.","Kayukawa, Seita (57203517444); Sato, Daisuke (55396865500); Murata, Masayuki (57195134352); Ishihara, Tatsuya (16238506200); Kosugi, Akihiro (55786159200); Takagi, Hironobu (7402956932); Morishima, Shigeo (7005317462); Asakawa, Chieko (6603028733)","57203517444; 55396865500; 57195134352; 16238506200; 55786159200; 7402956932; 7005317462; 6603028733","How Users, Facility Managers, and Bystanders Perceive and Accept a Navigation Robot for Visually Impaired People in Public Buildings","2022","RO-MAN 2022 - 31st IEEE International Conference on Robot and Human Interactive Communication: Social, Asocial, and Antisocial Robots","","","","546","553","7","10","10.1109/RO-MAN53752.2022.9900717","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85140750420&doi=10.1109%2fRO-MAN53752.2022.9900717&partnerID=40&md5=1cfd197ce23d25c6e51b184fbc6e2982","Waseda University, Japan; Miraikan-The National Museum of Emerging Science and Innovation; Carnegie Mellon University, United States; Ibm Research; Waseda Research Institute for Science and Engineering, Japan","Kayukawa S., Waseda University, Japan, Miraikan-The National Museum of Emerging Science and Innovation; Sato D., Carnegie Mellon University, United States; Murata M., Ibm Research; Ishihara T., Ibm Research; Kosugi A., Ibm Research; Takagi H., Waseda University, Japan, Miraikan-The National Museum of Emerging Science and Innovation; Morishima S., Waseda Research Institute for Science and Engineering, Japan; Asakawa C., Miraikan-The National Museum of Emerging Science and Innovation, Ibm Research","Autonomous navigation robots have a considerable potential to offer a new form of mobility aid to people with visual impairments. However, to deploy such robots in public buildings, it is imperative to receive acceptance from not only robot users but also people that use the buildings and managers of those facilities. Therefore, we conducted three studies to investigate the acceptance and concerns of our prototype robot, which looks like a regular suitcase. First, an online survey revealed that people could accept the robot navigating blind users. Second, in the interviews with facility managers, they were cautious about the robot's camera and the privacy of their customers. Finally, focus group sessions with legally blind participants who experienced the robot navigation revealed that the robot may cause trouble when it collides with those who may not be aware of the user's blindness. Still, many participants liked the design of the robot which assimilated into the surroundings. © 2022 IEEE.","","Buildings; Machine design; Managers; Navigation; Autonomous navigation; Blind users; Facilities Managers; Mobility aids; New forms; Online surveys; Prototype robot; Public buildings; Visual impairment; Visually impaired people; Robots","","","Institute of Electrical and Electronics Engineers Inc.","","978-172818859-1","","","English","RO-MAN - IEEE Int. Conf. Robot Hum. Interact. Commun.: Soc., Asoc., Antisocial Robot.","Conference paper","Final","","Scopus","2-s2.0-85140750420"
"Oureshi M.S.; Khan I.U.; Qureshi S.B.; Khan F.M.; Aleshaiker S.","Oureshi, Muhammad Shuaib (58736046300); Khan, Inam Ullah (57958437100); Qureshi, SMuhammad Bilal (55922030800); Khan, Fida Muhammad (57219203258); Aleshaiker, Sama (57968482500)","58736046300; 57958437100; 55922030800; 57219203258; 57968482500","Empowering the Blind: AI-Assisted Solutions for Visually Impaired People","2023","Proceedings of 2023 IEEE International Smart Cities Conference, ISC2 2023","","","","","","","0","10.1109/ISC257844.2023.10293380","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85178373092&doi=10.1109%2fISC257844.2023.10293380&partnerID=40&md5=0ac9ce4cf4ad936fcc3e8af874751bdf","Chungnam National University, Department of Computer Engineering, Daejeon, South Korea; University of Lakki Marwat, Department of Computer Science & It, KPK, Pakistan; University of Science & Technology, Department of Computer Science, Bannu, Pakistan; School of Computing and Engineering, University of West London, United Kingdom","Oureshi M.S., Chungnam National University, Department of Computer Engineering, Daejeon, South Korea; Khan I.U., University of Lakki Marwat, Department of Computer Science & It, KPK, Pakistan; Qureshi S.B., University of Lakki Marwat, Department of Computer Science & It, KPK, Pakistan; Khan F.M., University of Science & Technology, Department of Computer Science, Bannu, Pakistan; Aleshaiker S., School of Computing and Engineering, University of West London, United Kingdom","Physical disability has an influence on individuals everywhere. One of these disabilities that significantly affect a large number of individuals is visual loss. Basic mobility problems, including as crossing the street, reading, driving, and socializing, are commonly a problem for blind people. To go around or complete other daily duties, they typically need on particular assistance devices, including walking sticks. There is still more work to be done in the field of blindness treatment, despite ongoing scientific research. In addition, research has suggested solutions to the problems faced by blind individuals, but these solutions need the necessary technology support. The goal of this research project is to employ smart devices to make daily routines simpler for blind persons of all categories. This smart gadget can recognize faces, different objects, and colors by employing artificial intelligence and picture processing. The notification of the visually impaired individual results from the detecting procedure and might take the form of a vibration or sound alarm. This study also includes a tangible survey that includes local residents who are blind or visually handicapped. In order to program and implement the project, Open CV and Python are both used. This project prototype's effort looks into the methods that are employed for object detection. It also shows how this smart gadget can recognize certain physical objects and how it may alert the user when faced with hurdles. By helping blind individuals with the use of smart technology, this research will, overall, be a helpful contribution to the field of healthcare.  © 2023 IEEE.","Artificial Intelligence; Face Recognition; Open CV; Smart Education; Smart Health","Artificial intelligence; Computer software; Object detection; Python; Walking aids; Blind individuals; Blind people; Open CV; Physical disability; Scientific researches; Smart education; Smart health; Technology support; Visual loss; Visually impaired people; Face recognition","","","Institute of Electrical and Electronics Engineers Inc.","","979-835039775-8","","","English","Proc. IEEE Int. Smart Cities Conf., ISC2","Conference paper","Final","","Scopus","2-s2.0-85178373092"
"Yang A.; Beheshti M.; Hudson T.E.; Vedanthan R.; Riewpaiboon W.; Mongkolwat P.; Feng C.; Rizzo J.-R.","Yang, Anbang (57223727680); Beheshti, Mahya (57208333398); Hudson, Todd E. (7203069715); Vedanthan, Rajesh (24823609000); Riewpaiboon, Wachara (8642392600); Mongkolwat, Pattanasak (6602619011); Feng, Chen (51461205400); Rizzo, John-Ross (56527876700)","57223727680; 57208333398; 7203069715; 24823609000; 8642392600; 6602619011; 51461205400; 56527876700","UNav: An Infrastructure-Independent Vision-Based Navigation System for People with Blindness and Low Vision","2022","Sensors","22","22","8894","","","","4","10.3390/s22228894","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85142757239&doi=10.3390%2fs22228894&partnerID=40&md5=872779967a7cf4381f34f66a780d3b34","Department of Mechanical and Aerospace Engineering, NYU Tandon School of Engineering, Brooklyn, 11201, NY, United States; Department of Rehabilitation Medicine, NYU Grossman School of Medicine, New York, 10016, NY, United States; Department of Population Health, NYU Grossman School of Medicine, New York, 10016, NY, United States; Department of Academic Services, Ratchasuda College, Mahidol University, Nakhon Pathom, 73170, Thailand; Faculty of Information and Communication Technology, Mahidol University, Nakhon Pathom, 73170, Thailand; Department of Biomedical Engineering, NYU Tandon School of Engineering, Brooklyn, 11201, NY, United States","Yang A., Department of Mechanical and Aerospace Engineering, NYU Tandon School of Engineering, Brooklyn, 11201, NY, United States; Beheshti M., Department of Mechanical and Aerospace Engineering, NYU Tandon School of Engineering, Brooklyn, 11201, NY, United States, Department of Rehabilitation Medicine, NYU Grossman School of Medicine, New York, 10016, NY, United States; Hudson T.E., Department of Rehabilitation Medicine, NYU Grossman School of Medicine, New York, 10016, NY, United States; Vedanthan R., Department of Population Health, NYU Grossman School of Medicine, New York, 10016, NY, United States; Riewpaiboon W., Department of Academic Services, Ratchasuda College, Mahidol University, Nakhon Pathom, 73170, Thailand; Mongkolwat P., Faculty of Information and Communication Technology, Mahidol University, Nakhon Pathom, 73170, Thailand; Feng C., Department of Mechanical and Aerospace Engineering, NYU Tandon School of Engineering, Brooklyn, 11201, NY, United States; Rizzo J.-R., Department of Mechanical and Aerospace Engineering, NYU Tandon School of Engineering, Brooklyn, 11201, NY, United States, Department of Rehabilitation Medicine, NYU Grossman School of Medicine, New York, 10016, NY, United States, Department of Biomedical Engineering, NYU Tandon School of Engineering, Brooklyn, 11201, NY, United States","Vision-based localization approaches now underpin newly emerging navigation pipelines for myriad use cases, from robotics to assistive technologies. Compared to sensor-based solutions, vision-based localization does not require pre-installed sensor infrastructure, which is costly, time-consuming, and/or often infeasible at scale. Herein, we propose a novel vision-based localization pipeline for a specific use case: navigation support for end users with blindness and low vision. Given a query image taken by an end user on a mobile application, the pipeline leverages a visual place recognition (VPR) algorithm to find similar images in a reference image database of the target space. The geolocations of these similar images are utilized in a downstream task that employs a weighted-average method to estimate the end user’s location. Another downstream task utilizes the perspective-n-point (PnP) algorithm to estimate the end user’s direction by exploiting the 2D–3D point correspondences between the query image and the 3D environment, as extracted from matched images in the database. Additionally, this system implements Dijkstra’s algorithm to calculate a shortest path based on a navigable map that includes the trip origin and destination. The topometric map used for localization and navigation is built using a customized graphical user interface that projects a 3D reconstructed sparse map, built from a sequence of images, to the corresponding a priori 2D floor plan. Sequential images used for map construction can be collected in a pre-mapping step or scavenged through public databases/citizen science. The end-to-end system can be installed on any internet-accessible device with a camera that hosts a custom mobile application. For evaluation purposes, mapping and localization were tested in a complex hospital environment. The evaluation results demonstrate that our system can achieve localization with an average error of less than 1 m without knowledge of the camera’s intrinsic parameters, such as focal length. © 2022 by the authors.","PnP; topometric map; visual-based localization; VPR; weighted average","Database systems; Graphical user interfaces; Mapping; Mobile computing; Navigation systems; Pipelines; Query processing; Robots; Statistical methods; End-users; Localisation; Low vision; Perspective n points; Place recognition; Topometric map; Vision based localization; Visual place recognition; Visual-based localization; Weighted averages; Cameras","C. Feng; Department of Mechanical and Aerospace Engineering, NYU Tandon School of Engineering, Brooklyn, 11201, United States; email: cfeng@nyu.edu; J.-R. Rizzo; Department of Mechanical and Aerospace Engineering, NYU Tandon School of Engineering, Brooklyn, 11201, United States; email: johnross.rizzo@nyulangone.org","","MDPI","14248220","","","","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85142757239"
"Madake J.; Bhatlawande S.; Solanke A.; Shilaskar S.","Madake, Jyoti (57222127908); Bhatlawande, Shripad (55212307900); Solanke, Anjali (58412645600); Shilaskar, Swati (57189017229)","57222127908; 55212307900; 58412645600; 57189017229","A Qualitative and Quantitative Analysis of Research in Mobility Technologies for Visually Impaired People","2023","IEEE Access","11","","","82496","82520","24","5","10.1109/ACCESS.2023.3291074","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85163716340&doi=10.1109%2fACCESS.2023.3291074&partnerID=40&md5=fa46766c73fbd5181136e6e3d411189e","Vishwakarma Institute of Technology, Department of Electronics and Telecommunication Engineering, Maharashtra, Pune, 411037, India; Marathwada Mitra Mandal's College of Engineering, Department of Electronics and Telecommunication Engineering, Maharashtra, Pune, 411052, India","Madake J., Vishwakarma Institute of Technology, Department of Electronics and Telecommunication Engineering, Maharashtra, Pune, 411037, India; Bhatlawande S., Vishwakarma Institute of Technology, Department of Electronics and Telecommunication Engineering, Maharashtra, Pune, 411037, India; Solanke A., Marathwada Mitra Mandal's College of Engineering, Department of Electronics and Telecommunication Engineering, Maharashtra, Pune, 411052, India; Shilaskar S., Vishwakarma Institute of Technology, Department of Electronics and Telecommunication Engineering, Maharashtra, Pune, 411037, India","Assistive technology in rehabilitation programs is vital for people with vision impairments worldwide. The term 'blind assistive technology' refers to mobility devices specifically designed to provide position, orientation and mobility assistance for visually impaired individuals during indoor and outdoor activities. The paper presents a comprehensive evaluation of 140 research articles published over the past 75 years (1946 to 2022). This research analyses the evolution of assistive technology aids in depth, in terms sensing technique followed, algorithms employed for obstacle detection, localization, object recognition, depth estimation and scene understanding. It also includes, the functional attributes of the aid, feedback type, and assistive solutions embedded in aid. It evaluates the assistive aids for their usability index, portability, battery life, feedback type, and aesthetics. The survey findings reveal that optical and sonic sensor-based aids prioritize speed, weight, and battery life but lack major functionalities, achieving an average performance score of 62%. Stereo, monocular, SLAM, and 3-D point cloud-based aids excel in obstacle distance estimation and avoidance but require greater memory resources, with a lower performance score of 41%. Artificial intelligence and cloud-based aids offer comprehensive scene details but demand complex computational capabilities, achieving a performance score of 44%. However, the most suitable technology for developing state-of-the-art solutions for blind individuals is the multisensor fusion-based and guide robot-based aids, providing a majority of the essential assistive functions with a performance score of 51%. The study highlights possible challenges associated with implementing assistive technology aids, emphasizes the importance of user acceptability, and stresses the need for real-time evaluation of blind aids. The paper lays a concrete foundation and direction for future development, emphasizing the critical challenges faced by blind users, including boarding trains, traveling on public transport, shopping in a supermarket, avoiding dynamic obstacles, and real-time understanding of the surrounding scene. Addressing these key concerns is crucial for the continued development and improvement of assistive technology aids for the visually impaired, leading to enhanced independence, mobility, and ultimately, a higher quality of life.  © 2013 IEEE.","assistive aids; Blind and visually impaired people; health care; mobility","Assistive technology; Object recognition; Assistive; Assistive aid; Assistive technology; Blind and visually impaired; Blind and visually impaired people; Blindness; Legged locomotion; Mobility; Visual impairment; Visually impaired people; Robots","S. Bhatlawande; Vishwakarma Institute of Technology, Department of Electronics and Telecommunication Engineering, Pune, Maharashtra, 411037, India; email: Shripad.bhatlawande@vit.edu","","Institute of Electrical and Electronics Engineers Inc.","21693536","","","","English","IEEE Access","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85163716340"
"Theodorou P.; Meliones A.","Theodorou, P. (57194044913); Meliones, A. (55949094700)","57194044913; 55949094700","Human–Machine Requirements’ Convergence for the Design of Assistive Navigation Software: Τhe Case of Blind or Visually Impaired People","2022","Learning and Analytics in Intelligent Systems","28","","","263","283","20","1","10.1007/978-3-030-87132-1_12","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85179868183&doi=10.1007%2f978-3-030-87132-1_12&partnerID=40&md5=01903e78a01710380d0fbca3dd5c3b0c","Department of Digital Systems, University of Piraeus, Piraeus, 18534, Greece","Theodorou P., Department of Digital Systems, University of Piraeus, Piraeus, 18534, Greece; Meliones A., Department of Digital Systems, University of Piraeus, Piraeus, 18534, Greece","Autonomous navigation is a desirable capability for various types of “smart” devices or vehicles. The development of software designed for this purpose has become a central research field for major companies, as well as in academia. This trend is accompanied by a tendency to equip moving devices with artificial intelligence (AI) features. Interestingly, however, the most (and not artificially) intelligent unit which may require assistance from digital applications in order to achieve autonomous navigation is a blind or a visually impaired person (BVI). It is argued that as the capabilities of AI are being enhanced, convergence will occur among a significant subset of the requirements concerning assistive navigation software for the BVI and AI-equipped moving devices, respectively. The corresponding requirements which have been elicited through interviews with BVI people are presented. A subset of these requirements, which exhibit direct or prospective convergence with the corresponding requirements of AI devices is then outlined, with emphasis on possible opportunities for interaction between the two research topics. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Artificial intelligence; Assistive navigation software; Autonomous navigation; Blind and visually impaired people; Requirements analysis","Artificial intelligence; Assistive technology; Assistive navigation software; Assistive navigations; Autonomous navigation; Blind and visually impaired; Blind and visually impaired people; Human-machine; Machine requirements; Navigation software; Requirement analysis; Visually impaired people; Navigation","P. Theodorou; Department of Digital Systems, University of Piraeus, Piraeus, 18534, Greece; email: van4.theodorou@gmail.com","","Springer Nature","26623447","","","","English","Learn. Anal. Intell.  Syst.","Book chapter","Final","","Scopus","2-s2.0-85179868183"
"Lei Y.; Phung S.L.; Bouzerdoum A.; Thanh Le H.; Luu K.","Lei, Yunjia (57919124900); Phung, Son Lam (7801638780); Bouzerdoum, Abdesselam (35570413200); Thanh Le, Hoang (57204049393); Luu, Khoa (35217996900)","57919124900; 7801638780; 35570413200; 57204049393; 35217996900","Pedestrian Lane Detection for Assistive Navigation of Vision-Impaired People: Survey and Experimental Evaluation","2022","IEEE Access","10","","","101071","101089","18","7","10.1109/ACCESS.2022.3208128","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85139422275&doi=10.1109%2fACCESS.2022.3208128&partnerID=40&md5=c10394fbb970aa8d635283246aeaf495","University of Wollongong, School of Electrical, Computer and Telecommunications Engineering, Wollongong, 2522, NSW, Australia; Hamad Bin Khalifa University, Division of Information and Computing Technology, College of Science and Engineering, Ar Rayyan, Qatar; Nha Trang University, Faculty of Information Technology, Nha Trang, 650000, Viet Nam; University of Arkansas, Computer Science and Computer Engineering Department, Fayetteville, 72701, AR, United States","Lei Y., University of Wollongong, School of Electrical, Computer and Telecommunications Engineering, Wollongong, 2522, NSW, Australia; Phung S.L., University of Wollongong, School of Electrical, Computer and Telecommunications Engineering, Wollongong, 2522, NSW, Australia; Bouzerdoum A., University of Wollongong, School of Electrical, Computer and Telecommunications Engineering, Wollongong, 2522, NSW, Australia, Hamad Bin Khalifa University, Division of Information and Computing Technology, College of Science and Engineering, Ar Rayyan, Qatar; Thanh Le H., University of Wollongong, School of Electrical, Computer and Telecommunications Engineering, Wollongong, 2522, NSW, Australia, Nha Trang University, Faculty of Information Technology, Nha Trang, 650000, Viet Nam; Luu K., University of Arkansas, Computer Science and Computer Engineering Department, Fayetteville, 72701, AR, United States","Pedestrian lane detection is a crucial task in assistive navigation for vision-impaired people. It can provide information on walkable regions, help blind people stay on the pedestrian lane, and assist with obstacle detection. An accurate and real-time lane detection algorithm can improve travel safety and efficiency for the visually impaired. Despite its importance, pedestrian lane detection in unstructured scenes for assistive navigation has not attracted sufficient attention in the research community. This paper aims to provide a comprehensive review and an experimental evaluation of methods that can be applied for pedestrian lane detection, thereby laying a foundation for future research in this area. Our study covers traditional and deep learning methods for pedestrian lane detection, general road detection, and general semantic segmentation. We also perform an experimental evaluation of the representative methods on a large benchmark dataset that is specifically created for pedestrian lane detection. We hope this paper can serve as an informative guide for researchers in assistive technologies, and facilitate urgently-needed research for vision-impaired people. © 2013 IEEE.","assistive navigation; deep networks; Pedestrian lane detection; semantic segmentation; vision impairment","Deep learning; Large dataset; Navigation; Semantic Segmentation; Semantic Web; Semantics; Assistive navigations; Deep network; Experimental evaluation; Impaired people; Lane detection; Pedestrian lane detection; Pedestrian lanes; Semantic segmentation; Vision impaired; Vision impairments; Obstacle detectors","S.L. Phung; University of Wollongong, School of Electrical, Computer and Telecommunications Engineering, Wollongong, 2522, Australia; email: phung@uow.edu.au","","Institute of Electrical and Electronics Engineers Inc.","21693536","","","","English","IEEE Access","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85139422275"
"Afif M.; Ayachi R.; Atri M.","Afif, Mouna (57194068439); Ayachi, Riadh (57210106980); Atri, Mohamed (23017853700)","57194068439; 57210106980; 23017853700","Indoor objects detection system implementation using multi-graphic processing units","2022","Cluster Computing","25","1","","469","483","14","2","10.1007/s10586-021-03419-9","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85115724662&doi=10.1007%2fs10586-021-03419-9&partnerID=40&md5=d02e58d9c3c7e8fc8a11cc517c317362","Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia; College of Computer Science, King Khalid University, Abha, Saudi Arabia","Afif M., Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia; Ayachi R., Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia; Atri M., College of Computer Science, King Khalid University, Abha, Saudi Arabia","Indoor objects detection and recognition plays an important role in computer science and artificial intelligence fields. This task plays also a crucial role especially for blind and visually impaired persons (VIP) assistance navigation. Aiming to address this problem, we propose in this paper to develop a new indoor object detection system based on deep learning algorithms. Unfortunately, this type of algorithms requires heavy computational resources, and energy consumption. To address this problem, we propose a CUDA multi-GPU framework implementation of the proposed system. Generally deep learning based algorithms require huge amount of data to train and test networks. We propose to develop a new indoor dataset which consists of 11,000 indoor images containing 25 indoor landmark objects highly recommended for blind and VIP navigation. Based on the obtained results, the developed system shows big efficiency in terms of detection accuracy as well as processing time. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Assistance navigation; Deep convolutional neural network (DCNNs); Deep learning; GPU implementation; Indoor object detection; Parallel computing","Convolutional neural networks; Deep neural networks; Energy utilization; Graphics processing unit; Indoor positioning systems; Learning algorithms; Navigation; Object detection; Assistance navigation; Blind and visually impaired; Deep convolutional neural network; Deep learning; GPU implementation; Indoor object detection; Object detection systems; Parallel com- puting; Systems implementation; Visually impaired persons; Object recognition","M. Afif; Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia; email: mouna.afif@outlook.fr","","Springer","13867857","","","","English","Cluster Comput.","Article","Final","","Scopus","2-s2.0-85115724662"
"Rusli S.H.; Liawatimena S.; Trisetyarso A.","Rusli, Satria Hanafi (57972541600); Liawatimena, Suryadiputra (48761523700); Trisetyarso, Agung (36337949500)","57972541600; 48761523700; 36337949500","CONTEXT AWARE NAVIGATION SYSTEM FOR ASSISTING BLIND PEOPLE","2022","ICIC Express Letters","16","12","","1351","1358","7","1","10.24507/icicel.16.12.1351","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85142307109&doi=10.24507%2ficicel.16.12.1351&partnerID=40&md5=31707d24b05d62f102afb9cc276af97e","Computer Science Department, BINUS Graduate Program – Master of Computer Science, Bina Nusantara University, Jl. K. H. Syahdan No. 9, Kemanggisan, Palmerah, Jakarta, 11480, Indonesia; Computer Engineering Department, Faculty of Engineering, Bina Nusantara University, Jl. K. H. Syahdan No. 9, Kemanggisan, Palmerah, Jakarta, 11480, Indonesia; Computer Science Department, BINUS Graduate Program, Doctor of Computer Science Bina Nusantara University, Jl. K. H. Syahdan No. 9, Kemanggisan, Palmerah, Jakarta, 11480, Indonesia","Rusli S.H., Computer Science Department, BINUS Graduate Program – Master of Computer Science, Bina Nusantara University, Jl. K. H. Syahdan No. 9, Kemanggisan, Palmerah, Jakarta, 11480, Indonesia; Liawatimena S., Computer Science Department, BINUS Graduate Program – Master of Computer Science, Bina Nusantara University, Jl. K. H. Syahdan No. 9, Kemanggisan, Palmerah, Jakarta, 11480, Indonesia, Computer Engineering Department, Faculty of Engineering, Bina Nusantara University, Jl. K. H. Syahdan No. 9, Kemanggisan, Palmerah, Jakarta, 11480, Indonesia; Trisetyarso A., Computer Science Department, BINUS Graduate Program, Doctor of Computer Science Bina Nusantara University, Jl. K. H. Syahdan No. 9, Kemanggisan, Palmerah, Jakarta, 11480, Indonesia","In order to assist VI (Visually Impaired people or person) with their navigation task, many ATs (Assistive Technologies) utilizing CV (Computer Vision) have been developed. However, they often come with many disadvantages such as being bulky, not user friendly, high maintenance, depending on other services to function, and expensive. They often only measure the model accuracy. They rarely utilize visual signs despite their advantages. We propose an independent, low-cost, efficient, portable, and user friendly AT system embedded with deep-learning model based on YOLOv4-tiny (You Only Look Once) converted and quantized into TensorFlow Lite (TFLite) model working on Deep Simple Online and Real-time Tracking (DeepSORT) working with sign recognition to help with their navigation problem in their daily life. The performance of the proposed system is technically acceptable. However, improvements on the physical design and information output method for better user experience are needed. ICIC International ©2022.","Assistive technology; Computer vision; Sign recognition; Visual impairment","","S.H. Rusli; Computer Science Department, BINUS Graduate Program – Master of Computer Science, Bina Nusantara University, Jakarta, Jl. K. H. Syahdan No. 9, Kemanggisan, Palmerah, 11480, Indonesia; email: satria.rusli@binus.ac.id","","ICIC International","1881803X","","","","English","ICIC Express Lett.","Article","Final","","Scopus","2-s2.0-85142307109"
"Aljarf A.; Almaghrabi G.; Albarakati H.; Ahmed H.; Alharbi R.; Aljuhani S.","Aljarf, Ahad (55658149600); Almaghrabi, Ghaidaa (58183816300); Albarakati, Halah (58183531800); Ahmed, Haneen (58183672800); Alharbi, Raghad (59158067600); Aljuhani, Sawsan (58183672900)","55658149600; 58183816300; 58183531800; 58183672800; 59158067600; 58183672900","EBSAR: Detecting of Objects that Hinder Visually Impaired in a Controlled Area Using Deep Learning","2022","Proceedings of 2022 5th National Conference of Saudi Computers Colleges, NCCC 2022","","","","19","25","6","3","10.1109/NCCC57165.2022.10067421","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85152582423&doi=10.1109%2fNCCC57165.2022.10067421&partnerID=40&md5=5f5cdac50868d2f86982b86ec56b4f80","Umm Al-Qura University, Dept. Information Systems, Makkah, Saudi Arabia","Aljarf A., Umm Al-Qura University, Dept. Information Systems, Makkah, Saudi Arabia; Almaghrabi G., Umm Al-Qura University, Dept. Information Systems, Makkah, Saudi Arabia; Albarakati H., Umm Al-Qura University, Dept. Information Systems, Makkah, Saudi Arabia; Ahmed H., Umm Al-Qura University, Dept. Information Systems, Makkah, Saudi Arabia; Alharbi R., Umm Al-Qura University, Dept. Information Systems, Makkah, Saudi Arabia; Aljuhani S., Umm Al-Qura University, Dept. Information Systems, Makkah, Saudi Arabia","With more than 285 million visually impaired people globally, there is a critical need for an assistive system that helps blind and visually impaired people to navigate easily. The EBSAR system is proposed to detect the objects using mobile video cameras and make a sound alert with the distance and right direction in Arabic language to the user. In the EBSAR system, the updated Convolutional Neural Networks (CNN) and You Only Look Once version four (YOLOv4) algorithms are used for object detection. Besides, the key matching as these algorithms give more accuracy. The camera of the phone is enough for detecting the objects and no special hardware is required. Thus, the system requires minimal effort from the user to use the system during everyday life.  © 2022 IEEE.","Artificial Intelligence; Convolutional Neural Networks; Object Detection; Visually Impaired; YOLOv4","Convolution; Convolutional neural networks; Deep learning; Object recognition; Video cameras; Arabic languages; Assistive system; Blind and visually impaired; Convolutional neural network; Matchings; Mobile video cameras; Objects detection; Visually impaired; Visually impaired people; YOLOv4; Object detection","A. Aljarf; Umm Al-Qura University, Dept. Information Systems, Makkah, Saudi Arabia; email: amjarf@uqu.edu.sa","Al-Shareef S.; Alsini A.; Alsubait T.M.; Alharbi A.","Institute of Electrical and Electronics Engineers Inc.","","979-835033368-8","","","English","Proc. Natl. Conf. Saudi Comput. Coll., NCCC","Conference paper","Final","","Scopus","2-s2.0-85152582423"
"Hoogsteen K.M.P.; Szpiro S.; Kreiman G.; Peli E.","Hoogsteen, Karst M. P. (58055065400); Szpiro, Sarit (56269780200); Kreiman, Gabriel (57216597570); Peli, Eli (7005237694)","58055065400; 56269780200; 57216597570; 7005237694","Beyond the Cane: Describing Urban Scenes to Blind People for Mobility Tasks","2022","ACM Transactions on Accessible Computing","15","3","3522757","","","","7","10.1145/3522757","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85144066162&doi=10.1145%2f3522757&partnerID=40&md5=27ddc6c2ebd4a8b31f99ca1ba0b95c9d","Schepens Eye Research Institute, Mass Eye and Ear, Department of Ophthalmology, Harvard Medical School, 20 Staniford Street, Boston, MA, United States; Department of Special Education, University of Haifa, Abba Khoushy Ave 199, Haifa, Israel; Boston Children's Hospital, Harvard Medical School, 3 Blackfan Circle, Boston, MA, United States","Hoogsteen K.M.P., Schepens Eye Research Institute, Mass Eye and Ear, Department of Ophthalmology, Harvard Medical School, 20 Staniford Street, Boston, MA, United States; Szpiro S., Department of Special Education, University of Haifa, Abba Khoushy Ave 199, Haifa, Israel; Kreiman G., Boston Children's Hospital, Harvard Medical School, 3 Blackfan Circle, Boston, MA, United States; Peli E., Schepens Eye Research Institute, Mass Eye and Ear, Department of Ophthalmology, Harvard Medical School, 20 Staniford Street, Boston, MA, United States","Blind people face difficulties with independent mobility, impacting employment prospects, social inclusion, and quality of life. Given the advancements in computer vision, with more efficient and effective automated information extraction from visual scenes, it is important to determine what information is worth conveying to blind travelers, especially since people have a limited capacity to receive and process sensory information. We aimed to investigate which objects in a street scene are useful to describe and how those objects should be described. Thirteen cane-using participants, five of whom were early blind, took part in two urban walking experiments. In the first experiment, participants were asked to voice their information needs in the form of questions to the experimenter. In the second experiment, participants were asked to score scene descriptions and navigation instructions, provided by the experimenter, in terms of their usefulness. The descriptions included a variety of objects with various annotations per object. Additionally, we asked participants to rank order the objects and the different descriptions per object in terms of priority and explain why the provided information is or is not useful to them. The results reveal differences between early and late blind participants. Late blind participants requested information more frequently and prioritized information about objects' locations. Our results illustrate how different factors, such as the level of detail, relative position, and what type of information is provided when describing an object, affected the usefulness of scene descriptions. Participants explained how they (indirectly) used information, but they were frequently unable to explain their ratings. The results distinguish between various types of travel information, underscore the importance of featuring these types at multiple levels of abstraction, and highlight gaps in current understanding of travel information needs. Elucidating the information needs of blind travelers is critical for the development of more useful assistive technologies.  © 2022 Association for Computing Machinery.","assistive technologies; Blindness; impaired vision; independence; mobility; navigation; outdoor; scene description","Assistive technology; Computer programming; Assistive technology; Blind people; Blindness; Impaired vision; Independence; Mobility; Outdoor; Scene description; Travel information; Urban scenes; Conveying","","","Association for Computing Machinery","19367228","","","","English","ACM Trans. Accessible Comput.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85144066162"
"Shahani S.; Gupta N.","Shahani, Siddharth (58549318500); Gupta, Nitin (58549141200)","58549318500; 58549141200","The Methods of Visually Impaired Navigating and Obstacle Avoidance","2023","International Conference on Applied Intelligence and Sustainable Computing, ICAISC 2023","","","","","","","1","10.1109/ICAISC58445.2023.10200915","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85168767945&doi=10.1109%2fICAISC58445.2023.10200915&partnerID=40&md5=b9ff783ee4b69b1e18d9735506a597a7","ATLAS Skill Tech University, ISDI - School of Design & Innovation, Mumbai, India; IIMT University, School of Hotel Management Catering and Tourism, Meerut, India","Shahani S., ATLAS Skill Tech University, ISDI - School of Design & Innovation, Mumbai, India; Gupta N., IIMT University, School of Hotel Management Catering and Tourism, Meerut, India","Blindness is a widespread problem that affects people all around the globe. People with this condition have a very difficult time navigating on their own and seeing impediments. Thus, an integrated implementation of the Web of Things & predictive analytics is necessary for properly recognizing impediments. For the blind and visually handicapped to be able to move about freely, they need to be able to sense obstacles and be warned of their presence. First, data on the location and kind of barrier has to be collected, and only then can it be sent to those with visual impairments through other means of communication, such as speech. Using the tensor flow object identification model and the Google voice model, we demonstrate a solution to assist the visually handicapped. The idea has two main parts: environment data and an analytical representation. Using a tensor flow object detection model, it first attempts to analyze the surroundings for obstacles that visually impaired individuals are likely to encounter, and then it tries to relay that knowledge to those persons in the form of spoken words. © 2023 IEEE.","Navigation and Obstacle Detection and Deep Learning Techniques; Visually Impaired People","Deep learning; Learning systems; Object detection; Obstacle detectors; Predictive analytics; Speech communication; Condition; Learning techniques; Navigation and obstacle detection and deep learning technique; Object identification; Obstacles avoidance; Obstacles detection; Visual impairment; Visually handicapped; Visually impaired; Visually impaired people; Tensors","S. Shahani; ATLAS Skill Tech University, ISDI - School of Design & Innovation, Mumbai, India; email: sid@atlasuniversity.edu.in","","Institute of Electrical and Electronics Engineers Inc.","","979-835032379-5","","","English","Int. Conf. Appl. Intell. Sustain. Comput., ICAISC","Conference paper","Final","","Scopus","2-s2.0-85168767945"
"Khandelwal D.; Chhabra R.S.; Jindal S.K.","Khandelwal, Divyansh (58557635200); Chhabra, Rishiraj Singh (57220182198); Jindal, Sumit Kumar (56092549900)","58557635200; 57220182198; 56092549900","A Secure Medical-IoT Device for Assisting the Visually Impaired in External Navigation Using a Portable Braille Pad","2023","Security Implementation in Internet of Medical Things","","","","59","85","26","0","10.1201/9781003269168-4","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85169404329&doi=10.1201%2f9781003269168-4&partnerID=40&md5=b955455d55bcb0a0ce3f23b1a485d25f","Vellore Institute of Technology, Tamil Nadu, India; Vellore Institute of Technology, Tamil Nadu, India; Vellore Institute of Technology, Tamil Nadu, India","Khandelwal D., Vellore Institute of Technology, Tamil Nadu, India; Chhabra R.S., Vellore Institute of Technology, Tamil Nadu, India; Jindal S.K., Vellore Institute of Technology, Tamil Nadu, India","Braille is a form of writing system that was introduced in 1824 to assist blind people to read texts and understand literature to a greater extent. Since then, innovations and advancements are being poured in to further enhance the experience of blind people and make their life better. This work presents a novel IoT-based solution that incorporates machine-to-machine communication to an electronic braille pad that a blind person can use for external navigation. The main advancement brought by this research is the fact that this device is portable and very versatile in terms of compatibility. The proposed system incorporates a mesh of radio beacons to be placed in an outdoor environment, at key and strategic positions which would publish relevant information to the braille devices when within the range. The beacon is a one-time installment that is capable of extracting data from the cloud and processing it from visually readable language to braille. This processed information is then sent across to the electronic braille pad, which acts as a client and receives the data. The data is then displayed on the braille pad with the help of micro actuators. © 2024 selection and editorial matter, Luxmi Sapra, Varun Sapra and Akashdeep Bhardwaj.","","","","","CRC Press","","978-100093401-4; 978-103221603-4","","","English","Security Implement. in Internet of Med. Things","Book chapter","Final","","Scopus","2-s2.0-85169404329"
"S Shrivastava R.; Singhal A.; Chandna S.","S Shrivastava, Rhea (58722585600); Singhal, Abhishek (56198942000); Chandna, Swati (57302046600)","58722585600; 56198942000; 57302046600","Towards Helping Visually Impaired People to Navigate Outdoor","2023","Lecture Notes in Electrical Engineering","1078 LNEE","","","83","92","9","0","10.1007/978-981-99-5974-7_8","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85177813972&doi=10.1007%2f978-981-99-5974-7_8&partnerID=40&md5=d48752420aa9e37262e34681966609e7","Indraprastha Institute of Information Technology, Delhi (IIIT-Delhi), New Delhi, India; Amity University, Uttar Pradesh, Noida, India; SRH University Heidelberg, Heidelberg, Germany","S Shrivastava R., Indraprastha Institute of Information Technology, Delhi (IIIT-Delhi), New Delhi, India; Singhal A., Amity University, Uttar Pradesh, Noida, India; Chandna S., SRH University Heidelberg, Heidelberg, Germany","Vision is one of the crucial senses and is the birthright of every human being. Its impairment or loss leads to various difficulties. Blind and Visually Impaired (BVI) people find it tough to maneuver outdoors daily. Even though the market is laden with countless aids for BVI people, there is still a lot to be achieved. The idea of every new research in the market is to assist these individuals in any possible way. Individuals deprived of vision require numerous reliable methods to overcome these barriers. Also, with the advent of science and technology, there is nothing that a human being can’t do. Researchers and manufacturers are coming up with new inventions and tech gadgets now and then. In this paper, Convolutional Neural Network (CNN) models, vgg16 and vgg19, are used along with the self-created dataset involving two classes: roads and crosswalks, which underwent the ML procedures resulting in the accurate detection of the respective classes. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd 2023.","Blind and visually impaired; BVI; CNN; Crosswalks; Machine learning; Neural network; Roads; VGG16; VGG19; Vision","Convolutional neural networks; Crosswalks; Machine learning; Blind and visually impaired; Convolutional neural network; Human being; Machine-learning; Neural-networks; Road; VGG16; VGG19; Visually impaired people; Commerce","R. S Shrivastava; Indraprastha Institute of Information Technology, Delhi (IIIT-Delhi), New Delhi, India; email: rheas@iiitd.ac.in","Unhelkar B.; Pandey H.M.; Agrawal A.P.; Choudhary A.","Springer Science and Business Media Deutschland GmbH","18761100","978-981995973-0","","","English","Lect. Notes Electr. Eng.","Conference paper","Final","","Scopus","2-s2.0-85177813972"
"Thomas L.; Thara K.T.; Sunil Kumar H.R.","Thomas, Likewin (37049292400); Thara, K.T. (58547247400); Sunil Kumar, H.R. (58547623300)","37049292400; 58547247400; 58547623300","Artificial Intelligence for Face recognition and Assistance for Visually Impaired","2023","5th International Conference on Energy, Power, and Environment: Towards Flexible Green Energy Technologies, ICEPE 2023","","","","","","","1","10.1109/ICEPE57949.2023.10201487","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85168688372&doi=10.1109%2fICEPE57949.2023.10201487&partnerID=40&md5=edb38b60391eae343636134b02b80fd2","Pes Institute of Technology and Management, Department of Ai&ml, Shivamogga, India; Pes Institute of Technology and Management, Department of Cse, Shivamogga, India","Thomas L., Pes Institute of Technology and Management, Department of Ai&ml, Shivamogga, India; Thara K.T., Pes Institute of Technology and Management, Department of Ai&ml, Shivamogga, India; Sunil Kumar H.R., Pes Institute of Technology and Management, Department of Cse, Shivamogga, India","In this world, many people are affected by blindness and they face so many difficulties in their daily life. According to World Health Organization (WHO), there are approximately 2.2 billion people who are completely blind. They need to depend on primitive solutions like white canes, trained dogs, or other people. But these helping hands cannot always assist them. The affected people need a smart assisting device that avoids bumping into an obstacle and helps in navigating from one place to another independently. This proposed work describes the smart walking stick which makes blind people walk safely. By using the latest technologies and IoT devices, this smart walking stick can be developed where it provides safe navigation to the user. The proposed system employs a novel solution for navigation in indoor with the help of deep learning algorithms. In case of panic situations or emergency conditions, the predefined message with the user's location will be sent to the caretaker using an API. This smart walking stick is affordable, durable and provides more convenience to the user to walk safely, and gives more confidence without depending on any other externals. © 2023 IEEE.","API; IoT; Visually impaired; Word Health Organization","Deep learning; Indoor positioning systems; Internet of things; API; Blind people; Daily lives; Health organizations; IoT; Trained dogs; Visually impaired; White cane; Word health organization; World Health Organization; Face recognition","","","Institute of Electrical and Electronics Engineers Inc.","","979-835031312-3","","","English","Int. Conf. Energy, Power, Environ.: Towards Flex. Green Energy Technol., ICEPE","Conference paper","Final","","Scopus","2-s2.0-85168688372"
"Almurayziq T.S.; Alotibi N.; Alshammari G.; Alshammari A.; Alsaffar M.","Almurayziq, Tariq S. (57344559300); Alotibi, Naif (58699959900); Alshammari, Gharbi (57194795614); Alshammari, Abdullah (57203855787); Alsaffar, Mohammad (57353189500)","57344559300; 58699959900; 57194795614; 57203855787; 57353189500","Smart and Guide Hat for Blind Persons in Smart Cities Using Deep Learning","2023","Journal of Advances in Information Technology","14","6","","1214","1220","6","0","10.12720/jait.14.6.1214-1220","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85177093543&doi=10.12720%2fjait.14.6.1214-1220&partnerID=40&md5=87289c1efd2dcfe53c89d00ef46bc1b1","Department of Information and Computer Science, College of Computer Science and Engineering, University of Ha’il, Ha’il, 81481, Saudi Arabia","Almurayziq T.S., Department of Information and Computer Science, College of Computer Science and Engineering, University of Ha’il, Ha’il, 81481, Saudi Arabia; Alotibi N., Department of Information and Computer Science, College of Computer Science and Engineering, University of Ha’il, Ha’il, 81481, Saudi Arabia; Alshammari G., Department of Information and Computer Science, College of Computer Science and Engineering, University of Ha’il, Ha’il, 81481, Saudi Arabia; Alshammari A., Department of Information and Computer Science, College of Computer Science and Engineering, University of Ha’il, Ha’il, 81481, Saudi Arabia; Alsaffar M., Department of Information and Computer Science, College of Computer Science and Engineering, University of Ha’il, Ha’il, 81481, Saudi Arabia","—In recent years, Artificial Intelligence (AI) technology has evolved significantly and is used in various fields including banking, email management, surgery, etc. The primary objective of the current study is to assist visually impaired (blind) individuals using AI technology. Blindness is a natural occurrence but it need not prevent blind people from experiencing the world similarly to sighted people. They must execute daily tasks such as walking with such precision so that various obstacles do not impede their progress. Typically, a blind individual uses a white cane as a guiding aid to navigate around obstacles. It is difficult, however, to design smart headgear that is both supportive and intelligent in order to provide numerous services that accurately and promptly anticipate obstacles at ground level and in traffic. Technological advances present an opportunity to develop intelligent headwear which helps its wearer to anticipate potential dangers. This paper presents a work-in-progress and analysis of the challenges blind people face when attempting to identify traffic signals, objects, plant types and QR codes on city maps. Through this endeavour, we hope to gain a better understanding of how blind individuals will be able to navigate smart cities. © 2023 by the authors.","artificial intelligence; deep learning; disabilities; smart cities; visual impairments; voice-activated personal assistant","","T.S. Almurayziq; Department of Information and Computer Science, College of Computer Science and Engineering, University of Ha’il, Ha’il, 81481, Saudi Arabia; email: t.almuraziq@uoh.edu.sa","","Engineering and Technology Publishing","17982340","","","","English","J. Adv. Inf.  Technol.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85177093543"
"Agrawal S.","Agrawal, Shivendra (57208160774)","57208160774","Assistive Robotics for Empowering Humans with Visual Impairments to Independently Perform Day-to-day Tasks","2023","Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS","2023-May","","","3023","3025","2","0","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85171281057&partnerID=40&md5=8af745e459a0b3b275c856e4c1ebc4d4","University of Colorado, Boulder, United States","Agrawal S., University of Colorado, Boulder, United States","The ability to perform common day-to-day tasks is essential for an independent lifestyle. However, many crucial tasks are unaddressed for blind or visually impaired (BVI) people with the current solutions. Our research goal aims to provide technical solutions to such problems to help support more autonomy for BVI people. Through this work, we present a proof-of-concept socially assistive robotic cane that can assist with 1) a navigation task which is finding a socially preferred seat in unknown public places and guiding the users toward it, 2) a manipulation task which is locating and retrieving the desired product from a grocery store shelf. We evaluated our system in an initial pilot study with sighted blindfolded testers, with encouraging results that show the system's potential to provide purposeful and effective navigation guidance optimizing for users' convenience, privacy, and intimacy while increasing their confidence in independent navigation. Another study we ran showed the system's success in locating and providing effective fine-grain manipulation guidance to retrieve desired products with novice users while eliciting a positive user experience. © 2023 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.","Computer Vision; Human-Robot Interaction; KEYWORDS Assistive Robotics; Manipulation Guidance; Markov Decision Process; Planner","Autonomous agents; Computer vision; Human robot interaction; Multi agent systems; Navigation; 'current; Assistive robotics; Humans-robot interactions; KEYWORDS assistive robotic; Manipulation guidance; Markov Decision Processes; Planner; Research goals; Visual impairment; Visually impaired people; Markov processes","S. Agrawal; University of Colorado, Boulder, United States; email: shivendra.agrawal@colorado.edu","","International Foundation for Autonomous Agents and Multiagent Systems (IFAAMAS)","15488403","","","","English","Proc. Int. Joint Conf. Auton. Agents Multiagent Syst., AAMAS","Conference paper","Final","","Scopus","2-s2.0-85171281057"
"Wang G.; Li L.; Fan J.; Shi S.; Xu Y.; Wang Y.","Wang, GuangYi (58430239200); Li, Lu (58444503200); Fan, JingJuan (57883314700); Shi, SongYun (57224977566); Xu, YiPeng (57884310700); Wang, Yuan (56572214700)","58430239200; 58444503200; 57883314700; 57224977566; 57884310700; 56572214700","Active Guide System for The Blind Based on The Internet of Things and Collaborative Perception","2022","Proceedings - 2022 11th International Conference of Information and Communication Technology, ICTech 2022","","","","22","27","5","2","10.1109/ICTech55460.2022.00012","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85137699953&doi=10.1109%2fICTech55460.2022.00012&partnerID=40&md5=81b8d4ad0e7bce69bff1953ec68bbe0b","College of Artificial Intelligence, Tianjin University Science and Technology, Thirteenth Avenue Binhai New District Tianjin, Tianjin, 300457, China","Wang G., College of Artificial Intelligence, Tianjin University Science and Technology, Thirteenth Avenue Binhai New District Tianjin, Tianjin, 300457, China; Li L., College of Artificial Intelligence, Tianjin University Science and Technology, Thirteenth Avenue Binhai New District Tianjin, Tianjin, 300457, China; Fan J., College of Artificial Intelligence, Tianjin University Science and Technology, Thirteenth Avenue Binhai New District Tianjin, Tianjin, 300457, China; Shi S., College of Artificial Intelligence, Tianjin University Science and Technology, Thirteenth Avenue Binhai New District Tianjin, Tianjin, 300457, China; Xu Y., College of Artificial Intelligence, Tianjin University Science and Technology, Thirteenth Avenue Binhai New District Tianjin, Tianjin, 300457, China; Wang Y., College of Artificial Intelligence, Tianjin University Science and Technology, Thirteenth Avenue Binhai New District Tianjin, Tianjin, 300457, China","Nowadays, the number of visually impaired people is increasing day by day. On average, one in every 100 people is blind. At the same time, due to the imperfect establishment of social barrier-free facilities, it is very difficult for the blind travelling, which seriously affects the degree of social participation of the blind. This paper introduces a blind guide method based on the Internet of Things and collaborative perception. Coordinate through the long-distance of digital map (eg. Google Maps) and the local navigation mode of the blind guide device to realize an active guide system for the blind. This method can effectively assist the blind to cope with the complex traffic environment and thus perceive the world.  © 2022 IEEE.","active; collaborative navigation; deep learning; electronic map; guide system for the blind; HSV; Internet of Things","Deep learning; E-learning; Active; Barrier-free; Collaborative navigation; Deep learning; Electronic map; Guide system; Guide system for the blind; HSV; Systems for the blinds; Visually impaired people; Internet of things","Y. Wang; College of Artificial Intelligence, Tianjin University Science and Technology, Thirteenth Avenue Binhai New District Tianjin, Tianjin, 300457, China; email: wangyuan23@tust.edu.cn","","Institute of Electrical and Electronics Engineers Inc.","","978-166549694-0","","","English","Proc. - Int. Conf. Inf. Commun. Technol., ICTech","Conference paper","Final","","Scopus","2-s2.0-85137699953"
"Monusha C.M.; Punitha G.R.; Karthik T.; Tarun S.N.; Kumar S.S.","Monusha, C.M. (58653935900); Punitha, G.R. (56086154300); Karthik, T. (58653762200); Tarun, S.N. (58654024500); Kumar, Saravana S. (58740442900)","58653935900; 56086154300; 58653762200; 58654024500; 58740442900","Review on Real Time Multi-Utility Shoe for Visually Impaired Persons using Internet of Things","2023","14th International Conference on Advances in Computing, Control, and Telecommunication Technologies, ACT 2023","2023-June","","","3155","3159","4","0","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85174420901&partnerID=40&md5=bb622757af40c70b758ea298c5d74ed3","Department of Electronics and Telecommunication, Bengaluru, 560078, India; Dayananda Sagar College of Engineering, Bengaluru, 560078, India","Monusha C.M., Department of Electronics and Telecommunication, Bengaluru, 560078, India; Punitha G.R., Dayananda Sagar College of Engineering, Bengaluru, 560078, India; Karthik T., Dayananda Sagar College of Engineering, Bengaluru, 560078, India; Tarun S.N., Dayananda Sagar College of Engineering, Bengaluru, 560078, India; Kumar S.S., Dayananda Sagar College of Engineering, Bengaluru, 560078, India","Making physical objects talk to one other or to people is the core idea behind the internet of things. It is an information technology that is expanding and developing quickly. The creation of assistive equipment is necessary due to the rise in the population of persons who are blind. This issue can be resolved by using a tool that acts as an intelligent guide for blind individuals. Our idea is to help blind people carry out their daily tasks, especially navigation. It is constructed utilising Internet of Things technology, and the shoe will have several sensors, a microcontroller, and a panic button incorporated in it. When the wearer steps in front of an obstacle, the shoe alerts them by sounding the panic button. Ultrasonic sensors are used for obstacle detection, and Bluetooth sends a voice command when an obstruction is found. Moisture sensor warns of slick surface. In order to locate the blind for simple tracking and to send an SMS to the guardian in case of emergency, the system uses GPS and GSM modules. To prevent the user from running into any obstacles in his path, the smart shoe collaborates and interacts with one another. It will also be helpful to use GPS to locate the person in real time and measure their temperature and heart rate. The device's main objective is to give blind people a simple and practical navigational tool that enables artificial vision by providing information about the environment and the static and moving objects nearby. © Grenze Scientific Society, 2023.","Arduino uno; Bluetooth; GPS-GSM tracking; IOT (Internet of Things); piezo-electric plates; Smart shoe; Ultrasonic sensor; Water sensor","Global positioning system; Global system for mobile communications; Internet of things; Obstacle detectors; Ultrasonic applications; Ultrasonic sensors; Arduino uno; Blind people; GPS-GSM tracking; Internet of thing; Piezo electrics; Piezo-electric plate; Real- time; Simple++; Smart shoe; Water sensors; Bluetooth","","Stephen J.; Sharma P.; Chaba Y.; Abraham K.U.; Anooj P.K.; Mohammad N.; Universiti Kebangsaan Malaysia, 43600 UKM, Bangi Selangor; Thomas G.; Srikiran S.","Grenze Scientific Society","","","","","English","Int. Conf. Adv. Comput., Control, Telecommun. Technol., ACT","Conference paper","Final","","Scopus","2-s2.0-85174420901"
"Mu Z.; Fang W.; Zhu S.; Jin T.; Song W.; Xi X.; Huang Q.; Gu J.; Yuan S.","Mu, Zonghao (57763997400); Fang, Wei (58089296900); Zhu, Shiqiang (7404391617); Jin, Tianlei (57463030400); Song, Wei (55459523400); Xi, Xiangming (55604506300); Huang, Qiulan (58089297000); Gu, Jason (57420224100); Yuan, Songyu (57289568300)","57763997400; 58089296900; 7404391617; 57463030400; 55459523400; 55604506300; 58089297000; 57420224100; 57289568300","A Multi-modal Behavior Planning Framework for Guide Robot","2022","2022 IEEE International Conference on Robotics and Biomimetics, ROBIO 2022","","","","469","474","5","1","10.1109/ROBIO55434.2022.10011739","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85147326212&doi=10.1109%2fROBIO55434.2022.10011739&partnerID=40&md5=adcbcfc8426e104259d26f0b9cbd830b","Zhejiang Lab, Hangzhou, China","Mu Z., Zhejiang Lab, Hangzhou, China; Fang W., Zhejiang Lab, Hangzhou, China; Zhu S., Zhejiang Lab, Hangzhou, China; Jin T., Zhejiang Lab, Hangzhou, China; Song W., Zhejiang Lab, Hangzhou, China; Xi X., Zhejiang Lab, Hangzhou, China; Huang Q., Zhejiang Lab, Hangzhou, China; Gu J., Zhejiang Lab, Hangzhou, China; Yuan S., Zhejiang Lab, Hangzhou, China","In this paper we propose a multi-modal behavior planning framework for guide robots, to better assist the visually impaired to select safe paths in a cluttered space. Most prior robotic guiding systems only use physical contact, limiting their ability from operating in narrow and cluttered environments. Our multi-modal behavior planning framework is based on the Social Force Model(SFM) and the Monte Carlo Tree Search(MCTS). The proposed framework extracts robot behaviors' impact as the social force on human and predicts human motion, then employs the MCTS to search best multi-modal behavior policy. The proposed approach is deployed on a humanoid robot to guide a blind-folded person to safely travel in a complicated space. © 2022 IEEE.","","Robot programming; Behavior planning; Cluttered environments; Guide robots; Guiding systems; Multi-modal; Narrow environment; Physical contacts; Planning framework; Tree-search; Visually impaired; Anthropomorphic robots","W. Song; Zhejiang Lab, Hangzhou, China; email: weisong@zhejianglab.com","","Institute of Electrical and Electronics Engineers Inc.","","978-166548109-0","","","English","IEEE Int. Conf. Robot. Biomimetics, ROBIO","Conference paper","Final","","Scopus","2-s2.0-85147326212"
"Raut R.; Jadhav A.; Jaiswal S.; Pathak P.","Raut, Roshani (56405465100); Jadhav, Anuja (57804066500); Jaiswal, Swati (57216150255); Pathak, Pranav (56661830600)","56405465100; 57804066500; 57216150255; 56661830600","IoT-assisted smart device for blind people","2022","Intelligent Systems for Rehabilitation Engineering","","","","129","150","21","2","10.1002/9781119785651.ch6","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85147930751&doi=10.1002%2f9781119785651.ch6&partnerID=40&md5=6837f13ece673aa7b51c3003eb28ad80","Pimpri Chinchwad College of Engineering Pune, Maharashtra, India; MIT School of Bioengineering Sciences and Research Pune, Maharashtra, India","Raut R., Pimpri Chinchwad College of Engineering Pune, Maharashtra, India; Jadhav A., Pimpri Chinchwad College of Engineering Pune, Maharashtra, India; Jaiswal S., Pimpri Chinchwad College of Engineering Pune, Maharashtra, India; Pathak P., MIT School of Bioengineering Sciences and Research Pune, Maharashtra, India","Those who are blind have a lot of trouble going through their daily lives. A lot of effort has gone into making it easier for blind people to complete tasks on their own rather than relying on others. With this inspiration in mind, we proposed and created an intelligent blind stick. The smart walking stick assists visually impaired people in identifying obstacles and getting to their destination. There are a variety of walking sticks and devices that assist users in moving around both indoor and outdoor environments, but none of them include run-time autonomous navigation, object detection and identification warnings, or face and voice recognition. The stick uses IoT, echolocation, image processing, artificial intelligence, and navigation system technology to identify close and far obstacles for the user. If the blind person falls or has some other problem, then the system will send a warning to the designated person. The system uses voice recognition to recognise loved ones. © 2022 Scrivener Publishing LLC. All rights reserved.","Artificial intelligence; Image processing; Input/output and data communication (hardware); Natural language; Sensors (artificial intelligence); Voice (input/output device)","","A. Jadhav; Pimpri Chinchwad College of Engineering Pune, Maharashtra, India; email: annuja.jadhav@gmail.com","","wiley","","978-111978565-1; 978-111978566-8","","","English","Intell. Syst. for Rehabil. Eng.","Book chapter","Final","","Scopus","2-s2.0-85147930751"
"Divya G.; Dasiga S.; Gurudatt B.M.; Guruduth S.; Akshatha A.H.; Gaurav G.","Divya, G. (58237497700); Dasiga, Sankar (56257851700); Gurudatt, B.M. (57299383400); Guruduth, S. (58549154100); Akshatha, A.H. (58549110100); Gaurav, G. (58549245700)","58237497700; 56257851700; 57299383400; 58549154100; 58549110100; 58549245700","Smart Object and Face Detection Assistant for Visually Impaired","2023","International Conference on Applied Intelligence and Sustainable Computing, ICAISC 2023","","","","","","","1","10.1109/ICAISC58445.2023.10200356","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85168757576&doi=10.1109%2fICAISC58445.2023.10200356&partnerID=40&md5=8765218324cb8d51913273101312664a","Nitte Meenakshi Institute of Technology, Dept. of Electronics and Communication Engineering, Bengaluru, India","Divya G., Nitte Meenakshi Institute of Technology, Dept. of Electronics and Communication Engineering, Bengaluru, India; Dasiga S., Nitte Meenakshi Institute of Technology, Dept. of Electronics and Communication Engineering, Bengaluru, India; Gurudatt B.M., Nitte Meenakshi Institute of Technology, Dept. of Electronics and Communication Engineering, Bengaluru, India; Guruduth S., Nitte Meenakshi Institute of Technology, Dept. of Electronics and Communication Engineering, Bengaluru, India; Akshatha A.H., Nitte Meenakshi Institute of Technology, Dept. of Electronics and Communication Engineering, Bengaluru, India; Gaurav G., Nitte Meenakshi Institute of Technology, Dept. of Electronics and Communication Engineering, Bengaluru, India","The World Health Organization (WHO) estimates that 253 million people worldwide are blind or visually impaired. 217 million of them have moderate to severe visual impairment, leaving 36 million of them entirely blind. The majority of people who are visually impaired live in low- and middle-income nations, and there is evidence that vision impairment rises with age. Visually challenged people need access to a face and object detection system in order to improve their mobility and capacity for social interaction. The suggested remedy makes use of a Raspberry Pi equipped with a camera module to capture real-time video feeds, OpenCV for image processing, and machine learning techniques for object and face recognition. The proposed methodology includes YOLO Algorithm and Convolutional Neural Network Architecture in implementation. The system is designed to provide real-time feedback to the user via audio and haptic feedback to notify them of the presence of people or objects in their vicinity. The project also includes a user-friendly interface that allows the user to customize the detection settings to suit their individual needs. The system was tested on various scenarios and showed promising results in terms of accuracy and responsiveness. Overall, the proposed system provides a practical solution for visually impaired individuals to overcome their daily navigation challenges and to improve their quality of life. The results are obtained in both software and hardware implementation. © 2023 IEEE.","Convolutional Neural Network; OpenCV; Raspberry Pi; YOLO Algorithm","Convolution; Convolutional neural networks; Face recognition; Learning systems; Network architecture; Vision; Convolutional neural network; Faces detection; Objects detection; Opencv; Raspberry pi; Smart objects; Visual impairment; Visually impaired; World Health Organization; YOLO algorithm; Object detection","G. Divya; Nitte Meenakshi Institute of Technology, Dept. of Electronics and Communication Engineering, Bengaluru, India; email: divya.g@nmit.ac.in","","Institute of Electrical and Electronics Engineers Inc.","","979-835032379-5","","","English","Int. Conf. Appl. Intell. Sustain. Comput., ICAISC","Conference paper","Final","","Scopus","2-s2.0-85168757576"
"Zulaikha Beevi S.; Harish Kumar P.; Harish S.; Lakshan S.J.","Zulaikha Beevi, S. (36663986800); Harish Kumar, P. (58107357700); Harish, S. (57225462579); Lakshan, S.J. (58127552200)","36663986800; 58107357700; 57225462579; 58127552200","Decision Making Algorithm for Blind Navigation Assistance using Deep Learning","2022","2022 1st International Conference on Computational Science and Technology, ICCST 2022 - Proceedings","","","","268","272","4","0","10.1109/ICCST55948.2022.10040269","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85149376853&doi=10.1109%2fICCST55948.2022.10040269&partnerID=40&md5=bda25d3abe8edad882028ccb6ec881ff","Vsb Engineering College, Dept. of Computer Science and Engineering, Karudayampalayam. Tamil Nadu, Karur, 639 111, India; Vsb Engineering College, Dept. of Artificial Intelligence and Data Science, Karudayampalayam, Tamil Nadu, Karur, 639 111, India","Zulaikha Beevi S., Vsb Engineering College, Dept. of Computer Science and Engineering, Karudayampalayam. Tamil Nadu, Karur, 639 111, India; Harish Kumar P., Vsb Engineering College, Dept. of Artificial Intelligence and Data Science, Karudayampalayam, Tamil Nadu, Karur, 639 111, India; Harish S., Vsb Engineering College, Dept. of Artificial Intelligence and Data Science, Karudayampalayam, Tamil Nadu, Karur, 639 111, India; Lakshan S.J., Vsb Engineering College, Dept. of Artificial Intelligence and Data Science, Karudayampalayam, Tamil Nadu, Karur, 639 111, India","Blind people face several obstacles in their daily lives and technological interventions can help overcome these obstacles. In this research, we provide an AI-based autonomous assisting device that recognizes many objects and it will provide acoustic input to the user to help visually blind people to understand the surrounding better to understand their environment better. Multiple photos of objects relevant to visually impaired people were used to build a deep-learning model. Training photos are enhanced and manually annotated to improve the trained model's resilience. A distance-measuring sensor is included which recognise the objects using computer vision. The gadget is made more inclusive by recognizing the obstacles coming out of one place to another. After stage segmentation and obstacle detection, the aural information sent to the user is adj usted to get a lot of details in minimum time and speed up video processing. © 2022 IEEE.","Deep learning model; Image processing; Image recognition; RCNN; Speech recognition; Voice output","Behavioral research; Decision making; Deep learning; Image enhancement; Learning systems; Obstacle detectors; Video signal processing; Blind navigation; Blind people; Daily lives; Decision-making algorithms; Deep learning model; Images processing; Learning models; RCNN; Visually impaired people; Voice output; Speech recognition","","","Institute of Electrical and Electronics Engineers Inc.","","978-166547655-3","","","English","Int. Conf. Comput. Sci. Technol., ICCST - Proc.","Conference paper","Final","","Scopus","2-s2.0-85149376853"
"Gupta M.; Laddha A.; Chouhan M.; Mishra Y.; Gutte V.","Gupta, Mansi (58538120500); Laddha, Aditi (57217211792); Chouhan, Maahi (58538433600); Mishra, Yashasvi (58538917400); Gutte, Vitthal (57191042006)","58538120500; 57217211792; 58538433600; 58538917400; 57191042006","Handy Smart Stick: A Guide for Visually Impaired People","2023","Proceedings of the 8th International Conference on Communication and Electronics Systems, ICCES 2023","","","","1588","1595","7","1","10.1109/ICCES57224.2023.10192815","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85168117517&doi=10.1109%2fICCES57224.2023.10192815&partnerID=40&md5=7201c36e1b6f6efcdf7c7a4e41585762","Dr. Vishwanath Karad Mit World Peace University, B. Tech School of Computer Science and Engineering, Rajsamand, India","Gupta M., Dr. Vishwanath Karad Mit World Peace University, B. Tech School of Computer Science and Engineering, Rajsamand, India; Laddha A., Dr. Vishwanath Karad Mit World Peace University, B. Tech School of Computer Science and Engineering, Rajsamand, India; Chouhan M., Dr. Vishwanath Karad Mit World Peace University, B. Tech School of Computer Science and Engineering, Rajsamand, India; Mishra Y., Dr. Vishwanath Karad Mit World Peace University, B. Tech School of Computer Science and Engineering, Rajsamand, India; Gutte V., Dr. Vishwanath Karad Mit World Peace University, B. Tech School of Computer Science and Engineering, Rajsamand, India","It's a perpetual human quest and a constant effort of science to develop and design equipment and aids to help physically challenged people live their lives independently and confidently. Blindness and visual impairment are prevalent disabilities that inhibit the independence of the patient and increase the reliance on family members, friends, and guide dogs for navigation and other daily duties. The proposed work outlines a reliable, low-cost, and power-efficient solution that enables safe and secure navigation for visually impaired people. Many researchers have worked on and proposed solutions to the typical problems, but those works have some shortcomings that are addressed in this study. Existing technologies have not focused on optimizing power consumption and reliability, thereby reducing the life of the system. It is vital to reduce power consumption in order to extend the operating lifetime of devices that use batteries as a power source. A methodology has been put forward to address the crucial concern of a gadget that is left on for an extended period of time without being switched off to improve the overall performance of the system. The IoT is entrusted with taking into account heterogeneous equipment that might be severely constrained by its nature and pose problems in the hardware layer of the system. So methods to make the system reliable, accurate, and resistant to failures are inscribed in this study. Additionally, existing systems aren't economical compared to the offered features, lack automation, and lack audio feedback, particularly notifying obstacles. This study proposes audio feedback in multiple languages and automates the command execution, thereby making it effortless for impaired people to engage with the product. Also, the present state of technology faces challenges in locating the misplaced stick, which means the person has to be dependent on loved ones, friends, or caretakers to find the stick. The proposed automated and handy Smart Stick for the blind using IOT (Internet of Things) is a potent Electronic Travel Aid (ETA) that gives artificial perceptivity of vision and discerns environmental hurdles and obstacles. © 2023 IEEE.","artificial vision; Global Positioning System (GPS) and Global System for Mobile Communication (GSM); IoT (Internet of Things); sensors; smart Stick; visually impaired people","Audio systems; Computer vision; Electric power utilization; Feedback; Global positioning system; Global system for mobile communications; Audio feedbacks; Design equipments; Global positioning system  and global system for mobile communication; Guide dogs; Internet of thing; Low Power; Low-costs; Smart stick; Visual impairment; Visually impaired people; Internet of things","M. Gupta; Dr. Vishwanath Karad Mit World Peace University, B. Tech School of Computer Science and Engineering, Rajsamand, India; email: mansigupta9491@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","","979-835039663-8","","","English","Proc. Int. Conf. Commun. Electron. Syst., ICCES","Conference paper","Final","","Scopus","2-s2.0-85168117517"
"Hwang H.; Xia T.; Keita I.; Suzuki K.; Biswas J.; Lee S.I.; Kim D.","Hwang, Hochul (57946718100); Xia, Tim (57217020178); Keita, Ibrahima (57459546600); Suzuki, Ken (57946678800); Biswas, Joydeep (35774160500); Lee, Sunghoon I. (55049449300); Kim, Donghyun (57201849434)","57946718100; 57217020178; 57459546600; 57946678800; 35774160500; 55049449300; 57201849434","System Configuration and Navigation of a Guide Dog Robot: Toward Animal Guide Dog-Level Guiding Work","2023","Proceedings - IEEE International Conference on Robotics and Automation","2023-May","","","9778","9784","6","5","10.1109/ICRA48891.2023.10160573","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85168676707&doi=10.1109%2fICRA48891.2023.10160573&partnerID=40&md5=7ecc7ea5ab0949eb89fd12e63c3080d1","University of Texas, Austin, United States; University of Massachusetts Amherst, 140 Governors Dr, United States","Hwang H.; Xia T.; Keita I.; Suzuki K.; Biswas J., University of Texas, Austin, United States; Lee S.I.; Kim D., University of Massachusetts Amherst, 140 Governors Dr, United States","A robot guide dog has compelling advantages over animal guide dogs for its cost-effectiveness, the potential for mass production, and low maintenance burden. However, despite the long history of guide dog robot research, previous studies were conducted with little or no consideration of how the guide dog handler and the guide dog work as a team for navigation. To develop a robotic guiding system that genuinely benefits blind or visually impaired individuals, we performed qualitative research, including interviews with guide dog handlers, trainers, and first-hand blindfold walking experiences with various guide dogs. We build a collaborative indoor navigation scheme for a guide dog robot that includes preferred features such as speed and directional control. For collaborative navigation, we propose a semantic-aware local path planner that enables safe and efficient guiding work by utilizing semantic information about the environment and considering the handler's position and directional cues to determine the collision-free path. We evaluate our integrated robotic system by testing blindfolded walking in indoor settings and demonstrate guide dog-like navigation behavior by avoiding obstacles at typical gait speed (0.7m/s). The following demonstration video link includes an audio description: https://youtu.be/YxlcMeaL7GA © 2023 IEEE.","","Animals; Cost effectiveness; Indoor positioning systems; Robots; Semantics; Guide dogs; Guiding systems; Indoor navigation; Low maintenance; Mass production; Preferred features; Qualitative research; Robot guides; Systems Configuration; Visually impaired; Navigation","","","Institute of Electrical and Electronics Engineers Inc.","10504729","979-835032365-8","PIIAE","","English","Proc IEEE Int Conf Rob Autom","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85168676707"
"Prathipa R.; Arun M.; Premkumar M.; Sathiyapriya S.; Sridevi P.","Prathipa, R. (56607338800); Arun, M. (56893358200); Premkumar, M. (36632999900); Sathiyapriya, S. (57647888300); Sridevi, P. (58145135000)","56607338800; 56893358200; 36632999900; 57647888300; 58145135000","CNN Based Personal Assistive System for Deaf-Blind","2022","4th International Conference on Circuits, Control, Communication and Computing, I4C 2022","","","","221","225","4","1","10.1109/I4C57141.2022.10057882","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85150289271&doi=10.1109%2fI4C57141.2022.10057882&partnerID=40&md5=6aceb71b8f423f6617c6369e83e494df","Panimalar Engineering College, Dept of ECE, Chennai, India; Panimalar Institute of Technology, Dept of ECE, Chennai, India","Prathipa R., Panimalar Engineering College, Dept of ECE, Chennai, India; Arun M., Panimalar Engineering College, Dept of ECE, Chennai, India; Premkumar M., Panimalar Engineering College, Dept of ECE, Chennai, India; Sathiyapriya S., Panimalar Engineering College, Dept of ECE, Chennai, India; Sridevi P., Panimalar Institute of Technology, Dept of ECE, Chennai, India","Deaf-Blindness restricts a person's ability to learn about their environment, making unaided navigation, object recognition, obstacle avoidance, and reading tasks difficult. As there is now relatively little available to visually impaired people, there is a need to deploy technology that supports them in their everyday chores. Available systems, such as Braille devices and Screen Reading software, assist visually impaired people in reading and acquiring access to various devices, but such technologies become ineffective when the blind want to execute fundamental activities including perceiving the world before him, such as people or objects. The Raspberry Pi 3 Model has been utilized to display the functionality of the suggested prototype advantages of tiny size, low cost and ease of integration. The aim of the proposed device is to develop a personal assistant system for deaf-blind people to help identify faces of people and objects in terms of Morse code. The current set up is designed to deaf-blind to live independent. The proposed device has been designed with CNN algorithm which is a major part of deep learning to identify the objects. The resultant output is produced through vibrators and morse code as well.  © 2022 IEEE.","CNN; Deep Learning; Facial Recognition; Morse code recognition; Object Detection; Raspberry Pi; Sign Language Recognition; Text to speech; Visually Impaired","Character recognition; Codes (symbols); Deep learning; Face recognition; Object recognition; Speech recognition; Code recognition; Deep learning; Facial recognition; Morse code recognition; Morse codes; Objects detection; Raspberry pi; Sign Language recognition; Text to speech; Visually impaired; Object detection","","","Institute of Electrical and Electronics Engineers Inc.","","979-835039747-5","","","English","Int. Conf. Circuits, Control, Commun. Comput., I4C","Conference paper","Final","","Scopus","2-s2.0-85150289271"
"Nambiappan H.R.; Karim E.; Saurav M.J.R.; Srivastav A.; Gans N.; Makedon F.","Nambiappan, Harish Ram (57202944566); Karim, Enamul (57225007425); Saurav, Md Jillur Rahman (57809695400); Srivastav, Anushka (57799003500); Gans, Nicholas (15750786600); Makedon, Fillia (7003437865)","57202944566; 57225007425; 57809695400; 57799003500; 15750786600; 7003437865","Smartphone Based IoT-Controller Framework for Assisting the Blind in Human Robot Interaction","2022","ACM International Conference Proceeding Series","","","","514","516","2","2","10.1145/3529190.3534782","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85134432960&doi=10.1145%2f3529190.3534782&partnerID=40&md5=485c9a5913b0d94029ff5695f7449359","University of Texas at Arlington, Arlington, TX, United States; University of Texas, Arlington Research Institute, Fort Worth, TX, United States","Nambiappan H.R., University of Texas at Arlington, Arlington, TX, United States; Karim E., University of Texas at Arlington, Arlington, TX, United States; Saurav M.J.R., University of Texas at Arlington, Arlington, TX, United States; Srivastav A., University of Texas at Arlington, Arlington, TX, United States; Gans N., University of Texas, Arlington Research Institute, Fort Worth, TX, United States; Makedon F., University of Texas at Arlington, Arlington, TX, United States","In this paper, a novel smartphone-based IoT-Controller Framework is proposed for effective interaction between robots and people who are blind. This framework focuses on assisting visually impaired users in a pick and place task scenario in contrast to previous works, which primarily focus on navigation and localization. The user can give speech commands to the robot using a smartphone application, which is sent to a server for recognition and retrieval of information such as positions and type of object to be grasped. The details are sent to the robot, which performs the commanded task. Preliminary tests with five participants over a total of 20 trials showed that the system had a 85% success rate, and the average time taken for the task to complete was approximately 65 seconds.  © 2022 ACM.","Human Robot Interaction; Internet of Things; Mobile Computing","Human robot interaction; Man machine systems; Mobile computing; Smartphones; Speech recognition; Effective interactions; Humans-robot interactions; Localisation; Mobile-computing; Pick and place; Smart phones; Smart-phone applications; Speech commands; Visually-impaired users; Internet of things","","","Association for Computing Machinery","","978-145039631-8","","","English","ACM Int. Conf. Proc. Ser.","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85134432960"
"Han Z.; Gu J.; Feng Y.","Han, Zhicheng (58602862100); Gu, Jason (57420224100); Feng, Ying (7404544420)","58602862100; 57420224100; 7404544420","Blind Lane Detection and Following for Assistive Navigation of Vision Impaired People","2023","2023 8th IEEE International Conference on Advanced Robotics and Mechatronics, ICARM 2023","","","","721","726","5","1","10.1109/ICARM58088.2023.10218843","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85171573608&doi=10.1109%2fICARM58088.2023.10218843&partnerID=40&md5=1394e42b9eb02fc20dfc9da047d4afad","Institute of Advanced Technology, University of Science and Technology of China, Hefei, 230031, China; Dalhousie University, Department of Electrical and Computer Engineering, Halifax, NS, Canada; College of Automation Science and Engineering, South China University of Technology, Guangzhou, 510641, China","Han Z., Institute of Advanced Technology, University of Science and Technology of China, Hefei, 230031, China; Gu J., Dalhousie University, Department of Electrical and Computer Engineering, Halifax, NS, Canada; Feng Y., College of Automation Science and Engineering, South China University of Technology, Guangzhou, 510641, China","The task of safely navigating outdoor environ-ments presents significant difficulties for people with visual impairments. This paper aims to address this issue by proposing a wearable assistance system for visually impaired individuals in blind lane detection and following scenarios. The system consists of two stages: the first stage employs a fully convolutional network to detect blind lanes and an object detection network to identify obstacles. The second stage utilizes an improved artificial potential field method to achieve real-time path following based on the detection results. Our experiments involving subjects in dynamic outdoor environments demonstrate the robustness of our proposed method in navigating and avoiding obstacles under challenging outdoor conditions.  © 2023 IEEE.","Blind lane Detection; Following; Navigation of visually impaired People","Assistive navigations; Blind lane detection; Following; Impaired people; Lane detection; Lane following; Navigation of visually impaired people; Vision impaired; Visual impairment; Visually impaired people; Object detection","","","Institute of Electrical and Electronics Engineers Inc.","","979-835030017-8","","","English","IEEE Int. Conf. Adv. Robot. Mechatronics, ICARM","Conference paper","Final","","Scopus","2-s2.0-85171573608"
"Shang K.; Li X.; Liu C.; Ming L.; Hu G.","Shang, Kejun (57216562814); Li, Xixi (58102883000); Liu, Chongliang (57579452000); Ming, Li (57265826500); Hu, Guangfeng (57221100896)","57216562814; 58102883000; 57579452000; 57265826500; 57221100896","An Integrated Navigation Method for UAV Autonomous Landing Based on Inertial and Vision Sensors","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13605 LNAI","","","182","193","11","3","10.1007/978-3-031-20500-2_15","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85147984652&doi=10.1007%2f978-3-031-20500-2_15&partnerID=40&md5=8dec930314d3b58c9f9df3d0abf7a99d","Beijing Institute of Automation Equipment, Beijing, 100074, China","Shang K., Beijing Institute of Automation Equipment, Beijing, 100074, China; Li X., Beijing Institute of Automation Equipment, Beijing, 100074, China; Liu C., Beijing Institute of Automation Equipment, Beijing, 100074, China; Ming L., Beijing Institute of Automation Equipment, Beijing, 100074, China; Hu G., Beijing Institute of Automation Equipment, Beijing, 100074, China","In the process of autonomous landing of unmanned aerial vehicles (UAV), the vision sensor is restricted by the field of view and UAV maneuvering process, which may make the acquired relative position/attitude parameters unstable or even odd (not unique), and there is a ‘blind area’ of vision measurement in the UAV rollout stage, which loses the navigation ability and seriously affects the safety of landing. In this paper, an autonomous landing navigation method based on inertial/visual sensor information fusion is proposed. When the UAV is far away from the airport and the runway imaging is complete, landing navigation parameters are determined by vision sensor based on the object image conjugate relationship of the runway sideline, and fuses with the inertial information to improve the measure performance. When the UAV is close to the airport and the runway imaging is incomplete, the measurement information of the vision sensor appears singular. The estimation of the landing navigation parameters is realized by inertial information in the aid of vision. When the UAV rollouts, the vision sensor enters the ‘blind area’, judges the UAV’s motion state through the imaging features of two adjacent frames, and suppresses the inertial sensor error by using the UAV’s motion state constraint, so as to achieve the high-precision maintenance of landing navigation parameters. The flight test shows that the lateral relative position error is less than 10m when the inertial with low accuracy and visual sensor are used, which can meet the requirement of UAV landing safely. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Autonomous landing navigation; Deep learning semantic segmentation; Inertial/Vision data fusion","Air navigation; Antennas; Deep learning; Image enhancement; Maneuverability; Parameter estimation; Semantic Segmentation; Semantics; Unmanned aerial vehicles (UAV); Aerial vehicle; Autonomous landing; Autonomous landing navigation; Deep learning semantic segmentation; Inertial/vision data fusion; Learning semantics; Navigation parameters; Semantic segmentation; Vision data; Vision sensors; Landing","K. Shang; Beijing Institute of Automation Equipment, Beijing, 100074, China; email: kjshang@163.com","Fang L.; Povey D.; Zhai G.; Mei T.; Wang R.","Springer Science and Business Media Deutschland GmbH","03029743","978-303120499-9","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","","Scopus","2-s2.0-85147984652"
"Luke A.K.; Kumar R.A.; Krishnan V.G.U.; Abhishek S.; Anjali T.","Luke, Alan Koshy (58765393200); Kumar, R Adithya (58836565400); Krishnan, V G Uday (59161812600); Abhishek, S. (57205017146); Anjali, T. (57195522716)","58765393200; 58836565400; 59161812600; 57205017146; 57195522716","Envision: Assistance System for the Visually Impaired","2023","2023 14th International Conference on Computing Communication and Networking Technologies, ICCCNT 2023","","","","","","","0","10.1109/ICCCNT56998.2023.10308142","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85179838991&doi=10.1109%2fICCCNT56998.2023.10308142&partnerID=40&md5=e0dcb50938de2534619a13162f8bdf2f","Amrita Vishwa Vidyapeetham, Amrita School of Computing, Department of Computer Science and Engineering, Amritapuri, India","Luke A.K., Amrita Vishwa Vidyapeetham, Amrita School of Computing, Department of Computer Science and Engineering, Amritapuri, India; Kumar R.A., Amrita Vishwa Vidyapeetham, Amrita School of Computing, Department of Computer Science and Engineering, Amritapuri, India; Krishnan V.G.U., Amrita Vishwa Vidyapeetham, Amrita School of Computing, Department of Computer Science and Engineering, Amritapuri, India; Abhishek S., Amrita Vishwa Vidyapeetham, Amrita School of Computing, Department of Computer Science and Engineering, Amritapuri, India; Anjali T., Amrita Vishwa Vidyapeetham, Amrita School of Computing, Department of Computer Science and Engineering, Amritapuri, India","People who are visually impaired experience many difficulties completing daily duties and need technology solutions to enhance their quality of life. This study proposes to build a mobile application using deep learning and the Flutter framework to help visually impaired individuals perform their daily tasks. The application includes object detection, currency recognition, image-to-text conversion, and text-to-speech ability. It has features like vibrations on screen swiping and larger buttons for easy navigation. The suggested system operates online and in real-time, making it possible to utilize it without an Internet connection. The study emphasizes the need of addressing the difficulties faced by those who are blind and the requirement for technical solutions to help them in their everyday routines. © 2023 IEEE.","Assistive Technology; Deep Learning; Flutter; Object Recognition; Text-to-Speech; Visual Aid","Character recognition; Deep learning; Flutter (aerodynamics); Image recognition; Object detection; Speech recognition; Assistance system; Assistive technology; Deep learning; Flutter; Objects recognition; Quality of life; Technology solutions; Text to speech; Visual aids; Visually impaired; Object recognition","A.K. Luke; Amrita Vishwa Vidyapeetham, Amrita School of Computing, Department of Computer Science and Engineering, Amritapuri, India; email: amenu4cse20205@am.students.amrita.edu","","Institute of Electrical and Electronics Engineers Inc.","","979-835033509-5","","","English","Int. Conf. Comput. Commun. Netw. Technol., ICCCNT","Conference paper","Final","","Scopus","2-s2.0-85179838991"
"Ge S.; Lin Y.-N.; Lai S.-N.; Xu J.-J.; He Y.-L.; Zhao Q.; Zhang H.; Xu S.-Y.","Ge, Song (57222866846); Lin, Yan-Ni (57222864228); Lai, Shun-Nan (57191500062); Xu, Jing-Jing (57189596265); He, Yu-Li (57226395775); Zhao, Qi (58190628700); Zhang, Hong (57103774200); Xu, Sheng-Yong (57226226930)","57222866846; 57222864228; 57191500062; 57189596265; 57226395775; 58190628700; 57103774200; 57226226930","A Virtual Vision Navigation System for The Blind Using Wearable Touch-vision Devices; [一种基于可穿戴器件触觉感知信息的盲人虚拟视觉导航系统]","2022","Progress in Biochemistry and Biophysics","49","8","","1543","1554","11","2","10.16476/j.pibb.2021.0320","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85153043253&doi=10.16476%2fj.pibb.2021.0320&partnerID=40&md5=e58fe94fd76ce6c9191a56ca19c2b4b7","School of Electronics, Peking University, Key Laboratory for the Physics & Chemistry of Nanodevices, Beijing, 100871, China; School of Microelectronics, Shandong University, Jinan, 250100, China; School of Electronic and Information Engineering, Beihang University, Beijing, 100191, China; School of Astronautics, Beihang University, Beijing, 100191, China","Ge S., School of Electronics, Peking University, Key Laboratory for the Physics & Chemistry of Nanodevices, Beijing, 100871, China; Lin Y.-N., School of Electronics, Peking University, Key Laboratory for the Physics & Chemistry of Nanodevices, Beijing, 100871, China; Lai S.-N., School of Electronics, Peking University, Key Laboratory for the Physics & Chemistry of Nanodevices, Beijing, 100871, China; Xu J.-J., School of Microelectronics, Shandong University, Jinan, 250100, China; He Y.-L., School of Electronic and Information Engineering, Beihang University, Beijing, 100191, China; Zhao Q., School of Electronic and Information Engineering, Beihang University, Beijing, 100191, China; Zhang H., School of Astronautics, Beihang University, Beijing, 100191, China; Xu S.-Y., School of Electronics, Peking University, Key Laboratory for the Physics & Chemistry of Nanodevices, Beijing, 100871, China","Objective More than 200 million people are visually impaired or blind worldwide. Artificial vision system has been widely studied for a long time. There are two main technical paths for the research of artificial vision system, the first is implantable artificial vision device, and the second is non-implantable and wearable device. This study demonstrates a non-implantable system prototype based on a wearable touch-vision device designed for the head that can help blind people and visually impaired people complete complex tasks in life such as walking. Methods The image information front of the subjects is collected, and transmitted wirelessly to the operator. After analyzed and processed, it is wirelessly transmitted to wearable devices such as headgears in the form of tactile coding to trigger the multi-point head tactile sensation, thus the subject's information or precise action instructions is obtained. The system also assists the subjects with voice information, allowing them to learn more about road conditions and environmental information. Results 5 healthy volunteers (2 males and 3 females) and 1 young blind person (male) were tested with the prototype. The results show that, the prototype makes full use of the head's natural distinguishing ability for directions including front, back, left and right, and quick response ability to the touch, and the prototype can send clear instructions such as going straight, adjusting to the left or right, and turning. The test showed that the response time of subjects to tactile commands was within 0.5 s, and the mean value of the standard deviation of the deviation of walking for about 7 m is reduced to (16± 10) cm compared with the case without wearing the device. Conclusion Experimental results show that the system can provide environmental image information in a concealed and accurate manner, and help visually impaired people complete daily activities such as walking, avoiding obstacles, going up steps, entering cafes and other public places, fetching objects on the table, which meets expectations of assisting blind people improve their quality of life. In the future, the prototype system will be further improved, especially miniaturization and smart chip. The tactile devices are made into neck rings, belts, bracelets, foot rings, etc., which will be widely used in a variety of work scenarios such as travel assistance for the blind, night field trips, and deep sea diving. © 2022 Institute of Biophysics,Chinese Academy of Sciences. All rights reserved.","artificial vision system; head touch; the blind; voice assistance; wireless transmission","","J.-J. Xu; School of Microelectronics, Shandong University, Jinan, 250100, China; email: xujj@sdu.edu.cn; S.-Y. Xu; School of Electronics, Peking University, Key Laboratory for the Physics & Chemistry of Nanodevices, Beijing, 100871, China; email: xusy@pku.edu.cn","","Institute of Biophysics,Chinese Academy of Sciences","10003282","","","","Chinese","Prog. Biochem. Biophys.","Article","Final","","Scopus","2-s2.0-85153043253"
"Leong K.Y.; Lim S.M.","Leong, Kuan Yew (58366739500); Lim, Siew Mooi (56041868800)","58366739500; 56041868800","SurDis: A Surface Discontinuity Dataset for Wearable Technology to Assist Blind Navigation in Urban Environments","2022","Advances in Neural Information Processing Systems","35","","","","","","0","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85163150472&partnerID=40&md5=31d384e16c9d4b21ed0b0e8b7c590733","A.I. System Research Co., Ltd., Kyoto, 606-8302, Japan; Tunku Abdul Rahman, University of Management and Technology, Malaysia","Leong K.Y., A.I. System Research Co., Ltd., Kyoto, 606-8302, Japan; Lim S.M., Tunku Abdul Rahman, University of Management and Technology, Malaysia","According to World Health Organization, there is an estimated 2.2 billion people with a near or distance vision impairment worldwide. Difficulty in self-navigation is one of the greatest challenges to independence for the blind and low vision (BLV) people. Through consultations with several BLV service providers, we realized that negotiating surface discontinuities is one of the very prominent challenges when navigating an outdoor environment within the urban. Surface discontinuities are commonly formed by rises and drop-offs along a pathway. They could be a threat to balancing during a walk and perceiving such a threat is highly challenging to the BLVs. In this paper, we introduce SurDis, a novel dataset of depth maps and stereo images that exemplifies the issue of surface discontinuity in the urban areas of Klang Valley, Malaysia. We seek to address the limitation of existing datasets of such nature in these areas. Current mobility tools for the BLVs predominantly focus on furniture, indoor built environments, traffic signs, vehicles, humans and various types of objects' detection above the surface of a pathway. We emphasize a specific purpose for SurDis - to support the development of assistive wearable technology for the BLVs to negotiate surface discontinuity. We consulted BLV volunteers on the specifications of surface condition that could become hazardous for navigation using 3D printed replicas of actual scaled-down scenes, and identified locations that are frequented by the BLVs as our target data collection fields. With feedback from these volunteers, we developed a lightweight, small and unobtrusive prototype equipped with a tiny stereo camera and an embedded system on a single board computer to capture the samples from 10 different locations. We describe instrument development, data collection, preprocessing, annotation, and experiments conducted. The dataset contains: (1) more than 17000 depth maps generated from 200 sets of stereo image sequences, (2) annotations of surface discontinuity in the depth maps, and (3) bitmap stereo image pairs corresponding to the depth maps in (1). © 2022 Neural information processing systems foundation. All rights reserved.","","3D printing; Data acquisition; Object detection; Stereo image processing; Traffic signs; Wearable technology; Blind navigation; Data collection; Depthmap; Low vision; Service provider; Stereoimages; Surface discontinuities; Urban environments; Vision impairments; World Health Organization; Navigation","","Koyejo S.; Mohamed S.; Agarwal A.; Belgrave D.; Cho K.; Oh A.","Neural information processing systems foundation","10495258","978-171387108-8","","","English","Adv. neural inf. proces. syst.","Conference paper","Final","","Scopus","2-s2.0-85163150472"
"Hasan M.Z.; Sikder S.; Rahaman M.A.","Hasan, Md. Zahidul (57217411850); Sikder, Shovon (58246435100); Rahaman, Muhammad Aminur (56605926600)","57217411850; 58246435100; 56605926600","Real-Time Computer Vision Based Autonomous Navigation System for Assisting Visually Impaired People using Machine Learning","2022","2022 4th International Conference on Sustainable Technologies for Industry 4.0, STI 2022","","","","","","","1","10.1109/STI56238.2022.10103268","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85159073850&doi=10.1109%2fSTI56238.2022.10103268&partnerID=40&md5=cc6904ea8ba93b6c47e738e3c3974ca8","Green University of Bangladesh, Dept of Computer Science and Engineering, Bangladesh","Hasan M.Z., Green University of Bangladesh, Dept of Computer Science and Engineering, Bangladesh; Sikder S., Green University of Bangladesh, Dept of Computer Science and Engineering, Bangladesh; Rahaman M.A., Green University of Bangladesh, Dept of Computer Science and Engineering, Bangladesh","Visual impairment is a global problem and people without vision suffer more than other impaired people. A companion is always needed for the movement of blind people and there may not be anyone by their side in case of emergency. Walking alone on the street, detecting the person closest to him and avoiding obstacles are always problems. Researchers have been working for visually impaired people using sensor based distance measurement systems for years. This paper proposes a Computer Vision based system to navigate visually impaired people by using Artificial Intelligence, and also a novel distance measuring approach. The system will capture real time images through a camera placed inside a sun-glass then process the video frames by trained YOLO V3 model. After processing, the program will identify a total of 80 pre-trained objects and additional 7 objects including person, car, bicycle, and broken roads and then will produce a navigation command through headphones. A comparative evaluation with other similar works is performed, and the result represents the primary accomplishments of this article. Several testing and validation procedures were carried out in order to achieve optimal performance and accurate distance measurement. The proposed system outperforms the state-ofthe-art in terms of object detection, distance measurement, computational costs calculation, and accessibility for the visually impaired, according to the results, which were validated using mathematical calculations and the necessary measuring devices. Since Industry 4.0 demands smart automation, this system has a significant impact not just on disabled persons but also on the development of a smart city.  © 2022 IEEE.","Autonomous Navigation System; Computer Vision; Deep Learning; Visual Aid; Visually Impaired People","Deep learning; Disabled persons; Distance measurement; Learning systems; Navigation systems; Object detection; Autonomous navigation systems; Deep learning; Global problems; Impaired people; Machine-learning; Real-time computer vision; Vision based; Visual aids; Visual impairment; Visually impaired people; Computer vision","M.Z. Hasan; Green University of Bangladesh, Dept of Computer Science and Engineering, Bangladesh; email: zahid.183002076@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","","978-166549045-0","","","English","Int. Conf. Sustain. Technol. Ind. 4.0, STI","Conference paper","Final","","Scopus","2-s2.0-85159073850"
"Rajesh Kannan S.; Ezhilarasi P.; Rajagopalan V.G.; Krishnamithran S.; Ramakrishnan H.; Balaji H.K.","Rajesh Kannan, S. (57191429283); Ezhilarasi, P. (55935559100); Rajagopalan, V.G. (58160735400); Krishnamithran, Sushanth (58160867700); Ramakrishnan, H. (58160824000); Balaji, Harish Kumar (58251577300)","57191429283; 55935559100; 58160735400; 58160867700; 58160824000; 58251577300","Integrated AI Based Smart Wearable Assistive Device for Visually and Hearing-Impaired People","2023","ICRTEC 2023 - Proceedings: IEEE International Conference on Recent Trends in Electronics and Communication: Upcoming Technologies for Smart Systems","","","","","","","3","10.1109/ICRTEC56977.2023.10111863","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85159436856&doi=10.1109%2fICRTEC56977.2023.10111863&partnerID=40&md5=e2b4dc609521e39c4ddffd1c34ade316","St. Joseph's College of Engineering, Department of Electronics and Communication Engineering, Chennai, India; ThoughtWorks, Chennai, India","Rajesh Kannan S., St. Joseph's College of Engineering, Department of Electronics and Communication Engineering, Chennai, India; Ezhilarasi P., St. Joseph's College of Engineering, Department of Electronics and Communication Engineering, Chennai, India; Rajagopalan V.G., St. Joseph's College of Engineering, Department of Electronics and Communication Engineering, Chennai, India; Krishnamithran S., St. Joseph's College of Engineering, Department of Electronics and Communication Engineering, Chennai, India; Ramakrishnan H., St. Joseph's College of Engineering, Department of Electronics and Communication Engineering, Chennai, India; Balaji H.K., ThoughtWorks, Chennai, India","An Intelligent Robotics Device (IRD) is developed in order help the disabled people and elderly people with day-to-day activities. The system offers five main functions: obstacle detection and avoidance through bone conduction, live tracking, GPS navigation, GSM SOS alert system and AI based image and face recognition System. It works with a combination of ultrasonic detection and bone conduction, detecting obstacles and letting the user know about them. To assure safe mobility, the product offers some core features namely, GPS live tracking and GPS navigation. GPS live tracking feature is useful in case if any of the user's relative wants to continuously monitor the movements of the blind user. The GSM - SOS alert system is included in the product, which comes into handy when the user finds any exigencies while travelling with just a push of button, the SOS alert system will send customized help request SMS along with location link to their career. GPS navigation system sends alert vibrations regarding the directions to be taken for navigating from one place to another. Those alerts are conveyed to the user through bone conduction phones. Finally, AI-facial and image recognition feature is also included which helps the blind to distinguish between known person and unknown person. All the electronic components which support these features are assembled and embedded on a wearable vest for ease of access.  © 2023 IEEE.","Artificial Intelligence; Cloud; False Acceptance Rate (FAR); False Rejection Rate (FRR); GPS; GSM; IoT; Machine Learning","Audition; Face recognition; Global positioning system; Global system for mobile communications; Internet of things; Obstacle detectors; Robots; Ultrasonic applications; Wearable technology; Alert systems; Bone conduction; False acceptance rate; False rejection rate; GPS navigation; IoT; Machine-learning; Smart wearables; Machine learning","","","Institute of Electrical and Electronics Engineers Inc.","","979-835039619-5","","","English","ICRTEC - Proc.: IEEE Int. Conf. Recent Trends Electron. Commun.: Upcom. Technol. Smart Syst.","Conference paper","Final","","Scopus","2-s2.0-85159436856"
"Rahman M.A.; Siddika S.; Al-Baky M.A.; Mia M.J.","Rahman, Md. Atiqur (59208084000); Siddika, Sadia (57257958200); Al-Baky, Md. Abdullah (57208161944); Mia, Md. Jueal (57218145526)","59208084000; 57257958200; 57208161944; 57218145526","An automated navigation system for blind people","2022","Bulletin of Electrical Engineering and Informatics","11","1","","201","212","11","4","10.11591/eei.v11i1.3452","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85124508142&doi=10.11591%2feei.v11i1.3452&partnerID=40&md5=a32da3cd2df502a0e241a27ee514e237","Department of Computer Science and Engineering, Daffodil International University, Dhaka, 1207, Bangladesh","Rahman M.A., Department of Computer Science and Engineering, Daffodil International University, Dhaka, 1207, Bangladesh; Siddika S., Department of Computer Science and Engineering, Daffodil International University, Dhaka, 1207, Bangladesh; Al-Baky M.A., Department of Computer Science and Engineering, Daffodil International University, Dhaka, 1207, Bangladesh; Mia M.J., Department of Computer Science and Engineering, Daffodil International University, Dhaka, 1207, Bangladesh","Proper navigation and detailed perception in familiar or unfamiliar environments are the main roles for human life. Eyesight sense helps humans to abstain from all kinds of dangers and navigate to indoor and outdoor environments. These are challenging activities for blind people in all environments. Many assistive tools have been developed by the blessing of technology like braille compasses and white canes that help them to navigate around in the environment. A vision and cloud-based navigation system for the visually impaired or blind person was developed. Our aim was not only to navigate them but also to perceive the environment in as much detail as a normal person. The proposed system includes ultrasonic sensors detecting obstacles, stereo camera to capture videos to perceive the environment using deep learning algorithms. Face recognition approach identified known faces in front of him. Blind people interacted with the whole system through a speech recognition module and all the information was stored in the cloud. Web and android applications were developed to track blinds so that guardians were monitoring them while visiting and reached them in an emergency. The experimental results showed the proposed system could provide more plenty information and user-friendly interaction. © 2022, Institute of Advanced Engineering and Science. All rights reserved.","Blind navigation; Deep learning; Face recognition; Optical character recognition; Speech processing; Yolo","","M.A. Rahman; Department of Computer Science and Engineering, Daffodil International University, Dhaka, 1207, Bangladesh; email: atiq.misterrahman@gmail.com","","Institute of Advanced Engineering and Science","20893191","","","","English","Bull. Electr. Eng. Inform.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85124508142"
"Pawar A.; Nainani J.; Hotchandani P.; Patil G.","Pawar, Anish (58123988700); Nainani, Jatin (57914061400); Hotchandani, Priyanka (58123692300); Patil, Gayatri (57208855639)","58123988700; 57914061400; 58123692300; 57208855639","Smartphone Based Tactile Feedback System Providing Navigation and Obstacle Avoidance to the Blind and Visually Impaired","2022","5th IEEE International Conference on Advances in Science and Technology, ICAST 2022","","","","236","242","6","3","10.1109/ICAST55766.2022.10039535","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85149177589&doi=10.1109%2fICAST55766.2022.10039535&partnerID=40&md5=c9461dae9dd0e82624986da77a044aad","K.J. Somaiya College of Engineering, Department of Electronics and Telecommunication, Mumbai, India; K.J. Somaiya College of Engineering, Department of Computer Science, Mumbai, India","Pawar A., K.J. Somaiya College of Engineering, Department of Electronics and Telecommunication, Mumbai, India; Nainani J., K.J. Somaiya College of Engineering, Department of Electronics and Telecommunication, Mumbai, India; Hotchandani P., K.J. Somaiya College of Engineering, Department of Computer Science, Mumbai, India; Patil G., K.J. Somaiya College of Engineering, Department of Computer Science, Mumbai, India","People with vision impairments and other visual disorders require support to complete daily tasks like moving around and discovering new places. They may find it difficult to navigate through a new place and could be put in danger if they run into unforeseen barriers or could get lost easily. This paper discusses a solution to aid the blind and visually impaired navigate independently while avoiding obstacles in their path. The system has two operational modes, Outdoor Navigation and Indoor Navigation, and it alerts the user through vibration (tactile) and audio feedback. While the Indoor mode accompanies a user to a labelled site, the Outdoor mode leads them to a geographical destination. The solution was able to reduce the Clearance time by 27.35 percent and the Obstacle hit rate by 66.6 percent. The system is hassle-free, comfortable to use and affordable because it requires only the user's smartphone and a custom-made hardware waist belt.  © 2022 IEEE.","Assistive Technology; Blind Navigation; Computer Vision; GPS; Machine Learning; Obstacle Avoidance; Smart System; Smartphone; Tactile Feedback","Computer vision; Indoor positioning systems; Machine learning; Smartphones; Assistive technology; Blind and visually impaired; Blind navigation; Feedback systems; Machine-learning; Obstacles avoidance; Smart phones; Smart System; Tactile feedback; Vision impairments; Navigation","","","Institute of Electrical and Electronics Engineers Inc.","","978-166549263-8","","","English","IEEE Int. Conf. Adv. Sci. Technol., ICAST","Conference paper","Final","","Scopus","2-s2.0-85149177589"
"Messaoudi M.D.; Menelas B.-A.J.; Mcheick H.","Messaoudi, Mohamed Dhiaeddine (57945921900); Menelas, Bob-Antoine J. (25522432900); Mcheick, Hamid (6508070583)","57945921900; 25522432900; 6508070583","Review of Navigation Assistive Tools and Technologies for the Visually Impaired","2022","Sensors","22","20","7888","","","","34","10.3390/s22207888","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85140841463&doi=10.3390%2fs22207888&partnerID=40&md5=8015ea1b4b93dc353d9e7cb8f096ea44","Department of Computer Sciences and Mathematics, University of Quebec at Chicoutimi, 555 Blv Universite, Chicoutimi, G7H 2B1, QC, Canada","Messaoudi M.D., Department of Computer Sciences and Mathematics, University of Quebec at Chicoutimi, 555 Blv Universite, Chicoutimi, G7H 2B1, QC, Canada; Menelas B.-A.J., Department of Computer Sciences and Mathematics, University of Quebec at Chicoutimi, 555 Blv Universite, Chicoutimi, G7H 2B1, QC, Canada; Mcheick H., Department of Computer Sciences and Mathematics, University of Quebec at Chicoutimi, 555 Blv Universite, Chicoutimi, G7H 2B1, QC, Canada","The visually impaired suffer greatly while moving from one place to another. They face challenges in going outdoors and in protecting themselves from moving and stationary objects, and they also lack confidence due to restricted mobility. Due to the recent rapid rise in the number of visually impaired persons, the development of assistive devices has emerged as a significant research field. This review study introduces several techniques to help the visually impaired with their mobility and presents the state-of-the-art of recent assistive technologies that facilitate their everyday life. It also analyses comprehensive multiple mobility assistive technologies for indoor and outdoor environments and describes the different location and feedback methods for the visually impaired using assistive tools based on recent technologies. The navigation tools used for the visually impaired are discussed in detail in subsequent sections. Finally, a detailed analysis of various methods is also carried out, with future recommendations. © 2022 by the authors.","blind people; deep learning; navigation method; speech and voice recognition; visually impaired","Humans; Self-Help Devices; Technology; Visually Impaired Persons; Assistive technology; Deep learning; Navigation; Assistive technology; Assistive tool; Blind people; Deep learning; Moving objects; Navigation methods; Restricted mobilities; Stationary objects; Tools and technologies; Visually impaired; human; self help device; technology; visually impaired person; Speech recognition","M.D. Messaoudi; Department of Computer Sciences and Mathematics, University of Quebec at Chicoutimi, Chicoutimi, 555 Blv Universite, G7H 2B1, Canada; email: mohamed-dhiaeddine.messaoudi1@uqac.ca; B.-A.J. Menelas; Department of Computer Sciences and Mathematics, University of Quebec at Chicoutimi, 555 Blv Universite, Chicoutimi, G7H 2B1, Canada; email: bamenela@uqac.ca","","MDPI","14248220","","","36298237","English","Sensors","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85140841463"
"Azmat Shah P.; Ali Rashed Ali Alhefeiti H.; Hussain Mohamed Ali Alrayssi R.","Azmat Shah, Peer (57215905773); Ali Rashed Ali Alhefeiti, Hessah (58545164900); Hussain Mohamed Ali Alrayssi, Rayan (58545271300)","57215905773; 58545164900; 58545271300","Machine Learning-based Smart Assistance System for the Visually Impaired","2023","2023 9th International Conference on Information Technology Trends, ITT 2023","","","","139","144","5","3","10.1109/ITT59889.2023.10184257","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85168542734&doi=10.1109%2fITT59889.2023.10184257&partnerID=40&md5=3ffb03b0a6215643f4d7a79bc3308010","Higher Colleges of Technology, Department of Computer & Information Sciences (CIS), Fujairah, United Arab Emirates","Azmat Shah P., Higher Colleges of Technology, Department of Computer & Information Sciences (CIS), Fujairah, United Arab Emirates; Ali Rashed Ali Alhefeiti H., Higher Colleges of Technology, Department of Computer & Information Sciences (CIS), Fujairah, United Arab Emirates; Hussain Mohamed Ali Alrayssi R., Higher Colleges of Technology, Department of Computer & Information Sciences (CIS), Fujairah, United Arab Emirates","Individuals with visual impairments face significant challenges in their daily lives, including mobility issues and difficulties recognizing people and differentiating between places. As a result, there is a pressing need for an intelligent device that can assist these individuals and help alleviate some of the difficulties they encounter. The proposed solution aims to address three of the primary challenges faced by visually impaired individuals: recognizing people, familiarizing themselves with indoor spaces, and receiving notifications in case of danger. This research work seeks to assist individuals who are blind in identifying others using a camera-on-glasses that can perform facial recognition and detect and identify faces. Additionally, the system can provide audio feedback through headphones to inform the user of the identity of individuals in their vicinity. Moreover, the system features a unique indoor navigation feature that utilizes radio frequency and machine learning technology to inform the user of their current location within a building. The system can also learn from its environment to detect emergency situations and alert a loved one in case the visually impaired individual is in danger.  © 2023 IEEE.","indoor navigation; machine learning; smart glasses; visually impaired","Face recognition; Indoor positioning systems; Machine learning; Assistance system; Daily lives; Indoor navigation; Indoor space; Intelligent devices; Machine-learning; Pressung; Smart glass; Visual impairment; Visually impaired; Glass","P. Azmat Shah; Higher Colleges of Technology, Department of Computer & Information Sciences (CIS), Fujairah, United Arab Emirates; email: pshah@hct.ac.ae","","Institute of Electrical and Electronics Engineers Inc.","","979-835032750-2","","","English","Int. Conf. Inf. Technol. Trends, ITT","Conference paper","Final","","Scopus","2-s2.0-85168542734"
"Senthil Kumar A.; Kesavan S.; Jayakumar J.; Ananda Kumar K.S.; Maddula P.","Senthil Kumar, A. (56884313400); Kesavan, Selvaraj (56720202000); Jayakumar, J. (58189105400); Ananda Kumar, K.S. (57201022037); Maddula, Prasad (57211971496)","56884313400; 56720202000; 58189105400; 57201022037; 57211971496","Voice Enabled Deep Learning Based Image Captioning Solution for Guided Navigation","2023","2023 International Conference on Network, Multimedia and Information Technology, NMITCON 2023","","","","","","","1","10.1109/NMITCON58196.2023.10276134","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85175402759&doi=10.1109%2fNMITCON58196.2023.10276134&partnerID=40&md5=65b6a9e1b0c0238c1de71a802bfeeb99","Shri Vishnu Engg College for Women, Artificial Intelligence Department, AP, Bhimavaram, India; Technical Architect, Dxc Technology; Karunya University, Electrical & Electronics Engineering, TN, Coimbatore, India; Atria Institute of Technology, Information Science and Engineering, Karnataka, Bengaluru, India; Computer Science and Engineering, Shri Vishnu Engg, College for Women, AP, Bhimavaram, India","Senthil Kumar A., Shri Vishnu Engg College for Women, Artificial Intelligence Department, AP, Bhimavaram, India; Kesavan S., Technical Architect, Dxc Technology; Jayakumar J., Karunya University, Electrical & Electronics Engineering, TN, Coimbatore, India; Ananda Kumar K.S., Atria Institute of Technology, Information Science and Engineering, Karnataka, Bengaluru, India; Maddula P., Computer Science and Engineering, Shri Vishnu Engg, College for Women, AP, Bhimavaram, India","The use of technology to assist visually impaired individuals is crucial in addressing the global issue of vision impairment. Worldwide more than billion people suffer from a vision impairment that should have been avoided or is yet unaddressed. According to the statistics, there is a significant need for solutions that can help those who are visually impaired, mainly in the middle- and low-income countries where the vision impairment population is higher. It is anticipated that population expansion and ageing will increase the likelihood that more people may get vision impairment. The efficientnetB3 deep learning algorithm will be used in this project to caption images for blind people. so, they can learn about object identification, distance, and position. This has been accomplished by utilizing advanced picture captioning techniques, efficient net B3 algorithms, and tokenization approaches, where the computer learns the scenes with various captions. The computer recognizes and forecasts any image that is acquired using the camera. The significant objects are also anticipated, and the camera's distances are determined. Following the prediction, the user receives an audio output that can be used to determine the object's position and distance. Hence, with the aid of this research, we give the blind artificial eyesight that can give them confidence when they move on their own. The aim is to step forward in addressing the global issue of vision impairment. The use of technology to assist visually impaired individuals is crucial in providing them with the tools they need to navigate their environment and live their lives with greater ease. By utilizing advanced algorithms and image captioning techniques, the quality of life can be improved for people worldwide who are affected by vision impairment. The intension is to develop an artificial vision for vision impaired people by detecting real time objects, distance and the position of it from the person using Audio Output and to develop a model for image captioning to predict the captions.  © 2023 IEEE.","Deep learning; Guided navigation; Image captioning; tokenization","Deep learning; Eye movements; Forecasting; Image enhancement; Learning algorithms; Object detection; Population statistics; Deep learning; Global issues; Guided navigation; Image captioning; Learn+; Object distance; Object positions; Tokenization; Vision impairments; Visually impaired; Cameras","","","Institute of Electrical and Electronics Engineers Inc.","","979-835030082-6","","","English","Int. Conf. Netw., Multimed. Inf. Technol., NMITCON","Conference paper","Final","","Scopus","2-s2.0-85175402759"
"Delloul K.; Larabi S.","Delloul, K. (57939900900); Larabi, S. (55955680300)","57939900900; 55955680300","Image Captioning State-of-the-Art: Is It Enough for the Guidance of Visually Impaired in an Environment?","2022","Lecture Notes in Networks and Systems","513","","","385","394","9","1","10.1007/978-3-031-12097-8_33","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85140473291&doi=10.1007%2f978-3-031-12097-8_33&partnerID=40&md5=e9d60dead8da34135310e9b5f35b3263","RIIMA Laboratory, Faculty of Computer Science, USTHB University, BP 32 EL ALIA, Algiers, 16111, Algeria","Delloul K., RIIMA Laboratory, Faculty of Computer Science, USTHB University, BP 32 EL ALIA, Algiers, 16111, Algeria; Larabi S., RIIMA Laboratory, Faculty of Computer Science, USTHB University, BP 32 EL ALIA, Algiers, 16111, Algeria","Although image captioning has made great progress in describing images, current methods are limited in 2D recognition of salient and moving objects. This leads to sentences that lack information about static and background objects, with poor performance on words’ order and prepositions, which cannot be enough for blind people to completely understand a scene. They also don’t give precise information about spatial relationships of the detected objects, reducing by that the amount of information that the scene contains. In this paper, we will first explore the existing methods of state-of-the-art for image captioning, we will learn about the approaches, and highlight their shortcomings considering the egocentric guidance of blind and visually impaired people. We will explore and test some tools that may be needed in the future to augment them with new information and new functionalities that are needed by the visually impaired to be able to not only understand their environment, but to also move around it and have a complete awareness of the objects present in their scene. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Blind and visually impaired navigation; Computer vision; Deep learning; Image captioning; Image paragraph captioning; Segmentation","","K. Delloul; RIIMA Laboratory, Faculty of Computer Science, USTHB University, Algiers, BP 32 EL ALIA, 16111, Algeria; email: kdelloul@usthb.dz","Senouci M.R.; Boulahia S.Y.; Benatia M.A.","Springer Science and Business Media Deutschland GmbH","23673370","978-303112096-1","","","English","Lect. Notes Networks Syst.","Conference paper","Final","","Scopus","2-s2.0-85140473291"
"Gill S.; Pawluk D.T.V.","Gill, Satinder (55356634400); Pawluk, Dianne T. V. (57765699800)","55356634400; 57765699800","Design of a “Cobot Tactile Display” for Accessing Virtual Diagrams by Blind and Visually Impaired Users","2022","Sensors","22","12","4468","","","","3","10.3390/s22124468","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85132069324&doi=10.3390%2fs22124468&partnerID=40&md5=392b9f9db8b14815ac490cdf46019290","Department of Biomedical Engineering, Virginia Commonwealth University, Richmond, 23298, VA, United States","Gill S., Department of Biomedical Engineering, Virginia Commonwealth University, Richmond, 23298, VA, United States; Pawluk D.T.V., Department of Biomedical Engineering, Virginia Commonwealth University, Richmond, 23298, VA, United States","Access to graphical information plays a very significant role in today’s world. Access to this information can be particularly limiting for individuals who are blind or visually impaired (BVIs). In this work, we present the design of a low-cost, mobile tactile display that also provides robotic assistance/guidance using haptic virtual fixtures in a shared control paradigm to aid in tactile diagram exploration. This work is part of a larger project intended to improve the ability of BVI users to explore tactile graphics on refreshable displays (particularly exploration time and cognitive load) through the use of robotic assistance/guidance. The particular focus of this paper is to share information related to the design and development of an affordable and compact device that may serve as a solution towards this overall goal. The proposed system uses a small omni-wheeled robot base to allow for smooth and unlimited movements in the 2D plane. Sufficient position and orientation accuracy is obtained by using a low-cost dead reckoning approach that combines data from an optical mouse sensor and inertial measurement unit. A low-cost force-sensing system and an admittance control model are used to allow shared control between the Cobot and the user, with the addition of guidance/virtual fixtures to aid in diagram exploration. Preliminary semi-structured interviews, with four blind or visually impaired participants who were allowed to use the Cobot, found that the system was easy to use and potentially useful for exploring virtual diagrams tactually. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","admittance control; assistive technologies; dead reckoning; haptics; tactile displays; visually impaired","Animals; Blindness; Humans; Mice; Touch; User-Computer Interface; Visually Impaired Persons; Assistive technology; Costs; Display devices; Haptic interfaces; Mammals; Mice (computer peripherals); Navigation; Admittance control; Assistive technology; Blind and visually impaired users; Dead reckoning; Haptics; Low-costs; Shared control; Tactile display; Virtual fixture; Visually impaired; animal; blindness; computer interface; human; mouse; psychology; touch; visually impaired person; Robotics","S. Gill; Department of Biomedical Engineering, Virginia Commonwealth University, Richmond, 23298, United States; email: gills4@vcu.edu","","MDPI","14248220","","","35746250","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85132069324"
"Kulkarni M.; Chitale M.; Chitpur S.; Chivate A.; Chopade P.; Deshmukh S.","Kulkarni, Mukund (58091502900); Chitale, Maitrey (58511899400); Chitpur, Shreeshail (58512871700); Chivate, Atharva (58511899500); Chopade, Pratiksha (58511899600); Deshmukh, Sharvari (58584053500)","58091502900; 58511899400; 58512871700; 58511899500; 58511899600; 58584053500","Smart Assistive Navigator for the Blind using Image Processing","2023","International Conference on Sustainable Computing and Smart Systems, ICSCSS 2023 - Proceedings","","","","916","921","5","0","10.1109/ICSCSS57650.2023.10169767","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85166263545&doi=10.1109%2fICSCSS57650.2023.10169767&partnerID=40&md5=2a41a13c2daf1c1e264a9a424b976b80","Vishwakarma Institute of Technology, Department of Computer Engineering, Maharashtra, Pune, India","Kulkarni M., Vishwakarma Institute of Technology, Department of Computer Engineering, Maharashtra, Pune, India; Chitale M., Vishwakarma Institute of Technology, Department of Computer Engineering, Maharashtra, Pune, India; Chitpur S., Vishwakarma Institute of Technology, Department of Computer Engineering, Maharashtra, Pune, India; Chivate A., Vishwakarma Institute of Technology, Department of Computer Engineering, Maharashtra, Pune, India; Chopade P., Vishwakarma Institute of Technology, Department of Computer Engineering, Maharashtra, Pune, India; Deshmukh S., Vishwakarma Institute of Technology, Department of Computer Engineering, Maharashtra, Pune, India","The life of an individual depends on the basic five senses, and the ability to see is probably the most important one. Visually impaired individuals lack a sense of vision. Hence, daily activities are hampered by their inability to perceive their surroundings. They often face constraints related to independent mobility and navigation. This can lead to difficulties that can only be temporarily subdued by some assisting personnel, creating a feeling of dependency in such an individual. The current methods used nowadays, such as a smart cane, have a limited reach and can only detect obstacles in close proximity to the user. However, to provide comprehensive assistance and enhance independence for visually impaired individuals, it is clear that more techniques and solutions need to be invented. This paper proposes a sophisticated IoT-enabled wearable solution that also encompasses deep learning mechanisms. The architectural design includes a camera module and the Raspberry Pi as the processing unit. It is also equipped with buzzers. The camera module is used to detect obstacles in the user's route in real-time. Object detection is achieved using OpenCV and deep learning. The use of machine learning here enhances the effectiveness of the application model. Buzzers are interfaced to warn the user of any incoming obstacles. In this way, the system can guide and help the user reach his destination safely and independently, without any external aid.  © 2023 IEEE.","Assistive Aid; Blind; Image Processing; Internet of things; Obstacle Detection; Raspberry Pi","Cameras; Deep learning; Internet of things; Obstacle detectors; Assistive; Assistive aid; Blind; Camera modules; Daily activity; Five sense; Images processing; Obstacles detection; Raspberry pi; Visually impaired; Object detection","M. Kulkarni; Vishwakarma Institute of Technology, Department of Computer Engineering, Pune, Maharashtra, India; email: mukund.kulkarni@vit.edu","","Institute of Electrical and Electronics Engineers Inc.","","979-835033360-2","","","English","Int. Conf. Sustain. Comput. Smart Syst., ICSCSS - Proc.","Conference paper","Final","","Scopus","2-s2.0-85166263545"
"Alagarsamy S.; Rajkumar T.D.; Syamala K.P.L.; Niharika C.S.; Rani D.U.; Balaji K.","Alagarsamy, Saravanan (57218666701); Rajkumar, T. Dhiliphan (57194604617); Syamala, K.P.L. (57212905463); Niharika, Ch. Sandya (57224440753); Rani, D. Usha (57224440697); Balaji, K. (58729555200)","57218666701; 57194604617; 57212905463; 57224440753; 57224440697; 58729555200","An Real Time Object Detection Method for Visually Impaired Using Machine Learning","2023","2023 International Conference on Computer Communication and Informatics, ICCCI 2023","","","","","","","2","10.1109/ICCCI56745.2023.10128388","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85163058006&doi=10.1109%2fICCCI56745.2023.10128388&partnerID=40&md5=dd7f0e4292806ff20fddd2f44ca71f26","Kalasalingam Academy of Research and Education, Anand Nagar, Department of Computer Science and Engineering, Tamilnadu, Krishnankoil, India","Alagarsamy S., Kalasalingam Academy of Research and Education, Anand Nagar, Department of Computer Science and Engineering, Tamilnadu, Krishnankoil, India; Rajkumar T.D., Kalasalingam Academy of Research and Education, Anand Nagar, Department of Computer Science and Engineering, Tamilnadu, Krishnankoil, India; Syamala K.P.L., Kalasalingam Academy of Research and Education, Anand Nagar, Department of Computer Science and Engineering, Tamilnadu, Krishnankoil, India; Niharika C.S., Kalasalingam Academy of Research and Education, Anand Nagar, Department of Computer Science and Engineering, Tamilnadu, Krishnankoil, India; Rani D.U., Kalasalingam Academy of Research and Education, Anand Nagar, Department of Computer Science and Engineering, Tamilnadu, Krishnankoil, India; Balaji K., Kalasalingam Academy of Research and Education, Anand Nagar, Department of Computer Science and Engineering, Tamilnadu, Krishnankoil, India","Vision, one of the five fundamental human senses, is crucial for defining how people perceive the objects around them. Visual impairments affect more than 200 million people worldwide, severely limiting their ability to perform numerous activities of daily living. Thus, it is essential for blind people to understand their surroundings and the objects they are interacting with. In this work, we created a tool that helps blind persons recognize diverse items in their environment by utilizing the YOLO V3 algorithm combined with R-CNN. This comprises a variety of approaches to develop an app that not only instantly recognizes different objects in the visually impaired person's environment but also guides them using audio output. A convolutional neural network (CNN) called YOLO (You Only Look at Once) recognizes objects in real time. This suggested method is more effective and accurate than other algorithms for recognizing things, according to research results, and it produces results for object detection that are extremely similar in real time. It is crucial for persons who are blind or visually impaired to be able to reliably and effectively detect and recognize objects in order to navigate both common and unfamiliar situations safely, become stronger, and become more independent. © 2023 IEEE.","Computer vision; Convolutional Neural Network; Hyper Text Markup Language","Convolution; Convolutional neural networks; Machine learning; Markup languages; Object detection; Object recognition; Activities of Daily Living; Blind people; Convolutional neural network; Human sense; Hyper text markup language; Machine-learning; Object detection method; Real- time; Visual impairment; Visually impaired; Computer vision","S. Alagarsamy; Kalasalingam Academy of Research and Education, Anand Nagar, Department of Computer Science and Engineering, Krishnankoil, Tamilnadu, India; email: senthilsar@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","","979-835034821-7","","","English","Int. Conf. Comput. Commun. Informatics, ICCCI","Conference paper","Final","","Scopus","2-s2.0-85163058006"
"Kumar N.; Jain A.","Kumar, Nitin (57214110221); Jain, Anuj (57197766121)","57214110221; 57197766121","A DEEP LEARNING BASED MODEL TO ASSIST BLIND PEOPLE IN THEIR NAVIGATION","2022","Journal of Information Technology Education: Innovations in Practice","21","","","95","114","19","11","10.28945/5006","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85138066189&doi=10.28945%2f5006&partnerID=40&md5=c809a84c987e7ede8e300acc83763f39","Lovely Professional University, Phagwara, India","Kumar N., Lovely Professional University, Phagwara, India; Jain A., Lovely Professional University, Phagwara, India","Aim/Purpose This paper proposes a new approach to developing a deep learning-based pro-totyping wearable model which can assist blind and visually disabled people to recognize their environments and navigate through them. As a result, visually impaired people will be able to manage day-to-day activities and navigate through the world around them more easily. Background In recent decades, the development of navigational devices has posed chal-lenges for researchers to design smart guidance systems for visually impaired and blind individuals in navigating through known or unknown environments. Efforts need to be made to analyze the existing research from a historical per-spective. Early studies of electronic travel aids should be integrated with the use of assistive technology-based artificial vision models for visually impaired per-sons. Methodology This paper is an advancement of our previous research work, where we per-formed a sensor-based navigation system. In this research, the navigation of the visually disabled person is carried out with a vision-based 3D-designed wearable model and a vision-based smart stick. The wearable model used a neural net-work-based You Only Look Once (YOLO) algorithm to detect the course of the navigational path which is augmented by a GPS-based smart Stick. Over 100 images of each of the three classes, namely straight path, left path and right path, are being trained using supervised learning. The model accurately predicts a straight path with 79% mean average precision (mAP), the right path with 83% mAP, and the left path with 85% mAP. The average accuracy of the weara-ble model is 82.33% and that of the smart stick is 96.14% which combined gives an overall accuracy of 89.24%. Contribution This research contributes to the design of a low-cost navigational standalone system that will be handy to use and help people to navigate safely in real-time scenarios. The challenging self-built dataset of various paths is generated and transfer learning is performed on the YOLO-v5 model after augmentation and manual annotation. To analyze and evaluate the model, various metrics, such as model losses, recall value, precision, and maP, are used. Findings These were the main findings of the study: • To detect objects, the deep learning model uses a higher version of YOLO, i.e., a YOLOv5 detector, that may help those with visual im-pairments to improve their quality of navigational mobilities in known or unknown environments. • The developed standalone model has an option to be integrated into any other assistive applications like Electronic Travel Aids (ETAs) • It is the single neural network technology that allows the model to achieve high levels of detection accuracy of around 0.823 mAP with a custom dataset as compared to 0.895 with the COCO dataset. Due to its lightning-speed of 45 FPS object detection technology, it has be-come popular. Recommendations for Practitioners Practitioners can help the model’s efficiency by increasing the sample size and classes used in training the model. Recommendations for Researchers To detect objects in an image or live cam, there are various algorithms, e.g., R-CNN, Retina Net, Single Shot Detector (SSD), YOLO. Researchers can choose to use the YOLO version owing to its superior performance. Moreover, one of the YOLO versions, YOLOv5, outperforms its other versions such as YOLOv3 and YOLOv4 in terms of speed and accuracy. Impact on Society We discuss new low-cost technologies that enable visually impaired people to navigate effectively in indoor environments. Future Research The future of deep learning could incorporate recurrent neural networks on a larger set of data with special AI-based processors to avoid latency. © 2022, Journal of Information Technology Education: Innovations in Practice. All Rights Reserved.","Assistive technology; Blind; Handheld assistive technology; Navigation; Object detection; Visually impaired; Wearable devices","","N. Kumar; Lovely Professional University, Phagwara, India; email: nitin.14652@lpu.co.in","","Informing Science Institute","21653151","","","","English","J. Inf. Technol. Educ.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85138066189"
"Dabhade V.; Dhawalshankh D.; Thakare A.; Kulkarni M.; Ambekar P.","Dabhade, Vijay (58180624100); Dhawalshankh, Dnyaneshwar (58180754100); Thakare, Anuradha (55995624200); Kulkarni, Maithili (58180624200); Ambekar, Priyanka (58180689800)","58180624100; 58180754100; 55995624200; 58180624200; 58180689800","A Smart System for Obstacle Detection to Assist Visually Impaired in Navigating Autonomously Using Machine Learning Approach","2023","Artificial Intelligence Applications and Reconfigurable Architectures","","","","137","149","12","0","10.1002/9781119857891.ch7","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85152310275&doi=10.1002%2f9781119857891.ch7&partnerID=40&md5=982933a5a64655be1e78953cced2cd1c","Department of Computer Engineering, Pimpri Chinchwad College of Engineering, Pune, India","Dabhade V., Department of Computer Engineering, Pimpri Chinchwad College of Engineering, Pune, India; Dhawalshankh D., Department of Computer Engineering, Pimpri Chinchwad College of Engineering, Pune, India; Thakare A., Department of Computer Engineering, Pimpri Chinchwad College of Engineering, Pune, India; Kulkarni M., Department of Computer Engineering, Pimpri Chinchwad College of Engineering, Pune, India; Ambekar P., Department of Computer Engineering, Pimpri Chinchwad College of Engineering, Pune, India","Obstacle detection is a popular approach for detecting barriers in the region of a subject. Visual impairment affects a large number of people around the world. Visually impaired or blind people encounter numerous challenges in their daily lives; the white cane is still the most widely used instrument for obstacle detection; in an unfamiliar environment, they rely entirely on other people to reach their desired goal. This will allow blind people to navigate independently without the use of any aids by utilizing object detection systems that detect objects over a period of time. The proposed system in this article uses machine learning algorithms to see objects through the camera and then uses audio output to teach blind people about the item and its location. Obstacle detection approaches are discussed that can create and develop a system which will assist visually impaired people in navigating autonomously. © 2023 Scrivener Publishing LLC.","Image processing; IoT; machine learning; obstacle detection","","V. Dabhade; Department of Computer Engineering, Pimpri Chinchwad College of Engineering, Pune, India; email: vijayrajdabhade@gmail.com","","wiley","","978-111985789-1; 978-111985729-7","","","English","Artificial Intelligence Applications and Reconfigurable Architectures","Book chapter","Final","","Scopus","2-s2.0-85152310275"
"Vijayakumar P.; Yuvaraj T.; Moorthy C.A.S.; Upadhyaya M.; Dadheech P.; Kshirsagar P.R.","Vijayakumar, P. (55975531200); Yuvaraj, T. (56926781500); Moorthy, C. A. Sathiya (57723269700); Upadhyaya, Makarand (57190817554); Dadheech, Pankaj (57189097825); Kshirsagar, Pravin R. (57192542222)","55975531200; 56926781500; 57723269700; 57190817554; 57189097825; 57192542222","Artificial intelligence based algorithm to support disable person","2022","AIP Conference Proceedings","2393","","020073","","","","3","10.1063/5.0074090","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85131184018&doi=10.1063%2f5.0074090&partnerID=40&md5=7a6204c6e87b6637d2e31c0ae34238c9","Vellore Institute of Technology, Chennai, India; Saveetha Institute of Medical and Technical Sciences, Chennai, India; CMS College of Engineering, Namakkal, 63700, India; College of Business Administration, Bahrain, Bahrain; Swami Keshvanand Institute of Technology Management &gramotham, Rajasthan, Jaipur, 302017, India; AVNIET, Hyderabad, India","Vijayakumar P., Vellore Institute of Technology, Chennai, India; Yuvaraj T., Saveetha Institute of Medical and Technical Sciences, Chennai, India; Moorthy C.A.S., CMS College of Engineering, Namakkal, 63700, India; Upadhyaya M., College of Business Administration, Bahrain, Bahrain; Dadheech P., Swami Keshvanand Institute of Technology Management &gramotham, Rajasthan, Jaipur, 302017, India; Kshirsagar P.R., AVNIET, Hyderabad, India","The paper explores how the daily lives of people with vision impairments are changed by artificial intelligence. They suffer a great deal in circumstances they are not aware of. When they go alone in town, people are worried about their safety. The overall aim of the system is to provide low-cost navigation assistance to blind people that give a sense of artificial vision by informing people of the artificial intelligence environment of objects. An ultrasound sensor is used to detect the distance between objects to the blind person to guide voice and vibration, which can be heard and felt by the blind person. The software can help identify objects in the world by using the voice command, conduct text analysis and recognize the document's text on paper. It can be an important way for blind people to communicate and encourage blind people to live independently. © 2022 Author(s).","artificial intelligence; collision detection; image recognition; obstacle detection; visually impaired","","P. Vijayakumar; Vellore Institute of Technology, Chennai, India; email: vijayrgcet@gmail.com","Ramachandran M.; Kalita K.; Ghadai R.","American Institute of Physics Inc.","0094243X","978-073544198-9","","","English","AIP Conf. Proc.","Conference paper","Final","","Scopus","2-s2.0-85131184018"
"Mustafa A.; Omer A.; Mohammed O.","Mustafa, Ali (58079031900); Omer, Ahmed (58079793700); Mohammed, Ogba (58079285000)","58079031900; 58079793700; 58079285000","Intelligent Glasses for Visually Impaired People","2022","Proceedings - 2022 14th IEEE International Conference on Computational Intelligence and Communication Networks, CICN 2022","","","","29","33","4","3","10.1109/CICN56167.2022.10008291","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85146909849&doi=10.1109%2fCICN56167.2022.10008291&partnerID=40&md5=bbf2cf057c968cc1d1ae35f310957f55","Sudan University for Science and Technology, Electrical Engineering, Khartoum, Sudan","Mustafa A., Sudan University for Science and Technology, Electrical Engineering, Khartoum, Sudan; Omer A., Sudan University for Science and Technology, Electrical Engineering, Khartoum, Sudan; Mohammed O., Sudan University for Science and Technology, Electrical Engineering, Khartoum, Sudan","Vision impairment and blindness are a big concern in the research field of assistive technologies. An individual with sight impairment suffers from performing tasks that seem simple to a sighted person. However, these effortless tasks for sighted humans are very complex to be performed by machines. With fast processors and high-resolution cameras, researchers can develop accurate algorithms to assist blind and visually impaired (BVI) individuals in performing their daily activities. In this research, we designed an intelligent eyeglass that integrates various models to perform face recognition, object detection, and path navigation. When the system is powered, a welcoming message is played to the user asking him/her to choose one of the three modes. When the user pronounces the desired mode, the system uses Google speech recognition API to identify the request, tunes the required mode, and responds accordingly. Face recognition and object detection modes provide audio feedback of the names of the surrounding known people and objects, respectively. On the other hand, the navigation mode triggers a slight vibration whenever an obstacle is within a range of half a meter. Raspberry Pi 4 is used to control the system components because it can perform all the sophisticated computations needed in a short time, offering the user a realistic experience. Moreover, the low cost of Raspberry Pi, its small size, and its lightweight result in practical and simple-design eyeglasses.  © 2022 IEEE.","Assistive technology; Blind and Visually Impaired; deep learning; Face Recognition; Raspberry Pi; real-time; TensorFlow Lite object detector; Voice Assist","Assistive technology; Deep learning; Object detection; Object recognition; Speech recognition; Assistive technology; Blind and visually impaired; Deep learning; Object detectors; Objects detection; Raspberry pi; Real- time; Simple++; Tensorflow lite object detector; Voice assist; Face recognition","","","Institute of Electrical and Electronics Engineers Inc.","","978-166548771-9","","","English","Proc. - IEEE Int. Conf. Comput. Intell. Commun. Networks, CICN","Conference paper","Final","","Scopus","2-s2.0-85146909849"
"Patankar N.S.; Haribhau B.; Dhorde P.S.; Pravin Patil H.; Maind R.V.; Deshmukh Y.S.","Patankar, Nikhil S. (57982306900); Haribhau, Bhushan (58765782100); Dhorde, Prithviraj Shivaji (58765329600); Pravin Patil, Harshal (58765456200); Maind, Rohit Vijay (58765715700); Deshmukh, Yogesh S. (59128501700)","57982306900; 58765782100; 58765329600; 58765456200; 58765715700; 59128501700","An Intelligent IoT Based Smart Stick For Visually Impaired Person Using Image Sensing","2023","2023 14th International Conference on Computing Communication and Networking Technologies, ICCCNT 2023","","","","","","","1","10.1109/ICCCNT56998.2023.10306645","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85179851556&doi=10.1109%2fICCCNT56998.2023.10306645&partnerID=40&md5=0f2e5a33ae6307ba14eaa65d36b7f620","Sanjivani College of Engineering, Department of Information Technology, Kopargaon, India","Patankar N.S., Sanjivani College of Engineering, Department of Information Technology, Kopargaon, India; Haribhau B., Sanjivani College of Engineering, Department of Information Technology, Kopargaon, India; Dhorde P.S., Sanjivani College of Engineering, Department of Information Technology, Kopargaon, India; Pravin Patil H., Sanjivani College of Engineering, Department of Information Technology, Kopargaon, India; Maind R.V., Sanjivani College of Engineering, Department of Information Technology, Kopargaon, India; Deshmukh Y.S., Sanjivani College of Engineering, Department of Information Technology, Kopargaon, India","Visual impairment is a global concern affecting millions of people worldwide, with a significant proportion classified as blind. The traditional blind stick, while widely used, presents limitations such as skill requirements, costs, and extensive training. However, recent technological advancements have paved the way for innovative solutions to aid visually impaired individuals in navigating their surroundings effectively. This paper explores the development and potential of smart blind sticks, which leverage sensors, cameras, and artificial intelligence algorithms to provide enhanced assistance to users. These devices offer obstacle detection, auditory and tactile feedback, and even GPS navigation, empowering visually impaired individuals and improving their mobility and quality of life. Several research initiatives worldwide are actively contributing to the design and development of user-friendly, affordable smart blind sticks that require minimal training. With the use of an IoT stick, this research hopes to create an image of opportunity, autonomy, and certainty. To swiftly complete their everyday tasks, the proposed smart stick is designed with an obstacle recognition module, a worldwide positioning system (GPS), pit and flight of stairs detection, water detection, and a global system for mobile communication (GSM). © 2023 IEEE.","accessibility; Artificial Intelligence; assistive technology; inclusive design; independence; mobility; Obstacle detection; Smart Blind Stick; Visual impairment","Artificial intelligence; Assistive technology; Global positioning system; Internet of things; Obstacle detectors; Ophthalmology; Accessibility; Assistive technology; Inclusive design; Independence; Mobility; Obstacles detection; Smart blind stick; Visual impairment; Visually impaired; Visually impaired persons; Global system for mobile communications","N.S. Patankar; Sanjivani College of Engineering, Department of Information Technology, Kopargaon, India; email: nikhil.patankar1991@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","","979-835033509-5","","","English","Int. Conf. Comput. Commun. Netw. Technol., ICCCNT","Conference paper","Final","","Scopus","2-s2.0-85179851556"
"Ghatwary N.; Abouzeina A.; Kantoush A.; Eltawil B.; Ramadan M.; Yasser M.","Ghatwary, Noha (55647739300); Abouzeina, Ahmed (58093852500); Kantoush, Ahmed (58093777500); Eltawil, Bahaa (58093741900); Ramadan, Mohamed (57224356830); Yasser, Mohamed (58087617900)","55647739300; 58093852500; 58093777500; 58093741900; 57224356830; 58087617900","Intelligent Assistance System for Visually Impaired/Blind People (ISVB)","2022","2022 5th International Conference on Communications, Signal Processing, and their Applications, ICCSPA 2022","","","","","","","4","10.1109/ICCSPA55860.2022.10019201","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85147547395&doi=10.1109%2fICCSPA55860.2022.10019201&partnerID=40&md5=2e5a0334394e18f505470c69e5b7e86b","AAST, Computer Engineering, Smart Village, Egypt; AAST, Computer Engineering, Alexandria, Egypt","Ghatwary N., AAST, Computer Engineering, Smart Village, Egypt; Abouzeina A., AAST, Computer Engineering, Alexandria, Egypt; Kantoush A., AAST, Computer Engineering, Alexandria, Egypt; Eltawil B., AAST, Computer Engineering, Alexandria, Egypt; Ramadan M., AAST, Computer Engineering, Alexandria, Egypt; Yasser M., AAST, Computer Engineering, Alexandria, Egypt","Visual impairments pose a parsing need to develop new automated systems to assist persons presenting visual impairments. The visual impairments have trouble interacting and sensing their surroundings. Their movement is limited and has to rely on a guided stick for them to move safely from one place to another. However, traditional canes have the disadvantage of failing to detect far-away obstacles and small objects. Therefore, this project is proposed to design and develop an Intelligent Assistance System for Visually Impaired People (ISVB). Our proposed system is composed of three interconnected parts, a smart cap, a 3D-printed intelligent cane and a mobile application that connects the system through an online server. The smart cap uses the Raspberry Pi and camera module, along with a deep learning object detection module for obstacle detection. The intelligent cane will provide the feasibility for the visually impaired person to walk without encountering problems by analyzing the surrounding environment through a microcontroller with multiple sensors and a bluetooth module. The mobile application interacts with the cap and the cane. Additionally, it will provide virtual navigation to help visually impaired people in their movement. To evaluate the performance of the system, different experiments for object detection, sensors and mobile applications have been conducted. The overall performance of the model showed an efficiency of 94.6 %.  © 2022 IEEE.","Detection; Intelligent Cane; Navigation; Smart Cap; Visually Impaired","3D printing; Automation; Deep learning; Mobile computing; Object recognition; Obstacle detectors; Assistance system; Detection; Intelligent assistances; Intelligent cane; Mobile applications; Objects detection; Smart cap; Visual impairment; Visually impaired; Visually impaired people; Object detection","","","Institute of Electrical and Electronics Engineers Inc.","","978-166548237-0","","","English","Int. Conf. Commun., Signal Process., Appl., ICCSPA","Conference paper","Final","","Scopus","2-s2.0-85147547395"
"Zhang J.; Yang K.; Constantinescu A.; Peng K.; Muller K.; Stiefelhagen R.","Zhang, Jiaming (57221656967); Yang, Kailun (57189005098); Constantinescu, Angela (36184971800); Peng, Kunyu (57224632288); Muller, Karin (37044692500); Stiefelhagen, Rainer (6602180348)","57221656967; 57189005098; 36184971800; 57224632288; 37044692500; 6602180348","Trans4Trans: Efficient Transformer for Transparent Object and Semantic Scene Segmentation in Real-World Navigation Assistance","2022","IEEE Transactions on Intelligent Transportation Systems","23","10","","19173","19186","13","52","10.1109/TITS.2022.3161141","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85127523792&doi=10.1109%2fTITS.2022.3161141&partnerID=40&md5=e7f24af870c38bd6ae72a574d49b5d23","Karlsruhe Institute of Technology, Computer Vision for Human-Computer Interaction Laboratory, Karlsruhe, 76131, Germany; Karlsruhe Institute of Technology, Center for Digital Accessibility and Assistive Technology, Karlsruhe, 76131, Germany; Karlsruhe Institute of Technology, Computer Vision for Human-Computer Interaction Laboratory, The Center for Digital Accessibility and Assistive Technology, Karlsruhe, 76131, Germany","Zhang J., Karlsruhe Institute of Technology, Computer Vision for Human-Computer Interaction Laboratory, Karlsruhe, 76131, Germany; Yang K., Karlsruhe Institute of Technology, Computer Vision for Human-Computer Interaction Laboratory, Karlsruhe, 76131, Germany; Constantinescu A., Karlsruhe Institute of Technology, Center for Digital Accessibility and Assistive Technology, Karlsruhe, 76131, Germany; Peng K., Karlsruhe Institute of Technology, Computer Vision for Human-Computer Interaction Laboratory, Karlsruhe, 76131, Germany; Muller K., Karlsruhe Institute of Technology, Center for Digital Accessibility and Assistive Technology, Karlsruhe, 76131, Germany; Stiefelhagen R., Karlsruhe Institute of Technology, Computer Vision for Human-Computer Interaction Laboratory, The Center for Digital Accessibility and Assistive Technology, Karlsruhe, 76131, Germany","Transparent objects, such as glass walls and doors, constitute architectural obstacles hindering the mobility of people with low vision or blindness. For instance, the open space behind glass doors is inaccessible, unless it is correctly perceived and interacted with. However, traditional assistive technologies rarely cover the segmentation of these safety-critical transparent objects. In this paper, we build a wearable system with a novel dual-head Transformer for Transparency (Trans4Trans) perception model, which can segment general-and transparent objects. The two dense segmentation results are further combined with depth information in the system to help users navigate safely and assist them to negotiate transparent obstacles. We propose a lightweight Transformer Parsing Module (TPM) to perform multi-scale feature interpretation in the transformer-based decoder. Benefiting from TPM, the double decoders can perform joint learning from corresponding datasets to pursue robustness, meanwhile maintain efficiency on a portable GPU, with negligible calculation increase. The entire Trans4Trans model is constructed in a symmetrical encoder-decoder architecture, which outperforms state-of-the-art methods on the test sets of Stanford2D3D and Trans10K-v2 datasets, obtaining mIoU of 45.13% and 75.14%, respectively. Through a user study and various pre-tests conducted in indoor and outdoor scenes, the usability and reliability of our assistive system have been extensively verified. Meanwhile, the Tran4Trans model has outstanding performances on driving scene datasets. On Cityscapes, ACDC, and DADA-seg datasets corresponding to common environments, adverse weather, and traffic accident scenarios, mIoU scores of 81.5%, 76.3%, and 39.2% are obtained, demonstrating its high efficiency and robustness for real-world transportation applications.  © 2000-2011 IEEE.","Computer vision for the visually impaired; scene understanding; semantic segmentation; transparent object segmentation; wearable assistive system","Computer architecture; Computer vision; Decoding; Efficiency; Glass; Safety engineering; Semantic Segmentation; Assistive system; Computer vision for the visually impaired; Decoding; Images segmentations; Objects segmentation; Scene understanding; Scene understanding.; Semantic segmentation; Transformer; Transparent object segmentation; Transparent objects; Wearable assistive system; Semantics","K. Yang; Karlsruhe Institute of Technology, Computer Vision for Human-Computer Interaction Laboratory, Karlsruhe, 76131, Germany; email: kailun.yang@kit.edu","","Institute of Electrical and Electronics Engineers Inc.","15249050","","","","English","IEEE Trans. Intell. Transp. Syst.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85127523792"
"Gupta M.; Singh M.; Chauhan N.; Chauhan A.S.","Gupta, Mahima (58753795400); Singh, Mayank (58831126400); Chauhan, Nidhi (58753662300); Chauhan, Alok Singh (57924434100)","58753795400; 58831126400; 58753662300; 57924434100","A Novel Approach for Complete Aid to Blinds Using Voice Assisted Smart Glasses","2023","2023 International Conference on Sustainable Emerging Innovations in Engineering and Technology, ICSEIET 2023","","","","365","369","4","0","10.1109/ICSEIET58677.2023.10303298","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85179128469&doi=10.1109%2fICSEIET58677.2023.10303298&partnerID=40&md5=4ec781145d14de83547b4d48a152c1c3","Galgotias University, Department of Computer Application, Greater Noida, India","Gupta M., Galgotias University, Department of Computer Application, Greater Noida, India; Singh M., Galgotias University, Department of Computer Application, Greater Noida, India; Chauhan N., Galgotias University, Department of Computer Application, Greater Noida, India; Chauhan A.S., Galgotias University, Department of Computer Application, Greater Noida, India","In this research paper, a novel methodology is proposed that acts as a complete aid for blinds. It is embedded with wearable devices like normal glasses. The proposed methodology is artificial intelligence (AI) enabled to help visually impaired people make their daily routine easy. It has all kinds of basic features and functionalities, like a text reader, image captures, and storage of information, video recording, voice recording, and voice command to text converter, location identification, and report generation. To add novelty, we are putting additional features like a built-in voice assistance and sensors embedded in the glass itself. When a user wants to surf for something on the internet, he can easily access the internet through AI without a system. AI will search for results for the user and dictate the results it finds directly to their ears. Another new feature is the ultrasonic sensors, which can alarm the user when they accurately detect any blockage in the path. Crossing the road and walking on the street will be much easier with AI smart glasses.  © 2023 IEEE.","Artificial Intelligence and Navigation; Text Recognition; Visually Impaired","Air navigation; Artificial intelligence; Character recognition; Ultrasonic applications; Video recording; Artificial intelligence and navigation; Daily routines; Novel methodology; Research papers; Smart glass; Text recognition; Text-readers; Visually impaired; Visually impaired people; Wearable devices; Glass","","","Institute of Electrical and Electronics Engineers Inc.","","979-835032918-6","","","English","Int. Conf. Sustain. Emerg. Innov. Eng. Technol., ICSEIET","Conference paper","Final","","Scopus","2-s2.0-85179128469"
"Feng J.; Beheshti M.; Philipson M.; Ramsaywack Y.; Porfiri M.; Rizzo J.-R.","Feng, Junchi (57858341600); Beheshti, Mahya (57208333398); Philipson, Mira (58482006400); Ramsaywack, Yuvraj (58482207400); Porfiri, Maurizio (6701357619); Rizzo, John-Ross (56527876700)","57858341600; 57208333398; 58482006400; 58482207400; 6701357619; 56527876700","Commute Booster: A Mobile Application for First/Last Mile and Middle Mile Navigation Support for People With Blindness and Low Vision","2023","IEEE Journal of Translational Engineering in Health and Medicine","11","","","523","535","12","3","10.1109/JTEHM.2023.3293450","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85164432489&doi=10.1109%2fJTEHM.2023.3293450&partnerID=40&md5=ffdd6c32d39a244f091dfbcf4585adcf","New York University, Tandon School of Engineering, Department of Biomedical Engineering, Brooklyn, 11201, NY, United States; New York University, Center for Urban Science and Progress, Tandon School of Engineering, Brooklyn, 11201, NY, United States; New York University, Tandon School of Engineering, Department of Mechanical and Aerospace Engineering, Brooklyn, 11201, NY, United States; Nyu Langone Health, Department of Rehabilitation Medicine, New York, 10016, NY, United States; Metropolitan Transportation Authority, New York, 10004, NY, United States","Feng J., New York University, Tandon School of Engineering, Department of Biomedical Engineering, Brooklyn, 11201, NY, United States, New York University, Center for Urban Science and Progress, Tandon School of Engineering, Brooklyn, 11201, NY, United States; Beheshti M., New York University, Tandon School of Engineering, Department of Mechanical and Aerospace Engineering, Brooklyn, 11201, NY, United States, Nyu Langone Health, Department of Rehabilitation Medicine, New York, 10016, NY, United States; Philipson M., Metropolitan Transportation Authority, New York, 10004, NY, United States; Ramsaywack Y., Metropolitan Transportation Authority, New York, 10004, NY, United States; Porfiri M., New York University, Tandon School of Engineering, Department of Biomedical Engineering, Brooklyn, 11201, NY, United States, New York University, Center for Urban Science and Progress, Tandon School of Engineering, Brooklyn, 11201, NY, United States, New York University, Tandon School of Engineering, Department of Mechanical and Aerospace Engineering, Brooklyn, 11201, NY, United States; Rizzo J.-R., New York University, Tandon School of Engineering, Department of Biomedical Engineering, Brooklyn, 11201, NY, United States, Nyu Langone Health, Department of Rehabilitation Medicine, New York, 10016, NY, United States","Objective: People with blindness and low vision face substantial challenges when navigating both indoor and outdoor environments. While various solutions are available to facilitate travel to and from public transit hubs, there is a notable absence of solutions for navigating within transit hubs, often referred to as the 'middle mile'. Although research pilots have explored the middle mile journey, no solutions exist at scale, leaving a critical gap for commuters with disabilities. In this paper, we proposed a novel mobile application, Commute Booster, that offers full trip planning and real-time guidance inside the station. Methods and procedures: Our system consists of two key components: the general transit feed specification (GTFS) and optical character recognition (OCR). The GTFS dataset generates a comprehensive list of wayfinding signage within subway stations that users will encounter during their intended journey. The OCR functionality enables users to identify relevant navigation signs in their immediate surroundings. By seamlessly integrating these two components, Commute Booster provides real-time feedback to users regarding the presence or absence of relevant navigation signs within the field of view of their phone camera during their journey. Results: As part of our technical validation process, we conducted tests at three subway stations in New York City. The sign detection achieved an impressive overall accuracy rate of 0.97. Additionally, the system exhibited a maximum detection range of 11 meters and supported an oblique angle of approximately 110 degrees for field of view detection. Conclusion: The Commute Booster mobile application relies on computer vision technology and does not require additional sensors or infrastructure. It holds tremendous promise in assisting individuals with blindness and low vision during their daily commutes. Clinical and Translational Impact Statement: Commute Booster translates the combination of OCR and GTFS into an assistive tool, which holds great promise for assisting people with blindness and low vision in their daily commute.  © 2013 IEEE.","General transit feed specification; indoor navigation; low-vision aid; mobile application; optical character recognition","Blindness; Humans; Mobile Applications; Self-Help Devices; Transportation; Vision, Low; Eye protection; Indoor positioning systems; Navigation; Optical character recognition; Specifications; Subway stations; Code; Field of views; General transit feed specification; Indoor navigation; Low vision; Low-vision aid; Mobile applications; Public transportation; Transit hubs; Article; blindness; human; low vision; New York; optical character recognition; traffic and transport; validation process; blindness; Cameras","J.-R. Rizzo; New York University, Tandon School of Engineering, Department of Biomedical Engineering, Brooklyn, 11201, United States; email: JohnRoss.Rizzo@nyulangone.org","","Institute of Electrical and Electronics Engineers Inc.","21682372","","","38059065","English","IEEE J. Transl. Eng. Health Med.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85164432489"
"Logesh K.; Karthikeyan P.; Abishek T.; Jayamani S.","Logesh, K. (58708430900); Karthikeyan, P. (58584517300); Abishek, T. (58502206000); Jayamani, S. (35145931600)","58708430900; 58584517300; 58502206000; 35145931600","Assistance for Visually Impaired People Using Deep Learning","2023","ViTECoN 2023 - 2nd IEEE International Conference on Vision Towards Emerging Trends in Communication and Networking Technologies, Proceedings","","","","","","","1","10.1109/ViTECoN58111.2023.10157205","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85165532846&doi=10.1109%2fViTECoN58111.2023.10157205&partnerID=40&md5=b505f27c4abd79400cf558b41a6e781a","K. S. Rangasamy College of Technology, Electronics and Communication Engineering, Tiruchengode, India","Logesh K., K. S. Rangasamy College of Technology, Electronics and Communication Engineering, Tiruchengode, India; Karthikeyan P., K. S. Rangasamy College of Technology, Electronics and Communication Engineering, Tiruchengode, India; Abishek T., K. S. Rangasamy College of Technology, Electronics and Communication Engineering, Tiruchengode, India; Jayamani S., K. S. Rangasamy College of Technology, Electronics and Communication Engineering, Tiruchengode, India","A disability that disturbs the structure and operations of the vision system is known as a visual impairment. Many people believed that blindness meant being unable to see anything at all or, at most, being able to distinguish light from darkness. With regard to vision impairment, most of the visual impairment Individuals struggle to navigate on their own in an unfamiliar setting. The suggested study aims to provide blind persons the confidence to walk and the awareness to be attentive if their path is blocked by other objects, people, or related odds. In this study, an assistive system for blind persons is proposed, to help them to know what is surround, through deep learning along with visualization methods to identify an objects on walking route. The proposed model benefits the blind users to move around in unfamiliar indoor and outdoor environment. From this device the blind people can easily understand the surrounding through voice feedback. The suggested technique can improve people's protection and safety while going outside or in a strange environment. © 2023 IEEE.","ATmega micro-controller; ESP8266; GPS Module; YOLOv3","Deep learning; Gait analysis; Assistive system; Atmega micro controllers; Blind person; Esp8266; GPS module; Vision impairments; Vision systems; Visual impairment; Visually impaired people; YOLOv3; Ophthalmology","","Thanikaiselvan V T.; S R.D.; T S.; S K.","Institute of Electrical and Electronics Engineers Inc.","","979-835034798-2","","","English","ViTECoN - IEEE Int. Conf. Vis. Towards Emerg. Trends Commun. Netw. Technol., Proc.","Conference paper","Final","","Scopus","2-s2.0-85165532846"
"Sujatha C.N.; Gudipalli A.; Priya M.H.; Reddy P.S.; Hamsini R.","Sujatha, C.N. (57192542166); Gudipalli, Abhishek (55924319500); Priya, M. Hari (57490190900); Reddy, P. Sahithi (57489944700); Hamsini, R. (57202687169)","57192542166; 55924319500; 57490190900; 57489944700; 57202687169","Embedded Design of Smart Helmet for Physically Impaired","2021","3rd IEEE International Virtual Conference on Innovations in Power and Advanced Computing Technologies, i-PACT 2021","","","","","","","0","10.1109/i-PACT52855.2021.9696754","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85126463588&doi=10.1109%2fi-PACT52855.2021.9696754&partnerID=40&md5=c5eb173d4933c14f828c0c4713d34b4b","Electronics and Communication Engineering, Sreenidhi Institute of Science and Technology, Hyderabad, India; School of Electrical Engineering, Vellore Institute of Technology, Tamilnadu, Vellore, 623014, India","Sujatha C.N., Electronics and Communication Engineering, Sreenidhi Institute of Science and Technology, Hyderabad, India; Gudipalli A., School of Electrical Engineering, Vellore Institute of Technology, Tamilnadu, Vellore, 623014, India; Priya M.H., Electronics and Communication Engineering, Sreenidhi Institute of Science and Technology, Hyderabad, India; Reddy P.S., Electronics and Communication Engineering, Sreenidhi Institute of Science and Technology, Hyderabad, India; Hamsini R., Electronics and Communication Engineering, Sreenidhi Institute of Science and Technology, Hyderabad, India","The work proposed in this paper is a prototype for visually impaired people to identify common real objects, ensure their safety in public transportation and during panic situations. Here, we are to replace blind aided stick with Smart Helmet and integrating some features. This is carried over Embedded technology with the help of Machine Learning techniques. Computer Vision algorithm is implemented on Raspberry Pi detects Objects and warns the visually impaired person through speech in order to navigate properly. TensorFlow framework is used for real-time object detection in ML. This system has Panic Alert switch which the visually impaired can press when they are in an uncomfortable scenario. When a Panic Alert is triggered, our system sends the location, which was recorded using the GPS module, to the emergency contacts through GSM. GPS receiver will communicate with satellites and them it will calculate the latitude and longitude to track the location. Flite (Festival lite) is used to convert from text to speech. The primary goal of this work is to create a Helmet with convenient size and easy to navigation aid for Visually impaired which helps in Artificial Visualization by providing facts about the environmental scenario and forceful objects around them. © 2021 IEEE.","Alert switch; GPS module; GSM module; OpenCv; Raspberry Pi","Global system for mobile communications; Learning systems; Object detection; Safety devices; Alert switch; Embedded designs; GPS module; GSM module; Opencv; Public transportation; Raspberry pi; Real objects; Visually impaired; Visually impaired people; Global positioning system","","","Institute of Electrical and Electronics Engineers Inc.","","978-166542691-6","","","English","IEEE Int. Virtual Conf. Innov. Power Adv. Comput. Technol., i-PACT","Conference paper","Final","","Scopus","2-s2.0-85126463588"
"Bouteraa Y.","Bouteraa, Yassine (57202361442)","57202361442","Design and development of a wearable assistive device integrating a fuzzy decision support system for blind and visually impaired people","2021","Micromachines","12","9","1082","","","","19","10.3390/mi12091082","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85114962706&doi=10.3390%2fmi12091082&partnerID=40&md5=bed5714cd06d26078fcf455e32ac3a83","Department of Computer Engineering, College of Computer Engineering and Sciences, Prince Sattam bin Abdulaziz University, Al-Kharj, 11942, Saudi Arabia; Control and Energy Management Laboratory (CEM Lab), Ecole Nationale d Ingenieurs de Sfax (ENIS), Institut Superieur de Biotechnologie de Sfax (ISBS), University of Sfax, Sfax, 3038, Tunisia","Bouteraa Y., Department of Computer Engineering, College of Computer Engineering and Sciences, Prince Sattam bin Abdulaziz University, Al-Kharj, 11942, Saudi Arabia, Control and Energy Management Laboratory (CEM Lab), Ecole Nationale d Ingenieurs de Sfax (ENIS), Institut Superieur de Biotechnologie de Sfax (ISBS), University of Sfax, Sfax, 3038, Tunisia","In this article, a new design of a wearable navigation support system for blind and visually impaired people (BVIP) is proposed. The proposed navigation system relies primarily on sensors, real-time processing boards, a fuzzy logic-based decision support system, and a user interface. It uses sensor data as inputs and provides the desired safety orientation to the BVIP. The user is informed about the decision based on a mixed voice–haptic interface. The navigation aid system contains two wearable obstacle detection systems managed by an embedded controller. The control system adopts the Robot Operating System (ROS) architecture supported by the Beagle Bone Black master board that meets the real-time constraints. The data acquisition and obstacle avoidance are carried out by several nodes managed by the ROS to finally deliver a mixed haptic–voice message for guidance of the BVIP. A fuzzy logic-based decision support system was implemented to help BVIP to choose a safe direction. The system has been applied to blindfolded persons and visually impaired persons. Both types of users found the system promising and pointed out its potential to become a good navigation aid in the future. © 2021 by the author. Licensee MDPI, Basel, Switzerland.","Assistive technology; Blind and visually impaired people; Fuzzy classifier; Navigation aid; Sensor data fusion; Wearable devices","Computer circuits; Data acquisition; Embedded systems; Fuzzy logic; Haptic interfaces; Navigation systems; Obstacle detectors; Robots; Visual servoing; Wearable sensors; Blind and visually impaired; Design and Development; Fuzzy decision support system; Obstacle detection system; Real time constraints; Robot operating systems (ROS); Visually impaired persons; Wearable assistive devices; Decision support systems","Y. Bouteraa; Department of Computer Engineering, College of Computer Engineering and Sciences, Prince Sattam bin Abdulaziz University, Al-Kharj, 11942, Saudi Arabia; email: yassine.bouteraa@isbs.usf.tn","","MDPI","2072666X","","","","English","Micromachines","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85114962706"
"Saha S.; Shakal F.H.; Saleque A.M.; Trisha J.J.","Saha, Sagor (57218939007); Shakal, Farhan Hossain (57218934456); Saleque, Ahmed Mortuza (55490851300); Trisha, Jerin Jahan (57218935064)","57218939007; 57218934456; 55490851300; 57218935064","Vision Maker: An Audio Visual and Navigation Aid for Visually Impaired Person","2020","Proceedings of International Conference on Intelligent Engineering and Management, ICIEM 2020","","","9160169","266","271","5","3","10.1109/ICIEM48762.2020.9160169","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85090826189&doi=10.1109%2fICIEM48762.2020.9160169&partnerID=40&md5=f7a304333de84d4cb276ae6021b927c7","American International University Bangladesh, Electrical and Electronic Engineering, Dhaka, Bangladesh; American International University Bangladesh, Dhaka, Bangladesh","Saha S., American International University Bangladesh, Electrical and Electronic Engineering, Dhaka, Bangladesh; Shakal F.H., American International University Bangladesh, Electrical and Electronic Engineering, Dhaka, Bangladesh; Saleque A.M., American International University Bangladesh, Dhaka, Bangladesh; Trisha J.J., American International University Bangladesh, Electrical and Electronic Engineering, Dhaka, Bangladesh","People with low vision or complete loss of vision face challenging task to meet their daily demand. The barrier low vision hinders them participating in the society. The evolution of computer vision, artificial intelligence and machine learning proved to effective tool in revitalizing the situation of blind. The propounded design represents implementation of an assistive device that can aid them in recognizing the object. The low-cost device runs a pre-trained model (ssdlite-mobilenet-v2-coco) and can identify up to 80 classes. The user can read any text (English) from images. Ultrasonic sensors have been used in the custom-made device to continuously alert the user of obstacles from all directions. Additionally, it can help navigate them in outdoor using Google map API. The user can read news, listen to music and mail to the member of choice. The command is passed and received through earphone, which is connected in the audio jack of raspberry pi. © 2020 IEEE.","computer vision; Human-computer interaction; Object recognition; outdoor navigation; wearable device","Engineering; Industrial engineering; Assistive devices; Effective tool; Google map api; Low vision; Low-cost devices; Navigation aids; Visually impaired persons; Artificial intelligence","","","Institute of Electrical and Electronics Engineers Inc.","","978-172814097-1","","","English","Proc. Int. Conf. Intell. Eng. Manag., ICIEM","Conference paper","Final","","Scopus","2-s2.0-85090826189"
"Plikynas D.; Indriulionis A.; Laukaitis A.; Sakalauskas L.","Plikynas, Darius (57207667356); Indriulionis, Audrius (57208420603); Laukaitis, Algirdas (23088645200); Sakalauskas, Leonidas (6603419566)","57207667356; 57208420603; 23088645200; 6603419566","Indoor-Guided Navigation for People Who Are Blind: Crowdsourcing for Route Mapping and Assistance","2022","Applied Sciences (Switzerland)","12","1","523","","","","7","10.3390/app12010523","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85122201162&doi=10.3390%2fapp12010523&partnerID=40&md5=abd78ea8d6fe25ca622a921fe83bc60d","Department of Business Technologies and Entrepreneurship, Vilnius Gediminas Technical University, Vilnius, 10223, Lithuania; Department of Information Systems, Vilnius Gediminas Technical University, Vilnius, 10223, Lithuania; Faculty of Natural Sciences, Klaipeda University, Klaipeda, 92294, Lithuania","Plikynas D., Department of Business Technologies and Entrepreneurship, Vilnius Gediminas Technical University, Vilnius, 10223, Lithuania; Indriulionis A., Department of Business Technologies and Entrepreneurship, Vilnius Gediminas Technical University, Vilnius, 10223, Lithuania; Laukaitis A., Department of Information Systems, Vilnius Gediminas Technical University, Vilnius, 10223, Lithuania; Sakalauskas L., Faculty of Natural Sciences, Klaipeda University, Klaipeda, 92294, Lithuania","This paper presents an approach to enhance electronic traveling aids (ETAs) for people who are blind and severely visually impaired (BSVI) using indoor orientation and guided navigation by employing social outsourcing of indoor route mapping and assistance processes. This type of approach is necessary because GPS does not work well, and infrastructural investments are absent or too costly to install for indoor navigation. Our approach proposes the prior outsourcing of vision-based recordings of indoor routes from an online network of seeing volunteers, who gather and constantly update a web cloud database of indoor routes using specialized sensory equipment and web services. Computational intelligence-based algorithms process sensory data and prepare them for BSVI usage. In this way, people who are BSVI can obtain ready-to-use access to the indoor routes database. This type of service has not previously been offered in such a setting. Specialized wearable sensory ETA equipment, depth cameras, smartphones, computer vision algorithms, tactile and audio interfaces, and computational intelligence algorithms are employed for that matter. The integration of semantic data of points of interest (such as stairs, doors, WC, entrances/exits) and evacuation schemes could make the proposed approach even more attractive to BVSI users. Presented approach crowdsources volunteers’ real-time online help for complex navigational situations using a mobile app, a live video stream from BSVI wearable cameras, and digitalized maps of buildings’ evacuation schemes. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Blind and severely visually impaired; Computer vision; Crowdsourcing; Electronic travelling aids; Guided navigation; Social networking","","D. Plikynas; Department of Business Technologies and Entrepreneurship, Vilnius Gediminas Technical University, Vilnius, 10223, Lithuania; email: darius.plikynas@vilniustech.lt","","MDPI","20763417","","","","English","Appl. Sci.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85122201162"
"Khan M.A.; Paul P.; Rashid M.; Hossain M.; Ahad M.A.R.","Khan, Muiz Ahmed (57219926257); Paul, Pias (57208860581); Rashid, Mahmudur (57225694952); Hossain, Mainul (57203734814); Ahad, Md Atiqur Rahman (23491419800)","57219926257; 57208860581; 57225694952; 57203734814; 23491419800","An AI-Based Visual Aid with Integrated Reading Assistant for the Completely Blind","2020","IEEE Transactions on Human-Machine Systems","50","6","9234074","507","517","10","79","10.1109/THMS.2020.3027534","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85096098002&doi=10.1109%2fTHMS.2020.3027534&partnerID=40&md5=d09ec6ccf50432b5ec0174ad609cd635","Department of Electrical and Computer Engineering, North South University, Dhaka, Bangladesh; Department of Electrical and Electronic Engineering, University of Dhaka, Dhaka, Bangladesh","Khan M.A., Department of Electrical and Computer Engineering, North South University, Dhaka, Bangladesh; Paul P., Department of Electrical and Computer Engineering, North South University, Dhaka, Bangladesh; Rashid M., Department of Electrical and Computer Engineering, North South University, Dhaka, Bangladesh; Hossain M., Department of Electrical and Electronic Engineering, University of Dhaka, Dhaka, Bangladesh; Ahad M.A.R., Department of Electrical and Electronic Engineering, University of Dhaka, Dhaka, Bangladesh","Blindness prevents a person from gaining knowledge of the surrounding environment and makes unassisted navigation, object recognition, obstacle avoidance, and reading tasks a major challenge. In this work, we propose a novel visual aid system for the completely blind. Because of its low cost, compact size, and ease-of-integration, Raspberry Pi 3 Model B+ has been used to demonstrate the functionality of the proposed prototype. The design incorporates a camera and sensors for obstacle avoidance and advanced image processing algorithms for object detection. The distance between the user and the obstacle is measured by the camera as well as ultrasonic sensors. The system includes an integrated reading assistant, in the form of the image-to-text converter, followed by an auditory feedback. The entire setup is lightweight and portable and can be mounted onto a regular pair of eyeglasses, without any additional cost and complexity. Experiments are carried out with 60 completely blind individuals to evaluate the performance of the proposed device with respect to the traditional white cane. The evaluations are performed in controlled environments that mimic real-world scenarios encountered by a blind person. Results show that the proposed device, as compared with the white cane, enables greater accessibility, comfort, and ease of navigation for the visually impaired. © 2013 IEEE.","Blind people; completely blind; electronic navigation aid; Raspberry Pi; visual aid; visually impaired people; wearable system","Cameras; Object detection; Patient rehabilitation; Ultrasonic applications; Additional costs; Auditory feedback; Blind individuals; Controlled environment; Image processing algorithm; Real-world scenario; Surrounding environment; Visually impaired; Object recognition","M. Hossain; Department of Electrical and Electronic Engineering, University of Dhaka, Dhaka, Bangladesh; email: mainul.eee@du.ac.bd","","Institute of Electrical and Electronics Engineers Inc.","21682291","","","","English","IEEE Trans. Human Mach. Syst.","Article","Final","","Scopus","2-s2.0-85096098002"
"Vaidya S.; Shah N.; Shah N.; Shankarmani R.","Vaidya, Sunit (57217629457); Shah, Naisha (57217629067); Shah, Niti (57217629060); Shankarmani, Radha (36550572600)","57217629457; 57217629067; 57217629060; 36550572600","Real-Time Object Detection for Visually Challenged People","2020","Proceedings of the International Conference on Intelligent Computing and Control Systems, ICICCS 2020","","","9121085","311","316","5","32","10.1109/ICICCS48265.2020.9121085","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85087456202&doi=10.1109%2fICICCS48265.2020.9121085&partnerID=40&md5=8b166a6b1eb1deb0dc9d187e5b807c2e","Sardar Patel Institute of Technology, Information Technology Department, Mumbai, India","Vaidya S., Sardar Patel Institute of Technology, Information Technology Department, Mumbai, India; Shah N., Sardar Patel Institute of Technology, Information Technology Department, Mumbai, India; Shah N., Sardar Patel Institute of Technology, Information Technology Department, Mumbai, India; Shankarmani R., Sardar Patel Institute of Technology, Information Technology Department, Mumbai, India","One of the most important senses for a living is vision. Millions of people living in this world deal with visual impairment. These people encounter difficulties in navigating independently and safely, facing issues in accessing information and communication. The objective of the proposed work is to change the visual world into an audio world by notifying the blind people about the objects in their path. This will help visually impaired people to navigate independently without any external assistance just by using the real-time object detection system. The application uses image processing and machine learning techniques to determine real-time objects through the camera and inform blind people about the object and its location through the audio output. Inability to differentiate between objects has led to many limitations to the existing approach which includes less accuracy and lowperformance results. The main objective of the proposed work is to provide good accuracy, best performance results and a viable option for the visually impaired people to make the world a better place for them. © 2020 IEEE.","Image Processing; Machine learning; Object Detection; Visually Impaired; YOLO","Control systems; Intelligent computing; Learning systems; Object recognition; Audio-output; Better places; Blind people; Information and communication; Machine learning techniques; Object detection systems; Visual impairment; Visually impaired people; Object detection","","","Institute of Electrical and Electronics Engineers Inc.","","978-172814876-2","","","English","Proc. Int. Conf. Intell. Comput. Control Syst., ICICCS","Conference paper","Final","","Scopus","2-s2.0-85087456202"
"Afif M.; Ayachi R.; Said Y.; Atri M.","Afif, Mouna (57194068439); Ayachi, Riadh (57210106980); Said, Yahia (53867137900); Atri, Mohamed (23017853700)","57194068439; 57210106980; 53867137900; 23017853700","Indoor sign Detection System for Indoor Assistance Navigation","2021","18th IEEE International Multi-Conference on Systems, Signals and Devices, SSD 2021","","","9429495","1383","1387","4","1","10.1109/SSD52085.2021.9429495","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85107478777&doi=10.1109%2fSSD52085.2021.9429495&partnerID=40&md5=44318e7242bf12a98b3232868ed2da13","Laboratory of Electronics and Microelectronics (EE), University of Monastir, Faculty of Sciences of Monastir, Tunisia; College of Engineering, Northern Border University, Electrical Engineering Department, Arar, Saudi Arabia; College of Computer Science, King Khalid University, Abha, Saudi Arabia","Afif M., Laboratory of Electronics and Microelectronics (EE), University of Monastir, Faculty of Sciences of Monastir, Tunisia; Ayachi R., College of Engineering, Northern Border University, Electrical Engineering Department, Arar, Saudi Arabia; Said Y., College of Computer Science, King Khalid University, Abha, Saudi Arabia; Atri M., Laboratory of Electronics and Microelectronics (EE), University of Monastir, Faculty of Sciences of Monastir, Tunisia","Indoor signage plays an important role in finding specific destinations and way-finding especially for blind and visually impaired people (VIP). In this paper, we developed a new indoor signage classifier using deep convolutional neural Network (DCNN). Computer vision-based systems using cameras-based present a potential intermediate to assist blind and VIP persons on accessing unfamiliar buildings. Experiments were performed on a new dataset taken in an indoor building in France. The proposed dataset present 800 natural images divided into 4 indoor signs. Results achieved show that our proposed approach presents very encouraging results coming to 99.8% as recognition precision rate.  © 2021 IEEE.","Deep CNN; Indoor sign detection; Visually Impaired People (VIP); Way-finding","Convolutional neural networks; Deep neural networks; Blind and visually impaired; Natural images; Precision rates; Sign detection system; Vision based system; Way finding; Indoor positioning systems","M. Afif; Laboratory of Electronics and Microelectronics (EE), University of Monastir, Faculty of Sciences of Monastir, Tunisia; email: mouna.afif@outlook.fr","","Institute of Electrical and Electronics Engineers Inc.","","978-166541493-7","","","English","IEEE Int. Multi-Conf. Syst., Signals Devices, SSD","Conference paper","Final","","Scopus","2-s2.0-85107478777"
"Lu C.-L.; Liu Z.-Y.; Huang J.-T.; Huang C.-I.; Wang B.-H.; Chen Y.; Wu N.-H.; Wang H.-C.; Giarré L.; Kuo P.-Y.","Lu, Chen-Lung (57219589056); Liu, Zi-Yan (58365790400); Huang, Jui-Te (57215561521); Huang, Ching-I (57222062258); Wang, Bo-Hui (57225097383); Chen, Yi (57225101684); Wu, Nien-Hsin (57218769413); Wang, Hsueh-Cheng (36150277500); Giarré, Laura (6603769649); Kuo, Pei-Yi (59049202700)","57219589056; 58365790400; 57215561521; 57222062258; 57225097383; 57225101684; 57218769413; 36150277500; 6603769649; 59049202700","Assistive Navigation Using Deep Reinforcement Learning Guiding Robot With UWB/Voice Beacons and Semantic Feedbacks for Blind and Visually Impaired People","2021","Frontiers in Robotics and AI","8","","654132","","","","20","10.3389/frobt.2021.654132","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85109167488&doi=10.3389%2ffrobt.2021.654132&partnerID=40&md5=302e6b8b84b871b594e7a73800dbf4b1","Department of Electrical and Computer Engineering, Institute of Electrical and Control Engineering, National Chiao Tung University, Hsinchu, Taiwan; Department of Electrical and Computer Engineering, Institute of Electrical and Control Engineering, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; College of Technology Management, Institute of Service Science, National Tsing Hua University, Hsinchu, Taiwan; Department of Engineering, University of Modena and Reggio Emilia, Modena, Italy","Lu C.-L., Department of Electrical and Computer Engineering, Institute of Electrical and Control Engineering, National Chiao Tung University, Hsinchu, Taiwan, Department of Electrical and Computer Engineering, Institute of Electrical and Control Engineering, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Liu Z.-Y., Department of Electrical and Computer Engineering, Institute of Electrical and Control Engineering, National Chiao Tung University, Hsinchu, Taiwan, Department of Electrical and Computer Engineering, Institute of Electrical and Control Engineering, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Huang J.-T., Department of Electrical and Computer Engineering, Institute of Electrical and Control Engineering, National Chiao Tung University, Hsinchu, Taiwan, Department of Electrical and Computer Engineering, Institute of Electrical and Control Engineering, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Huang C.-I., Department of Electrical and Computer Engineering, Institute of Electrical and Control Engineering, National Chiao Tung University, Hsinchu, Taiwan, Department of Electrical and Computer Engineering, Institute of Electrical and Control Engineering, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Wang B.-H., Department of Electrical and Computer Engineering, Institute of Electrical and Control Engineering, National Chiao Tung University, Hsinchu, Taiwan, Department of Electrical and Computer Engineering, Institute of Electrical and Control Engineering, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Chen Y., Department of Electrical and Computer Engineering, Institute of Electrical and Control Engineering, National Chiao Tung University, Hsinchu, Taiwan, Department of Electrical and Computer Engineering, Institute of Electrical and Control Engineering, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Wu N.-H., College of Technology Management, Institute of Service Science, National Tsing Hua University, Hsinchu, Taiwan; Wang H.-C., Department of Electrical and Computer Engineering, Institute of Electrical and Control Engineering, National Chiao Tung University, Hsinchu, Taiwan, Department of Electrical and Computer Engineering, Institute of Electrical and Control Engineering, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Giarré L., Department of Engineering, University of Modena and Reggio Emilia, Modena, Italy; Kuo P.-Y., College of Technology Management, Institute of Service Science, National Tsing Hua University, Hsinchu, Taiwan","Facilitating navigation in pedestrian environments is critical for enabling people who are blind and visually impaired (BVI) to achieve independent mobility. A deep reinforcement learning (DRL)–based assistive guiding robot with ultrawide-bandwidth (UWB) beacons that can navigate through routes with designated waypoints was designed in this study. Typically, a simultaneous localization and mapping (SLAM) framework is used to estimate the robot pose and navigational goal; however, SLAM frameworks are vulnerable in certain dynamic environments. The proposed navigation method is a learning approach based on state-of-the-art DRL and can effectively avoid obstacles. When used with UWB beacons, the proposed strategy is suitable for environments with dynamic pedestrians. We also designed a handle device with an audio interface that enables BVI users to interact with the guiding robot through intuitive feedback. The UWB beacons were installed with an audio interface to obtain environmental information. The on-handle and on-beacon verbal feedback provides points of interests and turn-by-turn information to BVI users. BVI users were recruited in this study to conduct navigation tasks in different scenarios. A route was designed in a simulated ward to represent daily activities. In real-world situations, SLAM-based state estimation might be affected by dynamic obstacles, and the visual-based trail may suffer from occlusions from pedestrians or other obstacles. The proposed system successfully navigated through environments with dynamic pedestrians, in which systems based on existing SLAM algorithms have failed. © Copyright © 2021 Lu, Liu, Huang, Huang, Wang, Chen, Wu, Wang, Giarré and Kuo.","blind and visually impaired; deep reinforcement learning; guiding robot; indoor navigation; navigation; UWB beacon; verbal instruction","","H.-C. Wang; Department of Electrical and Computer Engineering, Institute of Electrical and Control Engineering, National Chiao Tung University, Hsinchu, Taiwan; email: hchengwang@g2.nctu.edu.tw","","Frontiers Media S.A.","22969144","","","","English","Front. Robot.  AI","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85109167488"
"Bhatlawande S.; Shilaskar S.; Kumari A.; Ambekar M.; Agrawal M.; Raj A.; Amilkanthwar S.","Bhatlawande, Shripad (55212307900); Shilaskar, Swati (57189017229); Kumari, Aditi (57836029600); Ambekar, Mahi (57835673000); Agrawal, Mohit (57836275300); Raj, Amit (58334729000); Amilkanthwar, Siddhi (57835913300)","55212307900; 57189017229; 57836029600; 57835673000; 57836275300; 58334729000; 57835913300","AI Based Handheld Electronic Travel Aid for Visually Impaired People","2022","2022 IEEE 7th International conference for Convergence in Technology, I2CT 2022","","","","","","","6","10.1109/I2CT54291.2022.9823962","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85135618468&doi=10.1109%2fI2CT54291.2022.9823962&partnerID=40&md5=9465f7fa80fabe12ec3dcbce7b6771d7","Vishwakarma Institute of Technology, Department of Electronics and Telecommunication Engineering, Pune, 411037, India","Bhatlawande S., Vishwakarma Institute of Technology, Department of Electronics and Telecommunication Engineering, Pune, 411037, India; Shilaskar S., Vishwakarma Institute of Technology, Department of Electronics and Telecommunication Engineering, Pune, 411037, India; Kumari A., Vishwakarma Institute of Technology, Department of Electronics and Telecommunication Engineering, Pune, 411037, India; Ambekar M., Vishwakarma Institute of Technology, Department of Electronics and Telecommunication Engineering, Pune, 411037, India; Agrawal M., Vishwakarma Institute of Technology, Department of Electronics and Telecommunication Engineering, Pune, 411037, India; Raj A., Vishwakarma Institute of Technology, Department of Electronics and Telecommunication Engineering, Pune, 411037, India; Amilkanthwar S., Vishwakarma Institute of Technology, Department of Electronics and Telecommunication Engineering, Pune, 411037, India","Visual Impairment is a major challenge. Blind people face significant problems in interacting with their surroundings. This paper presents an electronic travel aid for safe mobility of visually impaired people. This aid is implemented in the form of a hand-held torch. It consists of a monocular camera, an ultrasonic sensor, a raspberry pi board-based control system, and an earphone. It detects the obstacles using dual confirmation through a camera and ultrasonic sensor. The camera is used to detect and recognize the position of the object and an ultrasonic sensor is used for the depth perception. It can detect any obstacle in the path of visually impaired people. It can recognize a total of 80 indoor and outdoor objects/obstacles. It uses a simplified audio feedback to convey the details of detected obstacle/recognized object. The weight of the system is 410 grams. Total four experiments were conducted with blind-folded subjects to assess utility of this aid. The system consistently detected obstacles in the surrounding environment and helped the user to negotiate with the detected obstacle. © 2022 IEEE.","Computer Vision; Electronic Travel Aid; Machine Learning; Object Recognition; Obstacle Detection","Cameras; Depth perception; E-learning; Machine learning; Object detection; Object recognition; Obstacle detectors; Ultrasonic applications; Blind people; Electronic travel aidss; Handhelds; Machine-learning; Monocular cameras; Objects recognition; Obstacles detection; Safe mobility; Visual impairment; Visually impaired people; Computer vision","","","Institute of Electrical and Electronics Engineers Inc.","","978-166542168-3","","","English","IEEE Int. Conf. Converg. Technol., I2CT","Conference paper","Final","","Scopus","2-s2.0-85135618468"
"Afif M.; Ayachi R.; Said Y.; Pissaloux E.; Atri M.","Afif, Mouna (57194068439); Ayachi, Riadh (57210106980); Said, Yahia (53867137900); Pissaloux, Edwige (7003850978); Atri, Mohamed (23017853700)","57194068439; 57210106980; 53867137900; 7003850978; 23017853700","An Evaluation of RetinaNet on Indoor Object Detection for Blind and Visually Impaired Persons Assistance Navigation","2020","Neural Processing Letters","51","3","","2265","2279","14","81","10.1007/s11063-020-10197-9","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85078352208&doi=10.1007%2fs11063-020-10197-9&partnerID=40&md5=0128855fc4d8e3b4ecbe0e2d261d7a75","Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia; Electrical Engineering Department, College of Engineering, Northern Border University, Arar, Saudi Arabia; LITIS Laboratory & CNRS FR 3638, University of Rouen Normandy, Rouen, France; College of Computer Science, King Khalid University, Abha, Saudi Arabia","Afif M., Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia; Ayachi R., Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia; Said Y., Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia, Electrical Engineering Department, College of Engineering, Northern Border University, Arar, Saudi Arabia; Pissaloux E., LITIS Laboratory & CNRS FR 3638, University of Rouen Normandy, Rouen, France; Atri M., College of Computer Science, King Khalid University, Abha, Saudi Arabia","Indoor object detection presents a computer vision task that deals with the detection of specific indoor classes. This task attracts a lot of attention, especially in the last few years. The strong interest related to this field can be explained by the big importance of this task for indoor assistance navigation for visually impaired people and also by the phenomenal development of the deep convolutional neural networks (Deep CNN). In this paper, an effort is made to perform a new indoor object detector using the deep convolutional neural network-based framework. The framework is built based on the deep convolutional neural network “RetinaNet”. Evaluation is done by using various backbones as ResNet, DenseNet, and VGGNet in order to improve detection performances and processing time. We obtained very encouraging results coming up to 84.61% mAP as detection precision. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.","Deep convolutional neural network (DCNN); Deep learning; Indoor object detection and recognition dataset (IODR); Indoor object recognition; Visually impaired people (VIP)","Convolution; Deep learning; Indoor positioning systems; Neural networks; Object detection; Object recognition; Blind and visually impaired; Convolutional neural network; Detection performance; Detection precision; Object detection and recognition; Object detectors; Processing time; Visually impaired people; Deep neural networks","M. Afif; Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia; email: mouna.afif@outlook.fr","","Springer","13704621","","NPLEF","","English","Neural Process Letters","Article","Final","","Scopus","2-s2.0-85078352208"
"Nawaz W.; Khan K.U.; Bashir K.","Nawaz, Waqas (36700271700); Khan, Kifayat Ullah (56083359800); Bashir, Khalid (57217018591)","36700271700; 56083359800; 57217018591","A review on path selection and navigation approaches towards an assisted mobility of visually impaired people","2020","KSII Transactions on Internet and Information Systems","14","8","","3270","3294","24","7","10.3837/tiis.2020.08.007","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85090484720&doi=10.3837%2ftiis.2020.08.007&partnerID=40&md5=bdebfdecf5ac438266da39b6e715e9e5","Department of Computer and Information Systems, Islamic University of Madinah, Madinah, Saudi Arabia; Intelligent Knowledge Mining and Analytics (IKMA) Lab, Department of Computer Science, National University of Computer and Emerging Sciences, Islamabad, Pakistan","Nawaz W., Department of Computer and Information Systems, Islamic University of Madinah, Madinah, Saudi Arabia; Khan K.U., Intelligent Knowledge Mining and Analytics (IKMA) Lab, Department of Computer Science, National University of Computer and Emerging Sciences, Islamabad, Pakistan; Bashir K., Department of Computer and Information Systems, Islamic University of Madinah, Madinah, Saudi Arabia","Some things come easily to humans, one of them is the ability to navigate around. This capability of navigation suffers significantly in case of partial or complete blindness, restricting life activity. Advances in the technological landscape have given way to new solutions aiding navigation for the visually impaired. In this paper, we analyze the existing works and identify the challenges of path selection, context awareness, obstacle detection/identification and integration of visual and nonvisual information associated with real-time assisted mobility. In the process, we explore machine learning approaches for robotic path planning, multi constrained optimal path computation and sensor based wearable assistive devices for the visually impaired. It is observed that the solution to problem is complex and computationally intensive and significant effort is required towards the development of richer and comfortable paths for safe and smooth navigation of visually impaired people. We cannot overlook to explore more effective strategies of acquiring surrounding information towards autonomous mobility. © 2020 Korean Society for Internet Information. All rights reserved.","Autonomous Mobility; Information Systems; Navigation; Obstacle Detection; Path Planning; Visually Impaired","Obstacle detectors; Robot programming; Autonomous mobilities; Context- awareness; Machine learning approaches; Multi-constrained optimal paths; Obstacle detection; Visually impaired; Visually impaired people; Wearable assistive devices; Navigation","W. Nawaz; Department of Computer and Information Systems, Islamic University of Madinah, Madinah, Saudi Arabia; email: wnawaz@iu.edu.sa","","Korean Society for Internet Information","19767277","","","","English","KSII Trans. Internet Inf. Syst.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85090484720"
"","","","3rd Chinese Conference on Pattern Recognition and Computer Vision, PRCV 2020","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12305 LNCS","","","","","1936","0","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85093816655&partnerID=40&md5=74e631ea45de48fc1e60f3f126db388d","","","The proceedings contain 158 papers. The special focus in this conference is on Pattern Recognition and Computer Vision. The topics include: Underwater Image Processing by an Adversarial Network with Feedback Control; inception Parallel Attention Network for Small Object Detection in Remote Sensing Images; hyperspectral Image Denoising Based on Graph-Structured Low Rank and Non-local Constraint; multi-human Parsing with Pose and Boundary Guidance; m2E-Net: Multiscale Morphological Enhancement Network for Retinal Vessel Segmentation; DUDA: Deep Unsupervised Domain Adaptation Learning for Multi-sequence Cardiac MR Image Segmentation; learning from Rankings with Multi-level Features for No-Reference Image Quality Assessment; reversible Data Hiding Based on Prediction-Error-Ordering; aggregating Spatio-temporal Context for Video Object Segmentation; Position and Orientation Detection of Insulators in Arbitrary Direction Based on YOLOv3; R-PFN: Towards Precise Object Detection by Recurrent Pyramidal Feature Fusion; image Super-Resolution Based on Non-local Convolutional Neural Network; VH3D-LSFM: Video-Based Human 3D Pose Estimation with Long-Term and Short-Term Pose Fusion Mechanism; automatic Tooth Segmentation and 3D Reconstruction from Panoramic and Lateral Radiographs; unregistered Hyperspectral and Multispectral Image Fusion with Synchronous Nonnegative Matrix Factorization; Cloud Detection Algorithm Using Advanced Fully Convolutional Neural Networks in FY3D-MERSI Imagery; multi-layer Pointpillars: Multi-layer Feature Abstraction for Object Detection from Point Cloud; building Detection via Complementary Convolutional Features of Remote Sensing Images; hyperspectral Image Super-Resolution via Self-projected Smooth Prior; 3D Point Cloud Segmentation for Complex Structure Based on PointSIFT; completely Blind Image Quality Assessment with Visual Saliency Modulated Multi-feature Collaboration; blood Flow Velocity Detection of Nailfold Microcirculation Based on Spatiotemporal Analysis; blind Super-Resolution with Kernel-Aware Feature Refinement; preface.","","","","Peng Y.; Zha H.; Liu Q.; Lu H.; Sun Z.; Liu C.; Chen X.; Yang J.","Springer Science and Business Media Deutschland GmbH","03029743","978-303060632-9","","","English","Lect. Notes Comput. Sci.","Conference review","Final","","Scopus","2-s2.0-85093816655"
"Malini P.; Kumar U.S.; Vijayakumari G.; Thirukkumaran R.; Hanne L.S.; Karpakam S.","Malini, P. (58518464700); Kumar, Ushus.S. (57221912839); Vijayakumari, G. (55582126400); Thirukkumaran, R. (57226553850); Hanne, Lakshmi S (57638400000); Karpakam, S. (57679531800)","58518464700; 57221912839; 55582126400; 57226553850; 57638400000; 57679531800","Deep Learning and Fuzzy Decision Support System for visually impaired persons","2022","8th International Conference on Advanced Computing and Communication Systems, ICACCS 2022","","","","305","309","4","3","10.1109/ICACCS54159.2022.9785322","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85133182084&doi=10.1109%2fICACCS54159.2022.9785322&partnerID=40&md5=c8ad34eb3dc0c4d2464b9bff90c22c6f","Vivekanandha College of Technology for Women, Department of Ece, Tiruchengode, India; Srm Institute of Science and Tchnology, Department of Ece, Chennai, India; Builders Engineering College, Department of Ece, Tiruppur, India; Sri Eshwar College of Engineering, Department of Ece, Coimbatore, India","Malini P., Vivekanandha College of Technology for Women, Department of Ece, Tiruchengode, India; Kumar U.S., Srm Institute of Science and Tchnology, Department of Ece, Chennai, India; Vijayakumari G., Builders Engineering College, Department of Ece, Tiruppur, India; Thirukkumaran R., Vivekanandha College of Technology for Women, Department of Ece, Tiruchengode, India; Hanne L.S., Vivekanandha College of Technology for Women, Department of Ece, Tiruchengode, India; Karpakam S., Sri Eshwar College of Engineering, Department of Ece, Coimbatore, India","This paper presents the concept of wearable guidance device using deep learning to support blind or visually impaired people. Using suggested wearable technology, this effort seeks to provide supplemental support to them with more flexible movements. The large volume of device data also assures that its users are safer. The suggested device employs an RGB camera to transform RGB photos and compute a level surface for recognizing obstacles and safe walking paths, deep learning is used. The proposed navigation system takes sensor data as input and gives the BVIP the necessary safety orientation. To assist BVIP in choosing a safe course, a fuzzy logic-based decision support system was built. The method has been tested on blinded and visually challenged individuals. Both categories of users thought the framework was capable and saw it as having the possibility as a useful navigation tool in future. © 2022 IEEE.","CNN; deep learning; visually impaired persons; wearable device","Decision support systems; Deep learning; Fuzzy logic; Navigation systems; Deep learning; Device data; Fuzzy decision support system; Large volumes; RGB cameras; Supplemental supports; Visually impaired people; Visually impaired persons; Walking paths; Wearable devices; Wearable technology","","","Institute of Electrical and Electronics Engineers Inc.","","978-166540816-5","","","English","Int. Conf. Adv. Comput. Commun. Syst., ICACCS","Conference paper","Final","","Scopus","2-s2.0-85133182084"
"Ahmed F.; Tasnim Z.; Rana M.; Khan M.M.","Ahmed, Ferdaus (57818745600); Tasnim, Zarin (57219468234); Rana, Masud (36057121200); Khan, Mohammad Monirujjaman (36350785300)","57818745600; 57219468234; 36057121200; 36350785300","Development of Low Cost Smart Cane with GPS","2022","2022 IEEE World AI IoT Congress, AIIoT 2022","","","","715","724","9","8","10.1109/AIIoT54504.2022.9817322","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85134882718&doi=10.1109%2fAIIoT54504.2022.9817322&partnerID=40&md5=e7cb61180f9c4b9dec2e920060e3a554","North South University, Electrical and Computer Engineering, Dhaka, 1229, Bangladesh","Ahmed F., North South University, Electrical and Computer Engineering, Dhaka, 1229, Bangladesh; Tasnim Z., North South University, Electrical and Computer Engineering, Dhaka, 1229, Bangladesh; Rana M., North South University, Electrical and Computer Engineering, Dhaka, 1229, Bangladesh; Khan M.M., North South University, Electrical and Computer Engineering, Dhaka, 1229, Bangladesh","This paper introduces a smart stick system for assisting blind people. The smart stick is a solution which helps visually impaired people to detect obstacles, waters and dangers in front of them during walking time. It identifies almost everything around them. The system is modified to perform like an artificial vision and alarm unit the system consists of two sensors: water sensor, ultrasonic sensor and microcontroller (Arduino Uno R3) to receive the sensor signals. It processes them to the short pulses to the Arduino pins where buzzers are connected. GPS and GSM are used so that if the user thinks he/she is lost then they can press a button which will send the exact location of the stick to a particular number, which will be known as EC (Emergency Contact) button. Also, there is RF Module which will help to find the stick if it is lost in a certain range. GPS navigation system in the Mobile can be used to guide the blind people to find any places. The blind people can also use an earphone to listen to the navigation directions which are coming from the mobile phone and buzzer alarm in the stick to warn by sound. In this project, a smart stick is provided which is affordable and suitable for most of the blind people, and also it is light in weight. It can be made available to all aspects of the society and to the families who need it.  © 2022 IEEE.","Android App; Arduino Uno; Blind People; Buzzer; GPS; GSM; Low cost Smart Cane; RF Module; Ultra Sonic Sensors; Water Sensor","Android (operating system); Costs; Global positioning system; Ultrasonic applications; Ultrasonic sensors; Android apps; Arduino uno; Blind people; Buzzer; Low cost smart cane; Low-costs; RF module; Smart canes; Ultra sonic sensors; Water sensors; Global system for mobile communications","","","Institute of Electrical and Electronics Engineers Inc.","","978-166548453-4","","","English","IEEE World AI IoT Congr., AIIoT","Conference paper","Final","","Scopus","2-s2.0-85134882718"
"Shin K.; McConville R.; Metatla O.; Chang M.; Han C.; Lee J.; Roudaut A.","Shin, Kiyoung (55810687100); McConville, Ryan (55617133900); Metatla, Oussama (26422343700); Chang, Minhye (57198835090); Han, Chiyoung (57402862500); Lee, Junhaeng (57402005700); Roudaut, Anne (12445160900)","55810687100; 55617133900; 26422343700; 57198835090; 57402862500; 57402005700; 12445160900","Outdoor Localization Using BLE RSSI and Accessible Pedestrian Signals for the Visually Impaired at Intersections","2022","Sensors","22","1","371","","","","22","10.3390/s22010371","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85122348647&doi=10.3390%2fs22010371&partnerID=40&md5=69c8cf78c936a76faf09a6a97dd38437","RSS Center, Korea Electrotechnology Research Institute, Ansan, 15588, South Korea; Department of Computer Science, University of Bristol, Bristol, BS8 1TR, United Kingdom; Department of Engineering Mathematics, University of Bristol, Bristol, BS8 1TR, United Kingdom; Corporate Affiliated Research Institute, Human Care Co., Ltd, Ansan, 15258, South Korea","Shin K., RSS Center, Korea Electrotechnology Research Institute, Ansan, 15588, South Korea, Department of Computer Science, University of Bristol, Bristol, BS8 1TR, United Kingdom; McConville R., Department of Engineering Mathematics, University of Bristol, Bristol, BS8 1TR, United Kingdom; Metatla O., Department of Computer Science, University of Bristol, Bristol, BS8 1TR, United Kingdom; Chang M., RSS Center, Korea Electrotechnology Research Institute, Ansan, 15588, South Korea; Han C., Corporate Affiliated Research Institute, Human Care Co., Ltd, Ansan, 15258, South Korea; Lee J., Corporate Affiliated Research Institute, Human Care Co., Ltd, Ansan, 15258, South Korea; Roudaut A., Department of Computer Science, University of Bristol, Bristol, BS8 1TR, United Kingdom","One of the major challenges for blind and visually impaired (BVI) people is traveling safely to cross intersections on foot. Many countries are now generating audible signals at crossings for visually impaired people to help with this problem. However, these accessible pedestrian signals can result in confusion for visually impaired people as they do not know which signal must be interpreted for traveling multiple crosses in complex road architecture. To solve this problem, we propose an assistive system called CAS (Crossing Assistance System) which extends the principle of the BLE (Bluetooth Low Energy) RSSI (Received Signal Strength Indicator) signal for outdoor and indoor location tracking and overcomes the intrinsic limitation of outdoor noise to enable us to locate the user effectively. We installed the system on a real-world intersection and collected a set of data for demonstrating the feasibility of outdoor RSSI tracking in a series of two studies. In the first study, our goal was to show the feasibility of using outdoor RSSI on the localization of four zones. We used a k-nearest neighbors (kNN) method and showed it led to 99.8% accuracy. In the second study, we extended our work to a more complex setup with nine zones, evaluated both the kNN and an additional method, a Support Vector Machine (SVM) with various RSSI features for classification. We found that the SVM performed best using the RSSI average, standard deviation, median, interquartile range (IQR) of the RSSI over a 5 s window. The best method can localize people with 97.7% accuracy. We conclude this paper by discussing how our system can impact navigation for BVI users in outdoor and indoor setups and what are the implications of these findings on the design of both wearable and traffic assistive technology for blind pedestrian navigation. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","BLE RSSI; Localization at an intersection; Pedestrian navigation; Visually impaired","Blindness; Humans; Noise; Pedestrians; Self-Help Devices; Visually Impaired Persons; Indoor positioning systems; Navigation; Nearest neighbor search; Wearable technology; Bluetooth low energy received signal strength indicator; Localisation; Localization at an intersection; Lower energies; Outdoor localizations; Pedestrian navigation; Received signal strength indicators; Support vectors machine; Visually impaired; Visually impaired people; blindness; human; noise; pedestrian; self help device; visually impaired person; Support vector machines","A. Roudaut; Department of Computer Science, University of Bristol, Bristol, BS8 1TR, United Kingdom; email: Anne.Roudaut@bristol.ac.uk","","MDPI","14248220","","","35009910","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85122348647"
"Vorapatratorn S.","Vorapatratorn, Surapol (55360907300)","55360907300","AI-Based Obstacle Detection and Navigation for the Blind Using Convolutional Neural Network","2021","ICSEC 2021 - 25th International Computer Science and Engineering Conference","","","","17","22","5","2","10.1109/ICSEC53205.2021.9684607","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85125197515&doi=10.1109%2fICSEC53205.2021.9684607&partnerID=40&md5=0d4f0c0d2941a742865e56270ce552c9","Mae Fah Luang University, Ctr. of Excellence in Artif. Intelligence and Emerging Technologies School of Information Technology, Chiang Rai, Thailand","Vorapatratorn S., Mae Fah Luang University, Ctr. of Excellence in Artif. Intelligence and Emerging Technologies School of Information Technology, Chiang Rai, Thailand","Using stereo cameras and Convolutional Neural Networks, we present an AI-based obstacle detection and navigation system for the visually handicapped (CNN). The technology will employ wireless bone conductive headphones to guide users through stereo-directional sound patterns. First, we convert stereo photos to depth images, which may be utilized to determine each obstacle's depth level. Then, using our 2D-based Horizontal Depth Accumulative Information, we isolate obstacle pictures from the depth image (H-DAI image; side view projection). To train the AI model, the obstacle picture will be transformed to our Vertical Depth Accumulative Information (V-DAI image; top view projection). There are 34, 325 example photos in the training dataset, with 7 different obstacle kinds. The influence of each picture input type, such as depth image, obstacle image, and V-DAI image, is investigated in our experiment. According to the findings, utilizing a V-DAI picture with CNN achieves the maximum accuracy of classification of 93.61 percent and the quickest prediction speed of 10, 169 samples per second.  © 2021 IEEE.","computer vision; convolutional neural network; machine learning; obstacle detection; the blind; visually impaired","Computer vision; Convolution; Convolutional neural networks; Machine learning; Navigation systems; Stereo image processing; Convolutional neural network; Depth image; Obstacles detection; Side view; Stereo cameras; The blind; Top views; Training dataset; Visually handicapped; Visually impaired; Obstacle detectors","S. Vorapatratorn; Mae Fah Luang University, Ctr. of Excellence in Artif. Intelligence and Emerging Technologies School of Information Technology, Chiang Rai, Thailand; email: Surapol.vor@mfu.ac.th","","Institute of Electrical and Electronics Engineers Inc.","","978-166541197-4","","","English","ICSEC - Int. Comput. Sci. Eng. Conf.","Conference paper","Final","","Scopus","2-s2.0-85125197515"
"Tayyaba S.; Ashraf M.W.; Alquthami T.; Ahmad Z.; Manzoor S.","Tayyaba, Shahzadi (35811319900); Ashraf, Muhammad Waseem (7201371732); Alquthami, Thamer (36650413200); Ahmad, Zubair (55358599200); Manzoor, Saher (57217199762)","35811319900; 7201371732; 36650413200; 55358599200; 57217199762","Fuzzy-based approach using iot devices for smart home to assist blind people for navigation","2020","Sensors (Switzerland)","20","13","3674","1","13","12","25","10.3390/s20133674","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85087168021&doi=10.3390%2fs20133674&partnerID=40&md5=0bb4399745e3e250a5793476f359400c","Department of Computer Engineering, The University of Lahore, Lahore, 54000, Pakistan; Department of Physics (Electronics), Government College University, Lahore, 54000, Pakistan; Electrical and Computer Engineering Department, King Abdulaziz University, Jeddah, 21598, Saudi Arabia; Center for Advanced Material (CAM), Qatar University, PO Box 2713, Doha, Qatar","Tayyaba S., Department of Computer Engineering, The University of Lahore, Lahore, 54000, Pakistan; Ashraf M.W., Department of Physics (Electronics), Government College University, Lahore, 54000, Pakistan; Alquthami T., Electrical and Computer Engineering Department, King Abdulaziz University, Jeddah, 21598, Saudi Arabia; Ahmad Z., Center for Advanced Material (CAM), Qatar University, PO Box 2713, Doha, Qatar; Manzoor S., Department of Physics (Electronics), Government College University, Lahore, 54000, Pakistan","The demand of devices for safe mobility of blind people is increasing with advancement in wireless communication. Artificial intelligent devices with multiple input and output methods are used for reliable data estimation based on maximum probability. A model of a smart home for safe and robust mobility of blind people has been proposed. Fuzzy logic has been used for simulation. Outputs from the internet of things (IoT) devices comprising sensors and bluetooth are taken as input of the fuzzy controller. Rules have been developed based on the conditions and requirements of the blind person to generate decisions as output. These outputs are communicated through IoT devices to assist the blind person or user for safe movement. The proposed system provides the user with easy navigation and obstacle avoidance. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Bluetooth protocol; Fuzzy approach; IoT; Navigation; Smart home; Ultrasonic sensors","Artificial Intelligence; Computer Simulation; Environment Design; Fuzzy Logic; Housing; Humans; Internet of Things; Visually Impaired Persons; Air navigation; Automation; Fuzzy logic; Vision; Artificial intelligent; Data estimation; Fuzzy controllers; Internet of thing (IOT); Maximum probability; Multiple inputs; Safe mobility; Wireless communications; artificial intelligence; computer simulation; environmental planning; fuzzy logic; housing; human; visually impaired person; Internet of things","M.W. Ashraf; Department of Physics (Electronics), Government College University, Lahore, 54000, Pakistan; email: dr.waseem@gcu.edu.pk; Z. Ahmad; Center for Advanced Material (CAM), Qatar University, Doha, PO Box 2713, Qatar; email: zubairtarar@qu.edu.qa","","MDPI AG","14248220","","","32630055","English","Sensors","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85087168021"
"Sarkar A.N.; Jayita Saha; Das P.; Kumari R.; Chakraborty T.; Dey N.","Sarkar, Atindra Nath (57466707200); Jayita Saha (57467280800); Das, Pamela (57466560400); Kumari, Ritu (57201579680); Chakraborty, Tulika (57213937531); Dey, Naiwrita (57193691343)","57466707200; 57467280800; 57466560400; 57201579680; 57213937531; 57193691343","Navigation Enabling Application for Blind People","2022","Lecture Notes in Electrical Engineering","815","","","451","464","13","1","10.1007/978-981-16-7011-4_43","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85125279830&doi=10.1007%2f978-981-16-7011-4_43&partnerID=40&md5=e7183e1e7b527f6e350153dfbdf3f5bd","Department of AEIE, RCCIIT, Kolkata, India; Department of ECE, RCCIIT, Kolkata, India","Sarkar A.N., Department of AEIE, RCCIIT, Kolkata, India; Jayita Saha, Department of AEIE, RCCIIT, Kolkata, India; Das P., Department of AEIE, RCCIIT, Kolkata, India; Kumari R., Department of AEIE, RCCIIT, Kolkata, India; Chakraborty T., Department of AEIE, RCCIIT, Kolkata, India; Dey N., Department of ECE, RCCIIT, Kolkata, India","This paper proposes an assistive navigation application system to help visually impaired persons with independent travel in indoor. The proposed system is implemented in two parts: first, the obstacle detection is done by object identification using image processing and the images are stored in cloud further. You only look once (YOLO) version 3 which is a fully convolutional neural network (FCNN) is used for object identification. Twenty classes are considered with 2600 number of image samples. The application assembly is developed in android studio in Java platform. The application is named as “NETRA”. The blind person has to be logged in to the application beforehand. Later with text-to-speech conversion, the blind individual will be navigated. In the present work, the object identification using image processing and the overall application assembly is shown and discussed. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Application; Cloud; Image processing; Navigation; YOLO","Convolutional neural networks; Indoor positioning systems; Object detection; Obstacle detectors; Application systems; Assistive navigations; Blind people; Convolutional neural network; Images processing; Java platforms; Object identification; Obstacles detection; Visually impaired persons; You only look once; Navigation","N. Dey; Department of ECE, RCCIIT, Kolkata, India; email: naiwritadey@gmail.com","Bhaumik S.; Chattopadhyay S.; Chattopadhyay T.; Bhattacharya S.","Springer Science and Business Media Deutschland GmbH","18761100","978-981167010-7","","","English","Lect. Notes Electr. Eng.","Conference paper","Final","","Scopus","2-s2.0-85125279830"
"Varghese R.R.; Jacob P.M.; Shaji M.; Abhijith R.; John E.S.; Philip S.B.","Varghese, Renju Rachel (57211298447); Jacob, Pramod Mathew (57193714959); Shaji, Midhun (57695186800); Abhijith, R. (57694211300); John, Emil Saji (57694707600); Philip, Sebin Beebi (57694457300)","57211298447; 57193714959; 57695186800; 57694211300; 57694707600; 57694457300","An Intelligent Voice Assistance System for Visually Impaired using Deep Learning","2022","2022 International Conference on Decision Aid Sciences and Applications, DASA 2022","","","","449","453","4","1","10.1109/DASA54658.2022.9765171","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85130142125&doi=10.1109%2fDASA54658.2022.9765171&partnerID=40&md5=aaeb43864ca29007da0c0447ef60e16c","Providence College of Engineering, Computer Science Department, Kerala, Chengannur, India","Varghese R.R., Providence College of Engineering, Computer Science Department, Kerala, Chengannur, India; Jacob P.M., Providence College of Engineering, Computer Science Department, Kerala, Chengannur, India; Shaji M., Providence College of Engineering, Computer Science Department, Kerala, Chengannur, India; Abhijith R., Providence College of Engineering, Computer Science Department, Kerala, Chengannur, India; John E.S., Providence College of Engineering, Computer Science Department, Kerala, Chengannur, India; Philip S.B., Providence College of Engineering, Computer Science Department, Kerala, Chengannur, India","Unassisted navigation, object recognition, obstacle avoidance, and reading activities are extremely difficult for people who are completely blind. For those who are visually impaired, we present a new form of assistive technology. Raspberry Pi 3 Model B+ was selected to illustrate the proposed prototype's capability because of its inexpensive price, compact size, and ease of integration. Incorporated within the design is a camera, sensors for obstacle avoidance, and powerful image processing algorithms for detecting and classifying objects. Both the camera and the ultrasonic sensors are used to determine the user's distance from the impediment. The image-to-text converter, followed by audio feedback, is integrated into the system. A typical pair of eyeglasses can be used to mount the entire system, which is small, light, and simple to use. Using 60 completely blind people, researchers compare the suggested device to the classic white cane in terms of performance. Controlled environments based on real-world scenarios are used to conduct the evaluations. In comparison to a white cane, the proposed device provides higher accessibility and comfort, as well as simplicity of navigation for visually impaired people. © 2022 IEEE.","Haar cascade classifier; Raspberry Pi; TensorFlow; Tesseract","Cameras; Classification (of information); Object detection; Object recognition; Assistance system; Haar cascade classifiers; Objects recognition; Obstacles avoidance; Raspberry pi; Reading activities; Tensorflow; Tesseract; Visually impaired; White cane; Deep learning","R.R. Varghese; Providence College of Engineering, Computer Science Department, Chengannur, Kerala, India; email: renjuvarghese1991@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","","978-166549501-1","","","English","Int. Conf. Decis. Aid Sci. Appl., DASA","Conference paper","Final","","Scopus","2-s2.0-85130142125"
"Vorapatratorn S.; Suchato A.; Punyabukkana P.","Vorapatratorn, Surapol (55360907300); Suchato, Atiwong (12785303000); Punyabukkana, Proadpran (14018257100)","55360907300; 12785303000; 14018257100","Fast obstacle detection system for the blind using depth image and machine learning","2021","Engineering and Applied Science Research","48","5","","593","603","10","4","10.14456/easr.2021.61","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85111133600&doi=10.14456%2feasr.2021.61&partnerID=40&md5=2d8af5b8b14df3784bc7a9f032dc0ef1","Department of Computer Engineering, Faculty of Engineering, Chulalongkorn University, Bangkok, 10330, Thailand","Vorapatratorn S., Department of Computer Engineering, Faculty of Engineering, Chulalongkorn University, Bangkok, 10330, Thailand; Suchato A., Department of Computer Engineering, Faculty of Engineering, Chulalongkorn University, Bangkok, 10330, Thailand; Punyabukkana P., Department of Computer Engineering, Faculty of Engineering, Chulalongkorn University, Bangkok, 10330, Thailand","Our research proposes a novel obstacle detection and navigation system for the blind using stereo cameras with machine learning techniques. The obstacle classification result will navigate users through a difference directional sound patterns via bone conductive stereo headphones. In the first stage, the Semi-Global block-matching technique was used to transform stereo images to depth image which can be used to identify the depth level of each image pixel. Next, fast 2D-based ground plane estimation which was separate obstacle image from the depth image with our Horizontal Depth Accumulative Information (H-DAI). The obstacle image will be then converted to our Vertical Depth Accumulative Information (V-DAI) which was extracted by a feature vector to train the obstacle model. Our dataset consists of 34,325 stereo-gray images in 7 different obstacle class. Our experiment compared various machine learning algorithms (ANN, SVM, Naïve Bayes, Decision Tree, k-NN and Deep Learning) performance between classification accuracy and prediction speed. The results show that using ANN with our H-DAI and V-DAI reaches 96.45% in obstacle classification accuracy and 23.76 images per second for processing time which is 6.75 times faster than the recently ground plane estimate technique. © 2021, Paulus Editora. All rights reserved.","Assistive technology; Computer vision; Machine learning; Scene understanding; Visually impaired","","P. Punyabukkana; Department of Computer Engineering, Faculty of Engineering, Chulalongkorn University, Bangkok, 10330, Thailand; email: proadpran.p@chula.ac.th","","Paulus Editora","25396161","","","","English","Eng. Appl. Sci. Res.","Article","Final","","Scopus","2-s2.0-85111133600"
"Plikynas D.","Plikynas, Darius (57207667356)","57207667356","Indoor Visual Mapping and Navigation for Blind People","2022","Proceedings of SPIE - The International Society for Optical Engineering","12084","","120840S","","","","0","10.1117/12.2623893","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85134334726&doi=10.1117%2f12.2623893&partnerID=40&md5=f7337c5213d55591569d8c6d82942104","Vilnius Gediminas Technical University, Lithuania","Plikynas D., Vilnius Gediminas Technical University, Lithuania","Admittedly, machine vision-based assistive applications are beneficial for blind and visually impaired (BVI) persons. Such a need has numerous already implemented outdoor assistive solutions. However, there are much less effective solutions for indoor navigation and orientation. It is due to the absence of GPS signals and the need for infrastructural investments (such as WI-FI signals, beamers, RFID tags). In this paper, we present another way - a wearable electronic traveling aid (ETA) system for the BVI persons using outsourcing, i.e., volunteers’ mapping of buildings indoor routes. Volunteers use the proposed wearable ETA device to record indoor routes stored in the web cloud database using web services. Smartphones’ IMU and other sensors, stereo and depth camera, audio and haptic devices, computer vision algorithms, and computational intelligence are employed for objects detection and recognition, and consequently, intelligent routing and mapping of indoor spaces. Integration of semantic data of points of interest (such as stairs, doors, WC, entrances/exits) and building (evacuation) schemes makes the proposed approach even more attractive to the BVI users. The presented approach can also be employed to crowdsourcing real-time help in complex navigational situations such as dead reckoning, avoiding various obstacles, or unforeseen situations. © 2022 SPIE.","blind and visually impaired; electronic travelling aids; guided navigation; machine vision; outsourcing; visual navigation","Air navigation; Indoor positioning systems; Mapping; Object detection; Semantics; Stereo image processing; Web services; Blind and visually impaired; Blind people; Electronic traveling aid; Guided navigation; Machine-vision; Traveling aids; Vision based; Visual mapping; Visual Navigation; Visually impaired persons; Computer vision","","Osten W.; Nikolaev D.; Zhou J.","SPIE","0277786X","978-151065044-2","PSISD","","English","Proc SPIE Int Soc Opt Eng","Conference paper","Final","","Scopus","2-s2.0-85134334726"
"Plikynas D.; Zvironas A.; Gudauskis M.; Budrionis A.; Daniusis P.; Sliesoraityte I.","Plikynas, Darius (57207667356); Zvironas, Arunas (24771393100); Gudauskis, Marius (45561168000); Budrionis, Andrius (55735006700); Daniusis, Povilas (35409127300); Sliesoraityte, Ieva (36718074100)","57207667356; 24771393100; 45561168000; 55735006700; 35409127300; 36718074100","Research advances of indoor navigation for blind people: A brief review of technological instrumentation","2020","IEEE Instrumentation and Measurement Magazine","23","4","9126068","22","32","10","24","10.1109/MIM.2020.9126068","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85087437424&doi=10.1109%2fMIM.2020.9126068&partnerID=40&md5=e099bd4dccd4e7fe4596a5166061ad19","Vilnius Gediminas Technical University, Vilnius, Lithuania; Kaunas University of Technology, Institute of Mechatronics, Kaunas, Lithuania; Institut Arthur Vernes, Paris, France","Plikynas D., Vilnius Gediminas Technical University, Vilnius, Lithuania; Zvironas A., Kaunas University of Technology, Institute of Mechatronics, Kaunas, Lithuania; Gudauskis M., Kaunas University of Technology, Institute of Mechatronics, Kaunas, Lithuania; Budrionis A., Vilnius Gediminas Technical University, Vilnius, Lithuania; Daniusis P., Vilnius Gediminas Technical University, Vilnius, Lithuania; Sliesoraityte I., Institut Arthur Vernes, Paris, France","Blind persons need electronic traveling aid (ETA) solutions for better orientation and navigation in unfamiliar indoor environments, with embedded features for detection and recognition of both obstacles and desired destinations such as rooms, staircases, and elevators. Because the use of GPS for locational references is impractical indoors, the development of such navigation systems is challenging and requires a systematic review and evaluation of different technological approaches. Using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) method, we evaluated and compared current research papers that deal with the prototyping of assistive devices (visual sensory perception substitution with audio and haptic signals) for blind and visually impaired persons. We conducted an instructional assessment of selected indoor navigation prototypes using three main criteria: navigation technologies, sensors, and computer vision approaches. For the latter category, we conducted a separate systematic review, as papers in this research area primarily specialize in software computer vision solutions rather than hardware. The paper provides useful insights for researchers regarding technological instrumentation for the development of ETA solutions for blind and visually impaired (VI) persons in the field of indoor orientation and navigation.  © 1998-2012 IEEE.","","Computer hardware; Indoor positioning systems; Navigation systems; Vision; Blind and visually impaired; Blind people; Blind person; Indoor environment; Indoor navigation; Orientation and navigation; Research advances; Systematic Review; Traveling aids; Visually impaired persons; Computer vision","D. Plikynas; Vilnius Gediminas Technical University, Vilnius, Lithuania; email: darius.plikynas@vgtu.lt","","Institute of Electrical and Electronics Engineers Inc.","10946969","","IIMMF","","English","IEEE Instrum Meas Mag","Review","Final","","Scopus","2-s2.0-85087437424"
"Dissanayake D.M.L.V.; Rajapaksha R.G.M.D.R.P.; Prabhashawara U.P.; Solanga S.A.D.S.P.; Jayakody J.A.D.C.A.","Dissanayake, D.M.L.V. (57465838900); Rajapaksha, R.G.M.D.R.P. (57465398200); Prabhashawara, U.P. (57465986900); Solanga, S.A.D.S.P. (57465839000); Jayakody, J.A.D.C. Anuradha (55821069900)","57465838900; 57465398200; 57465986900; 57465839000; 55821069900","Guide-Me: Voice authenticated indoor user guidance system","2021","2021 IEEE 12th Annual Ubiquitous Computing, Electronics and Mobile Communication Conference, UEMCON 2021","","","","509","514","5","0","10.1109/UEMCON53757.2021.9666733","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85125183519&doi=10.1109%2fUEMCON53757.2021.9666733&partnerID=40&md5=fe29b43189d0d2ca8adb6c7c2c09ce65","Institue of Information Technology, Faculty of Computing Sri Lanka, Department of Computer Systems Engineering, Malabe, Sri Lanka; Sri Lanka Institue of Information Technology, Faculty of Computing, Department of Information Technology, Malabe, Sri Lanka; Curtin University, Department of Electrical and Computer Engineering, Perth, WA, Australia","Dissanayake D.M.L.V., Institue of Information Technology, Faculty of Computing Sri Lanka, Department of Computer Systems Engineering, Malabe, Sri Lanka; Rajapaksha R.G.M.D.R.P., Sri Lanka Institue of Information Technology, Faculty of Computing, Department of Information Technology, Malabe, Sri Lanka; Prabhashawara U.P., Institue of Information Technology, Faculty of Computing Sri Lanka, Department of Computer Systems Engineering, Malabe, Sri Lanka; Solanga S.A.D.S.P., Institue of Information Technology, Faculty of Computing Sri Lanka, Department of Computer Systems Engineering, Malabe, Sri Lanka; Jayakody J.A.D.C.A., Curtin University, Department of Electrical and Computer Engineering, Perth, WA, Australia","Due to a lack of knowledge about the building structure and possible impediments, the majority of blind persons require assistance when traveling through unknown regions. To solve this issue, this paper provides ""Guide-Me""as a strategy for indoor navigation with optimum accessibility, usability, and security, decreasing obstacles that the user may meet when traveling through indoor surroundings. Because the intended audience for this research is blind or visually impaired persons, ""Guide-Me""makes use of the user's voice-based inputs. This paper also includes Bluetooth beacon integration for localization, a Smart stick with sensors for obstacle detection, a machine learning model for voice authentication, and an algorithm protocol for a secure connection between server and application Integration driven architecture to assist vision impaired in navigating the known and unknown indoor environment.  © 2021 IEEE.","Guide-Me; Indoor navigation; Localization; Vision impaired; Voice authentication","Authentication; Machine learning; Network architecture; Obstacle detectors; Blind person; Building structure; Guidance system; Guide-me; Indoor navigation; Localisation; Usability and security; User guidance; Vision impaired; Voice authentication; Indoor positioning systems","","Paul R.","Institute of Electrical and Electronics Engineers Inc.","","978-166540690-1","","","English","IEEE Annu. Ubiquitous Comput., Electron. Mob. Commun. Conf., UEMCON","Conference paper","Final","","Scopus","2-s2.0-85125183519"
"Chandna S.; Singhal A.","Chandna, Swati (57302046600); Singhal, Abhishek (56198942000)","57302046600; 56198942000","Towards Outdoor Navigation System for Visually Impaired People using YOLOv5","2022","Proceedings of the Confluence 2022 - 12th International Conference on Cloud Computing, Data Science and Engineering","","","","617","622","5","12","10.1109/Confluence52989.2022.9734204","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85127550535&doi=10.1109%2fConfluence52989.2022.9734204&partnerID=40&md5=2312d31d1af0d136279e2379f9625d93","Srh University Heidelberg, Faculty Of Information Media, And Design, Heidelberg, Germany; Amity University Uttar Pradesh, Department Of Cse, Aset, Noida, India","Chandna S., Srh University Heidelberg, Faculty Of Information Media, And Design, Heidelberg, Germany; Singhal A., Amity University Uttar Pradesh, Department Of Cse, Aset, Noida, India","Navigation is one of the many difficulties the Blind and Visually Impaired (BVI) face, be it indoor or outdoor. BVI people use their remaining senses such as smell, touch, and memory to navigate in a familiar environment. But the real problem begins when the environment is new. In the new environment, they seek help from closed ones or use assistive tools. This paper proposes a deep learning based outdoor smartphone navigation system approach which will assist the user in crossing the road by detecting the crosswalks from a mobile device. CNN of Deep learning has been proven to give the best results for image classification. Hence we want to analyze and compare different variants of CNN. As mobile devices have computed limitations, we explore two lightweight state-of-the-art neural networks, namely YOLOv5 and MobileNetSSDV2, for this task. The image dataset of crosswalks for work is collected via web scraping and manual effort. To evaluate the performance of the two neural networks: mAP, loss, and training time of the models are compared. As a result, it is determined that YOLOv5 has a five times faster inference speed than MobileNetSSDv2, which makes YOLOv5 more suitable for real-time applications. This approach utilizes the smartphone camera to detect the crosswalks and provide voice feedback to help people with low vision understand their environment.  © 2022 IEEE.","MobileNetSSDv2 deep learning; neural network; Visually Impaired; YOLOv5","Air navigation; Navigation systems; Smartphones; Assistive tool; Blind and visually impaired; Mobilenetssdv2 deep learning; Neural-networks; Outdoor navigation systems; Real problems; Smart phones; Visually impaired; Visually impaired people; YOLOv5; Deep learning","","","Institute of Electrical and Electronics Engineers Inc.","","978-166543701-1","","","English","Proc. Conflu. - Int. Conf. Cloud Comput., Data Sci. Eng.","Conference paper","Final","","Scopus","2-s2.0-85127550535"
"Vijlyakumar K.; Ajitha K.; Alexia A.; Hemalashmi M.; Madhumitha S.","Vijlyakumar, K. (57192553778); Ajitha, K. (57221126154); Alexia, A. (57221117459); Hemalashmi, M. (57221123323); Madhumitha, S. (57221116611)","57192553778; 57221126154; 57221117459; 57221123323; 57221116611","Object Detection for Visually Impaired People Using SSD Algorithm","2020","2020 International Conference on System, Computation, Automation and Networking, ICSCAN 2020","","","9262344","","","","5","10.1109/ICSCAN49426.2020.9262344","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85098249812&doi=10.1109%2fICSCAN49426.2020.9262344&partnerID=40&md5=3c4ddef1efdff0b6431c90bac4b911b0","Manakula Vinayagar Institute of Technology, Department of Information Technology, Puducherry, India","Vijlyakumar K., Manakula Vinayagar Institute of Technology, Department of Information Technology, Puducherry, India; Ajitha K., Manakula Vinayagar Institute of Technology, Department of Information Technology, Puducherry, India; Alexia A., Manakula Vinayagar Institute of Technology, Department of Information Technology, Puducherry, India; Hemalashmi M., Manakula Vinayagar Institute of Technology, Department of Information Technology, Puducherry, India; Madhumitha S., Manakula Vinayagar Institute of Technology, Department of Information Technology, Puducherry, India","Visually impaired people are unaware of the danger that they are facing in their life. They may face many challenges while performing their daily activity even in their familiar environments. Vision is the necessary human senses and it plays the important role in human perception about surrounding environment. Hence, there are variety of computer vision products and services which are used in the development of new electronic aids for those blind people. In this paper we designed to provide navigation to those people. It guides the people about the object as well as provides the distance of the object. The algorithm itself calculates the distance of the object. Here it also provides the audio jack to insist them about the object. Here we are using SSD Algorithm for object detection and calculating the distance of the object by using monodepth algorithm. © 2020 IEEE.","Blind; Image Processing; Object Detection; Object Recognition","","","","Institute of Electrical and Electronics Engineers Inc.","","978-172816202-7","","","English","Int. Conf. Syst., Comput., Autom. Netw., ICSCAN","Conference paper","Final","","Scopus","2-s2.0-85098249812"
"Bizoń-Angov P.; Osiński D.; Wierzchoń M.; Konieczny J.","Bizoń-Angov, Patrycja (56610041800); Osiński, Dominik (56769866100); Wierzchoń, Michał (55255416500); Konieczny, Jarosław (24399292000)","56610041800; 56769866100; 55255416500; 24399292000","Visual echolocation concept for the colorophone sensory substitution device using virtual reality","2021","Sensors (Switzerland)","21","1","237","1","24","23","5","10.3390/s21010237","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85099000583&doi=10.3390%2fs21010237&partnerID=40&md5=f8b81e1f095ece9e044d06ae1c3a604d","Consciousness Lab, Institute of Psychology, Jagiellonian University, Kraków, 30-060, Poland; Department of Electronic Systems, Norwegian University of Science and Technology, Trondheim, NO-7491, Norway; Jagiellonian Human-Centered Artificial Intelligence Laboratory, Jagiellonian University, Kraków, 30-348, Poland; Department of Process Control, AGH University of Science and Technology, Kraków, 30-059, Poland","Bizoń-Angov P., Consciousness Lab, Institute of Psychology, Jagiellonian University, Kraków, 30-060, Poland; Osiński D., Department of Electronic Systems, Norwegian University of Science and Technology, Trondheim, NO-7491, Norway; Wierzchoń M., Consciousness Lab, Institute of Psychology, Jagiellonian University, Kraków, 30-060, Poland, Jagiellonian Human-Centered Artificial Intelligence Laboratory, Jagiellonian University, Kraków, 30-348, Poland; Konieczny J., Department of Process Control, AGH University of Science and Technology, Kraków, 30-059, Poland","Detecting characteristics of 3D scenes is considered one of the biggest challenges for visually impaired people. This ability is nonetheless crucial for orientation and navigation in the natural environment. Although there are several Electronic Travel Aids aiming at enhancing orientation and mobility for the blind, only a fewof themcombine passing both 2D and 3D information, including colour. Moreover, existing devices either focus on a small part of an image or allow interpretation of a mere few points in the field of view. Here, we propose a concept of visual echolocation with integrated colour sonification as an extension of Colorophone-an assistive device for visually impaired people. The concept aims at mimicking the process of echolocation and thus provides 2D, 3D and additionally colour information of the whole scene. Even though the final implementation will be realised by a 3D camera, it is first simulated, as a proof of concept, by using VIRCO-a Virtual Reality training and evaluation system for Colorophone. The first experiments showed that it is possible to sonify colour and distance of the whole scene, which opens up a possibility to implement the developed algorithm on a hardware-based stereo camera platform. An introductory user evaluation of the system has been conducted in order to assess the effectiveness of the proposed solution for perceiving distance, position and colour of the objects placed in Virtual Reality. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","3D camera; 3D scene sonification; Auditory SSD; Colour sonification; Distance sonification; Stereo vision; Virtual reality","Animals; Blindness; Echolocation; Humans; Male; Self-Help Devices; Virtual Reality; Visually Impaired Persons; Cameras; Color; Sonar; Stereo image processing; Assistive devices; Colour informations; Electronic travel aidss; Natural environments; Orientation and navigation; Sensory substitution; Virtual reality training; Visually impaired people; animal; blindness; echolocation; human; male; self help device; virtual reality; visually impaired person; Virtual reality","P. Bizoń-Angov; Consciousness Lab, Institute of Psychology, Jagiellonian University, Kraków, 30-060, Poland; email: patrycja.bizon@gmail.com","","MDPI AG","14248220","","","33401458","English","Sensors","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85099000583"
"Ouali I.; Ben Halima M.; Wali A.","Ouali, Imene (57212227268); Ben Halima, Mohamed (57188534608); Wali, Ali (35281296800)","57212227268; 57188534608; 35281296800","Real-Time Application for Recognition and Visualization of Arabic Words with Vowels based DL and AR","2022","2022 International Wireless Communications and Mobile Computing, IWCMC 2022","","","","678","683","5","6","10.1109/IWCMC55113.2022.9825089","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85135328369&doi=10.1109%2fIWCMC55113.2022.9825089&partnerID=40&md5=b1b1b7e6b8739af6d021177f913b4df7","REGIM: REsearch Groups on Intelligent Machines, National Engineering School of Sfax (ENIS), University of Sfax, Tunisia","Ouali I., REGIM: REsearch Groups on Intelligent Machines, National Engineering School of Sfax (ENIS), University of Sfax, Tunisia; Ben Halima M., REGIM: REsearch Groups on Intelligent Machines, National Engineering School of Sfax (ENIS), University of Sfax, Tunisia; Wali A., REGIM: REsearch Groups on Intelligent Machines, National Engineering School of Sfax (ENIS), University of Sfax, Tunisia","Text is difficult to read in some cases due to text orientation, writing style, very light colors, etc. Visually impaired or visually impaired people have difficulty reading text in all of these situations. The architecture proposed in this work is intended to detect and identify Arabic characters with vowels in natural environments. This architecture can help visually impaired or blind people to read the text correctly. It allows users to read the text in a better and more immersive way by combining augmented reality with digital material. The approach uses both deep learning and more specifically the VGG 19 model and augmented reality to improve the efficiency, clarity, and accuracy of text reading. For text detection and identification we use the VGG 19 model, and for text visualization, we use augmented reality. The implementation technique presented in this research for an augmented reality interactive virtual assistant system is for users to use their smartphone's camera to receive enhanced text information via a text image and a three-dimensional image to understand the displayed text. It offers an interesting way to understand their environment. The use of augmented reality to better display recognized text in 3D is a fantastic feature. User research studies are conducted to assess usability and user satisfaction. © 2022 IEEE.","Augmented Reality; Natural Scene; Text detection; Text recognition; Text Visualization; VGG19","Character recognition; Deep learning; Image enhancement; Three dimensional computer graphics; Visualization; Light color; Natural scenes; Real-time application; Text detection; Text recognition; Text visualization; VGG19; Visually impaired; Visually impaired people; Writing style; Augmented reality","","","Institute of Electrical and Electronics Engineers Inc.","","978-166546749-0","","","English","Int. Wirel. Commun. Mob. Comput., IWCMC","Conference paper","Final","","Scopus","2-s2.0-85135328369"
"Slade P.; Tambe A.; Kochenderfer M.J.","Slade, Patrick (56416007700); Tambe, Arjun (57323579900); Kochenderfer, Mykel J. (24824356000)","56416007700; 57323579900; 24824356000","Multimodal sensing and intuitive steering assistance improve navigation and mobility for people with impaired vision","2021","Science Robotics","6","59","eabg6594","","","","38","10.1126/scirobotics.abg6594","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85118520595&doi=10.1126%2fscirobotics.abg6594&partnerID=40&md5=153fa0e82d14706c4f789b17b4a8018d","Department of Mechanical Engineering, Stanford University, Stanford, CA, United States; Department of Aeronautics and Astronautics, Stanford University, Stanford, CA, United States","Slade P., Department of Mechanical Engineering, Stanford University, Stanford, CA, United States; Tambe A., Department of Mechanical Engineering, Stanford University, Stanford, CA, United States; Kochenderfer M.J., Department of Aeronautics and Astronautics, Stanford University, Stanford, CA, United States","Globally, more than 250 million people have impaired vision and face challenges navigating outside their homes, affecting their independence, mental health, and physical health. Navigating unfamiliar routes is challenging for people with impaired vision because it may require avoiding obstacles, recognizing objects, and wayfinding indoors and outdoors. Existing approaches such as white canes, guide dogs, and electronic travel aids only tackle some of these challenges. Here, we present the Augmented Cane, a white cane with a comprehensive set of sensors and an intuitive feedback method to steer the user, which addresses navigation challenges and improves mobility for people with impaired vision. We compared the Augmented Cane with a white cane by having sighted and visually impaired participants complete navigation challenges while blindfolded: walking along hallways, avoiding obstacles, and following outdoor waypoints. Across all experiments, the Augmented Cane increased the walking speed for participants with impaired vision by 18 ± 7% and sighted participants by 35 ± 12% compared with a white cane. The increase in walking speed may be due to accurate steering assistance, reduced cognitive load, fewer contacts with the environment, and higher participant confidence. We also demonstrate advanced navigation capabilities of the Augmented Cane: indoor wayfinding, recognizing and steering the participant to a key object, and navigating a sequence of indoor and outdoor challenges. The open-source and low-cost design of the Augmented Cane provides a platform that may improve the mobility and quality of life of people with impaired vision. Copyright © 2021 The Authors, some rights reserved.","","Algorithms; Blindness; Canes; Electronics; Equipment Design; Haptic Technology; Humans; Man-Machine Systems; Movement; Quality of Life; Robotics; Safety; Self-Help Devices; Visually Impaired Persons; Walking; Indoor positioning systems; Walking aids; Avoiding obstacle; Electronic travel aidss; Guide dogs; Impaired vision; Mental health; Multimodal sensing; Physical health; Walking speed; Way finding; White cane; algorithm; blindness; cane; electronics; equipment design; human; man machine interaction; movement (physiology); quality of life; robotics; safety; self help device; visually impaired person; walking; Navigation","P. Slade; Department of Mechanical Engineering, Stanford University, Stanford, United States; email: patslade@stanford.edu","","American Association for the Advancement of Science","24709476","","","34644159","English","Sci. Robotics","Article","Final","","Scopus","2-s2.0-85118520595"
"Presti G.; Ahmetovic D.; Ducci M.; Bernareggi C.; Ludovico L.A.; Baratè A.; Avanzini F.; Mascetti S.","Presti, Giorgio (56407087000); Ahmetovic, Dragan (53867884600); Ducci, Mattia (57200504661); Bernareggi, Cristian (56266643600); Ludovico, Luca A. (8684305800); Baratè, Adriano (14031183900); Avanzini, Federico (7005300654); Mascetti, Sergio (8919148700)","56407087000; 53867884600; 57200504661; 56266643600; 8684305800; 14031183900; 7005300654; 8919148700","Iterative Design of Sonification Techniques to Support People with Visual Impairments in Obstacle Avoidance","2021","ACM Transactions on Accessible Computing","14","4","3470649","","","","8","10.1145/3470649","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85118631013&doi=10.1145%2f3470649&partnerID=40&md5=44d5de07636d7669ed43abab324b0777","Department of Computer Science, University of Milan, Via Celoria 18, Milan, 20133, Italy","Presti G., Department of Computer Science, University of Milan, Via Celoria 18, Milan, 20133, Italy; Ahmetovic D., Department of Computer Science, University of Milan, Via Celoria 18, Milan, 20133, Italy; Ducci M., Department of Computer Science, University of Milan, Via Celoria 18, Milan, 20133, Italy; Bernareggi C., Department of Computer Science, University of Milan, Via Celoria 18, Milan, 20133, Italy; Ludovico L.A., Department of Computer Science, University of Milan, Via Celoria 18, Milan, 20133, Italy; Baratè A., Department of Computer Science, University of Milan, Via Celoria 18, Milan, 20133, Italy; Avanzini F., Department of Computer Science, University of Milan, Via Celoria 18, Milan, 20133, Italy; Mascetti S., Department of Computer Science, University of Milan, Via Celoria 18, Milan, 20133, Italy","Obstacle avoidance is a major challenge during independent mobility for blind or visually impaired (BVI) people. Typically, BVI people can only perceive obstacles at a short distance (about 1 m, in case they are using the white cane), and some obstacles are hard to detect (e.g., those elevated from the ground), or should not be hit by the white cane (e.g., a standing person). A solution to these problems can be found in recent computer-vision techniques that can run on mobile and wearable devices to detect obstacles at a distance. However, in addition to detecting obstacles, it is also necessary to convey information about them in real time. This contribution presents WatchOut, a sonification technique for conveying real-time information about the main properties of an obstacle to a BVI person, who can then use this additional feedback to safely navigate in the environment. WatchOut was designed with a user-centered approach, involving four iterations of online listening tests with BVI participants in order to define, improve and evaluate the sonification technique, eventually obtaining an almost perfect recognition accuracy. WatchOut was also implemented and tested as a module of a mobile app that detects obstacles using state-of-the-art computer vision technology. Results show that the system is considered usable and can guide the users to avoid more than 85% of the obstacles. © 2021 Association for Computing Machinery.","navigation assistance; orientation & mobility; Turn-by-turn navigation","Computer vision; Navigation; Computer vision techniques; Iterative design; Navigation assistance; Obstacles avoidance; Orientation & mobility; Sonifications; Turn-by-turn navigation; Visual impairment; Visually impaired people; White cane; Iterative methods","","","Association for Computing Machinery","19367228","","","","English","ACM Trans. Accessible Comput.","Article","Final","","Scopus","2-s2.0-85118631013"
"Kumar N.; Jain A.","Kumar, Nitin (57214110221); Jain, Anuj (57197766121)","57214110221; 57197766121","Smart Navigation Detection using Deep-learning for Visually Impaired Person","2021","2021 IEEE 2nd International Conference on Electrical Power and Energy Systems, ICEPES 2021","","","","","","","6","10.1109/ICEPES52894.2021.9699479","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85126686887&doi=10.1109%2fICEPES52894.2021.9699479&partnerID=40&md5=f2323a05888fec0207392d04892e8fde","Lovely Professional University, Punjab, India","Kumar N., Lovely Professional University, Punjab, India; Jain A., Lovely Professional University, Punjab, India","In order to minimize the problem of unknown paths movements for a VI person, we propose an assistive navigational system to help VI person to navigate in an known or unknown environment. A modified approach is developed in which a visual impaired person or blind person is assisted through a live feed. The model comprises of YOLO(You Only Look Once) based algorithm and a stick to detect objects through IOU (Intersection of Union). The collected pictures of path are annotated and then the model is developed by training of 300 images each of around 25 classes using Yolo v3 network. The path detection is executed through a Novel tracker system that utilizes an offline trained neural network. The accuracy of the proposed model with wearable mask founds to be 81% while with the stick it is found to be 96.14 %.  © 2021 IEEE.","ETA(Electronic Travelling Aid); MAP (mean average precision); Rehabilitation; SSD; Visually impaired; YOLO","Object detection; Assistive; Electronic traveling aid; Known environments; Mean average precision; Navigational systems; SSD; Traveling aids; Visually impaired; Visually impaired persons; You only look once; Deep learning","","","Institute of Electrical and Electronics Engineers Inc.","","978-166540236-1","","","English","IEEE Int. Conf. Electr. Power Energy Syst., ICEPES","Conference paper","Final","","Scopus","2-s2.0-85126686887"
"Zhang J.; Shi X.; Jin X.; Li F.; Chen X.","Zhang, Jianyi (56424692700); Shi, Xinyu (57219311580); Jin, Xinqin (55575707200); Li, Fengfeng (57219315675); Chen, Xin (57219316378)","56424692700; 57219311580; 55575707200; 57219315675; 57219316378","Design and research on guide blind device based on user experience","2020","Lecture Notes in Electrical Engineering","645","","","1029","1036","7","1","10.1007/978-981-15-6978-4_118","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85092207827&doi=10.1007%2f978-981-15-6978-4_118&partnerID=40&md5=aceb378aaf6369dc3b4911665844765b","School of Mechanical and Power Engineering, Harbin University of Science and Technology, Harbin, 150080, China","Zhang J., School of Mechanical and Power Engineering, Harbin University of Science and Technology, Harbin, 150080, China; Shi X., School of Mechanical and Power Engineering, Harbin University of Science and Technology, Harbin, 150080, China; Jin X., School of Mechanical and Power Engineering, Harbin University of Science and Technology, Harbin, 150080, China; Li F., School of Mechanical and Power Engineering, Harbin University of Science and Technology, Harbin, 150080, China; Chen X., School of Mechanical and Power Engineering, Harbin University of Science and Technology, Harbin, 150080, China","Facing the problem that the existing guide blind devices cannot fully meet the needs of visually impaired users, taking the guide blind device as study object, basing on user experience theory, studies the needs of visually impaired users and the future design direction of guide blind devices. Conduct user interviews and external observation experiments for visually impaired users, build user emotional experience maps, understand the road factors that affect visually impaired travel and the true needs of visually impaired users for guide blind devices. Transform user needs into practical and feasible function implementation directions, and explore suitable functions and interaction methods of guiding blind devices. The research results show that the future guide equipment should meet the needs of users at multiple levels and achieve simple operation, lightweight volume, and interactive humanization. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd 2020.","Guide Blind device; User experience; Visually impaired people","Man machine systems; Systems engineering; Emotional experiences; Future designs; Interaction methods; Multiple levels; Research results; Simple operation; Visually impaired; Visually-impaired users; User experience","J. Zhang; School of Mechanical and Power Engineering, Harbin University of Science and Technology, Harbin, 150080, China; email: jianyi990925@126.com","Long S.; Dhillon B.S.","Springer Science and Business Media Deutschland GmbH","18761100","978-981156977-7","","","English","Lect. Notes Electr. Eng.","Conference paper","Final","","Scopus","2-s2.0-85092207827"
"Pardeshi S.R.; Pawar V.J.; Kharat K.D.; Chavan S.","Pardeshi, Suraj R. (57194030474); Pawar, Vikul J. (57194031456); Kharat, Kailas D. (57194028917); Chavan, Sachin (57304877500)","57194030474; 57194031456; 57194028917; 57304877500","Assistive Technologies for Visually Impaired Persons Using Image Processing Techniques – A Survey","2021","Communications in Computer and Information Science","1380","","","95","110","15","2","10.1007/978-981-16-0507-9_9","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85103283446&doi=10.1007%2f978-981-16-0507-9_9&partnerID=40&md5=a86a74c9e5b3f9b1b8a3b0e515066d8c","Department of M.C.A, Govt. Engg. College, Aurangabad, Maharashtra, India; Department of Computer Science and Engineering, University College of Engineering, Osmania University, Hyderabad, Telangana, India; Department of CSE, Govt. Engg. College, Aurangabad, Maharashtra, India; Department of CS and IT, Dr. B.A.M. U., Aurangabad, Maharashtra, India","Pardeshi S.R., Department of M.C.A, Govt. Engg. College, Aurangabad, Maharashtra, India; Pawar V.J., Department of Computer Science and Engineering, University College of Engineering, Osmania University, Hyderabad, Telangana, India; Kharat K.D., Department of CSE, Govt. Engg. College, Aurangabad, Maharashtra, India; Chavan S., Department of CS and IT, Dr. B.A.M. U., Aurangabad, Maharashtra, India","Acquiring information of objects or obstacles ahead and accordingly deciding navigation direction is one of the major problems that visually impaired persons face in their day to day life. However there had been a lot of efforts taken for research towards helping the blinds, yet there is no any concrete, globally accepted and used solution available till date. Vision assistive devices are transferable electronic equipment that can also be hold in hands or easily wear by the users to help them sense the obstacles ahead. This paper tries to identify the currently available assistive technologies for visually impaired persons which use advanced image processing techniques. There is an increasing interest in using those techniques for the use of development of cost effective, easily wearable, hassle free, comfortable to operate and user approachable assistive technique for visually damaged population all over the world. These techniques if combined with some other technical components such as sensors, ultrasonic pulse-echo, infrared light transceivers, GPS systems, acoustic feedback techniques, vibration pads etc. can bring forth a very feature enriched assistive system that can overcome many of the lacunas of previously proposed solutions. © 2021, Springer Nature Singapore Pte Ltd.","Acoustic feedback; Assistive devices; GPS systems; Image processing; Infrared light transceivers; Obstacle detection; Sensors; Ultrasonic pulse-echo; Vibration pads; Visually impaired","Cost effectiveness; Pattern recognition; Radio transceivers; Ultrasonic applications; Acoustic feedback; Assistive devices; Assistive system; Assistive technology; Cost effective; Image processing technique; Ultrasonic pulse; Visually impaired persons; Image processing","S.R. Pardeshi; Department of M.C.A, Govt. Engg. College, Aurangabad, Maharashtra, India; email: surajrp@geca.ac.in","Santosh K.C.; Gawali B.","Springer Science and Business Media Deutschland GmbH","18650929","978-981160506-2","","","English","Commun. Comput. Info. Sci.","Conference paper","Final","","Scopus","2-s2.0-85103283446"
"Afif M.; Ayachi R.; Said Y.; Atri M.","Afif, Mouna (57194068439); Ayachi, Riadh (57210106980); Said, Yahia (53867137900); Atri, Mohamed (23017853700)","57194068439; 57210106980; 53867137900; 23017853700","Deep learning-based application for indoor wayfinding assistance navigation","2021","Multimedia Tools and Applications","80","18","","27115","27130","15","6","10.1007/s11042-021-10999-6","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85105851877&doi=10.1007%2fs11042-021-10999-6&partnerID=40&md5=0e26cfc747d1f0ce8f02e7ddf53cc3eb","Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia; Electrical Engineering Department, College of Engineering, Northern Border University, Arar, Saudi Arabia; College of Computer Science, King Khalid University, Abha, Saudi Arabia","Afif M., Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia; Ayachi R., Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia; Said Y., Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia, Electrical Engineering Department, College of Engineering, Northern Border University, Arar, Saudi Arabia; Atri M., Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia, College of Computer Science, King Khalid University, Abha, Saudi Arabia","There is an increasing need to develop new adaptive technologies and new wayfinding assistance systems for blind and visually impaired persons in order to improve their daily lives. To address this need, we propose in this paper to develop a new deep learning-based indoor wayfinding assistance system consisting of detecting landmark indoor signs. Assistive technologies used for blind and sighted persons used to support daily activities to improve social inclusion are developing very fast. Training and testing experiments were performed on the proposed indoor signage dataset. Through the experiments conducted, we demonstrated the efficiency of the proposed indoor wayfinding aid system. We obtained 93.45% as a mean average precision (mAP) of the proposed indoor wayfinding and signage detection system. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Blind and visually impaired persons; Convolution neural network (CNN); Deep learning; Indoor sign detection; Indoor wayfinding","Indoor positioning systems; Statistical tests; Adaptive technology; Assistive technology; Blind and visually impaired; Daily activity; Detection system; Social inclusion; Training and testing; Wayfinding assistance; Deep learning","M. Afif; Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia; email: mouna.afif@outlook.fr","","Springer","13807501","","MTAPF","","English","Multimedia Tools Appl","Article","Final","","Scopus","2-s2.0-85105851877"
"Liu K.; Huang R.","Liu, Ke (57315830400); Huang, Ran (56540445200)","57315830400; 56540445200","Semantic Segmentation and Topological Mapping of Floor Plans","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13015 LNAI","","","378","389","11","0","10.1007/978-3-030-89134-3_35","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85118157681&doi=10.1007%2f978-3-030-89134-3_35&partnerID=40&md5=dfdb351eb5572a0b9e8845dd4767f09a","College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, 100029, China","Liu K., College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, 100029, China; Huang R., College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, 100029, China","When visually impaired people walk in an unknown indoor environment, it is crucial to build a topological semantic map from the captured floor plan for navigation purposes. This paper proposes a topological mapping method from the floor plan model based on deep learning semantic segmentation. The topological semantic map can be used for assistive blind navigation purposes in unknown indoor environments. A deep learning network is developed for semantic segmentation, and disturbances such as image rotation, color transformation and Gaussian noises are taken into consideration in the training to enhance the robustness. With the semantic segmentation result as input, a topological semantic mapping algorithm is then proposed based on the graph theory. Experiments are presented to demonstrate the effectiveness of the proposed mapping method. © 2021, Springer Nature Switzerland AG.","Floor plan; Semantic segmentation; Topological map","Conformal mapping; Deep learning; Floors; Gaussian noise (electronic); Graph theory; Image enhancement; Image segmentation; Indoor positioning systems; Floorplans; Indoor environment; Mapping method; Model-based OPC; Semantic map; Semantic segmentation; Topological map; Topological mapping; Topological semantics; Visually impaired people; Semantic Segmentation","R. Huang; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, 100029, China; email: huangran@mail.buct.edu.cn","Liu X.; Nie Z.; Yu J.; Xie F.; Song R.","Springer Science and Business Media Deutschland GmbH","03029743","978-303089133-6","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","","Scopus","2-s2.0-85118157681"
"Diáz-Toro A.A.; Campanã-Bastidas S.E.; Caicedo-Bravo E.F.","Diáz-Toro, Andrés A. (57197198778); Campanã-Bastidas, Sixto E. (57192165933); Caicedo-Bravo, Eduardo F. (22949835800)","57197198778; 57192165933; 22949835800","Vision-Based System for Assisting Blind People to Wander Unknown Environments in a Safe Way","2021","Journal of Sensors","2021","","6685686","","","","16","10.1155/2021/6685686","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85102767183&doi=10.1155%2f2021%2f6685686&partnerID=40&md5=7de91d2b782969217b4b320b7cbc8b10","School of Basic Sciences, Technology and Engineering Ecbti, Universidad Nacional Abierta y A Distancia Unad, Pasto, 520001, Colombia; School of Electrical and Electronic Engineering Eiee, Universidad Del Valle, Cali, 76001, Colombia","Diáz-Toro A.A., School of Basic Sciences, Technology and Engineering Ecbti, Universidad Nacional Abierta y A Distancia Unad, Pasto, 520001, Colombia; Campanã-Bastidas S.E., School of Basic Sciences, Technology and Engineering Ecbti, Universidad Nacional Abierta y A Distancia Unad, Pasto, 520001, Colombia; Caicedo-Bravo E.F., School of Electrical and Electronic Engineering Eiee, Universidad Del Valle, Cali, 76001, Colombia","Vision is the principal source of information of the surrounding world. It facilitates our movement and development of everyday activities. In this sense, blind people have great difficulty for moving, especially in unknown environments, which reduces their autonomy and puts them at risk of suffering an accident. Electronic Travel AIDS (ETAs) have emerged and provided outstanding navigation assistance for blind people. In this work, we present the methodology followed for implementing a stereo vision-based system that assists blind people to wander unknown environments in a safe way, by sensing the world, segmenting the floor in 3D, fusing local 2D grids considering the camera tracking, creating a global occupancy 2D grid, reacting to close obstacles, and generating vibration patterns with an haptic belt. For segmenting the floor in 3D, we evaluate normal vectors and orientation of the camera obtained from depth and inertial data, respectively. Next, we apply RANSAC for computing efficiently the equation of the supporting plane (floor). The local grids are fused, obtaining a global map with data of free and occupied areas along the whole trajectory. For parallel processing of dense data, we leverage the capacity of the Jetson TX2, achieving high performance, low power consumption, and portability. Finally, we present experimental results obtained with ten (10) participants, in different conditions, with obstacles of different height, hanging obstacles, and dynamic obstacles. These results show high performance and acceptance by the participants, highlighting the easiness to follow instructions and the short period of training. © 2021 Andrés A. Diáz-Toro et al.","","Cameras; Data handling; Floors; Stereo image processing; Different heights; Dynamic obstacles; Electronic travel aidss; Low-power consumption; Parallel processing; Stereo vision-based systems; Vibration pattern; Vision based system; Stereo vision","A.A. Diáz-Toro; School of Basic Sciences, Technology and Engineering Ecbti, Universidad Nacional Abierta y A Distancia Unad, Pasto, 520001, Colombia; email: andres.diaz@unad.edu.co","","Hindawi Limited","1687725X","","","","English","J. Sensors","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85102767183"
"Patange N.; Dhutre V.; Wani V.; Oak S.","Patange, Nachiket (57258039400); Dhutre, Vaishnavi (57258253100); Wani, Vaishnavi (57258367300); Oak, Sujata (57211156253)","57258039400; 57258253100; 57258367300; 57211156253","Comprehensive Analysis of Indian Currency Recognition System and Location Tracking for Visually Impaired","2021","2021 International Conference on Intelligent Technologies, CONIT 2021","","","","","","","1","10.1109/CONIT51480.2021.9498430","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85114889710&doi=10.1109%2fCONIT51480.2021.9498430&partnerID=40&md5=66334f452b431d6ddffe42c04a16f0cc","Ramrao Adik Institute of Technology, Department of Information Technology, Navi Mumbai, India","Patange N., Ramrao Adik Institute of Technology, Department of Information Technology, Navi Mumbai, India; Dhutre V., Ramrao Adik Institute of Technology, Department of Information Technology, Navi Mumbai, India; Wani V., Ramrao Adik Institute of Technology, Department of Information Technology, Navi Mumbai, India; Oak S., Ramrao Adik Institute of Technology, Department of Information Technology, Navi Mumbai, India","According to a survey, approximately 5.4 million people face the problem of visual impairment in the country. Blind people face problems when moving from one place to another many of them use a white cane or stick or a working dog for assistance. When they travel from one place to another, their loved ones are worried about their location. Caretakers or their loved ones find it hard to trace the real-time location of the blind person. Moreover, blind folks face difficulties in contacting their loved ones in case of emergency. They even find it hard to recognize the currencies. Despite the advantages of technology and the enormous usage of electronic cards and e-payments, banknotes are still primarily used owing to its ease. There are about 11 different Indian currencies, and each banknote is distinct from others in various aspects like colour and dimensions. Blind folks find it hard to use the cards and other electronic payments, so they find it hard in recognizing the currencies. Currency Recognition System (CRS) could be useful in identifying currency for visually impaired persons. In the proposed system, the Android-based solution to aid the loved ones to track the real-time location of the blind folks with a smart calling facility and a Currency Recognition System based on Teachable Machine with Google design to recognize various currencies. In Teachable Machine, models are trained using the technique called transfer learning. It aims to provide fast and accurate results for the given image. The goal of this proposed scheme is to make life easier for a visually impaired person. © 2021 IEEE.","Android Application; Currency Recognition System (CRS); Real-time Location; TensorFlow Lite","Surveys; Tracking (position); Transfer learning; Comprehensive analysis; Currency recognition; Electronic cards; Electronic payment; Real-time location; Visual impairment; Visually impaired; Visually impaired persons; Location","","","Institute of Electrical and Electronics Engineers Inc.","","978-172818583-5","","","English","Int. Conf. Intell. Technol., CONIT","Conference paper","Final","","Scopus","2-s2.0-85114889710"
"Saha S.; Shakal F.H.; Mahmood M.","Saha, Sagor (57218939007); Shakal, Farhan Hossain (57218934456); Mahmood, Mufrath (57219987213)","57218939007; 57218934456; 57219987213","Visual, navigation and communication aid for visually impaired person","2021","International Journal of Electrical and Computer Engineering","11","2","","1276","1283","7","2","10.11591/ijece.v11i2.pp1276-1283","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85097842498&doi=10.11591%2fijece.v11i2.pp1276-1283&partnerID=40&md5=aab2a8df98de21996841c138a3c33614","Department of Electrical and Electronics Engineering, American International University, Bangladesh","Saha S., Department of Electrical and Electronics Engineering, American International University, Bangladesh; Shakal F.H., Department of Electrical and Electronics Engineering, American International University, Bangladesh; Mahmood M., Department of Electrical and Electronics Engineering, American International University, Bangladesh","The loss of vision restrained the visually impaired people from performing their daily task. This issue has impeded their free-movement and turned them into dependent a person. People in this sector did not face technologies revamping their situations. With the advent of computer vision, artificial intelligence, the situation improved to a great extent. The propounded design is an implementation of a wearable device which is capable of performing a lot of features. It is employed to provide visual instinct by recognizing objects, identifying the face of choices. The device runs a pre-trained model to classify common objects from household items to automobiles items. Optical character recognition and Google translate were executed to read any text from image and convert speech of the user to text respectively. Besides, the user can search for an interesting topic by the command in the form of speech. Additionally, ultrasonic sensors were kept fixed at three positions to sense the obstacle during navigation. The display attached help in communication with deaf person and GPS and GSM module aid in tracing the user. All these features run by voice commands which are passed through the microphone of any earphone. The visual input is received through the camera and the computation task is processed in the raspberry pi board. However, the device seemed to be effective during the test and validation. © 2021 Institute of Advanced Engineering and Science. All rights reserved.","Blind-deaf communication; Face identification; Human-computer interaction; Object recognition; Obstacle avoidance","","S. Saha; Department of Electrical and Electronics Engineering, American International Univerity Bangladesh, Dhaka, 408/1 Kuratoli Khilkhet, 1229, Bangladesh; email: sagarsaha455@gmail.com","","Institute of Advanced Engineering and Science","20888708","","","","English","Int. J. Electr. Comput. Eng.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85097842498"
"Nagenthiran N.; Priyanha P.; Nirosha S.; Vivek J.; De Silva H.; Sriyaratna D.","Nagenthiran, Nagaraj (57461750200); Priyanha, Paramananthan (57461321200); Nirosha, Segar (57218937837); Vivek, Jeevanaraj (57461184700); De Silva, Hansi (57204044971); Sriyaratna, Disni (57205440596)","57461750200; 57461321200; 57218937837; 57461184700; 57204044971; 57205440596","Machine Learning-Based Smart Shopping for Visually Impaired","2021","ICAC 2021 - 3rd International Conference on Advancements in Computing, Proceedings","","","","473","478","5","0","10.1109/ICAC54203.2021.9671170","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85125018675&doi=10.1109%2fICAC54203.2021.9671170&partnerID=40&md5=69805dcd196d8c7b3dadfaafee07f989","Sri Lankan Institute of Information Technology, Department of Information Technology, Malabe, Sri Lanka","Nagenthiran N., Sri Lankan Institute of Information Technology, Department of Information Technology, Malabe, Sri Lanka; Priyanha P., Sri Lankan Institute of Information Technology, Department of Information Technology, Malabe, Sri Lanka; Nirosha S., Sri Lankan Institute of Information Technology, Department of Information Technology, Malabe, Sri Lanka; Vivek J., Sri Lankan Institute of Information Technology, Department of Information Technology, Malabe, Sri Lanka; De Silva H., Sri Lankan Institute of Information Technology, Department of Information Technology, Malabe, Sri Lanka; Sriyaratna D., Sri Lankan Institute of Information Technology, Department of Information Technology, Malabe, Sri Lanka","There are diverse applications built for the e-commerce platform, each with its own set of advantages. All goods should be relevant to all members of society around the world, but there are people with unique needs who should be considered when technology advancements are made for the common good. People who are blind require particular attention since they require assistance from others. For this research, we referred to several studies with comparable goals and approaches, which our group closely examined to enhance our study and outcomes, and which were highlighted when relevant work was discussed. The purpose of this study is to provide accessibility for all members of society, including the blind, as well as location-based solutions for consumers. These include voice navigation through the app, product suggestions, offering quick paths to the shop location by comparing existing algorithms for identifying short paths and providing a function to give voice feedback. This paper thoroughly examined the findings and provided appropriate evidence to support the answer to the challenge mentioned above. This sort of research will have a beneficial influence on our IT companies' consideration for those who live with specific needs that technology may help them meet.  © 2021 IEEE.","accessibility; analysis; e-commerce; location-based; technology; voice navigation","Location; Machine learning; Analyse; Application builds; Commerce platforms; Diverse applications; E- commerces; Location based; Machine-learning; Technology; Visually impaired; Voice navigation; Electronic commerce","","","Institute of Electrical and Electronics Engineers Inc.","","978-166540862-2","","","English","ICAC - Int. Conf. Adv. Comput., Proc.","Conference paper","Final","","Scopus","2-s2.0-85125018675"
"Hu X.; Song A.; Wei Z.; Zeng H.","Hu, Xuhui (57206667503); Song, Aiguo (7102693451); Wei, Zhikai (57750624400); Zeng, Hong (7401472202)","57206667503; 7102693451; 57750624400; 7401472202","StereoPilot: A Wearable Target Location System for Blind and Visually Impaired Using Spatial Audio Rendering","2022","IEEE Transactions on Neural Systems and Rehabilitation Engineering","30","","","1621","1630","9","14","10.1109/TNSRE.2022.3182661","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85132279802&doi=10.1109%2fTNSRE.2022.3182661&partnerID=40&md5=70ba64442bca70fdfc41131568ec95a8","Southeast University, State Key Laboratory of Bioelectronics, the Jiangsu Key Laboratory of Remote Measurement and Control, School of Instrument Science and Engineering, Jiangsu, Nanjing, 210096, China","Hu X., Southeast University, State Key Laboratory of Bioelectronics, the Jiangsu Key Laboratory of Remote Measurement and Control, School of Instrument Science and Engineering, Jiangsu, Nanjing, 210096, China; Song A., Southeast University, State Key Laboratory of Bioelectronics, the Jiangsu Key Laboratory of Remote Measurement and Control, School of Instrument Science and Engineering, Jiangsu, Nanjing, 210096, China; Wei Z., Southeast University, State Key Laboratory of Bioelectronics, the Jiangsu Key Laboratory of Remote Measurement and Control, School of Instrument Science and Engineering, Jiangsu, Nanjing, 210096, China; Zeng H., Southeast University, State Key Laboratory of Bioelectronics, the Jiangsu Key Laboratory of Remote Measurement and Control, School of Instrument Science and Engineering, Jiangsu, Nanjing, 210096, China","Vision loss severely impacts object recognition and spatial cognition for limited vision individuals. It is a challenge to compensate for this using other sensory modalities, such as touch or hearing. This paper introduces StereoPilot, a wearable target location system to facilitate the spatial cognition of BVI. Through wearing a head-mounted RGB-D camera, the 3D spatial information of the environment is measured and processed into navigation cues. Leveraging spatial audio rendering (SAR) technology, it allows the navigation cues to be transmitted in a type of 3D sound from which the sound orientation can be distinguished by the sound localization instincts in humans. Three haptic and auditory display strategies were compared with SAR through experiments with three BVI and four sighted subjects. Compared with mainstream speech instructional feedback, the experimental results of the Fitts' law test showed that SAR increases the information transfer rate (ITR) by a factor of three for spatial navigation, while the positioning error is reduced by 40%. Furthermore, SAR has a lower learning effect than other sonification approaches such as vOICe. In desktop manipulation experiments, StereoPilot was able to obtain precise localization of desktop objects while reducing the completion time of target grasping tasks in half as compared to the voice instruction method. In summary, StereoPilot provides an innovative wearable target location solution that swiftly and intuitively transmits environmental information to BVI individuals in the real world. copy 2001-2011 IEEE. © 2022 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.","Blind assistance; environment perception; sensory feedback; spatial audio","Blindness; Humans; Vision, Ocular; Visual Perception; Visually Impaired Persons; Wearable Electronic Devices; Audition; Job analysis; Location; Navigation; Object recognition; Rendering (computer graphics); Sensory analysis; Stereo image processing; Three dimensional computer graphics; Three dimensional displays; Audio rendering; Blind and visually impaired; Blind assistance; Environment perceptions; Objects recognition; Rendering (computer graphic); Spatial audio; Task analysis; Three-dimensional display; adult; Article; auditory feedback; blindness; congenital blindness; controlled study; depth perception; female; hearing; human; human experiment; image analysis; male; pilot study; sensory feedback; spatial audio rendering; touch; ultrasound; visual impairment; blindness; electronic device; vision; visually impaired person; Cameras","A. Song; Southeast University, Nanjing, State Key Laboratory of Bioelectronics, the Jiangsu Key Laboratory of Remote Measurement and Control, School of Instrument Science and Engineering, Jiangsu, 210096, China; email: a.g.song@seu.edu.cn","","Institute of Electrical and Electronics Engineers Inc.","15344320","","ITNSB","35696467","English","IEEE Trans. Neural Syst. Rehabil. Eng.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85132279802"
"Eguiluz A.G.; Rodriguez-Gomez J.P.; Tapia R.; Maldonado F.J.; Acosta J.A.; Martinez-De Dios J.R.; Ollero A.","Eguiluz, A. Gomez (57220148225); Rodriguez-Gomez, J.P. (57209746441); Tapia, R. (57221553674); Maldonado, F.J. (57222349637); Acosta, J.A. (7102205615); Martinez-De Dios, J.R. (8677983300); Ollero, A. (7004325905)","57220148225; 57209746441; 57221553674; 57222349637; 7102205615; 8677983300; 7004325905","Why fly blind Event-based visual guidance for ornithopter robot flight","2021","IEEE International Conference on Intelligent Robots and Systems","","","","1958","1965","7","9","10.1109/IROS51168.2021.9636315","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85124353447&doi=10.1109%2fIROS51168.2021.9636315&partnerID=40&md5=30a8726326f1aa15b03cbaae40628777","Universidad de Sevilla, GRVC Robotics Lab Sevilla, Seville, 41092, Spain","Eguiluz A.G., Universidad de Sevilla, GRVC Robotics Lab Sevilla, Seville, 41092, Spain; Rodriguez-Gomez J.P., Universidad de Sevilla, GRVC Robotics Lab Sevilla, Seville, 41092, Spain; Tapia R., Universidad de Sevilla, GRVC Robotics Lab Sevilla, Seville, 41092, Spain; Maldonado F.J., Universidad de Sevilla, GRVC Robotics Lab Sevilla, Seville, 41092, Spain; Acosta J.A., Universidad de Sevilla, GRVC Robotics Lab Sevilla, Seville, 41092, Spain; Martinez-De Dios J.R., Universidad de Sevilla, GRVC Robotics Lab Sevilla, Seville, 41092, Spain; Ollero A., Universidad de Sevilla, GRVC Robotics Lab Sevilla, Seville, 41092, Spain","The development of perception and control methods that allow bird-scale flapping-wing robots (a.k.a. ornithopters) to perform autonomously is an under-researched area. This paper presents a fully onboard event-based method for ornithopter robot visual guidance. The method uses event cameras to exploit their fast response and robustness against motion blur in order to feed the ornithopter control loop at high rates (100 Hz). The proposed scheme visually guides the robot using line features extracted in the event image plane and controls the flight by actuating over the horizontal and vertical tail deflections. It has been validated on board a real ornithopter robot with real-time computation in low-cost hardware. The experimental evaluation includes sets of experiments with different maneuvers indoors and outdoors.  © 2021 IEEE.","computer vision; event camera; flapping-wing; line features; ornithopter; tracking; visual servoing","Air navigation; Aircraft control; Aircraft detection; Cameras; Computer vision; Research aircraft; Robots; Wings; Control methods; Event camera; Event-based; Event-based method; Fast response; Flapping-wing; Line features; Tracking; Visual guidance; Visual-servoing; Visual servoing","","","Institute of Electrical and Electronics Engineers Inc.","21530858","978-166541714-3","85RBA","","English","IEEE Int Conf Intell Rob Syst","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85124353447"
"Patil K.; Kharat A.; Chaudhary P.; Bidgar S.; Gavhane R.","Patil, Kanchan (57225409644); Kharat, Avinash (57223135123); Chaudhary, Pratik (57223127818); Bidgar, Shrikant (57223133678); Gavhane, Rushikesh (57223141987)","57225409644; 57223135123; 57223127818; 57223133678; 57223141987","Guidance System for Visually Impaired People","2021","Proceedings - International Conference on Artificial Intelligence and Smart Systems, ICAIS 2021","","","9395973","988","993","5","11","10.1109/ICAIS50930.2021.9395973","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85104999169&doi=10.1109%2fICAIS50930.2021.9395973&partnerID=40&md5=b9f6538ae86fd12ef553ffe5e27cfca0","SRES's Sanjivani College of Engineering, Department of Information Technology, Kopargaon, (MH), 423601, India","Patil K., SRES's Sanjivani College of Engineering, Department of Information Technology, Kopargaon, (MH), 423601, India; Kharat A., SRES's Sanjivani College of Engineering, Department of Information Technology, Kopargaon, (MH), 423601, India; Chaudhary P., SRES's Sanjivani College of Engineering, Department of Information Technology, Kopargaon, (MH), 423601, India; Bidgar S., SRES's Sanjivani College of Engineering, Department of Information Technology, Kopargaon, (MH), 423601, India; Gavhane R., SRES's Sanjivani College of Engineering, Department of Information Technology, Kopargaon, (MH), 423601, India","There are some visually impaired people throughout the world. Some of them may be around us. The visually impaired person finds difficulty while performing day-to-day life tasks. So this research work aims to develop a device which helps them as personal assistant. This paper represents the proposed device's integrated modules and functionalities that can help a blind person. The proposed idea is to provide a wearable device with a Virtual assistant system for the visually impaired person, for some of the basic tasks without requiring the help of others. The system is aimed to provide voice-over assistants for blind people to do tasks like understanding surroundings, looking for an object, recognizing the face of a person with emotion, and reading etc. There is a total of five components merged into one system in this project. The navigation through these components is possible through hardware buttons and voice-over commands given by the user. There are many deep learning methodologies and core libraries of python language used for programming. The complete project is dedicated to being simple to use by visually impaired people and making day to day tasks easy for them.  © 2021 IEEE.","AIML; Face Recognition; Facial Expression Recognition; gTTS; Image Captioning; JARVIS; Object Detection; Optical Character Recognition; Python 3.4.7; Pyttsx; Yolo v3","Artificial intelligence; Guidance system; Integrated module; Personal assistants; PYTHON language; Virtual assistants; Visually impaired people; Visually impaired persons; Wearable devices; Deep learning","","","Institute of Electrical and Electronics Engineers Inc.","","978-172819537-7","","","English","Proc. - Int. Conf. Artif. Intell. Smart Syst., ICAIS","Conference paper","Final","","Scopus","2-s2.0-85104999169"
"Achirei S.-D.; Opariuc I.-A.; Zvoristeanu O.; Caraiman S.; Manta V.-I.","Achirei, Stefan-Daniel (57221329680); Opariuc, Ioana-Ariana (57558009700); Zvoristeanu, Otilia (57204828288); Caraiman, Simona (36093924000); Manta, Vasile-Ion (6603231376)","57221329680; 57558009700; 57204828288; 36093924000; 6603231376","Pothole Detection for Visually Impaired Assistance","2021","Proceedings - 2021 IEEE 17th International Conference on Intelligent Computer Communication and Processing, ICCP 2021","","","","409","415","6","1","10.1109/ICCP53602.2021.9733610","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85127379314&doi=10.1109%2fICCP53602.2021.9733610&partnerID=40&md5=7ca5650e2d86da1e413a39e5fbe07ef5","Technical University of Iasi, Computer Engineering Department, Iasi, Romania","Achirei S.-D., Technical University of Iasi, Computer Engineering Department, Iasi, Romania; Opariuc I.-A., Technical University of Iasi, Computer Engineering Department, Iasi, Romania; Zvoristeanu O., Technical University of Iasi, Computer Engineering Department, Iasi, Romania; Caraiman S., Technical University of Iasi, Computer Engineering Department, Iasi, Romania; Manta V.-I., Technical University of Iasi, Computer Engineering Department, Iasi, Romania","The global number of visually impaired is growing fast due to aging world population. People suffering of severe visual impairments face many difficulties in their daily routine. Pavement holes are a major problem for their navigation and walking. The proposed algorithm detects holes in the sidewalks and roads using a Convolutional Neural Network. Starting from the previously published research, this paper proposes a practical solution for pothole detection used in the Navigation Module of the Sound of Vision Lite (SoV Lite) project. YoloV5s, Mobilenet V1 and Mobilenet V2 Lite were trained on Nvidia RTX using the obtained dataset, then deployed for testing on Nvidia Jetson NX. Due to the mobile platform constraints Mobilenet V1 was chosen to be integrated in SoV Lite. Because objects which are closer have higher detection confidence but also because the visually impaired person wearing the system is interested in the dangers close to him, in practice, we limit the detections of negative obstacles within a range of 8m. By creating a region of interest the run time is also enhanced. For training and experiments we used in-house acquired frames using a ZED 2 Stereo Camera as well as publicly available data and annotated it for the specific task of detecting potholes and drains in the pavement and streets. The obtained dataset was augmented and made publicly available. © 2021 IEEE.","blind; navigation assistant; path hole; pothole; visually impaired","Convolutional neural networks; Image segmentation; Navigation; Pavements; Statistical tests; Stereo image processing; Blind; Convolutional neural network; Daily routines; Mobile platform; Navigation assistant; Path hole; Practical solutions; Visual impairment; Visually impaired; World population; Landforms","","Nedevschi S.; Potolea R.; Slavescu R.R.","Institute of Electrical and Electronics Engineers Inc.","","978-166540976-6","","","English","Proc. - IEEE Int. Conf. Intell. Comput. Commun. Process., ICCP","Conference paper","Final","","Scopus","2-s2.0-85127379314"
"Lopatin S.; von Zabiensky F.; Kreutzer M.; Rinn K.; Bienhaus D.","Lopatin, Sergej (57356236800); von Zabiensky, Florian (57203134749); Kreutzer, Michael (57518502200); Rinn, Klaus (57355959600); Bienhaus, Diethelm (55533342600)","57356236800; 57203134749; 57518502200; 57355959600; 55533342600","An Electronic Guide Dog for the Blind Based on Artificial Neural Networks","2021","Communications in Computer and Information Science","1499 CCIS","","","23","30","7","0","10.1007/978-3-030-90179-0_4","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85120060157&doi=10.1007%2f978-3-030-90179-0_4&partnerID=40&md5=e2e9e7dc10b331a5d71ff76acabeaa67","Technische Hochschule Mittelhessen, Institute of Technology and Computer Science, University of Applied Sciences, Giessen, Germany","Lopatin S., Technische Hochschule Mittelhessen, Institute of Technology and Computer Science, University of Applied Sciences, Giessen, Germany; von Zabiensky F., Technische Hochschule Mittelhessen, Institute of Technology and Computer Science, University of Applied Sciences, Giessen, Germany; Kreutzer M., Technische Hochschule Mittelhessen, Institute of Technology and Computer Science, University of Applied Sciences, Giessen, Germany; Rinn K., Technische Hochschule Mittelhessen, Institute of Technology and Computer Science, University of Applied Sciences, Giessen, Germany; Bienhaus D., Technische Hochschule Mittelhessen, Institute of Technology and Computer Science, University of Applied Sciences, Giessen, Germany","This paper presents a feasibility study of an electronic assistance system to support blind and visually impaired people in finding their way in the area of public traffic. Optical recognition of walkways is implemented. For this purpose, a neural network for semantic segmentation is trained from scratch. In the practical test, an NVIDIA® Jetson NanoTM is used as the computing unit. A voice output gives the user feedback for orientation on the pavement. © 2021, Springer Nature Switzerland AG.","Blind sidewalk detection; Computer vision; Convolutional neural network; Electronic travel aid; Electronic travel aid technology; Portable eta system","Convolutional neural networks; Semantic Segmentation; Semantics; Blind and visually impaired; Blind sidewalk detection; Convolutional neural network; Electronic assistance system; Electronic travel aid technology; Electronic travel aidss; Feasibility studies; Guide dogs; Portable eta system; Computer vision","S. Lopatin; Technische Hochschule Mittelhessen, Institute of Technology and Computer Science, University of Applied Sciences, Giessen, Germany; email: sergej.lopatin@mni.thm.de","Stephanidis C.; Antona M.; Ntoa S.","Springer Science and Business Media Deutschland GmbH","18650929","978-303090178-3","","","English","Commun. Comput. Info. Sci.","Conference paper","Final","","Scopus","2-s2.0-85120060157"
"Kumar N.; Sharma S.; Abraham I.M.; Sathya Priya S.","Kumar, Naveen (58276484000); Sharma, Sanjeevani (57841441800); Abraham, Ilin Mariam (57842326100); Sathya Priya, S. (36139173200)","58276484000; 57841441800; 57842326100; 36139173200","Blind Assistance System Using Machine Learning","2022","Lecture Notes in Networks and Systems","514 LNNS","","","419","432","13","6","10.1007/978-3-031-12413-6_33","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85135792259&doi=10.1007%2f978-3-031-12413-6_33&partnerID=40&md5=efc7dc11b528f95ca1bda27be7927296","Department of Computer Science and Engineering, Hindustan Institute of Technology and Science, Chennai, India","Kumar N., Department of Computer Science and Engineering, Hindustan Institute of Technology and Science, Chennai, India; Sharma S., Department of Computer Science and Engineering, Hindustan Institute of Technology and Science, Chennai, India; Abraham I.M., Department of Computer Science and Engineering, Hindustan Institute of Technology and Science, Chennai, India; Sathya Priya S., Department of Computer Science and Engineering, Hindustan Institute of Technology and Science, Chennai, India","Blindness is one of the most frequent and debilitating of the various disabilities. There are million visually impaired people in the globe, according to the World Health Organization (WHO). The proposed system is designed to aid visually impaired persons with real-time obstacle detection, avoidance, indoors and out navigation, and actual position tracking. The gadget proposed is a camera-visual detection hybrid that performs well in low light as part of the recommended technique, this method is utilized to detect and avoid impediments, as well as to aid visually impaired persons in identifying the environment around them. A simple and effective method for people with visual impairments to identify things in their environment and convert them into speech for improved comprehension and navigation. Along with these, the depth estimation, which calculates the safe distance between the object and the person, allowing them to be more self-sufficient and less reliant on others. This were able to achieve this model with the help of TensorFlow and pre-trained models. The approach is suggest is dependable, inexpensive, practical, and practicable. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Depth estimation; Object detection; Single shot detection; TensorFlow","","S. Sathya Priya; Department of Computer Science and Engineering, Hindustan Institute of Technology and Science, Chennai, India; email: sathyap@hindustanuniv.ac.in","Chen J.I.-Z.; Tavares J.M.R.S.; Shi F.","Springer Science and Business Media Deutschland GmbH","23673370","978-303112412-9","","","English","Lect. Notes Networks Syst.","Conference paper","Final","","Scopus","2-s2.0-85135792259"
"Rao S.; Singh V.M.","Rao, Sneha (57222721225); Singh, Vishwa Mohan (57221605526)","57222721225; 57221605526","Computer vision and IoT based smart system for visually impaired people","2021","Proceedings of the Confluence 2021: 11th International Conference on Cloud Computing, Data Science and Engineering","","","9377120","552","556","4","23","10.1109/Confluence51648.2021.9377120","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85103854753&doi=10.1109%2fConfluence51648.2021.9377120&partnerID=40&md5=43c9a978d45d5ab735918d34f517213f","Computer Engineering and Technology, MIT World Peace University, Pune, India","Rao S., Computer Engineering and Technology, MIT World Peace University, Pune, India; Singh V.M., Computer Engineering and Technology, MIT World Peace University, Pune, India","For people who are visually impaired, navigation is a challenge they encounter on a daily basis. For the same, use of walking sticks have become a common practice. Although, there are a lot of limitation of just relying on a blind stick. A more suitable method will alert the user about the nature of the obstacle and should also be of assistance in guiding the user to their location. In this paper, we propose an architecture of an assistance system revolving around a shoe that employees IoT devices and sensors along with computer vision algorithms to provide functionalities like obstacle detection, avoidance and navigation. The method uses a smartphone based voice assistance and guides the user with appropriate haptic feedback calculated using various sensors and actuators. © 2021 IEEE","Computer Vision; IoT; Machine Learning; Navigation; Object Detection","Cloud computing; Computer vision; Data Science; Obstacle detectors; Smartphones; Assistance system; Computer vision algorithms; Haptic feedbacks; Obstacle detection; Sensors and actuators; Smart System; Visually impaired; Visually impaired people; Internet of things","S. Rao; Computer Engineering and Technology, MIT World Peace University, Pune, India; email: raosneha321@gmail.com; V.M. Singh; Computer Engineering and Technology, MIT World Peace University, Pune, India; email: vishwa20@outlook.com","","Institute of Electrical and Electronics Engineers Inc.","","978-073813160-3","","","English","Proc. Conflu.: Int. Conf. Cloud Comput., Data Sci. Eng.","Conference paper","Final","","Scopus","2-s2.0-85103854753"
"Wei Z.; Song A.; Hu X.","Wei, Zhikai (57750624400); Song, Aiguo (7102693451); Hu, Xuhui (57206667503)","57750624400; 7102693451; 57206667503","Object Localization Assistive System Based on CV and Vibrotactile Encoding","2022","Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS","2022-July","","","2882","2885","3","5","10.1109/EMBC48229.2022.9871382","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85133569838&doi=10.1109%2fEMBC48229.2022.9871382&partnerID=40&md5=9374e8ce1c26b8288120a15f86b99c14","State Key Laboratory of Bioelectronics and Jiangsu Key Laboratory of Remote Measurement and Control, School of Instrument Science & Engineering, Southeast University, Jiangsu, Nanjing, 210096, China","Wei Z., State Key Laboratory of Bioelectronics and Jiangsu Key Laboratory of Remote Measurement and Control, School of Instrument Science & Engineering, Southeast University, Jiangsu, Nanjing, 210096, China; Song A., State Key Laboratory of Bioelectronics and Jiangsu Key Laboratory of Remote Measurement and Control, School of Instrument Science & Engineering, Southeast University, Jiangsu, Nanjing, 210096, China; Hu X., State Key Laboratory of Bioelectronics and Jiangsu Key Laboratory of Remote Measurement and Control, School of Instrument Science & Engineering, Southeast University, Jiangsu, Nanjing, 210096, China","Intelligent assistive systems can navigate blind people, but most of them could only give non-intuitive cues or inefficient guidance. Based on computer vision and vibrotactile encoding, this paper presents an interactive system that provides blind people with intuitive spatial cognition. Different from the traditional auditory feedback strategy based on speech cues, this paper firstly introduces a vibration-encoded feedback method that leverages the haptic neural pathway and enables the users to interact with objects other than manipulating an assistance device. Based on this strategy, a wearable visual module based on an RGB-D camera is adopted for 3D spatial object localization, which contributes to accurate perception and quick object localization in the real environment. The experimental results on target blind individuals indicate that vibrotactile feedback reduces the task completion time by over 25% compared with the mainstream voice prompt feedback scheme. The proposed object localization system provides a more intuitive spatial navigation and comfortable wearability for blindness assistance. © 2022 IEEE.","","Blindness; Feedback; Feedback, Sensory; Humans; Vision, Ocular; Visually Impaired Persons; Encoding (symbols); Object recognition; Assistive system; Auditory feedback; Blind people; Encodings; Feedback strategies; Haptics; Interactive system; Object localization; Spatial cognition; Vibrotactile; blindness; feedback system; human; sensory feedback; vision; visually impaired person; Signal encoding","A. Song; State Key Laboratory of Bioelectronics and Jiangsu Key Laboratory of Remote Measurement and Control, School of Instrument Science & Engineering, Southeast University, Nanjing, Jiangsu, 210096, China; email: a.g.song@seu.edu.cn","","Institute of Electrical and Electronics Engineers Inc.","1557170X","978-172812782-8","","36086052","English","Proc. Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. EMBS","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85133569838"
"Shenjin C.; Chuanchun Z.","Shenjin, Chen (58505460700); Chuanchun, Zhan (57230075700)","58505460700; 57230075700","Design of barrier free travel system for the visually impaired","2021","2021 IEEE International Conference on Artificial Intelligence and Industrial Design, AIID 2021","","","9456567","482","487","5","0","10.1109/AIID51893.2021.9456567","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85113353677&doi=10.1109%2fAIID51893.2021.9456567&partnerID=40&md5=acb6f0a517c64e7194ea2350e926fdb0","School of Electrical and Computer Engineering, NanFang College Guangzhou, Guangzhou, China; Guangzhou HuaTu InfoTech Co. Ltd, Guangzhou, China","Shenjin C., School of Electrical and Computer Engineering, NanFang College Guangzhou, Guangzhou, China; Chuanchun Z., Guangzhou HuaTu InfoTech Co. Ltd, Guangzhou, China","The use of intelligent perception technology and mobile Internet technology allows visually impaired people to perceive their surroundings through smart phone, and integrates intelligent perception, precise positioning and navigation, virtual blind roads and cloud services to provide barrier-free travel services for visually impaired people. Through the deployment of the blind information infrastructure to provide the visually impaired with real-time bus travel and guidance services, the experimental results show that the method of intelligent sensing and processing technology for the visually impaired is innovative and effectively solves the problem. The problem of autonomous travel of the visually impaired has important practical significance to enhance the travel experience of the visually impaired in a typical travel place.  © 2021 IEEE.","cloud services; free travel; guide blind; Intelligence precision; positioning and navigation","Artificial intelligence; Product design; Smartphones; Information infrastructures; Intelligent perception; Intelligent sensing; Mobile Internet technology; Precise positioning; Processing technologies; Travel experiences; Visually impaired people; Advanced traveler information systems","","","Institute of Electrical and Electronics Engineers Inc.","","978-073811083-7","","","English","IEEE Int. Conf. Artif. Intell. Ind. Des., AIID","Conference paper","Final","","Scopus","2-s2.0-85113353677"
"Tyagi N.; Sharma D.; Singh J.; Sharma B.; Narang S.","Tyagi, Noopur (57461203300); Sharma, Deepika (57225937890); Singh, Jaiteg (26422494700); Sharma, Bhisham (56510992300); Narang, Sushil (57220188229)","57461203300; 57225937890; 26422494700; 56510992300; 57220188229","Assistive Navigation System for Visually Impaired and Blind People: A Review","2021","Proceedings - 2021 1st IEEE International Conference on Artificial Intelligence and Machine Vision, AIMV 2021","","","","","","","14","10.1109/AIMV53313.2021.9670951","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85125011408&doi=10.1109%2fAIMV53313.2021.9670951&partnerID=40&md5=a4ff3ac42efa81b8c646f4344a69039d","Chitkara University, Department of Computer Applications, Chitkara University Institute of Engineering and Technology, Punjab, India; Chitkara University, Chitkara University School of Engineering and Technology, Computer Science and Engineering Department, Himachal Pradesh, India","Tyagi N., Chitkara University, Department of Computer Applications, Chitkara University Institute of Engineering and Technology, Punjab, India; Sharma D., Chitkara University, Department of Computer Applications, Chitkara University Institute of Engineering and Technology, Punjab, India; Singh J., Chitkara University, Department of Computer Applications, Chitkara University Institute of Engineering and Technology, Punjab, India; Sharma B., Chitkara University, Chitkara University School of Engineering and Technology, Computer Science and Engineering Department, Himachal Pradesh, India; Narang S., Chitkara University, Chitkara University School of Engineering and Technology, Computer Science and Engineering Department, Himachal Pradesh, India","The emergence of modern technologies in healthcare systems like the Internet of Things, Wireless Sensor Network, Machine Learning, etc. has ameliorated the cognitive abilities of humans. The increased accessibility of healthcare data and the exponential growth of advanced analytics can be attributed to the innovative amalgamation of these technologies. These technologies have adaptive and self-correcting capabilities to enhance accuracy depending on the information. Assistive technology enables independence and attainment of quality of life for blind and visually impaired people. With the support of guided navigation tools, assistive technologies aid the people with the facility to move across inside as well as the outside environment. The major concern of a visually challenged and blind person is to live a life with quality and safety. This study contributes information about distinctive wearable and portable assistive tools and devices which are designed to provide support to visually impaired people. Also, it was revealed that traditional navigation devices lacked a few features that are crucial for independent navigation. To overcome those navigation deficiencies, IoT technology is exploited to provide better solutions. Global Positioning System (GPS) tracker can assist to discover several opportunities in numerous areas such as location detection, mapping, healthcare, security, etc. Navigation gadgets embedded with sensors have a huge variety of programs and benefits. The major objective of this comprehensive study is to showcase a clearer perspective about the wearable or embedded devices used by visually impaired or blind persons.  © 2021 IEEE.","Assistive Navigation; GPS; Internet of things (IoTs); Sensor; Visually Impaired","Cognitive systems; Global positioning system; Health care; Machine learning; Metals; Wireless sensor networks; Assistive navigations; Assistive technology; Blind people; Blind person; Healthcare systems; Internet of thing; Modern technologies; Sensor; Visually impaired; Visually impaired people; Internet of things","","Patel S.; Bharti S.K.; Gupta R.K.","Institute of Electrical and Electronics Engineers Inc.","","978-166544211-4","","","English","Proc. - IEEE Int. Conf. Artif. Intell. Mach. Vis., AIMV","Conference paper","Final","","Scopus","2-s2.0-85125011408"
"Sedighi P.; Norouzi M.H.; Delrobaei M.","Sedighi, Paniz (57222708284); Norouzi, Mohammad Hesam (57222714156); Delrobaei, Mehdi (24824009300)","57222708284; 57222714156; 24824009300","An RFID-Based Assistive Glove to Help the Visually Impaired","2021","IEEE Transactions on Instrumentation and Measurement","70","","9389748","","","","15","10.1109/TIM.2021.3069834","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85103772470&doi=10.1109%2fTIM.2021.3069834&partnerID=40&md5=9bb960ea1d1da4bd6f75e87625ac64e9","Center for Research and Technology (CREATECH), Faculty of Electrical Engineering, K. N. Toosi University of Technology, Tehran, 1631714191, Iran; Department of Electrical and Computer Engineering, Western University, London, N6A 3K7, ON, Canada","Sedighi P., Center for Research and Technology (CREATECH), Faculty of Electrical Engineering, K. N. Toosi University of Technology, Tehran, 1631714191, Iran; Norouzi M.H., Center for Research and Technology (CREATECH), Faculty of Electrical Engineering, K. N. Toosi University of Technology, Tehran, 1631714191, Iran; Delrobaei M., Center for Research and Technology (CREATECH), Faculty of Electrical Engineering, K. N. Toosi University of Technology, Tehran, 1631714191, Iran, Department of Electrical and Computer Engineering, Western University, London, N6A 3K7, ON, Canada","Recent studies have focused on facilitating perception and outdoor navigation for people who live with blindness or some form of vision loss. However, a significant portion of these studies is centered around treatment and vision rehabilitation, leaving some immediate needs, such as interaction with the surrounding objects or recognizing colors and fine patterns without tactile feedback. This study targets such needs and delivers a straightforward communication method with the environment using a wearable, unobtrusive device. We initially discuss the advantages and limitations of related works to draw out the best-fitting design concepts. Then, we introduce the potential for emerging technologies such as radio frequency identification. We present the design details and the experimental results of an assistive glove to allow people with vision disabilities to interact with the environment more efficiently. Based on the collected data from 17 blindfolded healthy participants, the implemented system's success rate in identifying objects was about 96.32%. Overall, 70% of the users found the device very satisfactory.  © 1963-2012 IEEE.","Assistive devices; human-machine interaction; low vision; wearable technologies","Electrical engineering; Instruments; Communication method; Design concept; Emerging technologies; Outdoor navigation; Related works; Tactile feedback; Vision rehabilitation; Visually impaired; Radio frequency identification (RFID)","M. Delrobaei; Center for Research and Technology (CREATECH), Faculty of Electrical Engineering, K. N. Toosi University of Technology, Tehran, 1631714191, Iran; email: delrobaei@kntu.ac.ir","","Institute of Electrical and Electronics Engineers Inc.","00189456","","IEIMA","","English","IEEE Trans. Instrum. Meas.","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85103772470"
"Dissanayake D.M.L.V.; Rajapaksha R.G.M.D.R.P.; Prabhashawara U.P.; Solanga S.A.D.S.P.; Jayakody J.A.D.C.A.","Dissanayake, D.M.L.V. (57465838900); Rajapaksha, R.G.M.D.R.P. (57465398200); Prabhashawara, U.P. (57465986900); Solanga, S.A.D.S.P. (57465839000); Jayakody, J. A. D. C. Anuradha (55821069900)","57465838900; 57465398200; 57465986900; 57465839000; 55821069900","Navigate-Me: Secure voice authenticated indoor navigation system for blind individuals","2021","21st International Conference on Advances in ICT for Emerging Regions, ICter 2021 - Proceedings","","","","219","224","5","2","10.1109/ICter53630.2021.9774790","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85130772264&doi=10.1109%2fICter53630.2021.9774790&partnerID=40&md5=c4bafbb17a21467a645ea4657d51c8bb","Institue of Information Technology, Faculty of Computing, Sri Lanka, Department of Computer Systems Engineering, Malabe, Sri Lanka; Institue of Information Technology, Faculty of Computing, Sri Lanka, Department of Information Technology, Malabe, Sri Lanka; Curtin University, Department of Electrical and Computer Engineering, Perth, WA, Australia","Dissanayake D.M.L.V., Institue of Information Technology, Faculty of Computing, Sri Lanka, Department of Computer Systems Engineering, Malabe, Sri Lanka; Rajapaksha R.G.M.D.R.P., Institue of Information Technology, Faculty of Computing, Sri Lanka, Department of Information Technology, Malabe, Sri Lanka; Prabhashawara U.P., Institue of Information Technology, Faculty of Computing, Sri Lanka, Department of Computer Systems Engineering, Malabe, Sri Lanka; Solanga S.A.D.S.P., Institue of Information Technology, Faculty of Computing, Sri Lanka, Department of Computer Systems Engineering, Malabe, Sri Lanka; Jayakody J.A.D.C.A., Curtin University, Department of Electrical and Computer Engineering, Perth, WA, Australia","The majority of blind people require assistance when navigating through unfamiliar places due to a lack of information about the building structure and encounterable obstacles. To address this aspect of the problem, this paper presents ""Navigate-Me""as an approach for indoor navigation with maximum accessibility, usability, and security, reducing the problems that the user might encounter while navigating through indoor environments. As the targeted audience of this paper is blind or visually impaired people, Navigate-Me utilizes voice-based inputs from the user. In addition, this paper includes Bluetooth beacon integration for localization, White Cane with sensors for obstacle detection, a machine learning model for voice authentication, and an algorithm protocol for a secure connection between server and application integration-driven architecture to assist the visually impaired in navigating known and unknown indoor environments. © 2021 IEEE.","indoor navigation; localization; vision impaired; voice authentication","Air navigation; Authentication; Cryptography; Machine learning; Navigation systems; Network architecture; Obstacle detectors; Blind individuals; Blind people; Building structure; Indoor environment; Indoor navigation; Indoor navigation system; Localisation; Usability and security; Vision impaired; Voice authentication; Indoor positioning systems","","","Institute of Electrical and Electronics Engineers Inc.","","978-166546683-7","","","English","Int. Conf. Adv. ICT Emerg. Reg., ICter - Proc.","Conference paper","Final","","Scopus","2-s2.0-85130772264"
"Wang K.; Chen C.-M.; Hossain M.S.; Muhammad G.; Kumar S.; Kumari S.","Wang, Ke (57222049054); Chen, Chien-Ming (35072405000); Hossain, M. Shamim (24066717900); Muhammad, Ghulam (56605566900); Kumar, Sachin (23474214900); Kumari, Saru (48361454000)","57222049054; 35072405000; 24066717900; 56605566900; 23474214900; 48361454000","Transfer reinforcement learning-based road object detection in next generation IoT domain","2021","Computer Networks","193","","108078","","","","33","10.1016/j.comnet.2021.108078","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85104603461&doi=10.1016%2fj.comnet.2021.108078&partnerID=40&md5=e13383124627632698010f42c9c84e40","Jinan University, Guangzhou, China; Shandong University of Science and Technology, Shandong, China; Research Chair of Pervasive and Mobile Computing, King Saud University, Riyadh, 11543, Saudi Arabia; Department of Software Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, 11543, Saudi Arabia; Department of Computer Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, 11543, Saudi Arabia; Department of Computer Science and Engineering, Ajay Kumar Garg Engineering College, Ghaziabad, 201009, India; Department of Mathematics, Ch. Charan Singh University, Meerut, India","Wang K., Jinan University, Guangzhou, China; Chen C.-M., Shandong University of Science and Technology, Shandong, China; Hossain M.S., Research Chair of Pervasive and Mobile Computing, King Saud University, Riyadh, 11543, Saudi Arabia, Department of Software Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, 11543, Saudi Arabia; Muhammad G., Department of Computer Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, 11543, Saudi Arabia; Kumar S., Department of Computer Science and Engineering, Ajay Kumar Garg Engineering College, Ghaziabad, 201009, India; Kumari S., Department of Mathematics, Ch. Charan Singh University, Meerut, India","The landscape of fifth generation (5G) and beyond 5G (B5G)-enabled Internet of Things(IoT) is expected to seamlessly and ubiquitously connect everything, which includes 5G, cloud computing, artificial intelligence and other cutting-edge technologies to realize truly intelligent applications in smart cities. In this paper, we present an important key technology for smart city, which is a road target recognition algorithm for smart city applications and designs a set of corresponding programs to assist automatic drivers, pedestrians and visually impaired people in road safety, or to manage city infrastructure. The system can connect robots in cars, wearable devices and body area network in pedestrians or blind people. A target recognition algorithm based on scene fusion is designed to recognize the specific target in the road environment, and transfer reinforcement learning method is used to improve the accuracy and real-time performance of target recognition. The system provides them with travel assistance, identify dangerous or useful objects for them through high-performance target recognition services. It can collect the road visual scene data by road cameras and transmit it to edge devices for training model. The model is collaborated trained in the edge devices and aggregated by the cloud. Based on the transfer reinforcement learning method, the vision-based road target recognition has been implemented, and the accurate and reliable target recognition can be realized. Many details of experiments verify the effectiveness of our technology. © 2021 Elsevier B.V.","5G; Deep learning; Internet of Things; Transfer reinforcement learning","Application programs; Deep learning; Internet of things; Motor transportation; Object detection; Pedestrian safety; Reinforcement learning; Roads and streets; Smart city; Wearable technology; Cloud-computing; Deep learning; Next generation Internet; Objects detection; Reinforcement learning method; Reinforcement learnings; Road targets; Target recognition; Target recognition algorithms; Transfer reinforcement learning; 5G mobile communication systems","M.S. Hossain; Research Chair of Pervasive and Mobile Computing, King Saud University, Riyadh, 11543, Saudi Arabia; email: mshossain@ksu.edu.sa","","Elsevier B.V.","13891286","","CNETD","","English","Comput. Networks","Article","Final","","Scopus","2-s2.0-85104603461"
"Du J.-Y.; Huang Z.-Q.; Chen D.-Y.; Lei T.-W.","Du, Jing-Yi (57264617300); Huang, Zhi-Qi (55388234400); Chen, Dong-Yi (55507126500); Lei, Tao-Wei (57265181300)","57264617300; 55388234400; 55507126500; 57265181300","Current situation of vibration tactile coding; [振动触觉编码的研究现状]","2021","Gongcheng Kexue Xuebao/Chinese Journal of Engineering","43","9","","1261","1268","7","1","10.13374/j.issn2095-9389.2021.01.12.007","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85115201625&doi=10.13374%2fj.issn2095-9389.2021.01.12.007&partnerID=40&md5=31f4b5d34e337a993c071bd48e5176de","School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, 610000, China","Du J.-Y., School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, 610000, China; Huang Z.-Q., School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, 610000, China; Chen D.-Y., School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, 610000, China; Lei T.-W., School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, 610000, China","In recent years, with the progress and development of science and technology, the research on artificial intelligence and wearable devices continues to develop, and researchers increasingly find ways to provide a kind of more comfortable and comprehensive user experience. Auditory and visual technologies have been fully developed and utilized, while as an emerging field, tactile sense is a research direction with great potential. As a type of unique human sensory channel, tactile sense has unique advantages. It can convey information about maximum joints in physiological structure of human body, such as hardness, texture, shape, size, and temperature, which cannot be transmitted by visual and auditory senses. Additionally, the tactile sense is fast and accurate, thus, it performs well in some special situations, such as supergravity scenarios, high-speed rotating scenarios, or very noisy environments. The design of vibrational tactile coding is an important way to develop tactile devices and achieve a better human computer interaction. Haptic coding has several defects. It is used in narrow application scenes and conveys unclear meaning. Compared with the mature development of vision and hearing, it is necessary to further design the vibration haptic coding patterns to overcome these defects. Research on information transmission of tactile sense is very meaningful, for example, it offers convenience for special groups such as people with visual impairment or workers engaged in their education. It provides a navigation service for visually impaired people by changing the vibration frequency and vibration intensity of a blind vest. When hearing or vision is impaired, the tactile sense is a considerate way to provide timely and accurate information assistance to the special groups. Besides, tactile sense can help express the flight attitude information in virtue of combined vibration tactile coding. However, these studies aimed at providing a set of specific coding for a specific scenario in which relatively vague pieces of information were conveyed, such as emotion and direction Based on these studies, designing a set of universal coding patterns for most usage scenarios to deliver exact and accessible information is essential. This paper discussed the resolution of tactile vibration based on the mechanism of tactile vibration perception. According to the application of directional navigation and text interaction, the vibration information coding was summarized and the experimental methods and conclusions of vibration information coding were introduced. Moreover, the prospect of the vibration information coding field was proposed. © 2021, Science Press. All right reserved.","Human perception property; Human-computer interaction; Tactile coding; Tactile language; Vibration tactile","Artificial intelligence; Audition; Defects; Textures; User experience; Wearable technology; Accessible information; Development of science and technologies; Experimental methods; Information transmission; Physiological structures; Vibration frequency; Vibration intensity; Visually impaired people; Human computer interaction","Z.-Q. Huang; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, 610000, China; email: zhiqih@uestc.edu.cn","","Science Press","20959389","","","","Chinese","Gongcheng Kexue Xuebao","Article","Final","","Scopus","2-s2.0-85115201625"
"Das U.; Namboodiri V.; He H.","Das, Uddipan (57189055056); Namboodiri, Vinod (8323861400); He, Hongsheng (56231174200)","57189055056; 8323861400; 56231174200","PathLookup: A Deep Learning-Based Framework to Assist Visually Impaired in Outdoor Wayfinding","2021","2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events, PerCom Workshops 2021","","","9431007","111","116","5","4","10.1109/PerComWorkshops51409.2021.9431007","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85107569510&doi=10.1109%2fPerComWorkshops51409.2021.9431007&partnerID=40&md5=68173f521aa3bd3bf05b01d30ba66327","College of New Jersey, Department of Computer Science, Ewing, 08628, NJ, United States; Wichita State University, Department of Eecs, Wichita, 67260, KS, United States","Das U., College of New Jersey, Department of Computer Science, Ewing, 08628, NJ, United States; Namboodiri V., Wichita State University, Department of Eecs, Wichita, 67260, KS, United States; He H., Wichita State University, Department of Eecs, Wichita, 67260, KS, United States","Reading and following visual signs remains the predominant mechanism for navigation and receiving wayfinding information in areas without accurate GPS coverage. This puts people who are blind or visually impaired (BVI) at a great disadvantage. There still remains a great need to provide a low-cost, easy to use, and reliable auxiliary wayfinding system within indoor and outdoor spaces that complements existing satellite-based systems. Through both a user study and a quantitative study of GPS accuracies in outdoor environments, this paper highlights the need for auxiliary outdoor wayfinding tools for people with visual impairments. A deep learning-based image localization framework called PathLookup is proposed in this work for accurately providing path advancement information for outdoor wayfinding. Evaluation results show PathLookup to be highly accurate and fast potentially proving to be a valuable tool for future integration into outdoor wayfinding systems.  © 2021 IEEE.","Accessibility technologies; computer vision; deep learning; outdoor wayfinding; visually impaired persons","Global positioning system; Ubiquitous computing; Evaluation results; Image localization; Outdoor environment; Quantitative study; Satellite-based system; Visual impairment; Visually impaired; Way-finding systems; Deep learning","","","Institute of Electrical and Electronics Engineers Inc.","","978-166540424-2","","","English","IEEE Int. Conf. Pervasive Comput. Commun. Workshops Affil. Events, PerCom Workshops","Conference paper","Final","","Scopus","2-s2.0-85107569510"
"See A.R.; Van Costillas L.M.; Advincula W.D.C.; Bugtai N.T.","See, Aaron Raymond (36476641600); Van Costillas, Leonheart M. (57224561415); Advincula, Welsey Daniel C. (57208444205); Bugtai, Nilo T. (36056199500)","36476641600; 57224561415; 57208444205; 36056199500","Haptic feedback to detect obstacles in multiple regions for visually impaired and blind people","2021","Sensors and Materials","33","6","","1799","1808","9","9","10.18494/SAM.2021.3221","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85107934892&doi=10.18494%2fSAM.2021.3221&partnerID=40&md5=ae81eddc645cb3e5d98608630d705d84","Department of Electrical Engineering, Southern Taiwan University of Science and Technology, Tainan, 71005, Taiwan; Manufacturing Engineering and Management Department, De La Salle University, Metro Manila, 1004, Philippines; Institute of Biomedical Engineering and Health Technologies (IBEHT), De La Salle University, Metro Manila, 1004, Philippines","See A.R., Department of Electrical Engineering, Southern Taiwan University of Science and Technology, Tainan, 71005, Taiwan; Van Costillas L.M., Department of Electrical Engineering, Southern Taiwan University of Science and Technology, Tainan, 71005, Taiwan; Advincula W.D.C., Department of Electrical Engineering, Southern Taiwan University of Science and Technology, Tainan, 71005, Taiwan; Bugtai N.T., Manufacturing Engineering and Management Department, De La Salle University, Metro Manila, 1004, Philippines, Institute of Biomedical Engineering and Health Technologies (IBEHT), De La Salle University, Metro Manila, 1004, Philippines","Focus on the development of assistive devices for visually impaired and blind people (VIBs) to provide assistance in their safety and mobility has increased, but making such devices portable is still a challenge. We propose a system for localized obstacle avoidance with a haptic-based interface for VIBs implemented using a robotic operating system (ROS) to improve the obstacle detection of existing assistive devices. With a depth camera sensor, an obstacle localization algorithm was developed utilizing the ROS framework to identify key regions to detect head-level, left/right torso-level, and left/right ground-level obstacles. The proposed wearable device provides a discernible array of haptic feedback to convey the perceived locations of obstacles. The system was tested by blindfolded volunteers to determine the accuracy in determining object locations in various environments. Experimental results showed the consistency of the system across different setups. The obstacle detection algorithm was optimized and evaluated to discriminate noises and concurrently detect smaller obstacles, thus making detection more robust. Subsequently, the Eulerian video magnification method was used to determine the level of vibration isolation for a prototype. © MYU K.K.","Haptic interface; Localized obstacle detection; Robot operating system (ROS); Visually impaired and blind people (VIBs)","Feedback; Obstacle detectors; Assistive devices; Haptic feedbacks; Localization algorithm; Magnification method; Obstacle detection; Vibration isolations; Visually impaired; Wearable devices; Haptic interfaces","A.R. See; Department of Electrical Engineering, Southern Taiwan University of Science and Technology, Tainan, 71005, Taiwan; email: aaronsee@stust.edu.tw","","M Y U Scientific Publishing Division","09144935","","","","English","Sens. Mater.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85107934892"
"El-Taher F.E.-Z.; Taha A.; Courtney J.; McKeever S.","El-Taher, Fatma El-Zahraa (56429574300); Taha, Ayman (55920179600); Courtney, Jane (7101845997); McKeever, Susan (34877220600)","56429574300; 55920179600; 7101845997; 34877220600","A systematic review of urban navigation systems for visually impaired people","2021","Sensors","21","9","3103","","","","52","10.3390/s21093103","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85105449185&doi=10.3390%2fs21093103&partnerID=40&md5=413b85d2d1143b8b1074a8c173af809f","School of Computer Science, Technological University Dublin, Dublin, Ireland; Faculty of Computers and Artificial Intelligence, Cairo University, Cairo, 12613, Egypt","El-Taher F.E.-Z., School of Computer Science, Technological University Dublin, Dublin, Ireland; Taha A., School of Computer Science, Technological University Dublin, Dublin, Ireland, Faculty of Computers and Artificial Intelligence, Cairo University, Cairo, 12613, Egypt; Courtney J., School of Computer Science, Technological University Dublin, Dublin, Ireland; McKeever S., School of Computer Science, Technological University Dublin, Dublin, Ireland","Blind and Visually impaired people (BVIP) face a range of practical difficulties when undertaking outdoor journeys as pedestrians. Over the past decade, a variety of assistive devices have been researched and developed to help BVIP navigate more safely and independently. In addition, research in overlapping domains are addressing the problem of automatic environment interpretation using computer vision and machine learning, particularly deep learning, approaches. Our aim in this article is to present a comprehensive review of research directly in, or relevant to, assistive outdoor navigation for BVIP. We breakdown the navigation area into a series of navigation phases and tasks. We then use this structure for our systematic review of research, analysing articles, methods, datasets and current limitations by task. We also provide an overview of commercial and non-commercial navigation applications targeted at BVIP. Our review contributes to the body of knowledge by providing a comprehensive, structured analysis of work in the domain, including the state of the art, and guidance on future directions. It will support both researchers and other stakeholders in the domain to establish an informed view of research progress. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Assistive systems; Autonomous driving; Independent children navigation; Navigation systems; Obstacle avoidance; Planning journeys; Robot navigation; Smart cities; Visually impaired people","Blindness; Humans; Machine Learning; Self-Help Devices; Sensory Aids; Visually Impaired Persons; Navigation systems; Assistive devices; Blind and visually impaired; Current limitation; Navigation phasis; Outdoor navigation; Structured analysis; Systematic Review; Visually impaired people; blindness; human; machine learning; self help device; sensory aid; visually impaired person; Deep learning","S. McKeever; School of Computer Science, Technological University Dublin, Dublin, Ireland; email: susan.mckeever@tudublin.ie","","MDPI AG","14248220","","","33946857","English","Sensors","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85105449185"
"Barontini F.; Bettelani G.C.; Leporini B.; Averta G.; Bianchi M.","Barontini, Federica (57209824559); Bettelani, Gemma Carolina (57197733448); Leporini, Barbara (9133799500); Averta, Giuseppe (57195416332); Bianchi, Matteo (57202798446)","57209824559; 57197733448; 9133799500; 57195416332; 57202798446","A User-Centered Approach to Artificial Sensory Substitution for Blind People Assistance","2022","Biosystems and Biorobotics","28","","","599","603","4","0","10.1007/978-3-030-70316-5_96","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85116935780&doi=10.1007%2f978-3-030-70316-5_96&partnerID=40&md5=a42e99ee800a1e34207979525784607b","Centro di Ricerca “Enrico Piaggio”, Universita di Pisa, Largo Lucio Lazzarino 1, Pisa, 56126, Italy; Dipartimento di Ingegneria dell’Informazione, University of Pisa, Via G. Caruso 16, Pisa, 56122, Italy; Fondazione Istituto Italiano di Tecnologia, via Morego 30, Genova, 16163, Italy; Istituto di Scienza e Tecnologie dell’Informazione (ISTI) “A. Faedo”, Area della Ricerca del CNR di Pisa, Via G. Moruzzi, 1, Pisa, 56124, Italy","Barontini F., Centro di Ricerca “Enrico Piaggio”, Universita di Pisa, Largo Lucio Lazzarino 1, Pisa, 56126, Italy, Dipartimento di Ingegneria dell’Informazione, University of Pisa, Via G. Caruso 16, Pisa, 56122, Italy, Fondazione Istituto Italiano di Tecnologia, via Morego 30, Genova, 16163, Italy; Bettelani G.C., Centro di Ricerca “Enrico Piaggio”, Universita di Pisa, Largo Lucio Lazzarino 1, Pisa, 56126, Italy, Dipartimento di Ingegneria dell’Informazione, University of Pisa, Via G. Caruso 16, Pisa, 56122, Italy; Leporini B., Istituto di Scienza e Tecnologie dell’Informazione (ISTI) “A. Faedo”, Area della Ricerca del CNR di Pisa, Via G. Moruzzi, 1, Pisa, 56124, Italy; Averta G., Centro di Ricerca “Enrico Piaggio”, Universita di Pisa, Largo Lucio Lazzarino 1, Pisa, 56126, Italy; Bianchi M., Centro di Ricerca “Enrico Piaggio”, Universita di Pisa, Largo Lucio Lazzarino 1, Pisa, 56126, Italy, Dipartimento di Ingegneria dell’Informazione, University of Pisa, Via G. Caruso 16, Pisa, 56122, Italy","Artificial sensory substitution plays a crucial role in different domains, including prosthetics, rehabilitation and assistive technologies. The sense of touch has historically represented the ideal candidate to convey information on the external environment, both contact-related and visual, when the natural action-perception loop is broken or not available. This is particularly true for blind people assistance, in which touch elicitation has been used to make content perceivable (e.g. Braille text or graphical reproduction), or to deliver informative cues for navigation. However, despite the significant technological advancements for what concerns both devices for touch-mediated access to alphanumeric stimuli, and technology-enabled haptic navigation supports, the majority of the proposed solutions has met with scarce acceptance in end users community. Main reason for this, in our opinion, is the poor involvement of the blind people in the design process. In this work, we report on a user-centric approach that we successfully applied for haptics-enabled systems for blind people assistance, whose engineering and validation have received significant inputs from the visually-impaired people. We also present an application of our approach to the design of a single-cell refreshable Braille device and to the development of a wearable haptic system for indoor navigation. After a summary of our previous results, we critically discuss next avenues and propose novel solutions for touch-mediated delivery of information for navigation, whose implementation has been totally driven by the feedback collected from real end-users. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","","","F. Barontini; Centro di Ricerca “Enrico Piaggio”, Universita di Pisa, Pisa, Largo Lucio Lazzarino 1, 56126, Italy; email: matteo.bianchi@unipi.it","","Springer Science and Business Media Deutschland GmbH","21953562","","","","English","Biosyst. Biorobotics","Book chapter","Final","","Scopus","2-s2.0-85116935780"
"Mukhiddinov M.; Cho J.","Mukhiddinov, Mukhriddin (57215927861); Cho, Jinsoo (55474141500)","57215927861; 55474141500","Smart glass system using deep learning for the blind and visually impaired","2021","Electronics (Switzerland)","10","22","2756","","","","55","10.3390/electronics10222756","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85118716667&doi=10.3390%2felectronics10222756&partnerID=40&md5=fc5009a58b8421e7f71fa17d139b492a","Department of Computer Engineering, Gachon University, Sujeong-gu, Seongnam-si, 13120, South Korea","Mukhiddinov M., Department of Computer Engineering, Gachon University, Sujeong-gu, Seongnam-si, 13120, South Korea; Cho J., Department of Computer Engineering, Gachon University, Sujeong-gu, Seongnam-si, 13120, South Korea","Individuals suffering from visual impairments and blindness encounter difficulties in moving independently and overcoming various problems in their routine lives. As a solution, artificial intelligence and computer vision approaches facilitate blind and visually impaired (BVI) people in fulfilling their primary activities without much dependency on other people. Smart glasses are a potential assistive technology for BVI people to aid in individual travel and provide social comfort and safety. However, practically, the BVI are unable move alone, particularly in dark scenes and at night. In this study we propose a smart glass system for BVI people, employing computer vision techniques and deep learning models, audio feedback, and tactile graphics to facilitate independent movement in a night-time environment. The system is divided into four models: a low-light image enhancement model, an object recognition and audio feedback model, a salient object detection model, and a text-to-speech and tactile graphics generation model. Thus, this system was developed to assist in the following manner: (1) enhancing the contrast of images under low-light conditions employing a two-branch exposure-fusion network; (2) guiding users with audio feedback using a transformer encoder–decoder object detection model that can recognize 133 categories of sound, such as people, animals, cars, etc., and (3) accessing visual information using salient object extraction, text recognition, and refreshable tactile display. We evaluated the performance of the system and achieved competitive performance on the challenging Low-Light and ExDark datasets. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Artificial intelligence; Assistive technologies; Blind and visually impaired; Deep learning; Low-light images; Object detection; Refreshable tactile display; Smart glasses","","J. Cho; Department of Computer Engineering, Gachon University, Seongnam-si, Sujeong-gu, 13120, South Korea; email: jscho@gachon.ac.kr","","MDPI","20799292","","","","English","Electronics (Switzerland)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85118716667"
"Fernandes S.; D’souza J.; Kattikaren A.; Jadhav D.","Fernandes, Salil (57422863700); D’souza, Jordan (57422863800); Kattikaren, Anthony (57422409900); Jadhav, Dipti (57214606320)","57422863700; 57422863800; 57422409900; 57214606320","NAYAN-DRISHTI: A Revolutionary Navigation/Visual Aid for the Visually Impaired","2022","Lecture Notes in Networks and Systems","314","","","319","333","14","0","10.1007/978-981-16-5655-2_31","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85123310474&doi=10.1007%2f978-981-16-5655-2_31&partnerID=40&md5=f52b22c35e9c714d8f6945fc84430b39","Don Bosco Institute of Technology, Mumbai, 400070, India","Fernandes S., Don Bosco Institute of Technology, Mumbai, 400070, India; D’souza J., Don Bosco Institute of Technology, Mumbai, 400070, India; Kattikaren A., Don Bosco Institute of Technology, Mumbai, 400070, India; Jadhav D., Don Bosco Institute of Technology, Mumbai, 400070, India","The project/proposed product hinges on three domains of computational technology, i.e., machine learning, convolutional neural networks, and Internet of Things. The aim of the project is to invent a product that is helpful to the disabled section of society as ideally as possible try to as well as to acquaint ourselves with the much talked about and ever-growing domains of computer technology. The main functions that our proposed product will offer are detection of the obstructing object and alerting via a speaker (along with classification and distance of the object from the user) and a navigation system (which obtains live data of the current location of the user with the help of the UBLOX GPS module). The proposed product is designed in such a way so as to provide an all in one multitasking and hassle-free solution to our user and to ease the burden that comes along with the disability of blindness. The proposed product is touted as a boon to our users since it not only will help them in identifying the obstructions ahead them but will also help them to navigate from their current location to their destination with freedom and no fear. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Batch normalization (BN); Convolutional neural networks (CNN); Rectified linear unit (ReLU)","","S. Fernandes; Don Bosco Institute of Technology, Mumbai, 400070, India; email: salilfernandes72@gmail.com","Fong S.; Dey N.; Joshi A.","Springer Science and Business Media Deutschland GmbH","23673370","978-981165654-5","","","English","Lect. Notes Networks Syst.","Conference paper","Final","","Scopus","2-s2.0-85123310474"
"Koutny R.; Günther S.; Dhingra N.; Kunz A.; Miesenberger K.; Mühlhäuser M.","Koutny, Reinhard (55948081100); Günther, Sebastian (55804000700); Dhingra, Naina (57209979094); Kunz, Andreas (7005939819); Miesenberger, Klaus (57216983728); Mühlhäuser, Max (7003434700)","55948081100; 55804000700; 57209979094; 7005939819; 57216983728; 7003434700","Accessible multimodal tool support for brainstorming meetings","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12377 LNCS","","","11","20","9","1","10.1007/978-3-030-58805-2_2","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85091528363&doi=10.1007%2f978-3-030-58805-2_2&partnerID=40&md5=b4a460ae2e2d2e0998a0a352a24f3812","Institut Integriert Studieren, Johannes Kepler University, Linz, Austria; Technische Universität Darmstadt, Darmstadt, Germany; Innovation Center Virtual Reality, ETH Zurich, Zurich, Switzerland","Koutny R., Institut Integriert Studieren, Johannes Kepler University, Linz, Austria; Günther S., Technische Universität Darmstadt, Darmstadt, Germany; Dhingra N., Innovation Center Virtual Reality, ETH Zurich, Zurich, Switzerland; Kunz A., Innovation Center Virtual Reality, ETH Zurich, Zurich, Switzerland; Miesenberger K., Institut Integriert Studieren, Johannes Kepler University, Linz, Austria; Mühlhäuser M., Technische Universität Darmstadt, Darmstadt, Germany","In recent years, assistive technology and digital accessibility for blind and visually impaired people (BVIP) has been significantly improved. Yet, group discussions, especially in a business context, are still challenging as non-verbal communication (NVC) is often depicted on digital whiteboards, including deictic gestures paired with visual artifacts. However, as NVC heavily relies on the visual perception, whichrepresents a large amount of detail, an adaptive approach is required that identifies the most relevant information for BVIP. Additionally, visual artifacts usually rely on spatial properties such as position, orientation, and dimensions to convey essential information such as hierarchy, cohesion, and importance that is often not accessible to the BVIP. In this paper, we investigate the requirements of BVIP during brainstorming sessions and, based on our findings, provide an accessible multimodal tool that uses non-verbal and spatial cues as an additional layer of information. Further, we contribute by presenting a set of input and output modalities that encode and decode information with respect to the individual demands of BVIP and the requirements of different use cases. © 2020, The Author(s).","2D haptic output device; Blind and visually impaired people; Brainstorming; Business meeting; Non verbal communication","Artificial intelligence; Computer science; Computers; Assistive technology; Blind and visually impaired; Brainstorming sessions; Business contexts; Input and outputs; Non-verbal communications; Spatial properties; Visual perception; Arts computing","R. Koutny; Institut Integriert Studieren, Johannes Kepler University, Linz, Austria; email: Reinhard.Koutny@jku.at","Miesenberger K.; Manduchi R.; Covarrubias Rodriguez M.; Penáz P.","Springer Science and Business Media Deutschland GmbH","03029743","978-303058804-5","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85091528363"
"Bhatlawande S.; Deshpande A.; Deshpande S.; Shilaskar S.","Bhatlawande, Shripad (55212307900); Deshpande, Atharva (58526375900); Deshpande, Shreyas (59060378000); Shilaskar, Swati (57189017229)","55212307900; 58526375900; 59060378000; 57189017229","Proactive Detection of Pothole and Walkable Path for Safe Mobility of Visually Challenged","2022","2022 3rd International Conference for Emerging Technology, INCET 2022","","","","","","","5","10.1109/INCET54531.2022.9824637","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85136324296&doi=10.1109%2fINCET54531.2022.9824637&partnerID=40&md5=3967334e64123248276fbac5e34ed180","Vishwakarma Institute of Technology, Department of Electronics and Telecommunication Engineering, Pune, 411037, India","Bhatlawande S., Vishwakarma Institute of Technology, Department of Electronics and Telecommunication Engineering, Pune, 411037, India; Deshpande A., Vishwakarma Institute of Technology, Department of Electronics and Telecommunication Engineering, Pune, 411037, India; Deshpande S., Vishwakarma Institute of Technology, Department of Electronics and Telecommunication Engineering, Pune, 411037, India; Shilaskar S., Vishwakarma Institute of Technology, Department of Electronics and Telecommunication Engineering, Pune, 411037, India","Potholes are primary cause for road accidents; hence it is important to identify and classify the potholes using efficient Computer vision-based techniques. Potholes not only cause problems to car drivers but even pedestrians face problem while walking on a road having potholes. It is also a major problem for blind people who must be extremely careful while walking. This paper presents a machine learning and computer vision-based approach to identify potholes from an image and alert visually impaired person about the same. Dataset is compiled from various available open-source datasets along with few images clicked from smartphone. Five different classification algorithms are used for classification of the images after applying image pre-processing and feature vector extraction. The algorithm implemented in this paper uses machine learning based approach which is computationally efficient. It provides an accuracy at par with computationally expensive deep learning algorithms. The method involves feature extraction, dimensionality reduction and finally classification algorithms are applied. Binary classification algorithms are used to classify the images. Performance evaluation metrics like Precision, Recall, F1-Score and Accuracy are used to evaluate the model's performance. Random forest model showed the best performance among all the algorithms with an accuracy of 88% on test dataset. © 2022 IEEE.","Assistive aid for Blind; Computer Vision; Machine Learning; Pothole Detection","Classification (of information); Decision trees; Deep learning; Extraction; Feature extraction; Image classification; Landforms; Learning algorithms; Roads and streets; Statistical tests; Assistive; Assistive aid for blind; Blind people; Car driver; Classification algorithm; Machine-learning; Pothole detection; Safe mobility; Vision based; Walkable paths; Computer vision","","","Institute of Electrical and Electronics Engineers Inc.","","978-166549499-1","","","English","Int. Conf. for Emerg. Technol., INCET","Conference paper","Final","","Scopus","2-s2.0-85136324296"
"Dev S.; Jaiswal S.; Kokamkar Y.; Deshpande K.B.; Upadhyaya K.","Dev, Sameer (57221909413); Jaiswal, Sudama (57221916885); Kokamkar, Yogendra (57221906967); Deshpande, Kiran B. (57217631380); Upadhyaya, Kaushiki (57216898851)","57221909413; 57221916885; 57221906967; 57217631380; 57216898851","Voice based smart assistive device for the visually challenged","2020","2020 International Conference on Convergence to Digital World - Quo Vadis, ICCDW 2020","","","9318604","","","","5","10.1109/ICCDW45521.2020.9318604","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85100588461&doi=10.1109%2fICCDW45521.2020.9318604&partnerID=40&md5=69ecc05b5868cf51f7a8bfe00e878d85","A. P. Shah Institute of Technology, Department of Information Technology, Thane, India","Dev S., A. P. Shah Institute of Technology, Department of Information Technology, Thane, India; Jaiswal S., A. P. Shah Institute of Technology, Department of Information Technology, Thane, India; Kokamkar Y., A. P. Shah Institute of Technology, Department of Information Technology, Thane, India; Deshpande K.B., A. P. Shah Institute of Technology, Department of Information Technology, Thane, India; Upadhyaya K., A. P. Shah Institute of Technology, Department of Information Technology, Thane, India","Blind, the dictionary defines it in one simple word, sightless. The life for a visually challenged person is extremely hard for obvious reasons. In this era of cutting-edge technology, it is still extremely difficult for visually challenged people to carry out day to day chores or enjoy the simple pleasures of life such as going for a walk, socializing, and so on. Hence, developing new solutions that allow those individuals to interact with sighted people, and the sighted world, in a way that lessens any of the problems that can arise from being visually impaired is becoming increasingly important. This paper presents a Smart Device built using a Raspberry Pi that can be controlled via Voice Commands and carry out various tasks such as Object Detection, Navigation, and notify the user through Audio feedback. The device will also take help of Image Recognition and Image Processing in order to convey information about specific places to the user as soon as the user is in that particular vicinity, hence allowing the person using the device know their surrounding environment in a better way. © 2020 IEEE.","Assistive Technology; Conversational AI; Deep Learning; Intelligent Device; Raspberry Pi; Speech Recognition","Image recognition; Assistive devices; Audio feedbacks; Cutting edge technology; New solutions; Smart devices; Surrounding environment; Visually impaired; Voice command; Object detection","","","Institute of Electrical and Electronics Engineers Inc.","","978-172814635-5","","","English","Int. Conf. Converg. Digit. World - Quo Vadis, ICCDW","Conference paper","Final","","Scopus","2-s2.0-85100588461"
"Fink P.D.S.; Holz J.A.; Giudice N.A.","Fink, Paul D. S. (57216695324); Holz, Jessica A. (57248485900); Giudice, Nicholas A. (15724751900)","57216695324; 57248485900; 15724751900","Fully Autonomous Vehicles for People with Visual Impairment","2021","ACM Transactions on Accessible Computing","14","3","15","","","","18","10.1145/3471934","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85114323580&doi=10.1145%2f3471934&partnerID=40&md5=986cce206d4eb455f6a8db4c4a25dc20","School of Computing and Information Science, 5711 Boardman Hall, Orono, 04469, ME, United States; VEMI Lab, The University of Maine, Orono, 04469, ME, United States","Fink P.D.S., School of Computing and Information Science, 5711 Boardman Hall, Orono, 04469, ME, United States; Holz J.A., VEMI Lab, The University of Maine, Orono, 04469, ME, United States; Giudice N.A., School of Computing and Information Science, 5711 Boardman Hall, Orono, 04469, ME, United States","A significant number of individuals in the United States report a disability that limits their ability to travel, including many people who are blind or visually impaired (BVI). The implications of restricted transportation result in negative impacts related to economic security, physical and mental health, and overall quality of life. Fully autonomous vehicles (FAVs) present a means to mitigate travel barriers for this population by providing new, safe, and independent travel opportunities. However, current policies governing interactions with the artificial intelligence (AI) 'at the wheel' of FAVs do not reflect the accessibility needs articulated by BVI people in the extant literature, failing to encourage use cases that would result in life changing mobility. By reviewing the legislative and policy efforts surrounding FAVs, we argue that the heart of this problem is due to a disjointed, laissez-faire approach to FAV accessibility that has yet to actualize the full benefits of this new transportation mode, not only for BVI people, but also for all users. We outline the necessity for a policy framework that guides the design of FAVs to include the concerns of BVI people and then propose legislative and design recommendations aimed to promote enhanced accessibility, transparency, and fairness during FAV travel.  © 2021 ACM.","accessibility (blind and visually impaired); accessible design; artificial intelligence; Autonomous vehicles; transportation policy","Artificial intelligence; Design recommendations; Economic security; Fully-autonomous vehicles; Overall quality; Policy framework; Transportation mode; Visual impairment; Visually impaired; Autonomous vehicles","","","Association for Computing Machinery","19367228","","","","English","ACM Trans. Accessible Comput.","Article","Final","","Scopus","2-s2.0-85114323580"
"Ramsewak T.P.; Appadoo D.; Mungloo-Dilmohamud Z.","Ramsewak, Tarini Prsti (57223303685); Appadoo, Dave (57223303947); Mungloo-Dilmohamud, Zahra (56993759900)","57223303685; 57223303947; 56993759900","EyeSee: Camera to Caption with Attention Mechanism","2020","2020 IEEE Asia-Pacific Conference on Computer Science and Data Engineering, CSDE 2020","","","9411571","","","","0","10.1109/CSDE50874.2020.9411571","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85105529826&doi=10.1109%2fCSDE50874.2020.9411571&partnerID=40&md5=6b30fd209f26f0e4ac78f3ba5bc2e5a1","University of Mauritius, FoICDT, Department of Digital Technologies, Reduit, Mauritius","Ramsewak T.P., University of Mauritius, FoICDT, Department of Digital Technologies, Reduit, Mauritius; Appadoo D., University of Mauritius, FoICDT, Department of Digital Technologies, Reduit, Mauritius; Mungloo-Dilmohamud Z., University of Mauritius, FoICDT, Department of Digital Technologies, Reduit, Mauritius","According to the WHO, there are currently around 2.2 billion people who are either visually impaired or blind in the world. Previously, these people had to rely only on classic AIDS such as the white cane and the guide dog for mobility and magnifiers and screen readers amongst others for reading. The massive use of smartphones has opened many new possibilities for the visually impaired and blind. They can now use their smartphones to help them navigate around cities and other places. In this project it is proposed to have an app for smartphones which automatically tells the blind user the objects around him. However, automatically identifying and describing the content of an image is not such a simple task. It involves tasks from 2 complex fields namely computer vision and natural language processing. The proposed application, EyeSee, takes images from a real-time environment, processes these frame by frame and tells the user what the image represents. The app also annotates the images with text. The app uses Deep Learning, more specifically, Show, Attend and Tell and GRU. © 2020 IEEE.","Attend and Tell; Camera2Caption; GRU; blind and visually impaired; blind and visually impaired; deep learning; Show; Camera2Caption; deep learning; GRU; Show, Attend and Tell","Natural language processing systems; Smartphones; Attention mechanisms; Blind users; Complex fields; NAtural language processing; Real-time environment; Screen readers; Visually impaired; White cane; Deep learning","","","Institute of Electrical and Electronics Engineers Inc.","","978-166541974-1","","","English","IEEE Asia-Pacific Conf. Comput. Sci. Data Eng., CSDE","Conference paper","Final","","Scopus","2-s2.0-85105529826"
"Marzullo G.D.; Jo K.-H.; Caceres D.","Marzullo, Gabriel D. (57222705687); Jo, Kang-Hyun (56978116100); Caceres, Danilo (57210173223)","57222705687; 56978116100; 57210173223","Vision-based Assistive Navigation Algorithm for Blind and Visually Impaired People Using Monocular Camera","2021","2021 IEEE/SICE International Symposium on System Integration, SII 2021","","","9382783","640","645","5","1","10.1109/IEEECONF49454.2021.9382783","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85103741931&doi=10.1109%2fIEEECONF49454.2021.9382783&partnerID=40&md5=7025ec138ac23e541f8a04bf40e342a7","Universidad Tecnológica de Panamá, Laboratorio de Sistemas Inteligentes, Panama; University of Uslan, Intelligent Systems Lab, South Korea; Universidad Tecnológica de Panamá, Facultad de Ingeniería Eléctrica, Panama","Marzullo G.D., Universidad Tecnológica de Panamá, Laboratorio de Sistemas Inteligentes, Panama; Jo K.-H., University of Uslan, Intelligent Systems Lab, South Korea; Caceres D., Universidad Tecnológica de Panamá, Facultad de Ingeniería Eléctrica, Panama","The system aims to develop an algorithm, to be implemented in an open-source platform, that facilitate the navigation, and early door/window detection process, for people with visual difficulties in closed environments. The proposed algorithm relies in a three stage image process consisting of the removal of all unwanted information through grayscale conversion and edge detection (image pre-processing), followed by the second stage formed by the search of lines or patterns on the detected edges, with the help of a first order Hough Transform, in order to locate a region where the majority of these lines intersect with each other, or vanishing point, which is the pivot point of the entire algorithm. Finally, the last stage consist on the estimation of the main regions of the aisle (walls, floor and ceiling) in order to perform an early detection of special regions in the walls, or candidates, that could lead to the detection of doors and windows. The main idea is that, after the image processing, the algorithm will return instructions to the subject, through a physical medium to make the corrections on his trajectory. It was found that the estimation of the vanishing point through the use of Hough transformation and the second-order edge detection method using open source software tools is a viable method for navigation in closed environments and even its possible to use the same image processing tools to estimate regions that could be easily consider as the main regions of the aisle. It's been demonstrated that perform the early detection of candidates for doors is also possible using the same techniques. © 2021 IEEE.","","Chemical detection; Edge detection; Feature extraction; Hough transforms; Navigation; Open source software; Open systems; Assistive navigations; Blind and visually impaired; Closed environment; Edge detection methods; Hough Transformation; Image preprocessing; Image processing tools; Open source platforms; Image processing","","","Institute of Electrical and Electronics Engineers Inc.","","978-172817658-1","","","English","IEEE/SICE Int. Symp. Syst. Integr., SII","Conference paper","Final","","Scopus","2-s2.0-85103741931"
"Bleau M.; Paré S.; Djerourou I.; Chebat D.R.; Kupers R.; Ptito M.","Bleau, Maxime (57222074383); Paré, Samuel (57222072614); Djerourou, Ismaël (57222078957); Chebat, Daniel R. (11140655700); Kupers, Ron (7003484370); Ptito, Maurice (7005537267)","57222074383; 57222072614; 57222078957; 11140655700; 7003484370; 7005537267","Blindness and the reliability of downwards sensors to avoid obstacles: A study with the eyecane","2021","Sensors","21","8","2700","","","","8","10.3390/s21082700","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85103914325&doi=10.3390%2fs21082700&partnerID=40&md5=4d84e8e156ae80c835fa65b000edc7e2","École D’optométrie, Université de Montréal, Montréal, H3T 1P1, QC, Canada; Visual and Cognitive Neuroscience Laboratory (VCN Lab.), Department of Psychology, Faculty of Social Sciences and Humanities, Ariel University, Ari’el, 40700, Israel; Navigation and Accessibility Research Center of Ariel University (NARCA), Ari’el, 40700, Israel; Department of Neuroscience, University of Copenhagen, Copenhagen, 2200, Denmark","Bleau M., École D’optométrie, Université de Montréal, Montréal, H3T 1P1, QC, Canada; Paré S., École D’optométrie, Université de Montréal, Montréal, H3T 1P1, QC, Canada; Djerourou I., École D’optométrie, Université de Montréal, Montréal, H3T 1P1, QC, Canada; Chebat D.R., Visual and Cognitive Neuroscience Laboratory (VCN Lab.), Department of Psychology, Faculty of Social Sciences and Humanities, Ariel University, Ari’el, 40700, Israel, Navigation and Accessibility Research Center of Ariel University (NARCA), Ari’el, 40700, Israel; Kupers R., École D’optométrie, Université de Montréal, Montréal, H3T 1P1, QC, Canada, Department of Neuroscience, University of Copenhagen, Copenhagen, 2200, Denmark; Ptito M., École D’optométrie, Université de Montréal, Montréal, H3T 1P1, QC, Canada, Department of Neuroscience, University of Copenhagen, Copenhagen, 2200, Denmark","Vision loss has dramatic repercussions on the quality of life of affected people, particularly with respect to their orientation and mobility. Many devices are available to help blind people to navigate in their environment. The EyeCane is a recently developed electronic travel aid (ETA) that is inexpensive and easy to use, allowing for the detection of obstacles lying ahead within a 2 m range. The goal of this study was to investigate the potential of the EyeCane as a primary aid for spatial navigation. Three groups of participants were recruited: early blind, late blind, and sighted. They were first trained with the EyeCane and then tested in a life-size obstacle course with four obstacles types: cube, door, post, and step. Subjects were requested to cross the corridor while detecting, identifying, and avoiding the obstacles. Each participant had to perform 12 runs with 12 different obstacles configurations. All participants were able to learn quickly to use the EyeCane and successfully complete all trials. Amongst the various obstacles, the step appeared to prove the hardest to detect and resulted in more collisions. Although the EyeCane was effective for detecting obstacles lying ahead, its downward sensor did not reliably detect those on the ground, rendering downward obstacles more hazardous for navigation. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Avoidance; Blindness; Collision; EyeCane; Navigation; Obstacle detection; Sensory substitution","Blindness; Humans; Orientation; Quality of Life; Reproducibility of Results; Visually Impaired Persons; Avoid obstacles; Blind people; Electronic travel aidss; EYECANE; Quality of life; Spatial navigation; Vision loss; blindness; human; orientation; quality of life; reproducibility; visually impaired person; Artificial life","M. Ptito; École D’optométrie, Université de Montréal, Montréal, H3T 1P1, Canada; email: maurice.ptito@umontreal.ca; M. Ptito; Department of Neuroscience, University of Copenhagen, Copenhagen, 2200, Denmark; email: maurice.ptito@umontreal.ca","","MDPI AG","14248220","","","33921202","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85103914325"
"Luo L.; Zhang P.-J.; Hu P.-J.; Yang L.; Chang K.-C.","Luo, Ling (59102493000); Zhang, Ping-Jun (8042173100); Hu, Peng-Jun (57219292793); Yang, Liu (57221069603); Chang, Kuo-Chi (57095265500)","59102493000; 8042173100; 57219292793; 57221069603; 57095265500","Research Method of Blind Path Recognition Based on DCGAN","2021","Advances in Intelligent Systems and Computing","1261 AISC","","","90","99","9","0","10.1007/978-3-030-58669-0_8","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85092114880&doi=10.1007%2f978-3-030-58669-0_8&partnerID=40&md5=c21217853031da4ece2c5ab52c355713","School of Information Science and Engineering, Fujian University of Technology, Fuzhou, China; Fujian Provincial Key Laboratory of Big Data Mining and Applications, Fujian University of Technology, Fuzhou, China; College of Mechanical and Electrical Engineering, National Taipei University of Technology, Taipei, Taiwan; Department of Business Administration, North Borneo University College, Sabah, Malaysia","Luo L., School of Information Science and Engineering, Fujian University of Technology, Fuzhou, China; Zhang P.-J., School of Information Science and Engineering, Fujian University of Technology, Fuzhou, China; Hu P.-J., School of Information Science and Engineering, Fujian University of Technology, Fuzhou, China; Yang L., School of Information Science and Engineering, Fujian University of Technology, Fuzhou, China; Chang K.-C., School of Information Science and Engineering, Fujian University of Technology, Fuzhou, China, Fujian Provincial Key Laboratory of Big Data Mining and Applications, Fujian University of Technology, Fuzhou, China, College of Mechanical and Electrical Engineering, National Taipei University of Technology, Taipei, Taiwan, Department of Business Administration, North Borneo University College, Sabah, Malaysia","In order to solve the problem that there are few blind path data sets and a lot of manual data collection work in the current blind guide system, computer vision algorithm is used to automatically generate blind path images in different environments. Methods a blind path image generation method based on the depth convolution generative adversary network (DCGAN) is proposed. The method uses the characteristics of typical blind path, which is the combination of depression and bulge. The aim of long short memory network’ (LSTM) is to encode the depression part, and the aim of convolution neural network (CNN) is to encode the bulge part. The two aspects of information are combined to generate blind path images in different environments. It can effectively improve the blind path recognition rate of the instrument and improve the safe travel of the visually impaired. Conclusion generative adversarial networks (GANs) can be used to generate realistic blind image, which has certain application value in expanding blind channel recognition data, but it still needs to be improved in some details. © 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG.","Advanced blind path recognition system; Algorithm; Convolutional neural network (CNN); Depth convolution generative adversary network (DCGAN); Generative adversarial networks (GANs)","Convolution; Encoding (symbols); Image enhancement; Intelligent systems; Adversarial networks; Computer vision algorithms; Convolution neural network; Data collection; Image generations; Path recognition; research methods; Visually impaired; Long short-term memory","K.-C. Chang; School of Information Science and Engineering, Fujian University of Technology, Fuzhou, China; email: albertchangxuite@gmail.com","Hassanien A.E.; Slowik A.; Snášel V.; El-Deeb H.; Tolba F.M.","Springer Science and Business Media Deutschland GmbH","21945357","978-303058668-3","","","English","Adv. Intell. Sys. Comput.","Conference paper","Final","","Scopus","2-s2.0-85092114880"
"Tang J.; Sun M.; Zhu L.; Hu M.; Zhou M.; Zhang J.; Li Q.; Zhai G.","Tang, Jingyu (57326047800); Sun, Mingze (57326426600); Zhu, Lingjun (57326793600); Hu, Menghan (55818700700); Zhou, Mei (55235800400); Zhang, Jian (55940455200); Li, Qingli (23100123400); Zhai, Guangtao (15847120000)","57326047800; 57326426600; 57326793600; 55818700700; 55235800400; 55940455200; 23100123400; 15847120000","Design and Optimization of an Assistive Cane With Visual Odometry for Blind People to Detect Obstacles With Hollow Section","2022","IEEE Sensors Journal","21","21","","24759","24770","11","8","10.1109/JSEN.2021.3115854","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85118624528&doi=10.1109%2fJSEN.2021.3115854&partnerID=40&md5=983a5140a345c0ad2a7f6ac5eaaef215","Shanghai Key Laboratory of Multidimensional Information Processing, School of Communication and Electronic Engineering, East China Normal University, Shanghai, 200062, China; School of Statistics, East China Normal University, Shanghai, 200241, China; Institute of Image Communication and Information Processing, Shanghai Jiao Tong University, Shanghai, 200240, China","Tang J.; Sun M., Shanghai Key Laboratory of Multidimensional Information Processing, School of Communication and Electronic Engineering, East China Normal University, Shanghai, 200062, China, School of Statistics, East China Normal University, Shanghai, 200241, China; Zhu L., Shanghai Key Laboratory of Multidimensional Information Processing, School of Communication and Electronic Engineering, East China Normal University, Shanghai, 200062, China; Hu M.; Zhou M., Shanghai Key Laboratory of Multidimensional Information Processing, School of Communication and Electronic Engineering, East China Normal University, Shanghai, 200062, China; Zhang J., Shanghai Key Laboratory of Multidimensional Information Processing, School of Communication and Electronic Engineering, East China Normal University, Shanghai, 200062, China; Li Q., Shanghai Key Laboratory of Multidimensional Information Processing, School of Communication and Electronic Engineering, East China Normal University, Shanghai, 200062, China; Zhai G., Institute of Image Communication and Information Processing, Shanghai Jiao Tong University, Shanghai, 200240, China","Existing obstacle avoidance systems do not consider the situation of hollowed-out obstacles. Therefore, in this paper, we design an assistive cane with visual odometry based on these requirements of the blind to aid them in attaining safe indoor navigation. The proposed device is portable, compact and adaptable. The assistive cane with visual odometry can be used not only to directly detect obstacles, but also to draw the attention of people around, providing multiple protection for the blind. In order to optimize the device parameters, we select the height of the camera and the speed of the voice broadcast as factors to design a complete factorial design, aiming to explore the parameter level that can make the passing time shortest and the success rate of obstacle avoidance highest. We invited two visually impaired people and two blindfolded students with normal vision as the experimental subjects. Each subject was required to conduct the experiments under nine combinations of device parameters. The subjects tried to avoid obstacles according to the voice prompts. The passing time and the success rate of obstacle avoidance were recorded. The experimental results show that the broadcast speed causes the significant effect to the success rate and both the two factors affect the passing time. In addition, we discuss the results of different subjects’ response surfaces and perceptual patterns and get the conclusion that different visual perceptual patterns, namely route representation and two different survey representations, have a certain impact on the setting of the optimal parameter level and the optimal passing time. The congenitally blind subject with route representation has the shortest optimal passing time in an unfamiliar environment, and the camera height should be set at the center of the hollowed-out obstacle, while the results are somewhat different for the acquired visual impairment subjects and the blindfolded normal vision subjects with survey representation. The camera height should be set between the center and the upper position of the obstacle. In summary, the camera height should be set at the center of the hollowed-out barrier and the broadcast speed should be set at the highest level. At the same time, the device parameters should be adjusted appropriately according to the visual perception pattern of users. Further researches are needed to analyze the perceptual patterns in depth. © 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.","Device optimization; Factorial design; Obstacle avoidance; Visually assistive device","Cameras; Collision avoidance; Computer vision; Indoor positioning systems; Surveys; Assistive; Assistive devices; Collisions avoidance; Device optimization; Factorial design; Indoor navigation; Obstacles avoidance; Optimisations; Visual odometry; Visually assistive device; Vision","","","Institute of Electrical and Electronics Engineers Inc.","1530437X","","","","English","IEEE Sensors J.","Article","Final","","Scopus","2-s2.0-85118624528"
"Noorjahan M.; Punitha A.","Noorjahan, M. (57206727450); Punitha, A. (57207789956)","57206727450; 57207789956","An electronic travel guide for visually impaired–vehicle board recognition system through computer vision techniques","2020","Disability and Rehabilitation: Assistive Technology","15","2","","238","241","3","2","10.1080/17483107.2019.1574918","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85078821276&doi=10.1080%2f17483107.2019.1574918&partnerID=40&md5=46a0f650d825b09035c4010299a6dca0","Research Scholar, Department of Computer Science, Bharathiyar University, Coimbatore, Tamil Nadu, India; Research Supervisor, Department of Computer Applications, Bharathiyar University, Coimbatore, Tamil Nadu, India","Noorjahan M., Research Scholar, Department of Computer Science, Bharathiyar University, Coimbatore, Tamil Nadu, India; Punitha A., Research Supervisor, Department of Computer Applications, Bharathiyar University, Coimbatore, Tamil Nadu, India","Purpose: In the context of assistive technology, mobility takes the meaning of “moving safely, gracefully, and comfortably”.The aim of this article is to provide a system which will be a convenient means of navigation for the Visually Impaired people, in the public transport system. Method: A blind regular commuter who travels by public transport facility finds difficulty in identifying the vehicle that is nearing the stop. Hence, a real-time system that dynamically identifies the nearing vehicle and informs the commuters is necessary. This paper proposes such a system namely the “Vehicle Board Recognition System” (VBRS). Computer Vision techniques such as segmentation, object recognition, text detection and optical character recognition are utilized to build the system, which will detect, analyze, derive and communicate the information to the passengers. Results: Thanks to the rapid development in technology, there are several navigation systems both hand held and wearable, available to help visually impaired (VI) people move comfortably both indoor and outdoor. Many blind people are not comfortable in using these devices or they are not affordable for them. Thus the proposed system gives them the comfort of navigation. Conclusion: This system can be installed in the bus stop to assist the Visually Impaired, from externally rather than their hand held or wearable assistive devices.Implications for rehabilitation This proposed system will help the visually impaired to ensure secure navigation be independent of the others develop self confidence. overcome the training, affordability of wearable/ handheld devices. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.","assistive technology; Computer vision; object recognition; OCR; segmentation; template matching; visually impaired","Electronics; Humans; Pattern Recognition, Automated; Self-Help Devices; Sensory Aids; Transportation; Visually Impaired Persons; automated pattern recognition; electronics; human; procedures; self help device; sensory aid; traffic and transport; visually impaired person","M. Noorjahan; Department of Computer Science, Bharathiyar University, Coimbatore, India; email: noornainar@gmail.com","","Taylor and Francis Ltd","17483107","","","30856030","English","Disabil. Rehabil. Assistive Technol.","Article","Final","","Scopus","2-s2.0-85078821276"
"Das S.; Saxena S.; Rout N.K.","Das, Sourav (57483402300); Saxena, Shubham (57220883821); Rout, Nirmal Kumar (36722439800)","57483402300; 57220883821; 36722439800","Low Cost Smart-Glass using ESP-32","2021","IEEE 2nd International Conference on Applied Electromagnetics, Signal Processing, and Communication, AESPC 2021 - Proceedings","","","","","","","5","10.1109/AESPC52704.2021.9708478","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85126103909&doi=10.1109%2fAESPC52704.2021.9708478&partnerID=40&md5=375d595172aa9bcf39fae8af25fcf9ea","KIIT University, School of Electronics Engineering, Bhubaneswar, India","Das S., KIIT University, School of Electronics Engineering, Bhubaneswar, India; Saxena S., KIIT University, School of Electronics Engineering, Bhubaneswar, India; Rout N.K., KIIT University, School of Electronics Engineering, Bhubaneswar, India","Studies on developing smart wearable technology are taking its boom in recent years making efforts towards better human life. Prime examples of smart wearable technologies are 'Smart Glasses' and 'Smart Watches'. The integration of smart wearables with human well-being is becoming reality these days. The proposed smart glass is aimed to increase efficiency, productivity, and interlinking computing devices into our everyday lives by presenting the important information directly in front of his/her eyes for example: navigation information/directions, presenting critical data in medical operative procedures, gaming controls etc. Smart-glasses are also becoming useful for visually and hearing-impaired people. For blinds, several solutions are already available but they lack some functionalities and ease of use. In this paper, an effort in this direction is made, to rectify those problems. Proposed smart glasses are going to feature a transparent display setup, which is placed in the line of sight without marginally obstructing vision and voice assistant is also incorporated. By developing and providing this system at a lower price to the people of India will also assist ""Digital India"".  © 2021 IEEE.","Embedded systems; Internet of Things (IoT); Machine Learning (ML); Smart glass; Smart wearable","Audition; Big data; Costs; Data visualization; Embedded systems; Glass; Machine learning; Wearable computers; Computing devices; Embedded-system; Human lives; Internet of thing; Low-costs; Machine learning; Machine-learning; Smart glass; Smart wearables; Well being; Internet of things","","","Institute of Electrical and Electronics Engineers Inc.","","978-166544299-2","","","English","IEEE Int. Conf. Appl. Electromagn., Signal Process., Commun., AESPC - Proc.","Conference paper","Final","","Scopus","2-s2.0-85126103909"
"Jubril A.M.; Samuel S.J.","Jubril, Abimbola M. (33667743400); Samuel, Segun J. (57224078216)","33667743400; 57224078216","A multisensor electronic traveling aid for the visually impaired","2021","Technology and Disability","33","2","","99","107","8","1","10.3233/TAD-200280","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85106911101&doi=10.3233%2fTAD-200280&partnerID=40&md5=2026300d70e8c9d7ef10837b7ba963f5","Department of Electronic and Electrical Engineering, Obafemi Awolowo University, Nigeria; Department of Electrical and Electronic Engineering, University of Ibadan, Nigeria","Jubril A.M., Department of Electronic and Electrical Engineering, Obafemi Awolowo University, Nigeria; Samuel S.J., Department of Electrical and Electronic Engineering, University of Ibadan, Nigeria","Background: Identifying obstacles and potholes in the pathway of the visually impaired have received much attention. While much has been done on the detection of obstacles, in the design of electronic mobility aids, much more is needed to be done on the detection of holes and drop-offs, especially those that are affordable and hands-free. This paper therefore considers the development of a wearable electronic mobility aid. Methods: The developed system is based on the multisensor fusion approach of detection which combined three techniques, namely: a source of laser light, a camera and an ultrasonic sensor. A red line generating laser source is used to project a straight line and this is captured by the camera. The red line is deformed differently on coming in contact with holes or standing obstacles. The pattern of deformation is then extracted for obstacle and pothole recognition. The visibility of laser light is greatly reduced when the scene is extremely illuminated, so this is complemented with edge detection. The edge detection uses edges in the identification of holes and obstacles. This is combined with ultrasonic sensing, so that the presence of obstacles can be differentiated from that of holes. The outcome of detection and the distance of obstacles from the blind are relayed via an audio cue. Redults: Its evaluation showed better performance compared to the guide cane. It showed a reduction in collision rate by 83.25% and reduction in falling rate by 84.62%. The device received good acceptability from the users. © 2021 - IOS Press. All rights reserved.","multi-sensor system; visually impaired; Wearable electronic mobility aid","adolescent; adult; Article; clinical outcome; demography; edge detection; exercise; feedback system; female; human; image processing; male; middle aged; Nigeria; recognition; travel; visibility; visual impairment; young adult","A.M. Jubril; Department of Electronic and Electrical Engineering, Obafemi Awolowo University, Nigeria; email: ajubril@oauife.edu.ng","","IOS Press BV","10554181","","TEDIF","","English","Technol. Disabil.","Article","Final","","Scopus","2-s2.0-85106911101"
"Siriboyina L.P.; Thadikemalla V.S.G.","Siriboyina, Leela Pravallika (57738184300); Thadikemalla, Venkata Sainath Gupta (57195199859)","57738184300; 57195199859","A Hybrid System to Assist Visually Impaired People","2021","SN Computer Science","2","4","333","","","","2","10.1007/s42979-021-00703-8","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85131789768&doi=10.1007%2fs42979-021-00703-8&partnerID=40&md5=c2f6f5951c8e94b494678990fff1bda6","Department of E.C.E, V. R. Siddhartha Engineering College, Kanuru, Andhra Pradesh, Vijayawada, 520070, India","Siriboyina L.P., Department of E.C.E, V. R. Siddhartha Engineering College, Kanuru, Andhra Pradesh, Vijayawada, 520070, India; Thadikemalla V.S.G., Department of E.C.E, V. R. Siddhartha Engineering College, Kanuru, Andhra Pradesh, Vijayawada, 520070, India","Blindness is one of the serious problems that is affecting the vast majority of people globally. Traditionally, the affected ones use white cane or a dog to assist them. However, they still have a lot of disadvantages. So, the aim of this paper is to develop a system to assist visually impaired people which acts as a navigation and obstacle/object recognition system. The system discussed in this paper is a hybrid guidance system that uses traditional stick and sensors (camera and ultrasonic) for coping in indoor and outdoor environments. The user is assisted without any human guidance by using a design with sensors and auditory feedback. © 2021, The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd.","Computer vision; Hybrid model; Raspberry Pi; Tensor flow; Ultrasonic sensor; Visually impaired","","V.S.G. Thadikemalla; Department of E.C.E, V. R. Siddhartha Engineering College, Vijayawada, Kanuru, Andhra Pradesh, 520070, India; email: sainathguptatv@gmail.com","","Springer","2662995X","","","","English","SN COMPUT. SCI.","Article","Final","","Scopus","2-s2.0-85131789768"
"","","","3rd Chinese Conference on Pattern Recognition and Computer Vision, PRCV 2020","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12306 LNCS","","","","","1936","0","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85093915021&partnerID=40&md5=14f9746d33306c82579ffc98fa15ec17","","","The proceedings contain 158 papers. The special focus in this conference is on Pattern Recognition and Computer Vision. The topics include: Underwater Image Processing by an Adversarial Network with Feedback Control; inception Parallel Attention Network for Small Object Detection in Remote Sensing Images; hyperspectral Image Denoising Based on Graph-Structured Low Rank and Non-local Constraint; multi-human Parsing with Pose and Boundary Guidance; m2E-Net: Multiscale Morphological Enhancement Network for Retinal Vessel Segmentation; DUDA: Deep Unsupervised Domain Adaptation Learning for Multi-sequence Cardiac MR Image Segmentation; learning from Rankings with Multi-level Features for No-Reference Image Quality Assessment; reversible Data Hiding Based on Prediction-Error-Ordering; aggregating Spatio-temporal Context for Video Object Segmentation; Position and Orientation Detection of Insulators in Arbitrary Direction Based on YOLOv3; R-PFN: Towards Precise Object Detection by Recurrent Pyramidal Feature Fusion; image Super-Resolution Based on Non-local Convolutional Neural Network; VH3D-LSFM: Video-Based Human 3D Pose Estimation with Long-Term and Short-Term Pose Fusion Mechanism; automatic Tooth Segmentation and 3D Reconstruction from Panoramic and Lateral Radiographs; unregistered Hyperspectral and Multispectral Image Fusion with Synchronous Nonnegative Matrix Factorization; Cloud Detection Algorithm Using Advanced Fully Convolutional Neural Networks in FY3D-MERSI Imagery; multi-layer Pointpillars: Multi-layer Feature Abstraction for Object Detection from Point Cloud; building Detection via Complementary Convolutional Features of Remote Sensing Images; hyperspectral Image Super-Resolution via Self-projected Smooth Prior; 3D Point Cloud Segmentation for Complex Structure Based on PointSIFT; completely Blind Image Quality Assessment with Visual Saliency Modulated Multi-feature Collaboration; blood Flow Velocity Detection of Nailfold Microcirculation Based on Spatiotemporal Analysis; blind Super-Resolution with Kernel-Aware Feature Refinement; preface.","","","","Peng Y.; Zha H.; Liu Q.; Lu H.; Sun Z.; Liu C.; Chen X.; Yang J.","Springer Science and Business Media Deutschland GmbH","03029743","978-303060638-1","","","English","Lect. Notes Comput. Sci.","Conference review","Final","","Scopus","2-s2.0-85093915021"
"Khan S.; Nazir S.; Khan H.U.","Khan, Sulaiman (57212048687); Nazir, Shah (56028678500); Khan, Habib Ullah (55507524000)","57212048687; 56028678500; 55507524000","Analysis of Navigation Assistants for Blind and Visually Impaired People: A Systematic Review","2021","IEEE Access","9","","9328100","26712","26734","22","77","10.1109/ACCESS.2021.3052415","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85099725046&doi=10.1109%2fACCESS.2021.3052415&partnerID=40&md5=9df851f84ead8e6ec3ff419383e2ce38","Department of Computer Science, University of Swabi, Swabi, 23430, Pakistan; Department of Accounting and Information Systems, College of Business and Economics, Qatar University, Doha, Qatar","Khan S., Department of Computer Science, University of Swabi, Swabi, 23430, Pakistan; Nazir S., Department of Computer Science, University of Swabi, Swabi, 23430, Pakistan; Khan H.U., Department of Accounting and Information Systems, College of Business and Economics, Qatar University, Doha, Qatar","Over the last few decades, the development in the field of navigation and routing devices has become a hindering task for the researchers to develop smart and intelligent guiding mechanism at indoor and outdoor locations for blind and visually impaired people (BVIPs). The existing research need to be analysed from a historical perception including early research on the first electronic travel aids to the use of modern artificial vision models for the navigation of BVIPs. Diverse approaches such as: e-cane or guide dog, infrared-based cane, laser based walker and many others are proposed for the navigation of BVIPs. But most of these techniques have limitations such as: infrared and ultrasonic based assistance has short range capacities for object detection. While laser based assistance can harm other people if it directly hit them on their eyes or any other part of the body. These trade-offs are critical to bring this technology in practice.To systematically assess, analyze, and identify the primary studies in this specialized field and provide an overview of the trends and empirical evidence in the proposed field. This systematic research work is performed by defining a set of relevant keywords, formulating four research questions, defining selection criteria for the articles, and synthesizing the empirical evidence in this area. Our pool of studies include 191 most relevant articles to the proposed field reported between 2011 and 2020 (a portion of 2020 is included). This systematic mapping will help the researchers, engineers, and practitioners to make more authentic decisions for finding gaps in the available navigation assistants and suggest a new and enhanced smart assistant application accordingly to ensure safety and accurate guidance of the BVIPs. This research work have several implications in particular the impact of reducing fatalities and major injuries of BVIPs.  © 2013 IEEE.","Blind and visually impaired people; healthcare; smart devices; systematic literature review","Economic and social effects; Object detection; Ultrasonic applications; Vision aids; Blind and visually impaired; Electronic travel aidss; Indoor and outdoor locations; Research questions; Selection criteria; Systematic mapping; Systematic research; Technology-in-practice; Navigation","H.U. Khan; Department of Accounting and Information Systems, College of Business and Economics, Qatar University, Doha, Qatar; email: habib.khan@qu.edu.qa","","Institute of Electrical and Electronics Engineers Inc.","21693536","","","","English","IEEE Access","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85099725046"
"Khairnar D.P.; Karad R.B.; Kapse A.; Kale G.; Jadhav P.","Khairnar, Devashish Pradeep (57218834519); Karad, Rushikesh Balasaheb (57218836765); Kapse, Apurva (57485930100); Kale, Geetanjali (55446294200); Jadhav, Prathamesh (57721044000)","57218834519; 57218836765; 57485930100; 55446294200; 57721044000","PARTHA: A Visually Impaired Assistance System","2020","2020 3rd International Conference on Communication Systems, Computing and IT Applications, CSCITA 2020 - Proceedings","","","9137791","32","37","5","18","10.1109/CSCITA47329.2020.9137791","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85090383126&doi=10.1109%2fCSCITA47329.2020.9137791&partnerID=40&md5=a4e3b3c79453ca1a3f4bdc0bd0b4b7e1","Pune Institute of Computer Technology, Pune, India; University of Pune, PICT, Pune, India","Khairnar D.P., Pune Institute of Computer Technology, Pune, India; Karad R.B., Pune Institute of Computer Technology, Pune, India; Kapse A., Pune Institute of Computer Technology, Pune, India; Kale G., University of Pune, PICT, Pune, India; Jadhav P., Pune Institute of Computer Technology, Pune, India","Blindness is very common and unendurable disability among many disabilities. According to the World Health Organization(WHO), there are 285 million visually impaired people [1]. The proposed VI (Visually Impaired) Assistant System is developed to assist the visually impaired people have four modules which are obstacle recognition, obstacle avoidance, indoor and outdoor navigation, and real-time location sharing. The proposed system is a combination of smart glove and smart-phone application which works fine in the low light level also. The smart glove as a part of the proposed solution is used to detect and avoid obstacles and to enable visually impaired people to identify the world around them. The smart-phone-based obstacle and object detection is used to detect various objects in the surrounding. The system also provides seamless indoor navigation implemented using available Wi-Fi access points. The system also provides security to the blind via real-time location sharing in an outdoor environment. Our proposed system is reliable, affordable, practical and feasible. © 2020 IEEE.","Arduino; Image Processing; indoor localization; Indoor Navigation; Obstacle detection; Speech Commands; Ultrasonic sensors; visually impaired people","Object detection; Smartphones; In-door navigations; Obstacle recognition; Outdoor environment; Real-time location; Smart-phone applications; Visually impaired people; Wi-fi access points; World Health Organization; Indoor positioning systems","","","Institute of Electrical and Electronics Engineers Inc.","","978-172812339-4","","","English","Int. Conf. Commun. Syst., Comput. IT Appl., CSCITA - Proc.","Conference paper","Final","","Scopus","2-s2.0-85090383126"
"Boyuan Z.","Boyuan, Zhang (56708901200)","56708901200","AI guide products system based on mental model","2020","Proceedings - 2020 International Conference on Robots and Intelligent Systems, ICRIS 2020","","","","240","243","3","0","10.1109/ICRIS52159.2020.00067","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85115604817&doi=10.1109%2fICRIS52159.2020.00067&partnerID=40&md5=8b4a6afa9fc99bf814e353669393688e","School of Fine Art, Qiannan Normal College for Nationalities, Guizhou, Duyun, 558000, China","Boyuan Z., School of Fine Art, Qiannan Normal College for Nationalities, Guizhou, Duyun, 558000, China","In order to overcome the difficulties of visually impaired users to integrate into the society, and to meet the life needs of visually impaired users, this paper proposes a novel AI guide products system based on mental model. The system fully combines big data technology and artificial intelligence, and uses user analytic hierarchy process (AHP) to analyze user needs and build user psychological model. Based on this, the system can also give full play to the interaction of big data and artificial intelligence to obtain users' thinking mode and living habits, and then develop a series of exclusive services for users. The research results show that the system can develop the exclusive services according to the needs of each visually impaired user, and then meet the individual needs of visually impaired users to the maximum extent.  © 2020 IEEE.","Handicapped; Out-going blind guide equipment; Smart travel introduction; User mental model","Agricultural robots; Analytic hierarchy process; Big data; Cognitive systems; Intelligent systems; Analytic hierarchy process (ahp); Data technologies; Mental model; Psychological model; Research results; Thinking modes; User need; Visually-impaired users; Intelligent robots","Z. Boyuan; School of Fine Art, Qiannan Normal College for Nationalities, Duyun, Guizhou, 558000, China; email: zhbyid@163.com","","Institute of Electrical and Electronics Engineers Inc.","","978-073812407-0","","","English","Proc. - Int. Conf. Robot. Intell. Syst., ICRIS","Conference paper","Final","","Scopus","2-s2.0-85115604817"
"Zhang H.; Ye C.","Zhang, He (57031171700); Ye, Cang (7202201245)","57031171700; 7202201245","A Visual Positioning System for Indoor Blind Navigation","2020","Proceedings - IEEE International Conference on Robotics and Automation","","","9196782","9079","9085","6","15","10.1109/ICRA40945.2020.9196782","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85092734634&doi=10.1109%2fICRA40945.2020.9196782&partnerID=40&md5=ccced2894650d19421609a96b68ac49c","Virginia Commonwealth University, Computer Science Department, Richmond, 23284, VA, United States","Zhang H., Virginia Commonwealth University, Computer Science Department, Richmond, 23284, VA, United States; Ye C., Virginia Commonwealth University, Computer Science Department, Richmond, 23284, VA, United States","This paper presents a visual positioning system (VPS) for real-time pose estimation of a robotic navigation aid (RNA) for assistive navigation. The core of the VPS is a new method called depth-enhanced visual-inertial odometry (DVIO) that uses an RGB-D camera and an inertial measurement unit (IMU) to estimate the RNA's pose. The DVIO method extracts the geometric feature (the floor plane) from the camera's depth data and integrates its measurement residuals with that of the visual features and the inertial data in a graph optimization framework for pose estimation. A new measure based on the Sampson error is introduced to describe the measurement residuals of the near-range visual features with a known depth and that of the far-range visual features whose depths are unknown. The measure allows for the incorporation of both types of visual features into graph optimization. The use of the geometric feature and the Sampson error improves pose estimation accuracy and precision. The DVIO method is paired with a particle filter localization (PFL) method to locate the RNA in a 2D floor plan and the information is used to guide a visually impaired person. The PFL reduces the RNA's position and heading error by aligning the camera's depth data with the floor plan map. Together, the DVIO and the PFL allow for accurate pose estimation for wayfinding and 3D mapping for obstacle avoidance. Experimental results demonstrate the usefulness of the RNA in assistive navigation in indoor spaces. © 2020 IEEE.","","Agricultural robots; Cameras; Errors; Floors; Navigation; RNA; Robotics; Accuracy and precision; Assistive navigations; Graph optimization; Inertial measurement unit; Measurement residual; Robotic navigation; Visual positioning; Visually impaired persons; Indoor positioning systems","","","Institute of Electrical and Electronics Engineers Inc.","10504729","978-172817395-5","PIIAE","","English","Proc IEEE Int Conf Rob Autom","Conference paper","Final","","Scopus","2-s2.0-85092734634"
"Khalid L.; Gong W.","Khalid, Laila (57222084993); Gong, Wei (57190345166)","57222084993; 57190345166","Vision4All - A Deep Learning Fashion Assistance Solution For Blinds","2022","2022 IEEE 5th International Conference on Artificial Intelligence and Big Data, ICAIBD 2022","","","","156","161","5","1","10.1109/ICAIBD55127.2022.9820475","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85134878661&doi=10.1109%2fICAIBD55127.2022.9820475&partnerID=40&md5=a8c97b9f930a178b17e591574bbe0f5a","University of Science and Technology of China, Dept. of Computer Science, Hefei, China","Khalid L., University of Science and Technology of China, Dept. of Computer Science, Hefei, China; Gong W., University of Science and Technology of China, Dept. of Computer Science, Hefei, China","Fashion is an industry that never appears to slow down, and it would be incredible if the blind could participate in this growing trend. By 2050, nearly 120 million individuals are expected to be vision-impaired. In this paper, the proposed Vision4All model assists users in identifying colors, clothing categories, textures, fabric, style, graphic, and text-based content on clothes. We enhanced FashionNet, a deep model that learns clothing characteristics by predicting garment qualities and categories together. To improve prediction accuracy, the ResNet34 architecture replaced the obsolete VGG16 design. A Fine-Grained multilabel classification model was trained by tackling the noisy data problem for attribute prediction. For identifying the range of graphical content printed on clothes, we use Pythia's modular re-implementation of the bottom-up, top-down approach. Our solution allows users to navigate through speech eliminating the requirement for users to rely on their vision. Vision4All is the first complete solution to align with Fashion assistance for the visually impaired.  © 2022 IEEE.","deep learning; neural networks; visually impaired","Deep neural networks; Forecasting; Classification models; Deep learning; Fine grained; Garment qualities; Learn+; Multi-label classifications; Neural-networks; Prediction accuracy; Vision impaired; Visually impaired; Textures","","","Institute of Electrical and Electronics Engineers Inc.","","978-166549913-2","","","English","IEEE Int. Conf. Artif. Intell. Big Data, ICAIBD","Conference paper","Final","","Scopus","2-s2.0-85134878661"
"Shandu N.E.; Owolawi P.A.; Mapayi T.; Odeyerni K.","Shandu, Nkosinathi Emmanuel (57219286874); Owolawi, Pius Adewale (35095525200); Mapayi, Temitope (56241764800); Odeyerni, Kehinde (59159478100)","57219286874; 35095525200; 56241764800; 59159478100","AI based pilot system for visually impaired people","2020","2020 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems, icABCD 2020 - Proceedings","","","9183857","","","","7","10.1109/icABCD49160.2020.9183857","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85092037929&doi=10.1109%2ficABCD49160.2020.9183857&partnerID=40&md5=3c8e4265ed086b39fcbfe235fef6d11e","Department of Computer Systems Engineering, Tshwane University of Technology, Pretoria, South Africa","Shandu N.E., Department of Computer Systems Engineering, Tshwane University of Technology, Pretoria, South Africa; Owolawi P.A., Department of Computer Systems Engineering, Tshwane University of Technology, Pretoria, South Africa; Mapayi T., Department of Computer Systems Engineering, Tshwane University of Technology, Pretoria, South Africa; Odeyerni K., Department of Computer Systems Engineering, Tshwane University of Technology, Pretoria, South Africa","In today's world we live with visually impaired people struggling to do things at their full potential as they lack sight of the environment they live in. Affected individuals are of ten seen with slips, trips and fall over light obstacles on their walkway. To some extent, these blind people cannot relate to any objects they come across such as cars and people around. As technology evolves, there has been numerous attempts in solving this problems for the affected group of people and the proposed solutions need further improvement on how to effectively assist the affected individuals to navigate from one place to the other using real-time updates of their whereabouts. This paper presents the design of an intelligent walking stick for the blind using Raspberry Pi 3 b+ as a central microcontroller, Ultrasonic sensors and Global Positioning System (GPS). The ultrasonic sensors are used for scanning the environment on walkway and sideways using sound waves at certain defined distances, and GPS module is used for real time directions and navigation. It also contains Bluetooth-headset that is used for the audio navigation through the aid of interpretation from the real time feed of ultrasonic sensors and coordinates from the GPS, thereby giving the user the actual route and possible turns until the destination point. Special emergency messages using keywords of the location coordinates obtained through GPS are sent via SMS subscription account connected to the GPIO pins of the Raspberry Pi to care-givers for tracking purposes when required. The entire setup of the intelligent walking stick makes navigation and tracking possible, thus effectively assisting the visually impaired people. ©2020 IEEE.","Artificial Intelligence (AI); Bluetooth; CNN; GPS; Raspberry Pi; Short Message Service (SMS); Ultrasonic Sensors","Big data; Convolutional codes; Data communication systems; Global positioning system; Navigation; Ultrasonic sensors; Walking aids; Audio navigation; Bluetooth headsets; Destination points; Emergency messages; Pilot system; Real-time updates; Slips , trips and falls; Visually impaired people; Artificial intelligence","","Pudaruth S.; Singh U.","Institute of Electrical and Electronics Engineers Inc.","","978-172816770-1","","","English","Int. Conf. Artif. Intell., Big Data, Comput. Data Commun. Syst., icABCD - Proc.","Conference paper","Final","","Scopus","2-s2.0-85092037929"
"Jayashree S.; Jaswanthi R.; Harshitha S.V.","Jayashree, S. (57291853400); Jaswanthi, R. (57292084500); Harshitha, S.V. (57291403400)","57291853400; 57292084500; 57291403400","A robotic assisted system using deep learning for navigation and reading IoT based home automation for visually impaired","2021","Advances in Parallel Computing","38","","","316","320","4","0","10.3233/APC210058","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85116945359&doi=10.3233%2fAPC210058&partnerID=40&md5=8f24d99640418108137de431a6d66b76","Dept of ECE, Panimalar Engineering College, Chennai, India","Jayashree S., Dept of ECE, Panimalar Engineering College, Chennai, India; Jaswanthi R., Dept of ECE, Panimalar Engineering College, Chennai, India; Harshitha S.V., Dept of ECE, Panimalar Engineering College, Chennai, India","This paper presents a personal guidance system to help the visually impaired people to face the world independently; this is achieved by transferring visual world into the audio world by alerting them with a voice message. Navigation assistance is included with object detection, so object interference in the path of navigation the vision deprived person is alerted about the object in the path. The system is also implemented with an IOT based load control to help them with household objects; the load control can be operated with the help of Blynk Application which is made enable with voice assistant. In this project we have used an algorithm, real time object detection using deep learning. This model is trained with more images to recognize an object. OCR is implemented for the reading assistance module in this system.  © 2021 The authors and IOS Press.","Assistive technology; Blind people navigation; Deep learning Raspberry pi 3processor; Reading assistant; Visually impaired","","S. Jayashree; Dept of ECE, Panimalar Engineering College, Chennai, India; email: jayashrees22@gmail.com","Ambeth Kumar V.D.; Malathi S.; Balas V.E.; Favorskaya M.; Perumal T.","IOS Press BV","09275452","978-164368202-0","","","English","Adv. Parallel Comput.","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85116945359"
"Chitra P.; Balamurugan V.; Sumathi M.; Mathan N.; Srilatha K.; Narmadha R.","Chitra, P. (57207591949); Balamurugan, V. (26428036800); Sumathi, M. (19934576600); Mathan, N. (56513992300); Srilatha, K. (38663247800); Narmadha, R. (57188727499)","57207591949; 26428036800; 19934576600; 56513992300; 38663247800; 57188727499","Voice Navigation Based guiding Device for Visually Impaired People","2021","Proceedings - International Conference on Artificial Intelligence and Smart Systems, ICAIS 2021","","","9395981","911","915","4","10","10.1109/ICAIS50930.2021.9395981","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85104930994&doi=10.1109%2fICAIS50930.2021.9395981&partnerID=40&md5=e570c171ae82d84eca0b391e568437bc","Sathyabama Institute of Science and Technology, Department of Ece, Chennai, India","Chitra P., Sathyabama Institute of Science and Technology, Department of Ece, Chennai, India; Balamurugan V., Sathyabama Institute of Science and Technology, Department of Ece, Chennai, India; Sumathi M., Sathyabama Institute of Science and Technology, Department of Ece, Chennai, India; Mathan N., Sathyabama Institute of Science and Technology, Department of Ece, Chennai, India; Srilatha K., Sathyabama Institute of Science and Technology, Department of Ece, Chennai, India; Narmadha R., Sathyabama Institute of Science and Technology, Department of Ece, Chennai, India","Navigation of visually impaired people is one of the important disputes that requires significant research consideration. The visually impaired users generally use white canes for obstacle detection by remembering all the familiar locations. In a new-fangled and unacquainted environment, they totally depend on individuals passing by to enquire for certain places. In this contemporary world along with various sensors, there should be a system with the most basic invention to make their life a bit tranquil. A contactless, hands free, LVU (Lidars and Vibrotactile Units), discrete wearable device was designed in this proposed work that allows blind people to detect obstacles. To provide a safe mobility for the impaired people, a suitable mobile assistance device is necessary. This paper propose a safe wearable device with audio output for benign local navigation in both inside and outdoor environment which help in assisting the user to discriminate free space from obstacles. The device presented is composed of wearable strap with sensors. By using TOF sensor attached in the front of the belt worn by the users, the pulses from the LiDAR provide a reliable and correct measurements of the distances between the handler and obstacles. The image captured by the camera is processed and classified by the convolution neural network algorithm. The identified image is given as an audio input to the audio jockey. The vibratory motor and voice intimation by audio jockey provides the haptic feedback when the disabled person reach the obstacle. The vibration motor is placed with a pretension point-loaded applicator to transmit the isolated vibrations to incapacitate person. Distance between the obstacle and the disabled person is measured by using LiDAR sensor and it will be given as a feedback to the visually impaired person with the voice input. Thus, this wearable device helps in assisting the visually impaired people in a more comfortable way than white canes.  © 2021 IEEE.","Blind people assistance device; LiDAR Sensor; Mobile assistance device; Obstacle Detection","Artificial intelligence; Audio equipment; Disabled persons; Feedback; Navigation; Obstacle detectors; Optical radar; Convolution neural network; Local navigation; Obstacle detection; Outdoor environment; Visually impaired people; Visually impaired persons; Visually-impaired users; Wearable devices; Wearable sensors","P. Chitra; Sathyabama Institute of Science and Technology, Department of Ece, Chennai, India; email: chitraperumal@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","","978-172819537-7","","","English","Proc. - Int. Conf. Artif. Intell. Smart Syst., ICAIS","Conference paper","Final","","Scopus","2-s2.0-85104930994"
"Xiao A.; Tong W.; Yang L.; Zeng J.; Li Z.; Sreenath K.","Xiao, Anxing (57218837091); Tong, Wenzhe (57937663900); Yang, Lizhi (57219166822); Zeng, Jun (57211020953); Li, Zhongyu (57221150964); Sreenath, Koushil (15728602700)","57218837091; 57937663900; 57219166822; 57211020953; 57221150964; 15728602700","Robotic Guide Dog: Leading a Human with Leash-Guided Hybrid Physical Interaction","2021","Proceedings - IEEE International Conference on Robotics and Automation","2021-May","","","5069","5074","5","51","10.1109/ICRA48506.2021.9561786","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85111238176&doi=10.1109%2fICRA48506.2021.9561786&partnerID=40&md5=08ac75ec5f20d200a480fbc5e0cf1a83","The Department of Mechanical Engineering, University of California, Berkeley, 94720, CA, United States","Xiao A., The Department of Mechanical Engineering, University of California, Berkeley, 94720, CA, United States; Tong W., The Department of Mechanical Engineering, University of California, Berkeley, 94720, CA, United States; Yang L., The Department of Mechanical Engineering, University of California, Berkeley, 94720, CA, United States; Zeng J., The Department of Mechanical Engineering, University of California, Berkeley, 94720, CA, United States; Li Z., The Department of Mechanical Engineering, University of California, Berkeley, 94720, CA, United States; Sreenath K., The Department of Mechanical Engineering, University of California, Berkeley, 94720, CA, United States","An autonomous robot that is able to physically guide humans through narrow and cluttered spaces could be a big boon to the visually-impaired. Most prior robotic guiding systems are based on wheeled platforms with large bases with actuated rigid guiding canes. The large bases and the actuated arms limit these prior approaches from operating in narrow and cluttered environments. We propose a method that introduces a quadrupedal robot with a leash to enable the robot-guiding-human system to change its intrinsic dimension (by letting the leash go slack) in order to fit into narrow spaces. We propose a hybrid physical Human Robot Interaction model that involves leash tension to describe the dynamical relationship in the robot-guiding-human system. This hybrid model is utilized in a mixed-integer programming problem to develop a reactive planner that is able to utilize slack-taut switching to guide a blind-folded person to safely travel in a confined space. The proposed leash-guided robot framework is deployed on a Mini Cheetah quadrupedal robot and validated in experiments (Video). © 2021 IEEE","","Human robot interaction; Robot programming; Guide dogs; Guiding systems; Human-systems; Narrow environment; Physical interactions; Quadrupedal robot; Robot guiding; Robotic guides; Visually impaired; Wheeled platforms; Integer programming","","","Institute of Electrical and Electronics Engineers Inc.","10504729","978-172819077-8","PIIAE","","English","Proc IEEE Int Conf Rob Autom","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85111238176"
"Ackattupathil M.; Levenshus E.; Lee E.-S.; Chawla G.","Ackattupathil, Manuel (57465264800); Levenshus, Eli (57466140700); Lee, Eun-Soe (57465700800); Chawla, Gautam (57465847000)","57465264800; 57466140700; 57465700800; 57465847000","3D Depth Imaging for Assistive Guidance","2020","2020 IEEE MIT Undergraduate Research Technology Conference, URTC 2020","","","","","","","2","10.1109/URTC51696.2020.9668899","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85125172846&doi=10.1109%2fURTC51696.2020.9668899&partnerID=40&md5=821dc14eb76b80bda0a521e6d1d4eb7e","Engineering Science (AS) Bergen Community College, Paramus, United States; Northern Valley Regional at Old Tappan (HS) Bergen Community College, Paramus, United States; Bergen County Academies (HS) Bergen Community College, Paramus, United States; Princeton (HS) Bergen Community College, Paramus, United States","Ackattupathil M., Engineering Science (AS) Bergen Community College, Paramus, United States; Levenshus E., Northern Valley Regional at Old Tappan (HS) Bergen Community College, Paramus, United States; Lee E.-S., Bergen County Academies (HS) Bergen Community College, Paramus, United States; Chawla G., Princeton (HS) Bergen Community College, Paramus, United States","The standard white cane is the most widely used mobility tool in the blind community. However, traditional cane travel can be difficult, as standard canes are unable to detect fluctuations in terrain. To bolster the capabilities of cane travel, a novel 'smart cane' has been developed, which implements realtime depth detection mechanisms to help improve the safety and efficiency of cane travel. This system consists of a 3D depth detecting camera, a single-board computer, and a haptic feedback device to locate objects and alert the user by triggering a vibration in real-time. With the use of a 5V 3A portable charger, the system is completely self-operating and can last up to five hours of usage before recharging. The combination of these tools will warn users of obstructions up to 1.5 meters away from them. This direct feedback mechanism can help the visually impaired navigate with confidence and ease.  © 2020 IEEE.","Biomedical engineering; computer vision; Electronic travel aid; White cane","Biomedical engineering; Assistive; Depth imaging; Detection mechanism; Electronic travel aidss; Real- time; Safety and efficiencies; Single board computers; Single-board computers; Smart canes; White cane; Computer vision","","","Institute of Electrical and Electronics Engineers Inc.","","978-172817571-3","","","English","IEEE MIT Undergrad. Res. Technol. Conf., URTC","Conference paper","Final","","Scopus","2-s2.0-85125172846"
"Manjari K.; Verma M.; Singal G.","Manjari, Kanak (57212198734); Verma, Madhushi (57210742799); Singal, Gaurav (56892765700)","57212198734; 57210742799; 56892765700","A survey on Assistive Technology for visually impaired","2020","Internet of Things (Netherlands)","11","","100188","","","","78","10.1016/j.iot.2020.100188","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85096917047&doi=10.1016%2fj.iot.2020.100188&partnerID=40&md5=d495f77d52cd582173fc1251cdd13fd5","Department of Computer Science Engineering, Bennett University, Greater Noida, India","Manjari K., Department of Computer Science Engineering, Bennett University, Greater Noida, India; Verma M., Department of Computer Science Engineering, Bennett University, Greater Noida, India; Singal G., Department of Computer Science Engineering, Bennett University, Greater Noida, India","One-Sixth of the world population is suffering from vision impairment, as per WHO's report. In the past decades, many efforts have been done in developing several devices to provide support to visually blind and enhance the quality of their lives by making them capable. Many of those devices are either heavy or costly for general purposes. In this paper, a detailed comparative study of all the relevant devices developed for them has been presented which are wearable and handheld. The focus was on the prominent features of those devices and analysis has also been performed based on few factors like power consumption, weight, economic and user-friendliness. The idea was to build a path for the researchers who are trying to work in this field by either developing a portable device or through some efficient algorithm to ensure independence, mobility, and safety for visually impaired persons. © 2020 Elsevier B.V.","Assistive Technology; Deep Learning; Electronic Travel Aids; Image Processing; Obstacle Avoidance; Obstacle Detection; Visually Impaired; Wearable Device","","K. Manjari; Department of Computer Science Engineering, Bennett University, Greater Noida, India; email: KM5723@bennett.edu.in","","Elsevier B.V.","25426605","","","","English","Internet. Thing.","Article","Final","","Scopus","2-s2.0-85096917047"
"Shah B.; Shah S.; Shah P.; Shah A.","Shah, Bela (57219993731); Shah, Smeet (57219992251); Shah, Purvesh (57219993191); Shah, Aneri (57664713700)","57219993731; 57219992251; 57219993191; 57664713700","Survey on Object Detection, Distance Estimation and Navigation Systems for Blind People","2021","Lecture Notes in Networks and Systems","141","","","463","472","9","1","10.1007/978-981-15-7106-0_46","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85096438947&doi=10.1007%2f978-981-15-7106-0_46&partnerID=40&md5=49adb6f10c48575c638cfe8c93772cec","The Maharaja Sayajirao University of Baroda, Vadodara, Gujarat, India","Shah B., The Maharaja Sayajirao University of Baroda, Vadodara, Gujarat, India; Shah S., The Maharaja Sayajirao University of Baroda, Vadodara, Gujarat, India; Shah P., The Maharaja Sayajirao University of Baroda, Vadodara, Gujarat, India; Shah A., The Maharaja Sayajirao University of Baroda, Vadodara, Gujarat, India","Loss of vision is a huge problem that is faced by many of the people around the world either from birth or due to some accident or else disease. Due to this, they face many difficulties while interacting with surrounding. In this paper, we have given a brief case study on the existing systems for object detection, distance estimation and navigation for blind and visually impaired people. Many systems have been developed using the electronic sensors or using the concepts of machine learning and deep learning to assist them. These new techniques are far more efficient and reliable than the prior methods like walking cane, guiding dogs, etc. We have also proposed a system based on machine learning integrated with a voice assistant-mobile-based application and external camera using existing methodologies. It aims to help blind people to identify nearby objects along with their respective distances. To achieve this operation, we will be using existing technologies YOLOv3 and DisNet based on neural networks. The system also makes traveling task from one place to another easier by suggesting the fastest transportation system available at that specific time. © 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Blind people; Distance estimation; Navigation; Neural networks; Object detection; Ultrasonic sensors; Visually impaired people","","S. Shah; The Maharaja Sayajirao University of Baroda, Vadodara, India; email: shahsmeet2804@gmail.com","Joshi A.; Khosravy M.; Gupta N.","Springer Science and Business Media Deutschland GmbH","23673370","978-981157105-3","","","English","Lect. Notes Networks Syst.","Conference paper","Final","","Scopus","2-s2.0-85096438947"
"Budrionis A.; Plikynas D.; Daniušis P.; Indrulionis A.","Budrionis, Andrius (55735006700); Plikynas, Darius (57207667356); Daniušis, Povilas (35409127300); Indrulionis, Audrius (57216490315)","55735006700; 57207667356; 35409127300; 57216490315","Smartphone-based computer vision travelling aids for blind and visually impaired individuals: A systematic review","2022","Assistive Technology","34","2","","178","194","16","38","10.1080/10400435.2020.1743381","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85083679792&doi=10.1080%2f10400435.2020.1743381&partnerID=40&md5=8014faf7173d1c9d41e6bf19c55f6395","Department of Business Technologies and Entrepreneurship, Vilnius Gediminas Technical University, Vilnius, Lithuania; Norwegian Centre for E-health Research, University Hospital of North Norway, Tromsø, Norway","Budrionis A., Department of Business Technologies and Entrepreneurship, Vilnius Gediminas Technical University, Vilnius, Lithuania, Norwegian Centre for E-health Research, University Hospital of North Norway, Tromsø, Norway; Plikynas D., Department of Business Technologies and Entrepreneurship, Vilnius Gediminas Technical University, Vilnius, Lithuania; Daniušis P., Department of Business Technologies and Entrepreneurship, Vilnius Gediminas Technical University, Vilnius, Lithuania; Indrulionis A., Department of Business Technologies and Entrepreneurship, Vilnius Gediminas Technical University, Vilnius, Lithuania","Given the growth in the numbers of visually impaired (VI) people in low-income countries, the development of affordable electronic travel aid (ETA) systems employing devices, sensors, and apps embedded in ordinary smartphones becomes a potentially cost-effective and reasonable all-in-one solution of utmost importance for the VI. This paper offers an overview of recent ETA research prototypes that employ smartphones for assisted orientation and navigation in indoor and outdoor spaces by providing additional information about the surrounding objects. Scientific achievements in the field were systematically reviewed using PRISMA methodology. Comparative meta-analysis showed how various smartphone-based ETA prototypes could assist with better orientation, navigation, and wayfinding in indoor and outdoor environments. The analysis found limited interest among researchers in combining haptic interfaces and computer vision capabilities in smartphone-based ETAs for the blind, few attempts to employ novel state-of-the-art computer vision methods based on deep neural networks, and no evaluations of existing off-the-shelf navigation solutions. These results were contrasted with findings from a survey of blind expert users on their problems in navigating in indoor and outdoor environments. This revealed a major mismatch between user needs and academic development in the field. © 2020 RESNA.","computer vision techniques; electronic travel aid; object recognition; obstacle detection; smartphone device","Blindness; Humans; Smartphone; Visually Impaired Persons; Cost effectiveness; Deep neural networks; Embedded systems; Haptic interfaces; Indoor positioning systems; Interface states; Navigation; Smartphones; Vision aids; Blind and visually impaired; Electronic travel aidss; Low income countries; Navigation solution; Orientation and navigation; Outdoor environment; Research prototype; Scientific achievements; blindness; human; meta analysis; smartphone; visually impaired person; Computer vision","A. Budrionis; Department of Business Technologies and Entrepreneurship, Vilnius Gediminas Technical University, Vilnius, Lithuania; email: Andrius.Budrionis@ehealthresearch.no","","Taylor and Francis Ltd.","10400435","","ASTEF","32207640","English","Assistive Technol.","Review","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85083679792"
"Palchunov D.; Tregubov A.S.","Palchunov, Dmitry (16414211000); Tregubov, A.S. (57539208400)","16414211000; 57539208400","Semantic methods of intelligent assistant developing","2021","2021 International Symposium on Knowledge, Ontology, and Theory, KNOTH 2021","","","","30","35","5","2","10.1109/KNOTH54462.2021.9686335","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85126804899&doi=10.1109%2fKNOTH54462.2021.9686335&partnerID=40&md5=29f0ed68f3eb0832377984493441cc64","Sobolev Institute of Mathematics, Laboratory of Computability Theory and Applied Logic, Novosibirsk, Russian Federation; Novosibirsk State University, Department of General Informatics, Novosibirsk, Russian Federation","Palchunov D., Sobolev Institute of Mathematics, Laboratory of Computability Theory and Applied Logic, Novosibirsk, Russian Federation; Tregubov A.S., Novosibirsk State University, Department of General Informatics, Novosibirsk, Russian Federation","Human-computer interaction with people whose visual perception is limited is possible only with tactile and voice interfaces, the latter are being used more and more recently. The aim of the work is to create an intelligent assistant for an indoor navigation system designed for blind and visually impaired people. The development of an intelligent assistant is based on a semantic user model and a four-level ontological model of the subject domain. To build a dialogue between an intelligent assistant and a user, we use the theory of speech acts, argumentation theory and case-based reasoning. The developed software system is aimed at identifying the desires and user needs and proposing possible user actions aimed at achieving them. The system allows for the decomposition of user tasks and the formation of a sequence of their execution based on semantic models of the user and the subject domain.  © 2021 IEEE.","argumentation theory; case-based reasoning; Intelligent assistant; intent recognition; machine learning; natural language processing; ontology","Case based reasoning; Human computer interaction; Learning algorithms; Machine learning; Natural language processing systems; Navigation systems; Semantics; Argumentation theory; Blind and visually impaired; Casebased reasonings (CBR); Indoor navigation system; Intelligent assistants; Intent recognition; Ontology's; Visual perception; Visually impaired people; Voice interfaces; Ontology","","","Institute of Electrical and Electronics Engineers Inc.","","978-166546719-3","","","English","Int. Symp. Knowl., Ontol., Theory, KNOTH","Conference paper","Final","","Scopus","2-s2.0-85126804899"
"Kuriakose B.; Shrestha R.; Sandnes F.E.","Kuriakose, Bineeth (57189239916); Shrestha, Raju (37016669000); Sandnes, Frode Eika (35594004500)","57189239916; 37016669000; 35594004500","SceneRecog: A Deep Learning Scene Recognition Model for Assisting Blind and Visually Impaired Navigate using Smartphones","2021","Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics","","","","2464","2470","6","11","10.1109/SMC52423.2021.9658913","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85124300114&doi=10.1109%2fSMC52423.2021.9658913&partnerID=40&md5=89cf765b875309fe8ec9e61749b96084","Oslo Metropolitan University (OsloMet), Department of Computer Science, Oslo, Norway","Kuriakose B., Oslo Metropolitan University (OsloMet), Department of Computer Science, Oslo, Norway; Shrestha R., Oslo Metropolitan University (OsloMet), Department of Computer Science, Oslo, Norway; Sandnes F.E., Oslo Metropolitan University (OsloMet), Department of Computer Science, Oslo, Norway","Deep learning models have recently gained popularity in the research community due to their high classification success rates. In this paper, we proposed an EfficientNet-Lite based scene recognition model for scene recognition as a part of the smartphone-based navigation support application for the blind and visually impaired. We created a custom dataset with both indoor and outdoor scenes for training and testing of the model. The main objective of this work is to support people with visual impairments navigate by providing information about the scene via a smartphone application. The results from the experiment show encouraging performance from the proposed model. As a proof of concept, a prototype app was developed on the Android platform. However, the model can be implemented and deployed in any modern smartphone with good processing power.  © 2021 IEEE.","assistive technology; deep learning; navigation; scene recognition; smartphone; visual impairment","Deep learning; Smartphones; Statistical tests; Assistive technology; Blind and visually impaired; Deep learning; Learning models; Navigation support; Recognition models; Research communities; Scene recognition; Smart phones; Visual impairment; Navigation","B. Kuriakose; Oslo Metropolitan University (OsloMet), Department of Computer Science, Oslo, Norway; email: bineethk@oslomet.no","","Institute of Electrical and Electronics Engineers Inc.","1062922X","978-166544207-7","PICYE","","English","Conf. Proc. IEEE Int. Conf. Syst. Man Cybern.","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85124300114"
"Rekik S.; Alaqeel L.; Kordi R.; Al-Rashoud S.","Rekik, Siwar (24528813900); Alaqeel, Lamya (57221009773); Kordi, Roaa (57221005992); Al-Rashoud, Sara (57220987168)","24528813900; 57221009773; 57221005992; 57220987168","Visually impaired assistance with arabic speech recognition on GPS","2020","2020 2nd International Conference on Computer and Information Sciences, ICCIS 2020","","","9257692","","","","0","10.1109/ICCIS49240.2020.9257692","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85097993893&doi=10.1109%2fICCIS49240.2020.9257692&partnerID=40&md5=0595c6b39aacd9eed34f1f4a5ad47a4e","College of Computer and Information Sciences, Imam Mohammad Bin Saud Islamic University, Riyadh, Saudi Arabia","Rekik S., College of Computer and Information Sciences, Imam Mohammad Bin Saud Islamic University, Riyadh, Saudi Arabia; Alaqeel L., College of Computer and Information Sciences, Imam Mohammad Bin Saud Islamic University, Riyadh, Saudi Arabia; Kordi R., College of Computer and Information Sciences, Imam Mohammad Bin Saud Islamic University, Riyadh, Saudi Arabia; Al-Rashoud S., College of Computer and Information Sciences, Imam Mohammad Bin Saud Islamic University, Riyadh, Saudi Arabia","People who have impaired vision regularly need a guide to assist in obstacle avoidance. Several electronic devices are currently used to provide guidance for a remote location. One of the latest trends in technology is Automatic Speech Recognition (ASR) which has become a primary communication tool for special needs people such as visually impaired and blind people. Nowadays, these people in Saudi Arabia could not find public places offering services such as braille readings on menus and flyers, sound facilities, ease of movements, health care and so on. Our application (Ayn) provides a database of several locations and all the information needed. This project investigated the suitability of a user-centered and client-server approach for the development of a talking GPS planned to fill a niche for outdoor wayfinding. We highlight the importance of having more places serving blind people in public places such as restaurants, centers, hospitals, parks... etc. The developed application used a speech-recognition speech-synthesis interface. The prototype solution incorporates a custom web application that accesses the Google Maps API. The system is intended to be scalable and extensible with additional features. The quality of Arabic speech recognition is improved over Google Speech Recognition API for Arabic using one of the machine learning algorithms: Artificial Neural Network (ANN). © 2020 IEEE.","Automatic speech recognition; Global Positioning System; GPS; Impaired vision; Navigation; Speech recognition","Application programming interfaces (API); Learning algorithms; Machine learning; Neural networks; Speech communication; Speech synthesis; Arabic speech recognition; Automatic speech recognition; Braille readings; Communication tools; Developed applications; Electronic device; Provide guidances; Visually impaired; Speech recognition","S. Rekik; College of Computer and Information Sciences, Imam Mohammad Bin Saud Islamic University, Riyadh, Saudi Arabia; email: sarekik@imamu.edu.sa","","Institute of Electrical and Electronics Engineers Inc.","","978-172815467-1","","","English","Int. Conf. Comput. Inf. Sci., ICCIS","Conference paper","Final","","Scopus","2-s2.0-85097993893"
"Bhole S.; Dhok A.","Bhole, Swapnil (57205419085); Dhok, Aniket (57215871271)","57205419085; 57215871271","Deep Learning based Object Detection and Recognition Framework for the Visually-Impaired","2020","Proceedings of the 4th International Conference on Computing Methodologies and Communication, ICCMC 2020","","","9076530","725","728","3","10","10.1109/ICCMC48092.2020.ICCMC-000135","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85084645948&doi=10.1109%2fICCMC48092.2020.ICCMC-000135&partnerID=40&md5=163daa8098c49ce3afec3e054b9c3271","NIT, Dept. of ECE, Nagpur, India","Bhole S., NIT, Dept. of ECE, Nagpur, India; Dhok A., NIT, Dept. of ECE, Nagpur, India","Vision impairment or blindness is one of the top ten disabilities in humans, and unfortunately, India is home to the world's largest visually impaired population. In this study, we present a novel framework to assist the visually impaired in object detection and recognition, so that they can independently navigate, and be aware of their surroundings. The paper employs transfer learning on Single-Shot Detection (SSD) mechanism for object detection and classification, followed by recognition of human faces and currency notes, if detected, using Inception v3 model. SSD detector is trained on modified PASCAL VOC 2007 dataset, in which a new class is added, to enable the detection of currency as well. Furthermore, separate Inception v3 models are trained to recognize human faces and currency notes, thus making the framework scalable and adaptable according to the user preferences. Ultimately, the output from the framework can then be presented to the visually impaired person in audio format. Mean Accuracy and Precision (mAP) scores of standalone SSD detector of the added currency class was 67.8 percent, and testing accuracy of person and currency recognition of Inception v3 model were 92.5 and 90.2 percent respectively. © 2020 IEEE.","convolutional neural network; Inception v3; SSD; transfer learning","Deep learning; Object recognition; Transfer learning; Accuracy and precision; Currency recognition; Object detection and recognition; Single shots; Testing accuracy; Vision impairments; Visually impaired; Visually impaired persons; Object detection","","","Institute of Electrical and Electronics Engineers Inc.","","978-172814889-2","","","English","Proc. Int. Conf. Comput. Methodol. Commun., ICCMC","Conference paper","Final","","Scopus","2-s2.0-85084645948"
"Pundlik S.; Baliutaviciute V.; Moharrer M.; Bowers A.R.; Luo G.","Pundlik, Shrinivas (8728311600); Baliutaviciute, Vilte (57213351835); Moharrer, Mojtaba (57195987227); Bowers, Alex R. (7005211077); Luo, Gang (36499782000)","8728311600; 57213351835; 57195987227; 7005211077; 36499782000","Data acquisition, processing, and reduction for home-use trial of a wearable video camera-based mobility aid","2020","Translational Vision Science and Technology","9","7","14","1","11","10","2","10.1167/tvst.9.7.14","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85088229873&doi=10.1167%2ftvst.9.7.14&partnerID=40&md5=17621f03d97221ae6bb540aea84a5b26","Schepens Eye Research Institute of Mass Eye & Ear, Boston, MA, United States; Department of Ophthalmology, Harvard Medical School, Boston, MA, United States","Pundlik S., Schepens Eye Research Institute of Mass Eye & Ear, Boston, MA, United States, Department of Ophthalmology, Harvard Medical School, Boston, MA, United States; Baliutaviciute V., Schepens Eye Research Institute of Mass Eye & Ear, Boston, MA, United States, Department of Ophthalmology, Harvard Medical School, Boston, MA, United States; Moharrer M., Schepens Eye Research Institute of Mass Eye & Ear, Boston, MA, United States, Department of Ophthalmology, Harvard Medical School, Boston, MA, United States; Bowers A.R., Schepens Eye Research Institute of Mass Eye & Ear, Boston, MA, United States, Department of Ophthalmology, Harvard Medical School, Boston, MA, United States; Luo G., Schepens Eye Research Institute of Mass Eye & Ear, Boston, MA, United States, Department of Ophthalmology, Harvard Medical School, Boston, MA, United States","Purpose: Evaluating mobility aids in naturalistic conditions across many days is challenging owing to the sheer amount of data and hard-to-control environments. For a wearable video camera-based collision warning device, we present the methodology for acquisition, reduction, review, and coding of video data for quantitative analyses of mobility outcomes in blind and visually impaired participants. Methods: Scene videos along with collision detection information were obtained from a chest-mounted collision warning device during daily use of the device. The recorded data were analyzed after use. Collision risk events flagged by the device were manually reviewed and coded using a detailed annotation protocol by two independent masked reviewers. Data reduction was achieved by predicting agreements between reviewers based on a machine learning algorithm. Thus, only those events for which disagreements were predicted would be reviewed by the second reviewer. Finally, the ultimate disagreements were resolved via consensus, and mobility-related outcome measures such as percentage of body contacts were obtained. Results: There were 38 hours of device use from 10 participants that were reviewed by both reviewers, with an agreement level of 0.66 for body contacts. The machine learning algorithm trained on 2714 events correctly predicted 90.5% of disagreements. For another 1943 events, the trained model successfully predicted 82% of disagreements, resulting in 81% data reduction. Conclusions: The feasibility of mobility aid evaluation based on a large volume of naturalistic data is demonstrated. Machine learning–based disagreement prediction can lead to data reduction. Translational Relevance: These methods provide a template for determining the real-world benefit of a mobility aid. © 2020, Association for Research in Vision and Ophthalmology Inc. All rights reserved.","Mobility aid; Naturalistic mobility; Wearable video camera","adult; algorithm; article; clinical article; consensus; feasibility study; female; human; human experiment; machine learning; male; outcome assessment; prediction; thorax; videorecording","S. Pundlik; Schepens Eye Research Institute of Mass Eye & Ear, Boston, United States; email: shrinivas_pundlik@meei.harvard.edu; S. Pundlik; Department of Ophthalmology, Harvard Medical School, Boston, United States; email: shrinivas_pundlik@meei.harvard.edu","","Association for Research in Vision and Ophthalmology Inc.","21642591","","","","English","Translational Vis. Sci. Technol.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85088229873"
"Boddupalli H.; Manikandan V.M.","Boddupalli, Hemanthsrisai (57486420300); Manikandan, V.M. (58626529300)","57486420300; 58626529300","A Detailed Review of Recent Advancements in Assistive Technologies for Blind People","2022","Lecture Notes in Networks and Systems","417 LNNS","","","326","335","9","0","10.1007/978-3-030-96302-6_30","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85126218258&doi=10.1007%2f978-3-030-96302-6_30&partnerID=40&md5=221f798068a94748406c68db866aee18","SRM University-AP, Andhra Pradesh, Amaravati, India","Boddupalli H., SRM University-AP, Andhra Pradesh, Amaravati, India; Manikandan V.M., SRM University-AP, Andhra Pradesh, Amaravati, India","According to late insights, worldwide, essentially 2.2 billion individuals have some sort of difficulty related to the visual system. The blind people are facing various issues to travel from one place to another, to interact the with the other people in a social event, the blind people may completely fail to understand the emotions of the people with them. The advancement in technology gives various kinds of support for blind people. This manuscript gives a detailed review of various tools and technologies available for assisting blind people. This study deals with the techniques currently under research in this field, the products available in the market which will be useful for blind people, the cost details, and the other advancements in this area. This manuscript will be a useful reference for the researchers who wish to work to design and implement some new products to assist blind people. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Assistive technology; Blind people; Machine learning; Vision impaired","","V.M. Manikandan; SRM University-AP, Amaravati, Andhra Pradesh, India; email: manikandan.v@srmap.edu.in","Abraham A.; Engelbrecht A.; Scotti F.; Gandhi N.; Manghirmalani Mishra P.; Fortino G.; Sakalauskas V.; Pllana S.","Springer Science and Business Media Deutschland GmbH","23673370","978-303096301-9","","","English","Lect. Notes Networks Syst.","Conference paper","Final","","Scopus","2-s2.0-85126218258"
"Rajkumar K.; Thejaswini K.; Subarna S.; Yuvashri P.","Rajkumar, K. (57216021502); Thejaswini, K. (57224425157); Subarna, S. (57226708950); Yuvashri, P. (57226716254)","57216021502; 57224425157; 57226708950; 57226716254","Walk Assistance for Outwardly Challenged People","2021","Journal of Physics: Conference Series","1979","1","012065","","","","3","10.1088/1742-6596/1979/1/012065","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85112428560&doi=10.1088%2f1742-6596%2f1979%2f1%2f012065&partnerID=40&md5=9453d9a1c2cff2fd51fd3fd336c6f56c","Eee, Sri Sairam Engineering College, Chennai, India","Rajkumar K., Eee, Sri Sairam Engineering College, Chennai, India; Thejaswini K., Eee, Sri Sairam Engineering College, Chennai, India; Subarna S., Eee, Sri Sairam Engineering College, Chennai, India; Yuvashri P., Eee, Sri Sairam Engineering College, Chennai, India","The blind's competency is to navigate to a particular place and to commence their daily activities is of decisive importance for their prosperity. It is estimated about one billion people are blind out of 285 million people visually impaired of all ages, according to the statistics of World Health Organization. This work includes an affordable and more efficient navigation aid for the blind which provides artificial vision by providing knowledge about an environmental scenario of static and dynamic characteristics of objects around them. This system induces a smart cane with ultrasonic sensors placed to intimate the intermediaries to their acknowledgment through Bluetooth. ZIGBEE used in this project so as to met the sensor and control device communication standards for navigation for the blind with the smart cane via Google maps their destination.  © Published under licence by IOP Publishing Ltd.","Arduino microcontroller; Navigation aid; Smart cane; Ultrasonic sensor; Zigbee receiver","Communication standards; Control device; Daily activity; Decisive importance; Navigation aids; Static and dynamic characteristics; Visually impaired; World Health Organization; Physics","","Rajesh M.; Gnanasekar J.M.; Sitharthan R.","IOP Publishing Ltd","17426588","","","","English","J. Phys. Conf. Ser.","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85112428560"
"","","","3rd Chinese Conference on Pattern Recognition and Computer Vision, PRCV 2020","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12307 LNCS","","","","","1936","0","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85094139686&partnerID=40&md5=870540f31eafc518d3b15989e79b4673","","","The proceedings contain 158 papers. The special focus in this conference is on Pattern Recognition and Computer Vision. The topics include: Underwater Image Processing by an Adversarial Network with Feedback Control; inception Parallel Attention Network for Small Object Detection in Remote Sensing Images; hyperspectral Image Denoising Based on Graph-Structured Low Rank and Non-local Constraint; multi-human Parsing with Pose and Boundary Guidance; m2E-Net: Multiscale Morphological Enhancement Network for Retinal Vessel Segmentation; DUDA: Deep Unsupervised Domain Adaptation Learning for Multi-sequence Cardiac MR Image Segmentation; learning from Rankings with Multi-level Features for No-Reference Image Quality Assessment; reversible Data Hiding Based on Prediction-Error-Ordering; aggregating Spatio-temporal Context for Video Object Segmentation; Position and Orientation Detection of Insulators in Arbitrary Direction Based on YOLOv3; R-PFN: Towards Precise Object Detection by Recurrent Pyramidal Feature Fusion; image Super-Resolution Based on Non-local Convolutional Neural Network; VH3D-LSFM: Video-Based Human 3D Pose Estimation with Long-Term and Short-Term Pose Fusion Mechanism; automatic Tooth Segmentation and 3D Reconstruction from Panoramic and Lateral Radiographs; unregistered Hyperspectral and Multispectral Image Fusion with Synchronous Nonnegative Matrix Factorization; Cloud Detection Algorithm Using Advanced Fully Convolutional Neural Networks in FY3D-MERSI Imagery; multi-layer Pointpillars: Multi-layer Feature Abstraction for Object Detection from Point Cloud; building Detection via Complementary Convolutional Features of Remote Sensing Images; hyperspectral Image Super-Resolution via Self-projected Smooth Prior; 3D Point Cloud Segmentation for Complex Structure Based on PointSIFT; completely Blind Image Quality Assessment with Visual Saliency Modulated Multi-feature Collaboration; blood Flow Velocity Detection of Nailfold Microcirculation Based on Spatiotemporal Analysis; blind Super-Resolution with Kernel-Aware Feature Refinement; preface.","","","","Peng Y.; Zha H.; Liu Q.; Lu H.; Sun Z.; Liu C.; Chen X.; Yang J.","Springer Science and Business Media Deutschland GmbH","03029743","978-303060635-0","","","English","Lect. Notes Comput. Sci.","Conference review","Final","","Scopus","2-s2.0-85094139686"
"Afif M.; Ayachi R.; Said Y.; Atri M.","Afif, Mouna (57194068439); Ayachi, Riadh (57210106980); Said, Yahia (53867137900); Atri, Mohamed (23017853700)","57194068439; 57210106980; 53867137900; 23017853700","A Transfer Learning Approach for Indoor Object Identification","2021","SN Computer Science","2","6","424","","","","4","10.1007/s42979-021-00790-7","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85131839637&doi=10.1007%2fs42979-021-00790-7&partnerID=40&md5=efa882a1bd1b2996eb7a13d298163208","Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia; Electrical Engineering Department, College of Engineering, Northern Border University, Arar, Saudi Arabia; College of Computer Science, King Khalid University, Abha, Saudi Arabia","Afif M., Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia; Ayachi R., Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia; Said Y., Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia, Electrical Engineering Department, College of Engineering, Northern Border University, Arar, Saudi Arabia; Atri M., College of Computer Science, King Khalid University, Abha, Saudi Arabia","Accessing new indoor environments is a well-known challenge for blind and visually impaired persons (VIP). In this work, we propose a new computer vision-based indoor object recognition system used especially for indoor wayfinding and indoor assistance navigation. The proposed recognition system is based on transfer learning techniques. This system is able to detect with a big performance three categories of indoor classes (door, stairs and sign). We developed an efficient and a robust indoor landmark identification system based on a lightweight deep convolutional neural network (DCNN). The proposed detection system is generic and performant enough to handle the large intra-class variation provided in the training and the testing sets. Experimental results have shown the big efficiency of the obtained systems by achieving high recognition rates. © 2021, The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd.","Blind and visually impaired people (VIP); Deep convolutional neural network (DCNN); Deep learning; Indoor object recognition","","M. Afif; Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences of Monastir, University of Monastir, Monastir, Tunisia; email: mouna.afif@outlook.fr","","Springer","2662995X","","","","English","SN COMPUT. SCI.","Article","Final","","Scopus","2-s2.0-85131839637"
"Aharchi M.; Kbir M.’A.","Aharchi, Moncef (57213270690); Kbir, M.’hamed Ait (18036420700)","57213270690; 18036420700","Localization and Navigation System for Blind Persons Using Stereo Vision and a GIS","2022","Lecture Notes in Electrical Engineering","745","","","365","376","11","2","10.1007/978-981-33-6893-4_35","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85113318805&doi=10.1007%2f978-981-33-6893-4_35&partnerID=40&md5=d784d43fead17a26468a20e77f8df9a0","LIST Laboratory, Faculty of Sciences and Technologies, Tangier, Morocco","Aharchi M., LIST Laboratory, Faculty of Sciences and Technologies, Tangier, Morocco; Kbir M.’A., LIST Laboratory, Faculty of Sciences and Technologies, Tangier, Morocco","Loss of vision caused by infectious diseases has decreased significantly; however aging will increase the risk that more people acquire vision impairment. Visual information is the basis of most navigation tasks; a person is considered visually impaired when he has no appropriate information on the surrounding environment. With the latest evolution of digital technologies, the assistance provided to visually disabled people during their mobility can be improved. In this context, we propose a system to help the visually impaired move quickly and to know their environment. Indoors, the system uses a stereoscopic camera, a portable computer, and a headset to direct and help visually impaired persons navigate comfortably and securely in familiar and unfamiliar environments. Outdoors, a GPS is used as a positioning method to keep the visually impaired person on the right path; with its dynamic routing and rerouting capabilities, it provides the user with an optimized path. The system can work on an outdoor and indoor environment. A stereoscopic camera is used to detect visual indicators that are used to trace and validate user navigation, provide accurate indoor location measurements, and recognize objects in front of the user. This article is mainly focused on this system and detailed outlines description. © 2022, Springer Nature Singapore Pte Ltd.","Accessibility; Autonomy; Blind Navigation; Computer Vision; Geographic information system","Cameras; Embedded systems; Indoor positioning systems; Intelligent systems; Microcomputers; Navigation systems; Stereo image processing; Digital technologies; Indoor environment; Localization and navigation systems; Positioning methods; Stereoscopic camera; Surrounding environment; Visual information; Visually impaired persons; Stereo vision","M. Aharchi; LIST Laboratory, Faculty of Sciences and Technologies, Tangier, Morocco; email: maharchi@uae.ac.ma","Bennani S.; Lakhrissi Y.; Khaissidi G.; Mansouri A.; Khamlichi Y.","Springer Science and Business Media Deutschland GmbH","18761100","978-981336892-7","","","English","Lect. Notes Electr. Eng.","Conference paper","Final","","Scopus","2-s2.0-85113318805"
"Audomphon A.; Apavatjrut A.","Audomphon, Apiched (57219245722); Apavatjrut, Anya (24490860800)","57219245722; 24490860800","Smart Glasses for Sign Reading as Mobility Aids for the Blind Using a Light Communication System","2020","17th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology, ECTI-CON 2020","","","9158250","615","618","3","2","10.1109/ECTI-CON49241.2020.9158250","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85091852810&doi=10.1109%2fECTI-CON49241.2020.9158250&partnerID=40&md5=4f05229e7ee23a8d28a1ca5467924850","Northern Region 1, Institute of Vocational Education, Chiang Mai, Thailand; Chiang Mai University, Computer Engineering, Chiang Mai, Thailand","Audomphon A., Northern Region 1, Institute of Vocational Education, Chiang Mai, Thailand; Apavatjrut A., Chiang Mai University, Computer Engineering, Chiang Mai, Thailand","Vision impairment directly impacts the quality of life since the absence of vision limits proficiency i n various activities, especially those that require mobility. Mobility aid tools can provide useful information for movement and orientation. Many assistive tools have previously been proposed to serve obstacle detection, traffic pattern interpretation, and orientation assistance. However, many of these appliances are expensive and complex since they usually require high computational cost from image processing, obstacle detection, virtual reality, etc. In this work, we want to propose a design of a low cost and low complexity mobility aid platform. Our platform is composed of a pair of smart glasses and an illuminated sign. The light emit from the sign are modulated to issue warnings and instructions. The pair of smart glasses equipped with a photodetector is used as an optical signal receiver. This can interpret the sign and issue a warning through an earphone. Our objective is to make the system user-friendly. It is initially intended to be deployed in a school for the blind to facilitate the movement of the students within and between the buildings.  © 2020 IEEE.","light communication; optical transmission; smart glasses; visually impairment","Costs; Image processing; Obstacle detectors; Optical data processing; Pattern recognition; Signal receivers; Assistive tool; Computational costs; Mobility aids; Obstacle detection; Quality of life; Traffic pattern; User friendly; Vision impairments; Glass","","","Institute of Electrical and Electronics Engineers Inc.","","978-172816486-1","","","English","Int. Conf. Electr. Eng./Electron., Comput., Telecommun. Inf. Technol., ECTI-CON","Conference paper","Final","","Scopus","2-s2.0-85091852810"
"Swathi K.; Vamsi B.; Rao N.T.","Swathi, Kalam (57211294150); Vamsi, Bandi (57222591822); Rao, Nakka Thirupathi (57195593290)","57211294150; 57222591822; 57195593290","A Deep Learning-Based Object Detection System for Blind People","2021","Lecture Notes in Networks and Systems","210 LNNS","","","223","231","8","4","10.1007/978-981-16-1773-7_18","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85111359225&doi=10.1007%2f978-981-16-1773-7_18&partnerID=40&md5=140ee5f4e8221fec967467856ed6167a","Department of Computer Science and Engineering, Vignan’s Institute of Information Technology, Visakhapatnam, 530049, India","Swathi K., Department of Computer Science and Engineering, Vignan’s Institute of Information Technology, Visakhapatnam, 530049, India; Vamsi B., Department of Computer Science and Engineering, Vignan’s Institute of Information Technology, Visakhapatnam, 530049, India; Rao N.T., Department of Computer Science and Engineering, Vignan’s Institute of Information Technology, Visakhapatnam, 530049, India","Visual impairment is one of the top disabilities among men and women across the world of all ages. Object detection is the primary task for them, and it can be implemented by deep learning techniques. Earlier implementation techniques involve in object detection with a strategy of single labeling. The proposed model uses classification techniques which reduce the recognize time of multi-objects with best time complexities and can help the visually impaired people in assisting the accurate navigation, in both indoor and outdoor circumstances. The proposed hybrid model is a combination of U-Net with base as residual network (ResNet) which improves accuracy in detection of objects in indoor and outdoor for visually impaired people. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Deep learning; Object detection; ResNet; U-Net; Visually impaired people","","K. Swathi; Department of Computer Science and Engineering, Vignan’s Institute of Information Technology, Visakhapatnam, 530049, India; email: swathi.kalam@gmail.com","Saha S.K.; Pang P.S.; Bhattacharyya D.","Springer Science and Business Media Deutschland GmbH","23673370","978-981161772-0","","","English","Lect. Notes Networks Syst.","Conference paper","Final","","Scopus","2-s2.0-85111359225"
"Merchan F.; Poveda M.; Cáceres-Hernández D.E.; Sanchez-Galan J.E.","Merchan, Fernando (36768533500); Poveda, Martin (57211348099); Cáceres-Hernández, Danilo E. (36801666000); Sanchez-Galan, Javier E. (55746624600)","36768533500; 57211348099; 36801666000; 55746624600","Indoor navigation aid systems for the blind and visually impaired based on depth sensors","2021","Examining Optoelectronics in Machine Vision and Applications in Industry 4.0","","","","187","223","36","2","10.4018/978-1-7998-6522-3.ch007","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85137433674&doi=10.4018%2f978-1-7998-6522-3.ch007&partnerID=40&md5=ce67c7cbc5cf7c5f1314fafb9b7051c9","Universidad Tecnologica de Panamá, Panama; Electrical Engineering Department, Universidad Tecnológica de Panamá, Panama","Merchan F., Universidad Tecnologica de Panamá, Panama; Poveda M., Universidad Tecnologica de Panamá, Panama; Cáceres-Hernández D.E., Electrical Engineering Department, Universidad Tecnológica de Panamá, Panama; Sanchez-Galan J.E., Universidad Tecnologica de Panamá, Panama","This chapter focuses on the contributions made in the development of assistive technologies for the navigation of blind and visually impaired (BVI) individuals. A special interest is placed on vision-based systems that make use of image (RGB) and depth (D) information to assist their indoor navigation. Many commercial RGB-D cameras exist on the market, but for many years the Microsoft Kinect has been used as a tool for research in this field. Therefore, first-hand experience and advances on the use of Kinect for the development of an indoor navigation aid system for BVI individuals is presented. Limitations that can be encountered in building such a system are addressed at length. Finally, an overview of novel avenues of research in indoor navigation for BVI individuals such as integration of computer vision algorithms, deep learning for the classification of objects, and recent developments with stereo depth vision are discussed. © 2021, IGI Global.","","","","","IGI Global","","978-179986524-7; 978-179986522-3","","","English","Examining Optoelectron. in Mach. Vision and Appl. in Ind. 4.0","Book chapter","Final","","Scopus","2-s2.0-85137433674"
"Varpe K.M.; Dhore M.L.","Varpe, K.M. (57216790581); Dhore, M.L. (15057690200)","57216790581; 15057690200","A review of scene text recognition systems for blind","2020","International Journal of Advanced Science and Technology","29","9 Special Issue","","2316","2323","7","2","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85084663012&partnerID=40&md5=22cf12fa09573ccb5027a0e68f4a45fb","Computer Engineering, Vishwakarma Institute of Technology, Pune, India; Computer Engineering, SKN College of Engineering, Pune, India","Varpe K.M., Computer Engineering, Vishwakarma Institute of Technology, Pune, India, Computer Engineering, SKN College of Engineering, Pune, India; Dhore M.L., Computer Engineering, Vishwakarma Institute of Technology, Pune, India","Various portable or wearable assistive systems have been developed by researchers to help blind people in indoor or outdoor environments. In an effort to analyses about the capabilities of existing systems and track research progress, this paper summarizes studies that have been conducted to assist visually impaired people in two categories: electronic navigation aids and scene text recognition systems. It also describes advantages and disadvantages of the methods used in these studies. This review paper is first of its kind to summarize both sensor-based systems and machine vision-based systems for blind people. Further study also extends challenges faced by real-time scene text recognition methods and presents future research directions to overcome them. © 2019 SERSC.","Assistive devices; Navigation aid; Optical character recognition (OCR); Scene text detection and recognition; Text reading","","","","Science and Engineering Research Support Society","20054238","","","","English","Int. J. Adv. Sci. Technol.","Review","Final","","Scopus","2-s2.0-85084663012"
"Joshi R.; Tripathi M.; Kumar A.; Gaur M.S.","Joshi, Rashika (57219121258); Tripathi, Meenakshi (56038579300); Kumar, Amit (58944211500); Gaur, Manoj Singh (11339605200)","57219121258; 56038579300; 58944211500; 11339605200","Object Recognition and Classification System for Visually Impaired","2020","Proceedings of the 2020 IEEE International Conference on Communication and Signal Processing, ICCSP 2020","","","9182077","1568","1572","4","14","10.1109/ICCSP48568.2020.9182077","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85091337480&doi=10.1109%2fICCSP48568.2020.9182077&partnerID=40&md5=bdfd2454b17195f3a39ce340c35c6ebb","MNIT, JRF, Department of computer science, Jaipur, India; MNIT, Department of computer science, Jaipur, India; IIIT, Department of computer science, Kota, India; IIT, Jammu, India","Joshi R., MNIT, JRF, Department of computer science, Jaipur, India; Tripathi M., MNIT, Department of computer science, Jaipur, India; Kumar A., IIIT, Department of computer science, Kota, India; Gaur M.S., IIT, Jammu, India","There have been diverse accounts of research relating to navigation assistance for the visually impaired. Using the object recognition approach, we aim to assist the blind to travel independently with the ability to identify any obstacles the the path. The subject now is to identify a solution that is low powered, easily portable and still effective. The system consists of Jetson Nano ported with the trained deep learning model and is interfaced with camera, that acts as a easy-to-use platform for object recognition, speech processing and image classification. Tests indicate that system is accurate and functions as navigation assistant. © 2020 IEEE.","Google colab; Jetson Nano; MobileNet; SSD; voice Assistant","Deep learning; Speech processing; Speech recognition; Classification system; Learning models; Visually impaired; Object recognition","","","Institute of Electrical and Electronics Engineers Inc.","","978-172814988-2","","","English","Proc. IEEE Int. Conf. Commun. Signal Process., ICCSP","Conference paper","Final","","Scopus","2-s2.0-85091337480"
"Pardasani A.; Indi P.N.; Banerjee S.; Kamal A.; Garg V.","Pardasani, Arjun (57209979008); Indi, Prithviraj N. (57211202143); Banerjee, Sashwata (57209981004); Kamal, Aditya (57211201253); Garg, Vaibhav (57209973733)","57209979008; 57211202143; 57209981004; 57211201253; 57209973733","Smart assistive navigation devices for visually impaired people","2019","2019 IEEE 4th International Conference on Computer and Communication Systems, ICCCS 2019","","","8821654","725","729","4","22","10.1109/CCOMS.2019.8821654","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85072958923&doi=10.1109%2fCCOMS.2019.8821654&partnerID=40&md5=584c345a99cb2d28607fe2528ebb8dd7","School of Electronics Engineering, Vellore Institute of Technology, Vellore, India; School of Electrical Engineering, Vellore Institute of Technology, Vellore, India; School of Computer Science Engineering, Vellore Institute of Technology, Vellore, India; School of Information Technology, Vellore Institute of Technology, Vellore, India","Pardasani A., School of Electronics Engineering, Vellore Institute of Technology, Vellore, India; Indi P.N., School of Electrical Engineering, Vellore Institute of Technology, Vellore, India; Banerjee S., School of Electrical Engineering, Vellore Institute of Technology, Vellore, India; Kamal A., School of Computer Science Engineering, Vellore Institute of Technology, Vellore, India; Garg V., School of Information Technology, Vellore Institute of Technology, Vellore, India","This paper is based on the design of a smart assistive device to help visually impaired people. Visually impaired people have been facing a lot of hardship in their day to day life. Navigating from one place to another is a toilsome task for them and the conventional blind walking stick has its own drawbacks or limitations. It is not of much assistance to these people of our society. Our research is motivated by the inability of the physically disabled people in their ambulation and we took an attempt to make their routine life painless and trouble-free. We designed two devices which can be of immense utility to the sightless people. We designed a smart glass and a smart pair of shoes by integrating various sensors with raspberry pi. The paper presents the functionalities and the working principle of the smart glasses and shoes and the varieties of tasks they are capable of accomplishing. The novelty of our work lies in processing data from both the devices and provide a better solution for navigating and day to day activities. © 2019 IEEE.","Image processing; Object detection; OpenCV; Text to braille; Video processing","Data handling; Glass; Image processing; Object detection; Assistive devices; Assistive navigations; Blind walking; Disabled people; OpenCV; Text to braille; Video processing; Visually impaired people; Video signal processing","","","Institute of Electrical and Electronics Engineers Inc.","","978-172811322-7","","","English","IEEE Int. Conf. Comput. Commun. Syst., ICCCS","Conference paper","Final","","Scopus","2-s2.0-85072958923"
"Elachhab A.; Mikou M.","Elachhab, Adil (57193909393); Mikou, Mohammed (56251321700)","57193909393; 56251321700","Obstacle detection algorithm by stereoscopic image processing: navigation assistance for the blind and visually impaired","2018","Advances in Intelligent Systems and Computing","737","","","13","23","10","1","10.1007/978-3-319-76357-6_2","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85043993972&doi=10.1007%2f978-3-319-76357-6_2&partnerID=40&md5=affe4a3d488b6536c6a6f76eb5bc0fbc","Laboratory: Analysis of Systems and Information Processing, Faculty of Sciences and Techniques, University Hassan I, BP 577, Settat, 26000, Morocco","Elachhab A., Laboratory: Analysis of Systems and Information Processing, Faculty of Sciences and Techniques, University Hassan I, BP 577, Settat, 26000, Morocco; Mikou M., Laboratory: Analysis of Systems and Information Processing, Faculty of Sciences and Techniques, University Hassan I, BP 577, Settat, 26000, Morocco","The continued growth in the number of blind or visually impaired people calls for an urgent need to invest all efforts to improve the quality of life for this population. This paper presents a work forming part of the technical study of new aid to the autonomous and secure movement for blind and partially sighted persons. These are intelligent glasses that capture environmental information, via a pair of stereoscopic cameras. In the case where an obstacle is present in the path of visual impairment, it will be detected by the stereoscopic sensor and an obstacle avoidance algorithm incorporated in a laptop computer. The final result, which provides key data to describe its environment, is transmitted to the user via headphones. Under this goal, we have developed a specialized processing algorithm that uses a stereoscopic vision approach and consists to determine the distance to obstacles and their size from two views of the same scene. The results provided by the algorithm seem to be satisfactory in terms of computing time required and effective operating range. © Springer International Publishing AG, part of Springer Nature 2018.","Dynamic programming; Image segmentation; Obstacle detection; Pyramidal image; Stereoscopic vision; Subpixel; Super-pixels; Visually impaired","Cameras; Dynamic programming; Image processing; Image segmentation; Laptop computers; Obstacle detectors; Pixels; Population statistics; Signal detection; Soft computing; Stereo image processing; Obstacle detection; Pyramidal image; Stereoscopic vision; Sub pixels; Visually impaired; Pattern recognition","A. Elachhab; Laboratory: Analysis of Systems and Information Processing, Faculty of Sciences and Techniques, University Hassan I, Settat, BP 577, 26000, Morocco; email: adil.elachhab@gmail.com","Muda A.K.; Abraham A.; Gandhi N.; Haqiq A.","Springer Verlag","21945357","978-331976356-9","","","English","Adv. Intell. Sys. Comput.","Conference paper","Final","","Scopus","2-s2.0-85043993972"
"Ryan J.; Okazaki D.; Dallow M.; Dezfouli B.","Ryan, John (57216159632); Okazaki, Daniel (57216161873); Dallow, Michael (57216163553); Dezfouli, Behnam (35758506000)","57216159632; 57216161873; 57216163553; 35758506000","NavSense: A Navigation Tool for Visually Impaired","2019","2019 IEEE Global Humanitarian Technology Conference, GHTC 2019","","","9033125","","","","3","10.1109/GHTC46095.2019.9033125","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85082652000&doi=10.1109%2fGHTC46095.2019.9033125&partnerID=40&md5=f872903bb4aa73ae974274dbbc2eff91","Santa Clara University, Department of Computer Science and Engineering, Santa Clara, CA, United States","Ryan J., Santa Clara University, Department of Computer Science and Engineering, Santa Clara, CA, United States; Okazaki D., Santa Clara University, Department of Computer Science and Engineering, Santa Clara, CA, United States; Dallow M., Santa Clara University, Department of Computer Science and Engineering, Santa Clara, CA, United States; Dezfouli B., Santa Clara University, Department of Computer Science and Engineering, Santa Clara, CA, United States","The visually impaired rely heavily on hearing and touching (with their cane) to navigate through life. These senses cannot make up for the loss of vision when identifying objects in the user's path. In this paper, we propose NavSense, an assistive device that supplements existing technology to improve navigation and peace of mind in day to day life. NavSense relies on range detection, computer vision, and hardware acceleration mechanisms to provide real-time object identification and context to the user through auditory feedback. In particular, we use four hardware platforms - Raspberry Pi 3 B+, Coral Accelerator, Coral Development Board, and Intel Neural Computer Stick - to compare the efficiency of object detection in terms of time and energy during setup and inference phases. Based on these results, it is possible to tailor the design for specific energy-accuracy requirements. Also, we have implemented and used NavSense in real-world scenarios to show its effectiveness. © 2019 IEEE.","Accessibility; Blindness; Computer Vision; Image Classification; Machine Learning; Object Recognition","Audition; Computer vision; Image classification; Learning systems; Object detection; Object recognition; Accessibility; Assistive devices; Auditory feedback; Blindness; Hardware acceleration; Hardware platform; Object identification; Real-world scenario; Computer hardware","","","Institute of Electrical and Electronics Engineers Inc.","","978-172811780-5","","","English","IEEE Glob. Humanit. Technol. Conf., GHTC","Conference paper","Final","","Scopus","2-s2.0-85082652000"
"Ciocoiu T.I.; Moldoveanu F.D.","Ciocoiu, Titus Iulian (57195396921); Moldoveanu, Florin Dumitru (24773636200)","57195396921; 24773636200","Vision based localization stereo rig calibration and rectification","2017","Proceedings - 2017 International Conference on Optimization of Electrical and Electronic Equipment, OPTIM 2017 and 2017 Intl Aegean Conference on Electrical Machines and Power Electronics, ACEMP 2017","","","7975103","1013","1018","5","1","10.1109/OPTIM.2017.7975103","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85027697447&doi=10.1109%2fOPTIM.2017.7975103&partnerID=40&md5=baa62a6377bdab5d1f2e3befde37bd28","Department of Automation and Information Technology, Transilvania University of Brasov, Str. Mihai Viteazul 5, Brasov, 500174, Romania","Ciocoiu T.I., Department of Automation and Information Technology, Transilvania University of Brasov, Str. Mihai Viteazul 5, Brasov, 500174, Romania; Moldoveanu F.D., Department of Automation and Information Technology, Transilvania University of Brasov, Str. Mihai Viteazul 5, Brasov, 500174, Romania","Lately, the 3D applications have become a more popular topic in robotics, computer vision or augmented reality. Using cameras and computer vision techniques, it is possible to obtain accurate 3D models of large-scale environments, such as cities. Furthermore, the cameras are low-cost, non-intrusive sensors compared to other sensors such as laser scanners and also offer rich information about the environment. One of the applications of great interest in this way is the vision-based localization in a prior 3D map. Robots need to perform tasks in the environment autonomously, and for this purpose, is very important to know precisely the location of the robot in the map. In the same way, providing accurate information about the location and spatial orientation of the user in a large-scale environment can be of benefit for those who suffer from visual impairment problems. A safe and autonomous navigation in unknown or known environments can be a great challenge for those who are blind or are visually impaired. Most of the commercial solutions for visually impaired localization and navigation assistance are based on the satellite Global Positioning System (GPS), which are not suitable enough for the visually impaired community in urban-environments. The errors are about of the order of several meters and there are also other problems such GPS signal loss or line-of-sight restrictions. Thus, taking into consideration all of the above mentioned problems, it is important to do further research on new more robust and accurate localization systems. © 2017 IEEE.","Key points; Stereo camera; Stereo Rig; Vision-based localization in a prior 3D map","Augmented reality; Cameras; Computer vision; Electric machinery; Electronic equipment; Global positioning system; Navigation systems; Oscillators (electronic); Power electronics; Robots; Stereo vision; Autonomous navigation; Computer vision techniques; Keypoints; Localization and navigation; Non-intrusive sensors; Stereo cameras; Stereo rigs; Vision based localization; Stereo image processing","","","Institute of Electrical and Electronics Engineers Inc.","","978-150904489-4","","","English","Proc. - Int. Conf. Optim. Electr. Electron. Equip., OPTIM and Int. Aegean Conf. Electr. Mach. Power Electron., ACEMP","Conference paper","Final","","Scopus","2-s2.0-85027697447"
"Tan X.Z.; Steinfeld A.","Tan, Xiang Zhi (57198744765); Steinfeld, Aaron (7103257672)","57198744765; 7103257672","Using robot manipulation to assist navigation by people who are blind or low vision","2017","ACM/IEEE International Conference on Human-Robot Interaction","","","","379","380","1","8","10.1145/3029798.3034808","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85016430116&doi=10.1145%2f3029798.3034808&partnerID=40&md5=b15bcb62c21b1efbcd4d7f36d0f90199","Robotics Institute, Carnegie Mellon University, Pittsburgh, United States","Tan X.Z., Robotics Institute, Carnegie Mellon University, Pittsburgh, United States; Steinfeld A., Robotics Institute, Carnegie Mellon University, Pittsburgh, United States","This work explores the capability of bi-directional manipulation during assistance to blind or low vision users. We describe a haptic approach that utilizes the manipulator arm to support interaction and provide navigational information supplemented with landmarks and spatial cues to users. © 2017 Authors.","assistive robots; haptic interface; indoor navigation","Air navigation; Haptic interfaces; Man machine systems; Navigation; Robots; Assistive robots; Bi-directional; In-door navigations; Manipulator arms; Navigational information; Robot manipulation; Spatial cues; Support interaction; Human robot interaction","","","IEEE Computer Society","21672148","978-145034885-0","","","English","ACM/IEEE Int. Conf. Hum.-Rob. Interact.","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85016430116"
"","","","14th International Conference on Mobile Web and Intelligent Information Systems, MobiWIS 2017","2017","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10486 LNCS","","","1","333","332","0","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85028465882&partnerID=40&md5=fd72a9853796eb2dda6c0c1fb1aea36f","","","The proceedings contain 27 papers. The special focus in this conference is on Mobile Web and Intelligent Information Systems. The topics include: Route recommendations to business travelers exploiting crowd-sourced data; audio story and AR platform for youth engagement; short and reliable path selection for automatic evacuation guiding based on interactions between evacuees and their mobile devices; exploring behavior change features for mobile workout applications; prediction of conditions for drying clothes based on area and temperature data; Google glass used as assistive technology its utilization for blind and visually impaired people; an approach of solid waste monitoring with smart dumpsters and disposal; intelligent notepad for windows phone that uses GPS data; calculating the optimal price of products in an online store; smart security system based on android platform; mobile health application running on public cloud during hajj; an authentication mechanism for accessing mobile web services; aspects of application security for internet of things systems; visual moving objects tracking using shape detectors and object models; research of the mobile CDMA network for the operation of an intelligent information system of earth-moving and construction machines; blaubot - hassle-free multi-device applications; overhead analysis of inter domain mobility management in multicasting-supported FPMIPv6 networks; software techniques for implementing dynamic network-aware energy-efficient download managers; new formulations for an optimal connectivity approach for mobile ad-hoc networks; adopting SOA in public service provision and performance analysis of cloud computing infrastructure.","","","","Awan I.; Holubova I.; Younas M.","Springer Verlag","03029743","978-331965514-7","","","English","Lect. Notes Comput. Sci.","Conference review","Final","","Scopus","2-s2.0-85028465882"
"Patil K.; Jawadwala Q.; Shu F.C.","Patil, Kailas (49862163900); Jawadwala, Qaidjohar (57200643974); Shu, Felix Che (26421430800)","49862163900; 57200643974; 26421430800","Design and Construction of Electronic Aid for Visually Impaired People","2018","IEEE Transactions on Human-Machine Systems","48","2","","172","182","10","99","10.1109/THMS.2018.2799588","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85042080465&doi=10.1109%2fTHMS.2018.2799588&partnerID=40&md5=b9d2226820d906e799bfd77782bd2098","Department of Computer Engineering, Vishwakarma University, Pune, 411048, India; Vishwakarma Institute of Information Technology, Savitribai Phule Pune University, Pune, 411048, India; University of Bamenda, Bamenda, 00237, Cameroon","Patil K., Department of Computer Engineering, Vishwakarma University, Pune, 411048, India; Jawadwala Q., Vishwakarma Institute of Information Technology, Savitribai Phule Pune University, Pune, 411048, India; Shu F.C., University of Bamenda, Bamenda, 00237, Cameroon","The NavGuide is a novel electronic device to assist visually impaired people with obstacle free path-finding. The highlight of the NavGuide system is that it provides simplified information on the surrounding environment and deduces priority information without causing information overload. The priority information is provided to the user through vibration and audio feedback mechanisms. The proof-of-concept device consists of a low power embedded system with ultrasonic sensors, vibration motors, and a battery. To test the effectiveness of the NavGuide system in daily-life mobility of visually impaired people, we performed an evaluation using 70 blind people of the 'school & home for the blind.' All evaluations were performed in controlled, real-world test environments with the NavGuide and traditional white cane. The evaluation results show that NavGuide is a useful aid in the detection of obstacles, wet floors, and ascending staircases and its performance is better than that of a white cane. © 2018 IEEE.","Assistive technology; blind people; electronic navigation aid; man machine systems; navigation; obstacle detection; rehabilitation; visually impaired people; wearable system","Acoustics; Electromagnetic compatibility; Headphones; Interactive computer systems; Man machine systems; Navigation; Obstacle detectors; Patient rehabilitation; Sensors; Ultrasonic applications; Wearable technology; Assistive technology; Blind people; Dogs; Electronic navigation; Obstacle detection; Vibrations; Visually impaired people; Wearable systems; Wearable sensors","K. Patil; Department of Computer Engineering, Vishwakarma University, Pune, 411048, India; email: kailas.patil@vupune.ac.in","","Institute of Electrical and Electronics Engineers Inc.","21682291","","","","English","IEEE Trans. Human Mach. Syst.","Article","Final","","Scopus","2-s2.0-85042080465"
"Rayar P.; Prabhudesai A.; Pai S.; Parikh S.","Rayar, Pavan (57208576866); Prabhudesai, Adarsh (57218915496); Pai, Samruddhi (57218909595); Parikh, Shaival (58443855900)","57208576866; 57218915496; 57218909595; 58443855900","Autonomous Real-Time Navigation Based on Dynamic Line and Object Detection","2020","Lecture Notes in Mechanical Engineering","","","","349","357","8","2","10.1007/978-981-15-4485-9_36","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85090711487&doi=10.1007%2f978-981-15-4485-9_36&partnerID=40&md5=5b95b78b2e95b3d8dd7ea31ce41866b9","Department of Production Engineering, Dwarkadas J. Sanghvi College of Engineering, Mumbai, India; Department of Electronics Engineering, Dwarkadas J. Sanghvi College of Engineering, Mumbai, India","Rayar P., Department of Production Engineering, Dwarkadas J. Sanghvi College of Engineering, Mumbai, India; Prabhudesai A., Department of Electronics Engineering, Dwarkadas J. Sanghvi College of Engineering, Mumbai, India; Pai S., Department of Electronics Engineering, Dwarkadas J. Sanghvi College of Engineering, Mumbai, India; Parikh S., Department of Electronics Engineering, Dwarkadas J. Sanghvi College of Engineering, Mumbai, India","An Android application has been developed that captures images, detects a line and computes the lateral deviation as well as angular deviation from the path. It also takes into consideration the intensity variation of the surroundings which has been explained in the following paper. We have used the Sobel edge detection algorithm and Hough transform for the angular inference. Colour thresholding and matrix manipulation are done for calculating the lateral deviation. Finally, the counter-active actions and inferences are sent via BLE (low energy Bluetooth). An application was developed for directing the autonomous quadruped robot. The rare-view camera of the mobile was used for taking the input. This Android application can have multiple implementations apart from robotics with slight manipulations, such as the automobile industry, military, industrial automation and blind assistance, and these kinds of researches will boost concept of smart manufacturing. © 2020, Springer Nature Singapore Pte Ltd.","Android application; Automation; Filters; Image processing; Line and object detection; Matrix manipulation; Robots","Android (operating system); Automotive industry; Cameras; Inference engines; Object recognition; Robot vision; Android applications; Capture images; Filter; Images processing; Lateral deviation; Line detection; Matrix manipulation; Objects detection; ON dynamics; Real-time navigation; Object detection","P. Rayar; Department of Production Engineering, Dwarkadas J. Sanghvi College of Engineering, Mumbai, India; email: pavan.rayar@djsce.ac.in","Vasudevan H.; Kottur V.K.N.; Raina A.A.","Springer Science and Business Media Deutschland GmbH","21954356","978-981154484-2","","","English","Lect. Notes Mech. Eng.","Conference paper","Final","","Scopus","2-s2.0-85090711487"
"Morrison C.; Cutrell E.; Dhareshwar A.; Doherty K.; Thieme A.; Taylor A.","Morrison, Cecily (57211159610); Cutrell, Edward (57203053744); Dhareshwar, Anupama (57173194700); Doherty, Kevin (57192910271); Thieme, Anja (36773797400); Taylor, Alex (35581562900)","57211159610; 57203053744; 57173194700; 57192910271; 36773797400; 35581562900","Imagining artificial intelligence applications with people with visual disabilities using tactile ideation","2017","ASSETS 2017 - Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility","","","","81","90","9","42","10.1145/3132525.3132530","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85041424219&doi=10.1145%2f3132525.3132530&partnerID=40&md5=28834f510375ebdd56ecb6b1e7cae9fb","Microsoft, Cambridge, United Kingdom; Microsoft, Redmond, WA, United States; Microsoft, Bangalore, India","Morrison C., Microsoft, Cambridge, United Kingdom; Cutrell E., Microsoft, Redmond, WA, United States; Dhareshwar A., Microsoft, Bangalore, India; Doherty K., Microsoft, Cambridge, United Kingdom; Thieme A., Microsoft, Cambridge, United Kingdom; Taylor A., Microsoft, Cambridge, United Kingdom","There has been a surge in artificial intelligence (AI) technologies co-opted by or designed for people with visual disabilities. Re-searchers and engineers have pushed technical boundaries in areas such as computer vision, natural language processing, location in-ference, and wearable computing. But what do people with visual disabilities imagine as their own technological future? To explore this question, we developed and carried out tactile ideation work-shops with participants in the UK and India. Our participants gen-erated a large and diverse set of ideas, most focusing on ways to meet needs related to social interaction. In some cases, this was a matter of recognizing people. In other cases, they wanted to be able to participate in social situations without foregrounding their disability. It was striking that this finding was consistent across UK and India despite substantial cultural and infrastructural differ-ences. In this paper, we describe a new technique for working with people with visual disabilities to imagine new technologies that are tuned to their needs and aspirations. Based on our experience with these workshops, we provide a set of social dimensions to consid-er in the design of new AI technologies: social participation, social navigation, social maintenance, and social independence. We offer these social dimensions as a starting point to forefront users' social needs and desires as a more deliberate consideration for assistive technology design. © 2017 Copyright is held by the owner/author(s).","Accessibility; AI; Artificial intelligence; Blind; Design; Ideation; Multicultural; Visually impaired","Design; Natural language processing systems; Transportation; Accessibility; Blind; Ideation; Multicultural; Visually impaired; Artificial intelligence","","","Association for Computing Machinery, Inc","","978-145034926-0","","","English","ASSETS - Proc. Int. ACM SIGACCESS Conf. Comput. Accessib.","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85041424219"
"Anitha J.; Subalaxmi A.; Vijayalakshmi G.","Anitha, J. (57065181500); Subalaxmi, A. (57209471653); Vijayalakshmi, G. (57209465942)","57065181500; 57209471653; 57209465942","Real time object detection for visually challenged persons","2019","International Journal of Innovative Technology and Exploring Engineering","8","8","","312","314","2","7","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85067850088&partnerID=40&md5=f5db952503bf323d1411dfbc3aaebfea","Department of Information Technology, Sri Ramakrishna Engineering College, Coimbatore, India","Anitha J., Department of Information Technology, Sri Ramakrishna Engineering College, Coimbatore, India; Subalaxmi A., Department of Information Technology, Sri Ramakrishna Engineering College, Coimbatore, India; Vijayalakshmi G., Department of Information Technology, Sri Ramakrishna Engineering College, Coimbatore, India","The visually impaired and blind people face various challenges in their day to day life. The objective of the proposed work is to develop an application for visually challenged persons based on the Android smart phone. It will eliminate the need for dedicated devices and other wearable devices to assist them to recognize objects as they move around. The Android application helps the visually impaired to navigate independently using real-time object detection and identification technology. The application makes use of the image processing technique to detect the object and speech synthesis to produce the voice output. The objective of the system is to detect real time objects which are scanned through the mobile camera and notify the blind persons about the object through audio or vocal information. The detection of images on moving objects has been a significant research area in computer vision which has been highly worked upon, and integrated with residential, commercial and industrial environments. Due to lack of data analysis of the trained data, and dependence of the motion of the objects, inability to differentiate one object from the other has led to various limitations in the existing techniques which include less accuracy and performance. Hence, Fast R-CNN (Region-based Convolutional Neural Networks) algorithm has been implemented to detect the object with high accuracy and processing speed. The detected image information is provided as a voice output using a speech synthesizer to the visually challenged persons to assist them in their mobility. © BEIESP.","Computer Vision; Convolution Neural Networks; Deep Learning; Mobile Application; Object Detection; Tensorflow","","","","Blue Eyes Intelligence Engineering and Sciences Publication","22783075","","","","English","Int. J. Innov. Technol. Explor. Eng.","Article","Final","","Scopus","2-s2.0-85067850088"
"Jabnoun H.; Hashish M.A.; Benzarti F.","Jabnoun, Hanen (56516842000); Hashish, Mohammad Abu (57217868566); Benzarti, Faouzi (6507257083)","56516842000; 57217868566; 6507257083","Mobile Assistive Application for Blind People in Indoor Navigation","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12157 LNCS","","","395","403","8","4","10.1007/978-3-030-51517-1_36","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85087758495&doi=10.1007%2f978-3-030-51517-1_36&partnerID=40&md5=d52f01112db6718619a6238af3bf5e48","ESPRIT, School of Engineering, Tunis, 2083, Tunisia; LR-11-ES17 Signal, Images et Technologies de l’Information (LR-SITI-ENIT), Tunis Le Belvédère, 1002, Tunisia","Jabnoun H., ESPRIT, School of Engineering, Tunis, 2083, Tunisia; Hashish M.A., LR-11-ES17 Signal, Images et Technologies de l’Information (LR-SITI-ENIT), Tunis Le Belvédère, 1002, Tunisia; Benzarti F., LR-11-ES17 Signal, Images et Technologies de l’Information (LR-SITI-ENIT), Tunis Le Belvédère, 1002, Tunisia","Navigation is an important human task that needs the human sense of vision. In this context, recent technologies developments provide technical assistance to support the visually impaired in their daily tasks and improve their quality of life. In this paper, we present a mobile assistive application called “GuiderMoi” that retrieves information about directions using color targets and identifies the next orientation for the visually impaired. In order to avoid the failure in detection and the inaccurate tracking caused by the mobile camera, the proposed method based on the CamShift algorithm aims to introduce better location and identification of color targets. Tests were conduct in natural indoor scene. The results depending on the distance and the angle of view, defined the accurate values to have a highest rate of target recognition. This work has perspectives for this such as implicating the augmented reality and the intelligent navigation based on machine learning and real-time processing. © 2020, The Author(s).","Android application; Assistive application; Camshift algorithm; Color targets","Augmented reality; Automation; Developing countries; Indoor positioning systems; Intelligent buildings; Assistive applications; Cam-shift algorithms; In-door navigations; Intelligent navigation; Realtime processing; Target recognition; Technical assistance; Visually impaired; Navigation","H. Jabnoun; ESPRIT, School of Engineering, Tunis, 2083, Tunisia; email: hanene.jabnoun@esprit.tn","Jmaiel M.; Aloulou H.; Mokhtari M.; Abdulrazak B.; Kallel S.","Springer","03029743","978-303051516-4","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85087758495"
"Chinchole S.; Patel S.","Chinchole, Sandesh (57202970154); Patel, Samir (57202048887)","57202970154; 57202048887","Artificial intelligence and sensors based assistive system for the visually impaired people","2018","Proceedings of the International Conference on Intelligent Sustainable Systems, ICISS 2017","","","","16","19","3","13","10.1109/ISS1.2017.8389401","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85050028899&doi=10.1109%2fISS1.2017.8389401&partnerID=40&md5=73b8ab00012ca42ba2712c38e2453913","Department of Computer Engineering, Sinhgad Institute of Technology, Navi Mumbai, India","Chinchole S., Department of Computer Engineering, Sinhgad Institute of Technology, Navi Mumbai, India; Patel S., Department of Computer Engineering, Sinhgad Institute of Technology, Navi Mumbai, India","The need for developing a low-cost assistive system for the visually impaired and blind people has increased with steady increase in their population worldwide. The stick system presented in the paper uses artificial intelligence along with various sensors in real time to help the visually disabled people to navigate their environment independently. Image recognition, collision detection and obstacle detection are the three tasks performed by the system. The image recognition task was performed using a smartphone application powered by artificial intelligence. The tasks of collision detection and obstacle detection utilized ultrasonic sensors to alert the user of the obstacles appearing in his route. The stick system also managed to demonstrate the important characteristics of affordability, high efficiency, mobility and ease of use. © 2017 IEEE.","Artificial intelligence; Collision detection; Image recognition; Obstacle detection; Real-time system; Visually impaired","Artificial intelligence; Image recognition; Interactive computer systems; Obstacle detectors; Real time systems; Assistive system; Collision detection; Disabled people; High-efficiency; Obstacle detection; Smart-phone applications; Visually impaired; Visually impaired people; Ultrasonic applications","","","Institute of Electrical and Electronics Engineers Inc.","","978-153861959-9","","","English","Proc. Int. Conf. Intell. Sustain. Syst., ICISS","Conference paper","Final","","Scopus","2-s2.0-85050028899"
"Bruno D.R.; De Assis M.H.; Osorio F.S.","Bruno, Diego Renan (57202465858); De Assis, Marcelo Henrique (57215872054); Osorio, Fernando Santos (7005366193)","57202465858; 57215872054; 7005366193","Development of a mobile robot: Robotic guide dog for aid of visual disabilities in urban environments","2019","Proceedings - 2019 Latin American Robotics Symposium, 2019 Brazilian Symposium on Robotics and 2019 Workshop on Robotics in Education, LARS/SBR/WRE 2019","","","9018597","104","108","4","14","10.1109/LARS-SBR-WRE48964.2019.00026","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85082166102&doi=10.1109%2fLARS-SBR-WRE48964.2019.00026&partnerID=40&md5=16092399027f8ff4c12004206bbd6159","Institute of Mathematics and Computer Science, University of São Paulo, Brazil; São Paulo State Faculty of Technology, Brazil","Bruno D.R., Institute of Mathematics and Computer Science, University of São Paulo, Brazil; De Assis M.H., São Paulo State Faculty of Technology, Brazil; Osorio F.S., Institute of Mathematics and Computer Science, University of São Paulo, Brazil","The general objective of this work is to develop a mobile robotic platform that is able to avoid obstacles for the benefit of visually impaired people. The idea is that this platform is a robot 'guide dog' ~for visually impaired persons, and animal guide dogs like these have very high costs (on average 40 thousand dollars - Our current robotic prototype has a cost of 400 dollars), also need a long time for training (more than 1 year), being therefore inaccessible to many visually impaired persons. Our work then turns to social robotics, where we have a big problem with disabled people and who do not get freedom to move around in urban spaces without the help of a caregiver. By collecting data from ultrasound sensors, managing a possible path to the target point in an autonomous manner, using a computer vision system to recognize and treat the obstacles encountered, representing these in the form of audio to the user. The prototype was developed to serve as a platform for research in the development of navigational algorithms and computer vision systems of the automation laboratory of the (omitted for blind review). The robot presented good results in tests with navigation algorithms and computer vision that were applied to validate this platform in tests with visually impaired people in IDVC - (Catanduva Institute for the Visually Impaired). © 2019 IEEE.","Deep Learning; Mobile Robotics; Vision in robotics and automation","Deep learning; Robotics; Robots; Urban planning; Computer vision system; Mobile robotic; Navigation algorithms; Ultrasound sensors; Urban environments; Visually impaired; Visually impaired people; Visually impaired persons; Computer vision","","Colombini E.L.; Drews P.L.J.; Garcia L.T.S.; Goncalves L.M.G.; Sa S.; Estrada E.S.D.; Botelho S.S.C.","Institute of Electrical and Electronics Engineers Inc.","","978-172814268-5","","","English","Proc. - Lat. Am. Robot. Symp., Braz. Symp. Robot. Workshop Robot. Educ., LARS/SBR/WRE","Conference paper","Final","","Scopus","2-s2.0-85082166102"
"Lee C.-Y.; Lee J.-H.; Wu M.-Y.","Lee, Chien-Yi (57211246021); Lee, Jia-Hong (26643425100); Wu, Mei-Yi (36835106700)","57211246021; 26643425100; 36835106700","An indoor navigation system for the visually impaired using hololens","2019","Multi Conference on Computer Science and Information Systems, MCCSIS 2019 - Proceedings of the International Conferences on Interfaces and Human Computer Interaction 2019, Game and Entertainment Technologies 2019 and Computer Graphics, Visualization, Computer Vision and Image Processing 2019","","","","482","484","2","1","10.33965/cgv2019_201906p075","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85073115124&doi=10.33965%2fcgv2019_201906p075&partnerID=40&md5=c5fa926d3a84fee900dbb29a89d11eb4","Department of Information Management, National Kaohsiung University of Science and Technology, Taiwan; Graduate Institute of Food Culture and Innovation, National Kaohsiung University of Hospitality and Tourism, Taiwan","Lee C.-Y., Department of Information Management, National Kaohsiung University of Science and Technology, Taiwan; Lee J.-H., Department of Information Management, National Kaohsiung University of Science and Technology, Taiwan; Wu M.-Y., Graduate Institute of Food Culture and Innovation, National Kaohsiung University of Hospitality and Tourism, Taiwan","The technological progress has brought us many new discoveries and better facilities, but it has also changed our daily lives. Recently, with the rapid development of augmented reality and virtual reality techniques, there has been a lot of business opportunities through the perception of context to make human senses have new experiences. The mixed reality device such as Microsoft HoloLens can detect the 3D environment and respond to the user via voice instructions. This technology can be applied to blind people for more convenience in their daily moves. In this paper, we have implemented a blind indoor navigation system using Microsoft HoloLens. With the proposed system, we can get the camera's coordinates and direction and detect distance from obstacles via the HoloLens functions such as gaze detection, image recognition, audio recognition and output. This system can also provide various blind guidance functions such as planning of safety paths, display the name of near targets, and BB sound warnings when walking beyond the safe path range. The system mainly controls the activities and functions through voice commands. To evaluate the system performance, we invited 8 testers to use the indoor navigation system to guide users through voice commands. After the experiments, the testers filled in the scores and opinions of a questionnaire. The experimental results show that the proposed system has good performance and provides a potential indoor navigation application for the visually impaired. © Copyright 2019 IADIS Press. All rights reserved.","Augmented Reality; Indoor Navigation System; Microsoft Hololens; The Visually Impaired","Augmented reality; Computer games; Computer vision; Human computer interaction; Image recognition; Mixed reality; Navigation systems; Visualization; Business opportunities; Guidance functions; In-door navigations; Indoor navigation system; MicroSoft; Technological progress; Virtual reality techniques; Visually impaired; Indoor positioning systems","","Blashki K.; Xiao Y.; Rodrigues L.","IADIS Press","","978-989853391-3","","","English","Multi Conf. Comput. Sci. Inf. Syst., MCCSIS - Proc. Int. Conf. Interfaces Hum. Comput. Interact., Game Entertain. Technol. Comput. Graph., Vis., Comput. Vis. Image Process.","Conference paper","Final","","Scopus","2-s2.0-85073115124"
"Arora A.S.; Gaikwad V.","Arora, Akshay Salil (57194774873); Gaikwad, Vishakha (57983715400)","57194774873; 57983715400","Blind aid stick: Hurdle recognition, simulated perception, android integrated voice based cooperation via GPS along with panic alert system","2017","2017 International Conference on Nascent Technologies in Engineering, ICNTE 2017 - Proceedings","","","7947957","","","","9","10.1109/ICNTE.2017.7947957","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85022042244&doi=10.1109%2fICNTE.2017.7947957&partnerID=40&md5=31540821b4f101c69715591d614a3de7","Electronics and Telecommunication. Ramrao Adik, Institute of Technology, Navi Mumbai, India","Arora A.S., Electronics and Telecommunication. Ramrao Adik, Institute of Technology, Navi Mumbai, India; Gaikwad V., Electronics and Telecommunication. Ramrao Adik, Institute of Technology, Navi Mumbai, India","Evolution of technology has always been endeavored with making daily life simple. With a fast paced life everybody today is harnessing the benefits of technology except some parts of the society. One of them is the visually impaired who have to rely on others for travelling and other activities. This paper aims at providing one such theoretical model which incorporates the latest technologies to provide efficient and smart electronic aid to the blind. We have used IR sensors along with ultrasonic range finder circuit for hurdle detection. Bluetooth module which along with GPS technology and an Android application for blind, will provide voice assistance to desired location and in panic situations will send SMS alert to registered mobile numbers The basic objective of the system is to provide a convenient and easy navigation aid for unsighted which helps in artificial vision by providing information about the environmental scenario of static and dynamic objects around them. © 2017 IEEE.","Bluetooth module; GPS module; IR sensor formatting; Ultrasonic senesor","Android (operating system); Bluetooth; Infrared detectors; Ultrasonic applications; Vision aids; Android applications; Blue-tooth module; Evolution of technology; IR sensor; Smart electronics; Theoretical modeling; Ultrasonic range finders; Visually impaired; Environmental technology","","","Institute of Electrical and Electronics Engineers Inc.","","978-150902794-1","","","English","Int. Conf. Nascent Technol. Eng., ICNTE - Proc.","Conference paper","Final","","Scopus","2-s2.0-85022042244"
"Iakovidis D.K.; Diamantis D.; Dimas G.; Ntakolia C.; Spyrou E.","Iakovidis, Dimitris K. (6603967427); Diamantis, Dimitrios (56594997600); Dimas, George (57195485922); Ntakolia, Charis (57196087950); Spyrou, Evaggelos (13205703300)","6603967427; 56594997600; 57195485922; 57196087950; 13205703300","Digital Enhancement of Cultural Experience and Accessibility for the Visually Impaired","2020","EAI/Springer Innovations in Communication and Computing","","","","237","271","34","17","10.1007/978-3-030-16450-8_10","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85075697150&doi=10.1007%2f978-3-030-16450-8_10&partnerID=40&md5=ae090979fd1c647576f2e8ac3ecc0870","Department of Computer Science and Biomedical Informatics, University of Thessaly, Lamia, Greece; Institute of Informatics and Telecommunications, National Center for Scientific Research “DEMOKRITOS”, Athens, Greece","Iakovidis D.K., Department of Computer Science and Biomedical Informatics, University of Thessaly, Lamia, Greece; Diamantis D., Department of Computer Science and Biomedical Informatics, University of Thessaly, Lamia, Greece; Dimas G., Department of Computer Science and Biomedical Informatics, University of Thessaly, Lamia, Greece; Ntakolia C., Department of Computer Science and Biomedical Informatics, University of Thessaly, Lamia, Greece; Spyrou E., Institute of Informatics and Telecommunications, National Center for Scientific Research “DEMOKRITOS”, Athens, Greece","Visual impairment restricts everyday mobility and limits the accessibility of places, which for the non-visually impaired is taken for granted. A short walk to a close destination, such as a market or a school becomes an everyday challenge. In this chapter, we present a novel solution to this problem that can evolve into an everyday visual aid for people with limited sight or total blindness. The proposed solution is a digital system, wearable like smart-glasses, equipped with cameras. An intelligent system module, incorporating efficient deep learning and uncertainty-aware decision-making algorithms, interprets the video scenes, translates them into speech, and describes them to the user through audio. The user can almost naturally interact with the system via a speech-based user interface, which is also capable of understanding the user’s emotions. The capabilities of this system are investigated in the context of accessibility and guidance to outdoor environments of cultural interest, such as the historic triangle of Athens. A survey of relevant state-of-the-art systems, technologies and services is performed, identifying critical system components that better adapt to the goals of the system, user needs and requirements, toward a user-centered architecture design. © 2020, Springer Nature Switzerland AG.","Computer vision; Deep learning; Navigation; Visual measurements; Visually impaired","Behavioral research; Decision making; Deep learning; Intelligent systems; Architecture designs; Decision-making algorithms; Digital enhancement; Outdoor environment; Speech based user interface; State-of-the-art system; Visual impairment; Visually impaired; User interfaces","D.K. Iakovidis; Department of Computer Science and Biomedical Informatics, University of Thessaly, Lamia, Greece; email: diakovidis@uth.gr","","Springer Science and Business Media Deutschland GmbH","25228595","","","","English","EAI/Springer Inno. Comm. Comp.","Book chapter","Final","","Scopus","2-s2.0-85075697150"
"Hersh M.A.; García Ramírez A.R.","Hersh, Marion Ann (7005937731); García Ramírez, Alejandro R. (57198233820)","7005937731; 57198233820","Evaluation of the electronic long cane: improving mobility in urban environments","2018","Behaviour and Information Technology","37","12","","1203","1223","20","8","10.1080/0144929X.2018.1490454","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85049650307&doi=10.1080%2f0144929X.2018.1490454&partnerID=40&md5=641c71dd4b16787199a8ed39ff0c4c80","Biomedical Engineering Department, University of Glasgow, Glasgow, United Kingdom; Centro em Ciências Tecnológicas da Terra e do Mar (CTTMar), Universidade do Vale do Itajaí, São José, Brazil","Hersh M.A., Biomedical Engineering Department, University of Glasgow, Glasgow, United Kingdom; García Ramírez A.R., Centro em Ciências Tecnológicas da Terra e do Mar (CTTMar), Universidade do Vale do Itajaí, São José, Brazil","A wide range of portable and wearable electronic travel aids have been developed to enable visually impaired people to move around public spaces without a sighted guide. However, few of them have gone beyond the prototype stage and the long cane and guide dog are still the main mobility aids. Despite the importance of evaluation to determine, for instance, effective functioning and end-user satisfaction, a standard approach has not yet been developed for mobility aids. The paper reports the evaluation of a low-cost electronic long cane, developed by the authors and colleagues in Brazil. It used a two-part methodology involving an experimental investigation of performance of the electronic long cane and a questionnaire to explore user satisfaction. The results of the experiments and questionnaire demonstrated both the cane’s usefulness and the need for modifications to improve its functioning. This work is also important for the development of methodologies for effective evaluation, as this is the first evaluation of a mobility device developed and carried out in Brazil. In addition, it is one of only a small number of evaluations in real locations with real obstacles. Finally, a series of recommendations for evaluating mobility devices is presented. What this paper adds? A standard approach to evaluating electronic travel for visually impaired people has not yet been developed and the most appropriate approach may depend on the objectives of the evaluation. Existing approaches generally use participants with no previous experience of using the device being evaluated and is carried out indoors with artificial obstacles. The training or device familiarisation period usually provided might be insufficient for participants to obtain optimal device performance or an effective comparison to be made of different devices. The approach to evaluating an electronic long cane reported in this paper has three main advantages over previous methods. The participants were experienced users of the electronic long cane who had been using it to support their daily mobility for at least a month. The evaluation was carried out in two different real urban environments with real obstacles. This has the advantages of being close to real-life cane use and participants being able to make informed comments and suggestions for improvements as a result of their experience. A questionnaire included questions on user satisfaction with and evaluation of a number of different cane features based on their experiences of cane use over a period. The work is also significant as the first detailed mobility device evaluation carried out in Brazil and in the presentation of a series of recommendations divided into themes for effective evaluation of mobility devices. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group.","Assistive technology; blindness; electronic cane; mobility; user experience","Carrier mobility; Surveys; Urban planning; Assistive technology; blindness; electronic cane; Electronic travel aidss; End-user satisfactions; Experimental investigations; User experience; Visually impaired people; Human computer interaction","A.R. García Ramírez; Centro em Ciências Tecnológicas da Terra e do Mar (CTTMar), Universidade do Vale do Itajaí, São José, Brazil; email: garcia.ramirez@gmail.com","","Taylor and Francis Ltd.","0144929X","","BEITD","","English","Behav Inf Technol","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85049650307"
"Majeed A.; Bhana R.; Haq A.U.; Shah H.; Williams M.-L.; Till A.","Majeed, Asim (56496418900); Bhana, Rehan (55819377900); Haq, Anwar Ul (7003873016); Shah, Hanifa (7202873528); Williams, Mike-Lloyd (57198682774); Till, Andy (56405305000)","56496418900; 55819377900; 7003873016; 7202873528; 57198682774; 56405305000","Living Labs (LILA): An Innovative Paradigm for Community Development—Project of “XploR” Cane for the Blind","2017","Springer Proceedings in Business and Economics","","","","31","46","15","2","10.1007/978-3-319-43434-6_3","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85113687624&doi=10.1007%2f978-3-319-43434-6_3&partnerID=40&md5=41fbf931193d8ffaec1ff6afb97d7818","School of Computing and Digital Technology, Birmingham City University, Birmingham, United Kingdom; Department of Computing, QA Higher Education, Birmingham, United Kingdom; Department of Business Administration, QA Higher Education, Birmingham, United Kingdom","Majeed A., School of Computing and Digital Technology, Birmingham City University, Birmingham, United Kingdom; Bhana R., School of Computing and Digital Technology, Birmingham City University, Birmingham, United Kingdom; Haq A.U., Department of Computing, QA Higher Education, Birmingham, United Kingdom; Shah H., School of Computing and Digital Technology, Birmingham City University, Birmingham, United Kingdom; Williams M.-L., Department of Business Administration, QA Higher Education, Birmingham, United Kingdom; Till A., Department of Business Administration, QA Higher Education, Birmingham, United Kingdom","The community development in different domains (business, education, welfare, etc.) has been the prime focus over the last decade due to the evolution of digital technologies and the shift in working patterns. However, many public and private investments have failed to produce sustaining and real value from them. The observed deficiencies which are causing the failure of community development projects ranged from initiation within the artificial and closed laboratory to open learning environments. The community development is entailed without understanding the real community needs, community’s value chain, and potential problems with limited interactions. These shortcomings have resulted in failure to develop effective, prosperous, and world class communities, leveraging the new innovative and powerful approaches. An approach to developing collaborative systems, called Living Lab (LILA), is discussed in this paper and this approach has empowered and engaged the communities (students, lecturers, computer scientists, electronics engineers, visually impaired and blind people) to experiment and learn the innovative solutions of their real-world problems. The theme of this innovation-led approach is to embed community-driven solution within the communities. This paper presents the actual framework for the establishment of a Living Lab using specific case study at Birmingham City University (BCU), along with its impact on community development. This research determines the key features that the visually impaired would find useful in a mobility cane called “XploR”. The smart cane incorporates facial recognition technology to alert the user when they are approaching a relative or friend from up to 10 m away. This is a revolutionary ‘smart’ cane enabling blind people to instantly identify friends and family. The cane also features GPS functionality to aid navigation. This project is part of LILA, a European initiative encouraging entrepreneurship and fostering internationalisation. © 2017, Springer International Publishing Switzerland.","Assistive technology; Facial recognition; Innovation; Living labs; Visually impaired","","A. Majeed; School of Computing and Digital Technology, Birmingham City University, Birmingham, United Kingdom; email: Asim.Majeed@bcu.ac.uk","Benlamri R.; Sparer M.","Springer Science and Business Media B.V.","21987246","978-331943433-9","","","English","Springer Proc. Bus. Econ.","Conference paper","Final","","Scopus","2-s2.0-85113687624"
"","","","16th International Conference on Computers Helping People with Special Needs, ICCHP 2018","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10897 LNCS","","","","","1209","0","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85049791267&partnerID=40&md5=da13855ab2bbc404e84f80777dc8968d","","","The proceedings contain 179 papers. The special focus in this conference is on Computers Helping People with Special Needs. The topics include: Stereo vision based distance estimation and logo recognition for the visually impaired; intersection navigation for people with visual impairment; indoor localization using computer vision and visual-inertial odometry; hapticrein: Design and development of an interactive haptic rein for a guidance robot; echoVis: Training echolocation using binaural recordings – initial benchmark results; tactiBelt: Integrating spatial cognition and mobility theories into the design of a novel orientation and mobility assistive device for the blind; virtual navigation environment for blind and low vision people; visual shoreline detection for blind and partially sighted people; 3D-printing of personalized assistive technology; camassia: Monocular interactive mobile way sonification; a proposed method for producing embossed dots graphics with a 3D printer; accessibility as prerequisite for the production of individualized aids through inclusive maker spaces; Hackability: A methodology to encourage the development of DIY assistive devices; Universal design tactile graphics production system BPLOT4 for blind teachers and blind staffs to produce tactile graphics and ink print graphics of high quality; a user study to evaluate tactile charts with blind and visually impaired people; concept-building in blind readers with thematic tactile volumes; augmented reality for people with visual impairments: Designing and creating audio-tactile content from existing objects; recording of fingertip position on tactile picture by the visually impaired and analysis of tactile information; designing an interactive tactile relief of the meissen table fountain; one-handed braille in the air.","","","","Miesenberger K.; Kouroupetroglou G.","Springer Verlag","03029743","978-331994273-5","","","English","Lect. Notes Comput. Sci.","Conference review","Final","","Scopus","2-s2.0-85049791267"
"Ranaweera P.S.; Madhuranga S.H.R.; Fonseka H.F.A.S.; Karunathilaka D.M.L.D.","Ranaweera, P.S. (57192379935); Madhuranga, S.H.R. (57200144570); Fonseka, H.F.A.S. (57200146814); Karunathilaka, D.M.L.D. (57200147998)","57192379935; 57200144570; 57200146814; 57200147998","Electronic travel aid system for visually impaired people","2017","2017 5th International Conference on Information and Communication Technology, ICoIC7 2017","","","8074700","","","","16","10.1109/ICoICT.2017.8074700","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85039946866&doi=10.1109%2fICoICT.2017.8074700&partnerID=40&md5=e372b10191670184dee744c8a497dee7","Dept. of Electrical and Information Engineering, Faculty of Engineering, University of Ruhuna, Sri Lanka","Ranaweera P.S., Dept. of Electrical and Information Engineering, Faculty of Engineering, University of Ruhuna, Sri Lanka; Madhuranga S.H.R., Dept. of Electrical and Information Engineering, Faculty of Engineering, University of Ruhuna, Sri Lanka; Fonseka H.F.A.S., Dept. of Electrical and Information Engineering, Faculty of Engineering, University of Ruhuna, Sri Lanka; Karunathilaka D.M.L.D., Dept. of Electrical and Information Engineering, Faculty of Engineering, University of Ruhuna, Sri Lanka","Visually impaired people face unique challenges in their day to day life while navigating in unfamiliar public locations. Using a walking stick relies on trial and error, particularly in unfamiliar locations. Also from a walking stick the user can only identify obstacles which are touching the stick and cannot identify the obstacles which are above his waist height. Electronic Travel AIDS (ETAs) are devices that use sensor technology to assist and improve the blind users mobility in terms of safety. In this context, we implement a system whose objective is to give blind users the ability to move around in unfamiliar environment, whether indoor or outdoor, through an interface specifically designed to cater the visual imperfections. This ETA system grants the user the ability to travel independently with an automated alerting system designed for emergencies. The obstacles are detected using image processing and distance sensing through IR sensors. A navigation system was developed to assist the user via web access. © 2017 IEEE.","Electronic Travel AIDS; GPS; Image Processing; Infra red; Visually Impaired","Global positioning system; Human rehabilitation equipment; Navigation systems; Alerting systems; Blind users; Electronic travel aidss; Sensor technologies; Trial and error; Visually impaired; Visually impaired people; Web access; Image processing","","","Institute of Electrical and Electronics Engineers Inc.","","978-150904912-7","","","English","Int. Conf. Inf. Commun. Technol., ICoIC7","Conference paper","Final","","Scopus","2-s2.0-85039946866"
"Fernandes H.; Costa P.; Filipe V.; Paredes H.; Barroso J.","Fernandes, Hugo (35172835800); Costa, Paulo (7201895673); Filipe, Vitor (6507061487); Paredes, Hugo (23398113700); Barroso, João (20435746800)","35172835800; 7201895673; 6507061487; 23398113700; 20435746800","A review of assistive spatial orientation and navigation technologies for the visually impaired","2019","Universal Access in the Information Society","18","1","","155","168","13","59","10.1007/s10209-017-0570-8","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85028576315&doi=10.1007%2fs10209-017-0570-8&partnerID=40&md5=923edc79e8302297c6354c8404a6b7dc","INESC TEC and Universidade de Trás-os-Montes e Alto Douro, School of Science and Technology, Quinta de Prados, Apt.1013, Vila Real, 5001-801, Portugal; Polytechnic Institute of Leiria, Leiria, Portugal","Fernandes H., INESC TEC and Universidade de Trás-os-Montes e Alto Douro, School of Science and Technology, Quinta de Prados, Apt.1013, Vila Real, 5001-801, Portugal; Costa P., Polytechnic Institute of Leiria, Leiria, Portugal; Filipe V., INESC TEC and Universidade de Trás-os-Montes e Alto Douro, School of Science and Technology, Quinta de Prados, Apt.1013, Vila Real, 5001-801, Portugal; Paredes H., INESC TEC and Universidade de Trás-os-Montes e Alto Douro, School of Science and Technology, Quinta de Prados, Apt.1013, Vila Real, 5001-801, Portugal; Barroso J., INESC TEC and Universidade de Trás-os-Montes e Alto Douro, School of Science and Technology, Quinta de Prados, Apt.1013, Vila Real, 5001-801, Portugal","The overall objective of this work is to review the assistive technologies that have been proposed by researchers in recent years to address the limitations in user mobility posed by visual impairment. This work presents an “umbrella review.” Visually impaired people often want more than just information about their location and often need to relate their current location to the features existing in the surrounding environment. Extensive research has been dedicated into building assistive systems. Assistive systems for human navigation, in general, aim to allow their users to safely and efficiently navigate in unfamiliar environments by dynamically planning the path based on the user’s location, respecting the constraints posed by their special needs. Modern mobile assistive technologies are becoming more discrete and include a wide range of mobile computerized devices, including ubiquitous technologies such as mobile phones. Technology can be used to determine the user’s location, his relation to the surroundings (context), generate navigation instructions and deliver all this information to the blind user. © 2017, Springer-Verlag GmbH Germany.","Accessibility; Assistive technology; Blind; Computer vision; Location; Navigation; Orientation; Review","Computer vision; Crystal orientation; Location; Reviews; Accessibility; Assistive technology; Blind; Spatial orientations; Surrounding environment; Ubiquitous technology; Visual impairment; Visually impaired people; Navigation","H. Fernandes; INESC TEC and Universidade de Trás-os-Montes e Alto Douro, Vila Real, School of Science and Technology, Quinta de Prados, Apt.1013, 5001-801, Portugal; email: hugof@utad.pt","","Springer Verlag","16155289","","","","English","Univers. Access Inf. Soc.","Review","Final","","Scopus","2-s2.0-85028576315"
"Kubanek M.; Depta F.; Smorawa D.","Kubanek, Mariusz (7801545165); Depta, Filip (57199545231); Smorawa, Dorota (55860034400)","7801545165; 57199545231; 55860034400","System of acoustic assistance in spatial orientation for the blind","2017","Advances in Intelligent Systems and Computing","534","","","266","277","11","3","10.1007/978-3-319-48429-7_25","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84994482401&doi=10.1007%2f978-3-319-48429-7_25&partnerID=40&md5=f6aec98133aa9028c1d237ec8a67a9ae","Institute of Computer and Information Science, Czestochowa University of Technology, Dabrowskiego Street 73, Czestochowa, 42-201, Poland; Faculty of Mechanical Engineering and Computer Science, Armii Krajowej Street 21, Czestochowa, 42-201, Poland","Kubanek M., Institute of Computer and Information Science, Czestochowa University of Technology, Dabrowskiego Street 73, Czestochowa, 42-201, Poland; Depta F., Faculty of Mechanical Engineering and Computer Science, Armii Krajowej Street 21, Czestochowa, 42-201, Poland; Smorawa D., Institute of Computer and Information Science, Czestochowa University of Technology, Dabrowskiego Street 73, Czestochowa, 42-201, Poland","The technological development provides better and more mobile devices to aid disabled people. All work conducted to improve the life of disabled people constitute an important part of science. The paper describes the issues related to anatomy and physiology of sight and hearing organs, imagination abilities of the blind and assisting devices for those people. The authors of this work have developed a prototype of an electronic device which navigates a blind person by means of sound signals. Sounds are meant to provide the blind with a simplified map of the object depth in their path. What makes the work innovative is the Kinect sensor applied to scan the space in front of the user as well as the set of algorithms designed to learn and generate acoustic space, which also take into account the tilt of the head. The carried out experiments indicate the correct interpretation of the sound signals being modelled. The tests conducted on the people prove the developed concept to be highly efficient. © Springer International Publishing AG 2017.","Acoustic space; Bioengineering; Bioinformatics; The blind","Artificial intelligence; Audition; Bioinformatics; Mobile devices; Soft computing; Systems science; Acoustic assistance; Acoustic spaces; Disabled people; Electronic device; Kinect sensors; Spatial orientations; Technological development; The blind; Vision aids","M. Kubanek; Institute of Computer and Information Science, Czestochowa University of Technology, Czestochowa, Dabrowskiego Street 73, 42-201, Poland; email: mariusz.kubanek","Kacprzyk J.; Kobayashi S.; El Fray I.; Piegat A.; Pejas J.","Springer Verlag","21945357","978-331948428-0","","","English","Adv. Intell. Sys. Comput.","Conference paper","Final","","Scopus","2-s2.0-84994482401"
"Li B.; Munoz J.P.; Rong X.; Chen Q.; Xiao J.; Tian Y.; Arditi A.; Yousuf M.","Li, Bing (58842105100); Munoz, Juan Pablo (57188972093); Rong, Xuejian (56727669200); Chen, Qingtian (57202354227); Xiao, Jizhong (34874109100); Tian, Yingli (16556710700); Arditi, Aries (57207554001); Yousuf, Mohammed (57193586001)","58842105100; 57188972093; 56727669200; 57202354227; 34874109100; 16556710700; 57207554001; 57193586001","Vision-Based Mobile Indoor Assistive Navigation Aid for Blind People","2019","IEEE Transactions on Mobile Computing","18","3","8370712","702","714","12","138","10.1109/TMC.2018.2842751","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85061415404&doi=10.1109%2fTMC.2018.2842751&partnerID=40&md5=954c00cda1cb76256a298399f6c66076","Department of Electrical Engineering, City College (CCNY), City University of New York, 160 Convent Ave, New York, 10031, NY, United States; Department of Computer Science, Graduate Center, City University of New York, 365 5th Ave, New York, 10016, NY, United States; CCNY Robotics Lab, Department of Electrical Engineering, City College, City University of New York, 160 Convent Ave, New York, 10031, NY, United States; Visibility Metrics LLC, 49 Valley View Road, Chappaqua, 10514, NY, United States; Federal Highway Administration, Washington, 20590, DC, United States","Li B., Department of Electrical Engineering, City College (CCNY), City University of New York, 160 Convent Ave, New York, 10031, NY, United States; Munoz J.P., Department of Computer Science, Graduate Center, City University of New York, 365 5th Ave, New York, 10016, NY, United States; Rong X., Department of Electrical Engineering, City College (CCNY), City University of New York, 160 Convent Ave, New York, 10031, NY, United States; Chen Q., Department of Electrical Engineering, City College (CCNY), City University of New York, 160 Convent Ave, New York, 10031, NY, United States; Xiao J., CCNY Robotics Lab, Department of Electrical Engineering, City College, City University of New York, 160 Convent Ave, New York, 10031, NY, United States; Tian Y., CCNY Robotics Lab, Department of Electrical Engineering, City College, City University of New York, 160 Convent Ave, New York, 10031, NY, United States; Arditi A., Visibility Metrics LLC, 49 Valley View Road, Chappaqua, 10514, NY, United States; Yousuf M., Federal Highway Administration, Washington, 20590, DC, United States","This paper presents a new holistic vision-based mobile assistive navigation system to help blind and visually impaired people with indoor independent travel. The system detects dynamic obstacles and adjusts path planning in real-time to improve navigation safety. First, we develop an indoor map editor to parse geometric information from architectural models and generate a semantic map consisting of a global 2D traversable grid map layer and context-aware layers. By leveraging the visual positioning service (VPS) within the Google Tango device, we design a map alignment algorithm to bridge the visual area description file (ADF) and semantic map to achieve semantic localization. Using the on-board RGB-D camera, we develop an efficient obstacle detection and avoidance approach based on a time-stamped map Kalman filter (TSM-KF) algorithm. A multi-modal human-machine interface (HMI) is designed with speech-audio interaction and robust haptic interaction through an electronic SmartCane. Finally, field experiments by blindfolded and blind subjects demonstrate that the proposed system provides an effective tool to help blind individuals with indoor navigation and wayfinding. © 2002-2012 IEEE.","blind and visually impaired people; Google Tango device; Indoor assistive navigation; obstacle avoidance; semantic maps","Base stations; Battery management systems; Charging (batteries); Costs; Deep learning; Electric discharges; Mobile telecommunication systems; Monitoring; Secondary batteries; Solar cells; Uninterruptible power systems; Wireless networks; Back-up power; Battery allocation; Battery feature profiling; Power system reliability; Resource management; State of charge; Outages","B. Li; Department of Electrical Engineering, City College (CCNY), City University of New York, New York, 160 Convent Ave, 10031, United States; email: bli@ccny.cuny.edu","","Institute of Electrical and Electronics Engineers Inc.","15361233","","","","English","IEEE Trans. Mob. Comput.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85061415404"
"Chuang T.-K.; Lin N.-C.; Chen J.-S.; Hung C.-H.; Huang Y.-W.; Tengl C.; Huang H.; Yu L.-F.; Giarre L.; Wang H.-C.","Chuang, Tzu-Kuan (57207882603); Lin, Ni-Ching (57192594508); Chen, Jih-Shi (57207879045); Hung, Chen-Hao (57207877040); Huang, Yi-Wei (57207879336); Tengl, Chunchih (57207884514); Huang, Haikun (57192586295); Yu, Lap-Fai (49862480500); Giarre, Laura (6603769649); Wang, Hsueh-Cheng (36150277500)","57207882603; 57192594508; 57207879045; 57207877040; 57207879336; 57207884514; 57192586295; 49862480500; 6603769649; 36150277500","Deep trail-following robotic guide dog in pedestrian environments for people who are blind and visually impaired - Learning from virtual and real worlds","2018","Proceedings - IEEE International Conference on Robotics and Automation","","","8460994","5849","5855","6","49","10.1109/ICRA.2018.8460994","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85063132814&doi=10.1109%2fICRA.2018.8460994&partnerID=40&md5=1f08106adc74ea7ff2ec0d0c25c13f6c","Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan; Department of Computer Science, University of Massachusetts at Boston, United States; University of Modena and Reggio Emilia, Modena, Italy","Chuang T.-K., Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan; Lin N.-C., Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan; Chen J.-S., Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan; Hung C.-H., Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan; Huang Y.-W., Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan; Tengl C., Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan; Huang H., Department of Computer Science, University of Massachusetts at Boston, United States; Yu L.-F., Department of Computer Science, University of Massachusetts at Boston, United States; Giarre L., University of Modena and Reggio Emilia, Modena, Italy; Wang H.-C., Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan","Navigation in pedestrian environments is critical to enabling independent mobility for the blind and visually impaired (BVI) in their daily lives. White canes have been commonly used to obtain contact feedback for following walls, curbs, or man-made trails, whereas guide dogs can assist in avoiding physical contact with obstacles or other pedestrians. However, the infrastructures of tactile trails or guide dogs are expensive to maintain. Inspired by the autonomous lane following of self-driving cars, we wished to combine the capabilities of existing navigation solutions for BVI users. We proposed an autonomous, trail-following robotic guide dog that would be robust to variances of background textures, illuminations, and interclass trail variations. A deep convolutional neural network (CNN) is trained from both the virtual and realworld environments. Our work included major contributions: 1) conducting experiments to verify that the performance of our models trained in virtual worlds was comparable to that of models trained in the real world; 2) conducting user studies with 10 blind users to verify that the proposed robotic guide dog could effectively assist them in reliably following man-made trails. © 2018 IEEE.","","Deep neural networks; E-learning; Neural networks; Robots; Textures; Virtual reality; Background textures; Blind and visually impaired; Convolutional neural network; Lane following; Navigation solution; Physical contacts; Real world environments; Robotic guides; Robotics","","","Institute of Electrical and Electronics Engineers Inc.","10504729","978-153863081-5","PIIAE","","English","Proc IEEE Int Conf Rob Autom","Conference paper","Final","","Scopus","2-s2.0-85063132814"
"Kumar N.A.; Thangal Y.H.; Sunitha Beevi K.","Kumar, Nirmal A. (57203626794); Thangal, Yazin Haris (57216288960); Sunitha Beevi, K. (57216287429)","57203626794; 57216288960; 57216287429","IoT enabled navigation system for blind","2019","IEEE Region 10 Humanitarian Technology Conference, R10-HTC","2019-November","","9042483","","","","8","10.1109/R10-HTC47129.2019.9042483","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85083035111&doi=10.1109%2fR10-HTC47129.2019.9042483&partnerID=40&md5=4d392c96df0b8ac24d1c971828dfdc3c","Electrical and Electronics Engineering, TKM College of Engineering, Kollam, Kerala, India","Kumar N.A., Electrical and Electronics Engineering, TKM College of Engineering, Kollam, Kerala, India; Thangal Y.H., Electrical and Electronics Engineering, TKM College of Engineering, Kollam, Kerala, India; Sunitha Beevi K., Electrical and Electronics Engineering, TKM College of Engineering, Kollam, Kerala, India","One of the biggest issues faced by the visually impaired is the difficulty they face in navigating their path. This paper proposes an effective wearable system which is an IoT enabled navigation system for the blind which uses a raspberry pi to process a raspberry pi camera and ultrasonic sensors, to provide information about obstacles in the vicinity, with the help of audio assistance, for mobility of blind people. It uses an ultrasound sensor to measure the distance of an object to the user. This image is processed and the output is given in the form of an audio signal. This system also makes use of a NEO-6M GPS module to provide real time position status of the user. This enables easy location analysis of the user in time of distress and can project pathways to a particular destination using audio signals and pre-loaded maps. The system also uses an inbuilt GSM module in the Raspberry Pi to allow the user to send a distress signal to the authorities concerned. The signal transmitted consists of an alert message with the current location of the user. © 2019 IEEE.","GPS module; GSM module; Image processing; Indoor navigation; IoT; Mobility; Navigational aid; Obstacle detection; Outdoor navigation; Raspberry pi; Ultrasonic sensors; Visually impaired; Wearable system","Global system for mobile communications; Navigation systems; Signaling; Wearable sensors; Audio signal; Blind people; Distress signals; Gsm modules; Location analysis; Ultrasound sensors; Visually impaired; Wearable systems; Internet of things","","","Institute of Electrical and Electronics Engineers Inc.","25727621","978-172810834-6","","","English","IEEE Reg. Humanit. Technol. Conf.: Sustain. Technol. Humanit., R10-HTC","Conference paper","Final","","Scopus","2-s2.0-85083035111"
"Suresh A.; Arora C.; Laha D.; Gaba D.; Bhambri S.","Suresh, Aswath (56878538300); Arora, Chetan (57987555800); Laha, Debrup (57202397832); Gaba, Dhruv (57202407101); Bhambri, Siddhant (59174590000)","56878538300; 57987555800; 57202397832; 57202407101; 59174590000","Intelligent smart glass for visually impaired using deep learning machine vision techniques and robot operating system (ROS)","2019","Advances in Intelligent Systems and Computing","751","","","99","112","13","12","10.1007/978-3-319-78452-6_10","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85048225919&doi=10.1007%2f978-3-319-78452-6_10&partnerID=40&md5=d1498c12bd9fce39a95432512cc8b1d2","Department of Mechanical and Aerospace Engineering, New York University, New York, United States; Department of Electronics and Communication Engineering, Bharati Vidyapeeth’s College of Engineering, Pune, India","Suresh A., Department of Mechanical and Aerospace Engineering, New York University, New York, United States; Arora C., Department of Mechanical and Aerospace Engineering, New York University, New York, United States; Laha D., Department of Mechanical and Aerospace Engineering, New York University, New York, United States; Gaba D., Department of Mechanical and Aerospace Engineering, New York University, New York, United States; Bhambri S., Department of Electronics and Communication Engineering, Bharati Vidyapeeth’s College of Engineering, Pune, India","The Smart Glass represents potential aid for people who are visually impaired that might lead to improvements in the quality of life. The smart glass is for the people who need to navigate independently and feel socially convenient and secure while they do so. It is based on the simple idea that blind people do not want to stand out while using tools for help. This paper focuses on the significant work done in the field of wearable electronics and the features which comes as add-ons. The Smart glass consists of ultrasonic sensors to detect the object ahead in real-time and feeds the Raspberry for analysis of the object whether it is an obstacle or a person. It can also assist the person on whether the object is closing in very fast and if so, provides a warning through vibrations in the recognized direction. It has an added feature of GSM, which can assist the person to make a call during an emergency situation. The software framework management of the whole system is controlled using Robot Operating System (ROS). It is developed using ROS catkin workspace with necessary packages and nodes. The ROS was loaded on to Raspberry Pi with Ubuntu Mate. © Springer International Publishing AG, part of Springer Nature 2019.","","Computer programming; Deep learning; Glass; Intelligent robots; Object detection; Ultrasonic applications; Visual servoing; Blind people; Emergency situation; Learning machines; Quality of life; Raspberry pi; Robot operating systems (ROS); Software frameworks; Visually impaired; Computer vision","A. Suresh; Department of Mechanical and Aerospace Engineering, New York University, New York, United States; email: as10616@nyu.edu","Myung H.; Xu W.; Jung J.-W.; Choi H.-L.; Kim J.-H.; Kim J.; Matson E.T.","Springer Verlag","21945357","978-331978451-9","","","English","Adv. Intell. Sys. Comput.","Conference paper","Final","","Scopus","2-s2.0-85048225919"
"Li X.; Cui H.; Rizzo J.-R.; Wong E.; Fang Y.","Li, Xiang (57126877800); Cui, Hanzhang (57208671133); Rizzo, John-Ross (56527876700); Wong, Edward (7403161446); Fang, Yi (55469293700)","57126877800; 57208671133; 56527876700; 7403161446; 55469293700","Cross-Safe: A Computer Vision-Based Approach to Make All Intersection-Related Pedestrian Signals Accessible for the Visually Impaired","2020","Advances in Intelligent Systems and Computing","944","","","132","146","14","17","10.1007/978-3-030-17798-0_13","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85065474195&doi=10.1007%2f978-3-030-17798-0_13&partnerID=40&md5=6304ff78896ba31de3798b0f803cc376","NYU Multimedia and Visual Computing Lab, New York, United States; NYU Tandon School of Engineering, Brooklyn, United States; NYU Langone Medical Center, New York, United States; NYU Abu Dhabi, Abu Dhabi, United Arab Emirates","Li X., NYU Multimedia and Visual Computing Lab, New York, United States, NYU Tandon School of Engineering, Brooklyn, United States; Cui H., NYU Multimedia and Visual Computing Lab, New York, United States, NYU Tandon School of Engineering, Brooklyn, United States; Rizzo J.-R., NYU Tandon School of Engineering, Brooklyn, United States, NYU Langone Medical Center, New York, United States; Wong E., NYU Multimedia and Visual Computing Lab, New York, United States, NYU Tandon School of Engineering, Brooklyn, United States; Fang Y., NYU Multimedia and Visual Computing Lab, New York, United States, NYU Tandon School of Engineering, Brooklyn, United States, NYU Abu Dhabi, Abu Dhabi, United Arab Emirates","Intersections pose great challenges to blind or visually impaired travelers who aim to cross roads safely and efficiently given unpredictable traffic control. Due to decreases in vision and increasingly difficult odds when planning and negotiating dynamic environments, visually impaired travelers require devices and/or assistance (i.e. cane, talking signals) to successfully execute intersection navigation. The proposed research project is to develop a novel computer vision-based approach, named Cross-Safe, that provides accurate and accessible guidance to the visually impaired as one crosses intersections, as part of a larger unified smart wearable device. As a first step, we focused on the red-light-green-light, go-no-go problem, as accessible pedestrian signals are drastically missing from urban infrastructure in New York City. Cross-Safe leverages state-of-the-art deep learning techniques for real-time pedestrian signal detection and recognition. A portable GPU unit, the Nvidia Jetson TX2, provides mobile visual computing and a cognitive assistant provides accurate voice-based guidance. More specifically, a lighter recognition algorithm was developed and equipped for Cross-Safe, enabling robust walking signal sign detection and signal recognition. Recognized signals are conveyed to visually impaired end user by vocal guidance, providing critical information for real-time intersection navigation. Cross-Safe is also able to balance portability, recognition accuracy, computing efficiency and power consumption. A custom image library was built and developed to train, validate, and test our methodology on real traffic intersections, demonstrating the feasibility of Cross-Safe in providing safe guidance to the visually impaired at urban intersections. Subsequently, experimental results show robust preliminary findings of our detection and recognition algorithm. © 2020, Springer Nature Switzerland AG.","Assistive technology; Pedestrian safety; Portable device; Visual impairment","Air navigation; Computer software portability; Deep learning; Energy efficiency; Footbridges; Green computing; Pedestrian safety; Signal detection; Traffic control; Assistive technology; Dynamic environments; Portable device; Recognition accuracy; Recognition algorithm; Urban infrastructure; Vision-based approaches; Visual impairment; Computer vision","Y. Fang; NYU Multimedia and Visual Computing Lab, New York, United States; email: yfang@nyu.edu","Arai K.; Kapoor S.","Springer Verlag","21945357","978-303017797-3","","","English","Adv. Intell. Sys. Comput.","Conference paper","Final","","Scopus","2-s2.0-85065474195"
"De Belen R.A.J.; Bednarz T.; Del Favero D.","De Belen, Ryan Anthony J. (57194784108); Bednarz, Tomasz (23089950100); Del Favero, Dennis (23049466300)","57194784108; 23089950100; 23049466300","Integrating mixed reality and internet of things as an assistive technology for elderly people living in a smart home","2019","Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry","","","a47","","","","15","10.1145/3359997.3365742","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85077136634&doi=10.1145%2f3359997.3365742&partnerID=40&md5=d3cc2749aa7ad6875abf7a442c69976c","UNSW Art Design, Sydney, NSW, Australia; UNSW Art and Design, CSIRO Data61, Sydney, NSW, Australia","De Belen R.A.J., UNSW Art Design, Sydney, NSW, Australia; Bednarz T., UNSW Art and Design, CSIRO Data61, Sydney, NSW, Australia; Del Favero D., UNSW Art Design, Sydney, NSW, Australia","In the last few decades, there has been a significant increase in demand for Wearable Assistive Technologies (WATs) useful to overcome functional limitations of individuals. Although advances in Computer Graphics (CG), Computer Vision (CV), and Artificial Intelligence (AI) have the potential to address a wide range of human needs, fully integrated systems that consider age-related changes in elderly people are still pretty uncommon. In this work, we present a WAT that follows interaction design guidelines to ensure reliability, usability, and suitability for everyday use. The WAT enables elderly people to improve interactions with Mixed Reality (MR) and Internet of Things (IoT) technologies. It properly aids and assists elderly people in daily activities such as analysing the environment, recognising and searching for objects, wayfinding, and navigation. We believe that this technology is helpful for blind, low-vision, or hearing-impaired independent elderly people to improve their quality of life while maintaining self-independence. © 2019 Association for Computing Machinery.","Assistive technology; Elderly people; Internet of things; Mixed reality","Audition; Automation; Interactive computer graphics; Internet of things; Wearable technology; Age-related changes; Assistive technology; Assistive technology for elderly people; Elderly people; Fully integrated; Hearing impaired; Interaction design; Internet of Things (IOT); Mixed reality","","Spencer S.N.","Association for Computing Machinery, Inc","","978-145037002-8","","","English","Proc. - VRCAI: ACM SIGGRAPH Int. Conf. Virtual-Real. Continuum its Appl. Ind.","Conference paper","Final","","Scopus","2-s2.0-85077136634"
"Rizzo J.-R.; Pan Y.; Hudson T.; Wong E.K.; Fang Y.","Rizzo, John-Ross (56527876700); Pan, Yubo (57194650036); Hudson, Todd (7203069715); Wong, Edward K. (7403161446); Fang, Yi (55469293700)","56527876700; 57194650036; 7203069715; 7403161446; 55469293700","Sensor fusion for ecologically valid obstacle identification: Building a comprehensive assistive technology platform for the visually impaired","2017","2017 7th International Conference on Modeling, Simulation, and Applied Optimization, ICMSAO 2017","","","7934891","","","","28","10.1109/ICMSAO.2017.7934891","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85021427754&doi=10.1109%2fICMSAO.2017.7934891&partnerID=40&md5=b4342bfa002cea71a19a4836b707c6db","Multimedia and Visual Computing Lab, New York University, Brooklyn, United States; Tandon School of Engineering, New York University, Brooklyn, United States; NYU Abu Dhabi, Abu Dhabi, United Arab Emirates; NYU Langone Medical Center, New York, NY, United States","Rizzo J.-R., NYU Langone Medical Center, New York, NY, United States; Pan Y., Multimedia and Visual Computing Lab, New York University, Brooklyn, United States, Tandon School of Engineering, New York University, Brooklyn, United States; Hudson T., NYU Langone Medical Center, New York, NY, United States; Wong E.K., Multimedia and Visual Computing Lab, New York University, Brooklyn, United States, Tandon School of Engineering, New York University, Brooklyn, United States; Fang Y., Multimedia and Visual Computing Lab, New York University, Brooklyn, United States, Tandon School of Engineering, New York University, Brooklyn, United States, NYU Abu Dhabi, Abu Dhabi, United Arab Emirates","Sensor fusion represents a robust approach to ecologically valid obstacle identification in building a comprehensive electronic travel aid (ETA) for the blind and visually impaired. A stereoscopic camera system and an infrared sensor with 16 independent elements is proposed to be combined with a multi-scale convolutional neural network for this fusion framework. While object detection and identification can be combined with depth information from a stereo camera system, our experiments demonstrate that depth information may be inconsistent given material surfaces of specific potential collision hazards. This inconsistency can be easily remedied by supplementation with a more reliable depth signal from an alternate sensing modality. The sensing redundancy in this multi-modal strategy, as deployed in this platform, may enhance the situational awareness of a visually impaired end user, permitting more efficient and safer obstacle negotiation. © 2017 IEEE.","","Cameras; Infrared detectors; Neural networks; Object detection; Object recognition; Stereo image processing; Assistive technology; Blind and visually impaired; Convolutional neural network; Detection and identifications; Electronic travel aidss; Obstacle negotiation; Situational awareness; Stereo camera system; Vision aids","","","Institute of Electrical and Electronics Engineers Inc.","","978-150905454-1","","","English","Int. Conf. Model., Simul., Appl. Optim., ICMSAO 2017","Conference paper","Final","","Scopus","2-s2.0-85021427754"
"Narmatha K.; Aiswarya A.; Rout P.K.; Sharma Y.","Narmatha, K. (57192678725); Aiswarya, A. (57211299357); Rout, Pratyush Kumar (57214670148); Sharma, Yashraj (57214664037)","57192678725; 57211299357; 57214670148; 57214664037","Navigation system for blind people using ultrasonic sensor","2019","Journal of Advanced Research in Dynamical and Control Systems","11","3 Special Issue","","1357","1360","3","1","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85078935690&partnerID=40&md5=4cb0038a0dd0d83a4db6c40e775e0543","Department of CSE, SRM Institute of Science and Technology, Chennai, India","Narmatha K., Department of CSE, SRM Institute of Science and Technology, Chennai, India; Aiswarya A., Department of CSE, SRM Institute of Science and Technology, Chennai, India; Rout P.K., Department of CSE, SRM Institute of Science and Technology, Chennai, India; Sharma Y., Department of CSE, SRM Institute of Science and Technology, Chennai, India","The project is built for the visually impaired and blind individuals and to help them in their daily lives and to avoid obstacles using special detection sensors. This system uses a Raspberry pi board (microcontroller) as the core of the navigation system. The system uses various inputs it receives from the ultrasonic sensor, the infrared sensor, camera and microphone. The data and signals received from the various input devices is processed by the Raspberry board. A GPS module is also connected to the system so that the user’s current location can be tracked for safety purposes. The data from the GPS module is processed with the help of a GIS system, which sends the processed information to an image processing system, which with the help of the Google speech API. All the processed information is passed to the Raspberry board which processes it produces the audio output via headphones. The main purpose of the paper is to gather contextual information to aid the blind or visually impaired user and to lend them a helping hand in their daily lives. © 2019, Institute of Advanced Scientific Research, Inc.. All rights reserved.","Blind; Navigation; Raspberry Pi; Ultrasonic sensor","","","","Institute of Advanced Scientific Research, Inc.","1943023X","","","","English","J. Adv. Res. Dyn. Control. Syst.","Article","Final","","Scopus","2-s2.0-85078935690"
"Nanavati A.; Tan X.Z.; Steinfeld A.","Nanavati, Amal (57201556798); Tan, Xiang Zhi (57198744765); Steinfeld, Aaron (7103257672)","57201556798; 57198744765; 7103257672","Coupled Indoor Navigation for People Who Are Blind","2018","ACM/IEEE International Conference on Human-Robot Interaction","","","","201","202","1","14","10.1145/3173386.3176976","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85045257087&doi=10.1145%2f3173386.3176976&partnerID=40&md5=db165ec6f6e6fe1a573334ba75dc113e","Carnegie Mellon University, Pittsburgh, PA, United States","Nanavati A., Carnegie Mellon University, Pittsburgh, PA, United States; Tan X.Z., Carnegie Mellon University, Pittsburgh, PA, United States; Steinfeld A., Carnegie Mellon University, Pittsburgh, PA, United States","This paper presents our design of an autonomous navigation system for a mobile robot that guides people who are blind and low vision in indoor settings. It begins by presenting user studies that shaped our design of the system, moves on to describing our model of human-robot coupled motion, and concludes by describing our autonomous navigation system. © 2018 Authors.","accessibility; autonomous navigation; human-robot interaction","Indoor positioning systems; Machine design; Man machine systems; Mobile robots; Navigation systems; accessibility; Autonomous navigation; Autonomous navigation systems; Coupled motions; Human robots; In-door navigations; Low vision; User study; Human robot interaction","","","IEEE Computer Society","21672148","978-145035615-2","","","English","ACM/IEEE Int. Conf. Hum.-Rob. Interact.","Conference paper","Final","","Scopus","2-s2.0-85045257087"
"Zhang H.; Ye C.","Zhang, He (57031171700); Ye, Cang (7202201245)","57031171700; 7202201245","Human-Robot Interaction for Assisted Wayfinding of a Robotic Navigation Aid for the Blind","2019","International Conference on Human System Interaction, HSI","2019-June","","8942612","137","142","5","10","10.1109/HSI47298.2019.8942612","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85077803559&doi=10.1109%2fHSI47298.2019.8942612&partnerID=40&md5=30c5bc8f18532a8a61e00eade48ba902","Virginia Commonwealth University, Department of Computer Science, VA, United States","Zhang H., Virginia Commonwealth University, Department of Computer Science, VA, United States; Ye C., Virginia Commonwealth University, Department of Computer Science, VA, United States","This paper introduces a new robotic navigation aid (RNA) for the visually impaired (VI). Two fundamental functions - wayfinding and human-robot interaction (HRI) - are presented for assisted wayfinding. The problem of wayfinding involves planning a path from the RNA's current location to the destination and following the path to get to the destination. To address the problem, we developed a new visual inertial odometry to estimate the RNA's pose by using the image and depth data from an RGB-D camera and the inertial data of an IMU. The estimated pose is used for path planning. To guide the user to follow the planned path, we designed an HRI interface with two guiding modes - the robocane mode and white-came mode. In the robocane mode, the RNA uses a motorized rolling tip to steer itself into the desired direction of travel (DDT) for the user to follow and track the planned path. In the white-cane mode, the RNA uses its speech interface to indicate the DDT to the user by audio messages. In this mode, the user swings the RNA just like using a conventional white cane. To make mode selection effortless, we developed a human intent detection (HID) method based on the decision tree mode. The method can detect the user intent and automatically select the appropriate mode according to the detected intent. Experimental results demonstrate the efficacies of the VIO, HRI, and HID methods for assisted wayfinding. © 2019 IEEE.","Assisted Wayfinding; Human Intent Detection; Human-Robot Interaction; Robotic Navigation Aid; Visually Impaired","Decision trees; Man machine systems; Motion planning; Navigation; Patient rehabilitation; RNA; Robotics; Human robot Interaction (HRI); Intent detection; Mode selection; Rgb-d cameras; Robotic navigation; Speech interface; Visually impaired; Way-finding; Human robot interaction","","","IEEE Computer Society","21582246","978-172813980-7","","","English","Int. Conf. Hum. Syst. Interact., HSI","Conference paper","Final","","Scopus","2-s2.0-85077803559"
"Guo A.","Guo, Anhong (56789541900)","56789541900","Crowd-ai systems for non-visual information access in the real world","2018","Conference on Human Factors in Computing Systems - Proceedings","2018-April","","DC09","","","","0","10.1145/3170427.3173034","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85052026255&doi=10.1145%2f3170427.3173034&partnerID=40&md5=92d897126704c637fbf7d467a85117c9","Human-Computer Interaction Institute, Carnegie Mellon University, Pittsburgh, 15213, PA, United States","Guo A., Human-Computer Interaction Institute, Carnegie Mellon University, Pittsburgh, 15213, PA, United States","The world is full of information, interfaces and environments that are inaccessible to blind people. When navigating indoors, blind people are often unaware of key visual information, such as posters, signs, and exit doors. When accessing specific interfaces, blind people cannot independently do so without at least first learning their layout and labeling them with sighted assistance. My work investigates interactive systems that integrates computer vision, on-demand crowdsourcing, and wearables to amplify the abilities of blind people, offering solutions for real-time environment and interface navigation. My work provides more options for blind people to access information and increases their freedom in navigating the world. © 2018 Copyright is held by the owner/author(s).","3D printing; Accessibility; Blind; Computer vision; Crowdsourcing; Fabrication; Mobile; Non-visual interfaces; Visually impaired; Wearable","Behavioral research; Human engineering; AI systems; Blind people; Interactive system; Non visuals; Real-time environment; Real-world; Specific interface; Visual information; Real time systems","A. Guo; Human-Computer Interaction Institute, Carnegie Mellon University, Pittsburgh, 15213, United States; email: anhongg@cs.cmu.edu","","Association for Computing Machinery","","978-145035620-6; 978-145035621-3","","","English","Conf Hum Fact Comput Syst Proc","Conference paper","Final","","Scopus","2-s2.0-85052026255"
"Paul S.T.; Raimond K.","Paul, Sherin Tresa (57204726759); Raimond, Kumudha (24776550700)","57204726759; 24776550700","A hassle-free shopping experience for the visually impaired: An assistive technology application","2019","Lecture Notes in Computational Vision and Biomechanics","31","","","207","217","10","2","10.1007/978-3-030-04061-1_21","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85060207640&doi=10.1007%2f978-3-030-04061-1_21&partnerID=40&md5=b8762d5d86769e261f1dfdc9db4d8a56","Karunya University, Coimbatore, India","Paul S.T., Karunya University, Coimbatore, India; Raimond K., Karunya University, Coimbatore, India","The issues and challenges faced by the Visually Impaired (VI) bring forth an insurmountable inequality of living conditions which needs immediate attention. The objective of this work is to render an in-depth understanding of some of the issues faced by the VI and to propose an Assistive Technology (AT) framework for assisting the VI to have a smooth and carefree shopping experience, including navigation in a supermarket to the sections specified by the VI user, identification of objects of interest, hassle-free billing, and registration for a subscription-based offer notification system based on the user’s shopping habits and preferences. The ultimate goal is to improve the Quality of Life (QoL) led by the VI person in as many ways as possible. © Springer Nature Switzerland AG 2019.","Assistive technology for visually impaired (VI); BLE beacons; Blindness; Image processing; Inertial measurement unit (IMU); IoT; Optical head-mounted displays; Pattern recognition; Quality of life (QoL)","","S.T. Paul; Karunya University, Coimbatore, India; email: sherintresapaul777@gmail.com","","Springer Netherlands","22129391","","","","English","Lectur. Notes Comput. Vis. Biomech.","Book chapter","Final","","Scopus","2-s2.0-85060207640"
"Priya T.; Sravya K.S.; Umamaheswari S.","Priya, Tuhina (59063581200); Sravya, Kotla Sai (57190842409); Umamaheswari, S. (57225433179)","59063581200; 57190842409; 57225433179","Machine-Learning-Based Device for Visually Impaired Person","2020","Advances in Intelligent Systems and Computing","1056","","","79","88","9","5","10.1007/978-981-15-0199-9_7","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85079678126&doi=10.1007%2f978-981-15-0199-9_7&partnerID=40&md5=ebb62b62a0dc5deb2366660e334eb77e","Department of Electronics and Instrumentation Engineering, SRM Institute of Science and Technology, Kattankulathur, Chennai, India","Priya T., Department of Electronics and Instrumentation Engineering, SRM Institute of Science and Technology, Kattankulathur, Chennai, India; Sravya K.S., Department of Electronics and Instrumentation Engineering, SRM Institute of Science and Technology, Kattankulathur, Chennai, India; Umamaheswari S., Department of Electronics and Instrumentation Engineering, SRM Institute of Science and Technology, Kattankulathur, Chennai, India","The blind-assistive devices promote the detection of objects and alerting the user by a buzzer or an alarm. In this project, a camera is placed inside a cap which is used by the visually impaired person. The machine-learning algorithm is used for accurate detection of the object and provides an alarm. The ultrasonic sensor is used to measure the distance between the visually impaired person and the real-time object detected when the object is detected the alarm is actuated. Nowadays, the assistive devices include the involvement of both hardware and software section to assist the blind user. The proposed method for the visually impaired person aims to detect the object more accurately so that the visually impaired person can navigate to their full potential in real-time application. © Springer Nature Singapore Pte Ltd 2020.","Machine-learning; Raspberry Pi 3 processor; Real-time object detection; Ultrasonic sensor; Visually impaired person; Yolo algorithm","Alarm systems; Evolutionary algorithms; Learning algorithms; Learning systems; Object detection; Object recognition; Ultrasonic applications; Ultrasonic sensors; Assistive devices; Blind users; Hardware and software; Raspberry Pi 3 processor; Real time; Real-time application; Visually impaired persons; Machine learning","S. Umamaheswari; Department of Electronics and Instrumentation Engineering, SRM Institute of Science and Technology, Chennai, Kattankulathur, India; email: uma.success53@gmail.com","Dash S.S.; Lakshmi C.; Das S.; Panigrahi B.K.","Springer","21945357","978-981150198-2","","","English","Adv. Intell. Sys. Comput.","Conference paper","Final","","Scopus","2-s2.0-85079678126"
"Megalingam R.K.; Vishnu S.; Sasikumar V.; Sreekumar S.","Megalingam, Rajesh Kannan (35119069500); Vishnu, Souraj (57209477134); Sasikumar, Vishnu (57201392959); Sreekumar, Sajikumar (57203550939)","35119069500; 57209477134; 57201392959; 57203550939","Autonomous path guiding robot for visually impaired people","2019","Advances in Intelligent Systems and Computing","768","","","257","266","9","26","10.1007/978-981-13-0617-4_25","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85052195282&doi=10.1007%2f978-981-13-0617-4_25&partnerID=40&md5=836be74ce1a74a7a278a5cddd3bafed0","Department of Electronics and Communication Engineering, Amrita University, Amritapuri, India","Megalingam R.K., Department of Electronics and Communication Engineering, Amrita University, Amritapuri, India; Vishnu S., Department of Electronics and Communication Engineering, Amrita University, Amritapuri, India; Sasikumar V., Department of Electronics and Communication Engineering, Amrita University, Amritapuri, India; Sreekumar S., Department of Electronics and Communication Engineering, Amrita University, Amritapuri, India","This paper introduces an intelligent and efficient path guidance robot to assist the visually impaired people in their movements. This is a novel device for the replacement of strenuous guide dogs. The robot has the capability to move along multiple paths and then remember as well as retrace all of them, thus making it a perfect substitute for a guide dog which is often a luxury for the ninety percent of blind people living in low income settings. The robot is capable to guide the user to travel places which cannot be traced using GPS. Since most of the navigation systems that are developed so far for the blind people employ a complex conjunction of positioning systems, video cameras, location mapping and image processing algorithms, we have designed an affordable low-cost prototype navigation system which serves the purpose of guiding the visually impaired people both in indoor as well as outdoor environment. © 2019, Springer Nature Singapore Pte Ltd.","Blind guiding; Navigation; Obstacle avoiding; Retrace","Image processing; Navigation; Navigation systems; Soft computing; Video cameras; Vision aids; Blind guiding; Efficient path; Image processing algorithm; Obstacle-avoiding; Outdoor environment; Positioning system; Retrace; Visually impaired people; Intelligent robots","R.K. Megalingam; Department of Electronics and Communication Engineering, Amrita University, Amritapuri, India; email: rajeshkannan@ieee.org","Bhoi A.K.; Balas V.E.; Zobaa A.F.; Mallick P.K.","Springer Verlag","21945357","978-981130616-7","","","English","Adv. Intell. Sys. Comput.","Conference paper","Final","","Scopus","2-s2.0-85052195282"
"Bălan O.; Moldoveanu A.; Moldoveanu F.; Nagy H.; Wersényi G.; Unnþórsson R.","Bălan, Oana (57191473650); Moldoveanu, Alin (24438151800); Moldoveanu, Florica (6507837390); Nagy, Hunor (56689648800); Wersényi, György (6506858994); Unnþórsson, Rúnar (22942282900)","57191473650; 24438151800; 6507837390; 56689648800; 6506858994; 22942282900","Improving the audio game–playing performances of people with visual impairments through multimodal training","2017","Journal of Visual Impairment and Blindness","111","2","","148","164","16","6","10.1177/0145482x1711100206","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85020089826&doi=10.1177%2f0145482x1711100206&partnerID=40&md5=d16209feb1563544c8d47426064670c8","University Politehnica of Bucharest, Splaiul Independentei, 313, Bucharest, Romania; University Politehnica of Bucharest, Bucharest, Romania; Széchenyi István University, Egyetem tér 1, Gyõr, 9026, Hungary; Széchenyi István University, Egyetem tér 1., Hungary; University of Iceland, School of Engineering and Natural Sciences, Faculty of Industrial Engineering, Mechanical Engineering and Computer Science, VR-2/V02-237, Reykjavik, Iceland","Bălan O., University Politehnica of Bucharest, Splaiul Independentei, 313, Bucharest, Romania; Moldoveanu A., University Politehnica of Bucharest, Bucharest, Romania; Moldoveanu F., University Politehnica of Bucharest, Bucharest, Romania; Nagy H., Széchenyi István University, Egyetem tér 1, Gyõr, 9026, Hungary; Wersényi G., Széchenyi István University, Egyetem tér 1., Hungary; Unnþórsson R., University of Iceland, School of Engineering and Natural Sciences, Faculty of Industrial Engineering, Mechanical Engineering and Computer Science, VR-2/V02-237, Reykjavik, Iceland","Introduction: As the number of people with visual impairments (that is, those who are blind or have low vision) is continuously increasing, rehabilitation and engineering researchers have identified the need to design sensory-substitution devices that would offer assistance and guidance to these people for performing navigational tasks. Auditory and haptic cues have been shown to be an effective approach towards creating a rich spatial representation of the environment, so they are considered for inclusion in the development of assistive tools that would enable people with visual impairments to acquire knowledge of the surrounding space in a way close to the visually based perception of sighted individuals. However, achieving efficiency through a sensory substitution device requires extensive training for visually impaired users to learn how to process the artificial auditory cues and convert them into spatial information. Methods: Considering all the potential advantages game-based learning can provide, we propose a new method for training sound localization and virtual navigational skills of visually impaired people in a 3D audio game with hierarchical levels of difficulty. The training procedure is focused on a multimodal (auditory and haptic) learning approach in which the subjects have been asked to listen to 3D sounds while simultaneously perceiving a series of vibrations on a haptic headband that corresponds to the direction of the sound source in space. Results: The results we obtained in a sound-localization experiment with 10 visually impaired people showed that the proposed training strategy resulted in significant improvements in auditory performance and navigation skills of the subjects, thus ensuring behavioral gains in the spatial perception of the environment. © 2017 AFB, All Rights Reserved.","","","O. Bălan; University Politehnica of Bucharest, Bucharest, Splaiul Independentei, 313, Romania; email: oana.balan@cs.pub.ro","","AFB Press","0145482X","","JVIBD","","English","J. Vis. Impairm. Blindn.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85020089826"
"Gupta A.; Khandelwal S.; Gandhi T.","Gupta, Avaneep (57208547218); Khandelwal, Sumeet (57208548951); Gandhi, Tapan (24343318600)","57208547218; 57208548951; 24343318600","Blind navigation using ambient crowd analysis","2018","Proceedings of the 8th International Advance Computing Conference, IACC 2018","","","8692112","131","135","4","2","10.1109/IADCC.2018.8692112","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85065055452&doi=10.1109%2fIADCC.2018.8692112&partnerID=40&md5=9a442a1a1e2f2d89378f63ef7a6cacab","Department of Mathematics, IIT Delhi, New Delhi, India; Department of Electrical Engineering, IIT Delhi, New Delhi, India","Gupta A., Department of Mathematics, IIT Delhi, New Delhi, India; Khandelwal S., Department of Mathematics, IIT Delhi, New Delhi, India; Gandhi T., Department of Electrical Engineering, IIT Delhi, New Delhi, India","Research in assistive technology has been on the rise over the last decade. Numerous solutions and consumer products have flooded the market to guide visually impaired making use of beacon technology, depth cameras and many more. Though certain products and solutions are available for highly structured and regular indoor environments, we are still a long way from an industrial level product for unstructured, dynamic and irregular outdoor environments. Our work harnesses the decision making power of sighted individuals and crowd as a group surrounding the visually impaired. This information extraction from the crowd along with coarse terrain mapping of major obstacles like footpath edges, walls and large pot holes will help the subject to navigate dynamic and irregular environments. This out of the box approach provides us the margin to use low grade equipment and develop algorithms with low computational complexity. The paper explains the theoretical aspects of this approach along with its proof of concept and some remarkable results achieved in real life implementation. © 2018 IEEE.","Assistive technology; Computer vision; Gait recognition; navigation; Object detection; Path planning","Computer vision; Consumer products; Decision making; Motion planning; Navigation; Object detection; Assistive technology; Gait recognition; Indoor environment; Low computational complexity; Outdoor environment; Real-life implementations; Theoretical aspects; Visually impaired; Pattern recognition","","Goswami A.","Institute of Electrical and Electronics Engineers Inc.","","978-153866678-4","","","English","Proc. Int. Adv. Comput. Conf., IACC","Conference paper","Final","","Scopus","2-s2.0-85065055452"
"Kandil M.; Albaghdadi R.; Alattar F.; Damaj I.","Kandil, Marwa (57209255650); Albaghdadi, Reem (57209243074); Alattar, Fatemah (57209249347); Damaj, Issam (26643638100)","57209255650; 57209243074; 57209249347; 26643638100","AmIE: An Ambient Intelligent Environment for Assisted Living","2019","2019 Advances in Science and Engineering Technology International Conferences, ASET 2019","","","8714499","","","","5","10.1109/ICASET.2019.8714499","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85067048101&doi=10.1109%2fICASET.2019.8714499&partnerID=40&md5=d8e9433ada6e11d33b730a85fe2f4522","Electrical and Computer Engineering Department, American University of Kuwait, Salmiya, Kuwait; Electrical and Computer Engineering Department, Rafik Hariri University, Mechref, Lebanon","Kandil M., Electrical and Computer Engineering Department, American University of Kuwait, Salmiya, Kuwait; Albaghdadi R., Electrical and Computer Engineering Department, American University of Kuwait, Salmiya, Kuwait; Alattar F., Electrical and Computer Engineering Department, American University of Kuwait, Salmiya, Kuwait; Damaj I., Electrical and Computer Engineering Department, Rafik Hariri University, Mechref, Lebanon","In the modern world of technology Internet-of-things (IoT) systems strives to provide an extensive interconnected and automated solutions for almost every life aspect. This paper proposes an IoT context-aware system to present an Ambient Intelligence (AmI) environment; such as an apartment, house, or a building; to assist blind, visually-impaired, and elderly people. The proposed system aims at providing an easy-to-utilize voice-controlled system to locate, navigate and assist users indoors. The main purpose of the system is to provide indoor positioning, assisted navigation, outside weather information, room temperature, people availability, phone calls and emergency evacuation when needed. The system enhances the user's awareness of the surrounding environment by feeding them with relevant information through a wearable device to assist them. In addition, the system is voice-controlled in both English and Arabic languages and the information are displayed as audio messages in both languages. The system design, implementation, and evaluation consider the constraints in common types of premises in Kuwait and in challenges, such as the training needed by the users. This paper presents cost-effective implementation options by the adoption of a Raspberry Pi microcomputer, Bluetooth Low Energy devices and an Android smart watch. © 2019 IEEE.","ambient intelligence; blind and visually impaired; context aware; Internet of Things","Ambient intelligence; Artificial intelligence; Assisted living; Cost effectiveness; Internet of things; Ambient intelligent environments; Blind and visually impaired; Bluetooth low energies (BTLE); Context-Aware; Context-aware systems; Cost-effective implementations; Internet of Things (IOT); Surrounding environment; Iodine compounds","","","Institute of Electrical and Electronics Engineers Inc.","","978-153868271-5","","","English","Adv. Sci. Eng. Technol. Int. Conf., ASET","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85067048101"
"Sowmiya S.; Valarmathi K.; Sathyavenkateshwaren S.; Gobinath M.; Thillaisivakavi S.","Sowmiya, S. (58244853800); Valarmathi, K. (57898270800); Sathyavenkateshwaren, S. (57202991969); Gobinath, M. (57205761830); Thillaisivakavi, S. (57205761526)","58244853800; 57898270800; 57202991969; 57205761830; 57205761526","Snag Detection Robot for Visually Impaired Steering and Blind Individuals","2018","Proceedings of the International Conference on Inventive Research in Computing Applications, ICIRCA 2018","","","8597178","167","171","4","3","10.1109/ICIRCA.2018.8597178","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85061507349&doi=10.1109%2fICIRCA.2018.8597178&partnerID=40&md5=1c326d3170f441143c6dc27833388289","Department of Computer Science and Engineering, Panimalar Engineering College, Chennai, India","Sowmiya S., Department of Computer Science and Engineering, Panimalar Engineering College, Chennai, India; Valarmathi K., Department of Computer Science and Engineering, Panimalar Engineering College, Chennai, India; Sathyavenkateshwaren S., Department of Computer Science and Engineering, Panimalar Engineering College, Chennai, India; Gobinath M., Department of Computer Science and Engineering, Panimalar Engineering College, Chennai, India; Thillaisivakavi S., Department of Computer Science and Engineering, Panimalar Engineering College, Chennai, India","The analysis field that created several of today's robots supported computing currently being known as psychological feature computing. It makes fast progress towards human brain. Everyday new reports and stories started regarding innovations in AI that have the potential to alter our lives. Our final goal is to create a robotic system to assist visually impaired folks navigate reception and improve the standard of their life. The main aim of the proposed system is to expand the electronic travel aid for the blind and visually impaired folks by emerging into the ultrasonic technology. Blind individuals tackle variety of visual challenges each day. This system represents an innovative project design and implementation of an Ultrasonic Navigation system in order to provide fully automatic obstacle detection with audible notification for blind folks. This blind guidance system is safe, resolute and profitable. © 2018 IEEE.","Blind spots; collision avoidance; visually impaired","Collision avoidance; Electronic guidance systems; Navigation systems; Obstacle detectors; Robots; Blind spots; Electronic travel aidss; Innovative projects; Obstacle detection; Psychological features; Ultrasonic navigation systems; Ultrasonic technology; Visually impaired; Ultrasonic applications","","","Institute of Electrical and Electronics Engineers Inc.","","978-153862456-2","","","English","Proc. Int. Conf. Inven. Res. Comput. Appl., ICIRCA","Conference paper","Final","","Scopus","2-s2.0-85061507349"
"Saranya K.; Kiran Aditya G.V.; Mohana Muhil M.; Vijayashaarathi S.; Imtheyas Basha A.; Shyamala Devi R.; Kavin A.","Saranya, K. (57212317648); Kiran Aditya, G.V. (57218321652); Mohana Muhil, M. (57218318177); Vijayashaarathi, S. (56644832700); Imtheyas Basha, A. (57218320276); Shyamala Devi, R. (57218321432); Kavin, A. (57218318759)","57212317648; 57218321652; 57218318177; 56644832700; 57218320276; 57218321432; 57218318759","Design and development of neuro-ocr based assistive system for text detection with voice output","2020","Journal of Advanced Research in Dynamical and Control Systems","12","7 Special Issue","","593","596","3","0","10.5373/JARDCS/V12SP7/20202145","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85088804791&doi=10.5373%2fJARDCS%2fV12SP7%2f20202145&partnerID=40&md5=1eabd16224f0b010e4ed9bda03639887","Department of ECE, Sona College of Technology (An Autonomous Institution), Salem, India; Sona College of Technology (An Autonomous Institution), Salem, India; Department of ECE, M.A.M. College of Engineering (Affiliated to Anna University), Trichy, India","Saranya K., Department of ECE, Sona College of Technology (An Autonomous Institution), Salem, India; Kiran Aditya G.V., Sona College of Technology (An Autonomous Institution), Salem, India; Mohana Muhil M., Sona College of Technology (An Autonomous Institution), Salem, India; Vijayashaarathi S., Department of ECE, Sona College of Technology (An Autonomous Institution), Salem, India; Imtheyas Basha A., Sona College of Technology (An Autonomous Institution), Salem, India; Shyamala Devi R., Department of ECE, M.A.M. College of Engineering (Affiliated to Anna University), Trichy, India; Kavin A., Sona College of Technology (An Autonomous Institution), Salem, India","Our proposed project mainly focus the object’s text regions. By using artificial neural network, the project offers a novel text localization algorithm. It is prepared by learning gradient features of stroke orientations and distributions of edge pixels. Using off-the-shelf optical character identification software, the text font in the localized text regions is binarized. The renowned text codes are converted into audio output for the blind users. Object distance is measured using ultrasonic sensor. It automatically focuses the text regions from the object. Text extraction is done using neural network based OCR. Text to voice conversion is done using phonematic concatenation for visually impaired people. © 2020, Institute of Advanced Scientific Research, Inc.. All rights reserved.","Background Subtraction; Feature Extraction; OCR (Optical Character Recognition)","","","","Institute of Advanced Scientific Research, Inc.","1943023X","","","","English","J. Adv. Res. Dyn. Control. Syst.","Article","Final","","Scopus","2-s2.0-85088804791"
"Nijhawan S.S.; Kumar A.; Bhardwaj S.; Nijhawan G.","Nijhawan, Siddharth Sagar (57215414381); Kumar, Aditi (57216227915); Bhardwaj, Shubham (57551703300); Nijhawan, Geeta (56204069400)","57215414381; 57216227915; 57551703300; 56204069400","Real-time object detection for visually impaired with optimal combination of scores","2019","Proceedings of the 2019 6th International Conference on Computing for Sustainable Global Development, INDIACom 2019","","","8991199","307","311","4","2","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85080966019&partnerID=40&md5=45306c3a5fd45b0b47237568406b79b6","Ece, Netaji Subhas Institute of Technology, New Delhi, India; Statistics, Delhi University, Delhi, India; Manav Rachna International University, Faridabad, India","Nijhawan S.S., Ece, Netaji Subhas Institute of Technology, New Delhi, India; Kumar A., Statistics, Delhi University, Delhi, India; Bhardwaj S., Ece, Netaji Subhas Institute of Technology, New Delhi, India; Nijhawan G., Manav Rachna International University, Faridabad, India","In this paper, we propose a computer vision based object detection mechanism for visually impaired individuals by optimally combining the detection scores to aid their indoor navigation. Proposed framework comprises of three stages, viz. image acquisition, object detection and class estimation followed by score combination to generate the final classes of objects. The detection is performed on the image frames captured through wireless webcam and by applying two powerful deep learning algorithms, SSD and YOLO, separately. After generating the scores for each class, they are optimally combined using Proportional Conflict Resolving principle to generate fused set of scores. Based on these score values, object category is chosen with the category having the highest probabilistic score. The particular audio file narrating the object name is selected and the audio signal is sent to the blind user through the attached wearable headphone. The proposed framework achieved a high Mean Average Precision score of 86.15. Model performs well in an indoor environment and successfully aids a blind individual in mobility and mapping surrounding environment effectively. © 2019 Bharati Vidyapeeth, New Delhi. Copy Right in Bulk will be transferred to IEEE by Bharati Vidyapeeth.","Computer Vision; Deep Learning; Object Detection; Score Fusion","Computer vision; Deep learning; Learning algorithms; Object recognition; Sustainable development; Conflict resolving; Detection mechanism; In-door navigations; Indoor environment; Optimal combination; Probabilistic scores; Score fusion; Surrounding environment; Object detection","","","Institute of Electrical and Electronics Engineers Inc.","","978-938054434-2","","","English","Proc. Int. Conf. Comput. Sustain. Glob. Dev., INDIACom","Conference paper","Final","","Scopus","2-s2.0-85080966019"
"Lee S.; Kang M.","Lee, Sanghyeon (57205299287); Kang, Moonsik (53363725700)","57205299287; 53363725700","Object detection system for the blind with voice command and guidance","2019","IEIE Transactions on Smart Processing and Computing","8","5","","373","379","6","2","10.5573/IEIESPC.2019.8.5.373","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85087439537&doi=10.5573%2fIEIESPC.2019.8.5.373&partnerID=40&md5=b6af80eb97649fce3746be0fd33bde47","Department of Electronic Engineering, Gangneung-Wonju National University / Gangneung, Gangwon, 25457, South Korea","Lee S., Department of Electronic Engineering, Gangneung-Wonju National University / Gangneung, Gangwon, 25457, South Korea; Kang M., Department of Electronic Engineering, Gangneung-Wonju National University / Gangneung, Gangwon, 25457, South Korea","As object recognition technology has developed recently, various technologies have been applied to autonomous vehicles, robots, and industrial facilities. However, the benefits of these technologies are not reaching the visually impaired, who need it the most. In this paper, we proposed an object detection system for the blind using deep learning technologies. We use voice recognition technology in order to know what objects a blind person wants, and then to find the objects via object recognition. Furthermore, a voice guidance technique is used to inform sight-impaired persons as to the location of objects. The object recognition deep learning model utilizes the Single Shot Multibox Detector (SSD) neural network architecture, and voice recognition is designed through speech-to-text (STT) technology. In addition, a voice announcement is synthesized using text-to-speech (TTS) to make it easier for the blind to get information about objects. The control system is based on the Arduino microprocessor. As a result, we implement an efficient object-detection system that helps the blind find objects in a specific space without help from others, and the system is analyzed through experiments to verify performance. © 2019 The Institute of Electronics and Information Engineers.","Real-time object detection; SSD neural network; STT voice recognition; TTS-based technology; Voice synthesis","Character recognition; Deep learning; Engineering education; Industrial robots; Learning systems; Mobility aids for blind persons; Network architecture; Object detection; Object recognition; Efficient object detections; Industrial facilities; Learning models; Learning technology; Object detection systems; Various technologies; Visually impaired; Voice announcements; Speech recognition","M. Kang; Department of Electronic Engineering, Gangneung-Wonju National University / Gangneung, Gangwon, 25457, South Korea; email: mskang@gwnu.ac.kr","","Institute of Electronics Engineers of Korea","22875255","","","","English","IEIE Trans.  Smart Process  Comput.","Article","Final","","Scopus","2-s2.0-85087439537"
"Presti G.; Ahmetovic D.; Ducci M.; Bernareggi C.; Ludovico L.; Baratè A.; Avanzini F.; Mascetti S.","Presti, Giorgio (56407087000); Ahmetovic, Dragan (53867884600); Ducci, Mattia (57200504661); Bernareggi, Cristian (56266643600); Ludovico, Luca (8684305800); Baratè, Adriano (14031183900); Avanzini, Federico (7005300654); Mascetti, Sergio (8919148700)","56407087000; 53867884600; 57200504661; 56266643600; 8684305800; 14031183900; 7005300654; 8919148700","Watchout: Obstacle sonifcation for people with visual impairment or blindness","2019","ASSETS 2019 - 21st International ACM SIGACCESS Conference on Computers and Accessibility","","","","402","413","11","43","10.1145/3308561.3353779","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85074914222&doi=10.1145%2f3308561.3353779&partnerID=40&md5=a5a6f526d14beb6351437459de5008a2","Università degli Studi di Milano Dipartimento di Informatica, Italy; Università degli Studi di Torino, Dipartimento di Matematica, Italy","Presti G., Università degli Studi di Milano Dipartimento di Informatica, Italy; Ahmetovic D., Università degli Studi di Torino, Dipartimento di Matematica, Italy; Ducci M., Università degli Studi di Milano Dipartimento di Informatica, Italy; Bernareggi C., Università degli Studi di Milano Dipartimento di Informatica, Italy; Ludovico L., Università degli Studi di Milano Dipartimento di Informatica, Italy; Baratè A., Università degli Studi di Milano Dipartimento di Informatica, Italy; Avanzini F., Università degli Studi di Milano Dipartimento di Informatica, Italy; Mascetti S., Università degli Studi di Milano Dipartimento di Informatica, Italy","Independent mobility is one of the main challenges for blind or visually impaired (BVI) people. In particular, BVI people often need to identify and avoid nearby obstacles, for example a bicycle parked on the sidewalk. This is generally achieved with a combination of residual vision, hearing and haptic sensing using the white cane. However, in many cases, BVI people can only perceive obstacles at short distance (typically about 1m, i.e., the white cane detection range), in other situations obstacles are hard to detect (e.g., those elevated from the ground), while others should not be hit by the white cane (e.g., a standing person). Thus, some time and effort are required to identify the object in order to understand how to avoid it. A solution to these problems can be found in recent computer vision techniques that can run on mobile and wearable devices to detect obstacles at a distance. However, in addition to detecting obstacles, it is also necessary to convey information about them to a BVI user. This contribution presents WatchOut, a sonifcation technique for conveying real-time information about the main characteristics of an obstacle to a BVI person, who can then use this additional feedback to safely navigate in the environment. WatchOut was designed with a user-centric approach, involving two iterations of online questionnaires with BVI participants in order to defne, improve and evaluate the sonifcation technique. WatchOut was implemented and tested as a module of a mobile app that detects obstacles using state-of-the-art computer vision technology. Results show that the system is considered usable, and can guide the users to avoid more than 85% of the obstacles. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Navigation assistive technologies; Obstacle avoidance; Sonifcation; Visual impairment","Audition; Collision avoidance; Ophthalmology; Patient rehabilitation; Surveys; Assistive technology; Computer vision techniques; Computer vision technology; Online questionnaire; Real-time information; Sonifcation; Visual impairment; Visually impaired; Computer vision","","","Association for Computing Machinery, Inc","","978-145036676-2","","","English","ASSETS - Int. ACM SIGACCESS Conf. Comput. Access.","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85074914222"
"Alam M.M.; Arefin S.E.; Alim M.A.; Adib S.I.; Rahman M.A.","Alam, Md. Mazidul (57205486460); Arefin, Sayed Erfan (56582127900); Alim, Miraj Al (57194214204); Adib, Samiul Islam (57194205293); Rahman, Md. Abdur (57212184292)","57205486460; 56582127900; 57194214204; 57194205293; 57212184292","Indoor localization system for assisting visually impaired people","2017","ECCE 2017 - International Conference on Electrical, Computer and Communication Engineering","","","7912927","333","338","5","7","10.1109/ECACE.2017.7912927","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85019225753&doi=10.1109%2fECACE.2017.7912927&partnerID=40&md5=a2b52f403fc9ddb7823e1c678fa595a7","Department of EEE, American International University-Bangladesh, Bangladesh; Department of CSE, BRAC University, Bangladesh","Alam M.M., Department of EEE, American International University-Bangladesh, Bangladesh; Arefin S.E., Department of CSE, BRAC University, Bangladesh; Alim M.A., Department of EEE, American International University-Bangladesh, Bangladesh; Adib S.I., Department of EEE, American International University-Bangladesh, Bangladesh; Rahman M.A., Department of EEE, American International University-Bangladesh, Bangladesh","In this paper we developed an image processing based indoor localization system with color detection technique of connected objects. By using color detection technique our system can pinpoint a user's location with very high accuracy for real-Time applications. We used our system to filter out an image for a specific color and extracted pixel co-ordinates for that image. User's location is then determined by comparing the matrix for those values against a pre-created matrix of training images. We successfully conducted experiments in indoor environments as well and they yielded very good results. After analyzing these results, we propose to integrate our localization system with indoor navigation systems which can be used for blind people where accuracy is the most important element. To further facilitate the navigation process we also developed an android based application. © 2017 IEEE.","Color Segmentation; Connected object detection; Image Processing; Indoor localization; Wireless communication","Color; Color image processing; Image processing; Image segmentation; Navigation systems; Object detection; Object recognition; Wireless telecommunication systems; Color segmentation; Indoor localization; Indoor localization systems; Indoor navigation system; Localization system; Real-time application; Visually impaired people; Wireless communications; Indoor positioning systems","","","Institute of Electrical and Electronics Engineers Inc.","","978-150905626-2","","","English","ECCE - Int. Conf. Electr., Comput. Commun. Eng.","Conference paper","Final","","Scopus","2-s2.0-85019225753"
"Kotalwar G.; Prajapati J.; Patil S.; Moralwar D.; Jewani K.","Kotalwar, Ganesh (57226090798); Prajapati, Jigisha (57216490360); Patil, Sharayu (57216490041); Moralwar, Dilip (57216490638); Jewani, Kajal (57205288848)","57226090798; 57216490360; 57216490041; 57216490638; 57205288848","Smart Navigation Application for Visually Challenged People in Indoor Premises","2020","Lecture Notes on Data Engineering and Communications Technologies","31","","","998","1004","6","2","10.1007/978-3-030-24643-3_119","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85083669449&doi=10.1007%2f978-3-030-24643-3_119&partnerID=40&md5=212e46c702e5ff32eb23b7190e67e68e","Computer Engineering Department, Vivekanand Education Society’s Institute of Technology, Mumbai, India","Kotalwar G., Computer Engineering Department, Vivekanand Education Society’s Institute of Technology, Mumbai, India; Prajapati J., Computer Engineering Department, Vivekanand Education Society’s Institute of Technology, Mumbai, India; Patil S., Computer Engineering Department, Vivekanand Education Society’s Institute of Technology, Mumbai, India; Moralwar D., Computer Engineering Department, Vivekanand Education Society’s Institute of Technology, Mumbai, India; Jewani K., Computer Engineering Department, Vivekanand Education Society’s Institute of Technology, Mumbai, India","It is very difficult for a visually impaired person to perform its day to day job with ease. Since Mobile applications are largely used among people they have high potential in aiding blind people. In this paper, we are trying to present an application to assist visually disabled. The android application will be using Deep learning object detection and identification techniques such as YOLO, R-CNN etc. The need for navigation help among blind people and a broader look at the advanced technology becoming available in today’s world motivated us to develop this project. Technology is something which is there to ease tasks for human beings. Hence, in this project, we use technology to solve the problems of visually impaired people. The project aims to help users in navigation with the use of technology and our engineering profession motivates us to use the technology we have. © Springer Nature Switzerland AG 2020.","Deep learning; Object detection; R-CNN; YOLO","Deep learning; Indoor positioning systems; Navigation; Object detection; Professional aspects; Advanced technology; Android applications; Detection and identifications; Engineering profession; Learning objects; Mobile applications; Visually impaired people; Visually impaired persons; Engineering education","G. Kotalwar; Computer Engineering Department, Vivekanand Education Society’s Institute of Technology, Mumbai, India; email: 2015ganesh.kotalwar@ves.ac.in","","Springer Science and Business Media Deutschland GmbH","23674512","","","","English","Lecture. Notes. Data Eng. Commun. Tech.","Book chapter","Final","","Scopus","2-s2.0-85083669449"
"De Zoysa D.S.S.; Sampath J.M.; De Seram E.M.P.; Dissanayake D.M.I.D.; Wijerathna L.; Thelijjagoda S.","De Zoysa, D.S.S. (57204428012); Sampath, J.M. (57204419279); De Seram, E.M.P. (57204421794); Dissanayake, D.M.I.D. (58452410400); Wijerathna, L. (57221926263); Thelijjagoda, S. (17343051500)","57204428012; 57204419279; 57204421794; 58452410400; 57221926263; 17343051500","Project Bhashitha - Mobile based optical character recognition and text-to-speech system","2018","13th International Conference on Computer Science and Education, ICCSE 2018","","","8468858","623","628","5","3","10.1109/ICCSE.2018.8468858","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85055565948&doi=10.1109%2fICCSE.2018.8468858&partnerID=40&md5=8f66b0e9ff166a77ce93b838a188265b","Department of Software Engineering, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Department of Information Technology, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Department of Information Systems Engineering, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka","De Zoysa D.S.S., Department of Software Engineering, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Sampath J.M., Department of Software Engineering, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; De Seram E.M.P., Department of Software Engineering, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Dissanayake D.M.I.D., Department of Information Technology, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Wijerathna L., Department of Software Engineering, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Thelijjagoda S., Department of Information Systems Engineering, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka","In the modern era when computers play a vital role in people's day today activities, visually impaired people face numerous problems when accessing printed text using existing technologies. This will rise to the need for the improvement of devices that could bring relief to this tasks that the blind people have to go beginning to end. Due to digitization of books there are many excellent attempts at building a vigorous document analysis system in industries and research labs, but this is only for those who are able to visible aided. 'Bhashitha' is an android based mobile application contains OCR and TTS for Sinhala, Tamil and English languages as single product by resolving problems in existing systems. In order to make the proposed system, user needs to acquire printed document as optical image using a camera of the mobile phone. The image skew will reduce the OCR accuracy drastically due to the angle view of the document. Therefore after doing the image skew detection optical image is passing to the OCR engine to convert the image to character streams representing letters of recognized words. Finally, the converted text output is access by TTS system to convert the textual content into a voice output. Additionally, it consists audio assist system to navigate through the pages in the diligence for differently abled users. This is easier, portable and faster solution comparing to the existing systems which are made for visually impaired. © 2018 IEEE.","Image Processing; Image Skew Detection and Correction; Optical Character Recognition (OCR); Sinhala; Text-to-Speech (TTS); Visually Disabled Users","Cellular telephone systems; Education computing; Geometrical optics; Human rehabilitation equipment; Image processing; Optical character recognition; Optical data processing; Optical character recognition (OCR); Sinhala; Skew detection; Text to speech; Visually Disabled Users; Speech recognition","","","Institute of Electrical and Electronics Engineers Inc.","","978-153865495-8","","","English","Int. Conf. Comput. Sci. Educ., ICCSE","Conference paper","Final","","Scopus","2-s2.0-85055565948"
"Bashiri F.S.; LaRose E.; Badger J.C.; D’Souza R.M.; Yu Z.; Peissig P.","Bashiri, Fereshteh S. (57200214339); LaRose, Eric (57193547169); Badger, Jonathan C. (57195069524); D’Souza, Roshan M. (7103281962); Yu, Zeyun (8964323900); Peissig, Peggy (16043391700)","57200214339; 57193547169; 57195069524; 7103281962; 8964323900; 16043391700","Object detection to assist visually impaired people: A deep neural network adventure","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11241 LNCS","","","500","510","10","45","10.1007/978-3-030-03801-4_44","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85057187793&doi=10.1007%2f978-3-030-03801-4_44&partnerID=40&md5=e397ccd49e899c593e0b8d49111cd158","Marshfield Clinic Research Institute, Marshfield, United States; University of Wisconsin-Milwaukee, Milwaukee, United States","Bashiri F.S., Marshfield Clinic Research Institute, Marshfield, United States, University of Wisconsin-Milwaukee, Milwaukee, United States; LaRose E., Marshfield Clinic Research Institute, Marshfield, United States; Badger J.C., Marshfield Clinic Research Institute, Marshfield, United States; D’Souza R.M., University of Wisconsin-Milwaukee, Milwaukee, United States; Yu Z., University of Wisconsin-Milwaukee, Milwaukee, United States; Peissig P., Marshfield Clinic Research Institute, Marshfield, United States","Blindness or vision impairment, one of the top ten disabilities among men and women, targets more than 7 million Americans of all ages. Accessible visual information is of paramount importance to improve independence and safety of blind and visually impaired people, and there is a pressing need to develop smart automated systems to assist their navigation, specifically in unfamiliar healthcare environments, such as clinics, hospitals, and urgent cares. This contribution focused on developing computer vision algorithms composed with a deep neural network to assist visually impaired individual’s mobility in clinical environments by accurately detecting doors, stairs, and signages, the most remarkable landmarks. Quantitative experiments demonstrate that with enough number of training samples, the network recognizes the objects of interest with an accuracy of over 98% within a fraction of a second. © Springer Nature Switzerland AG 2018.","Assistive technology for visually impaired people; Data mining and knowledge discovery; Machine learning and predictive modeling; Mobile health and wearable devices","Automation; Data mining; Data visualization; Object detection; Wearable technology; Assistive technology for visually impaired; Blind and visually impaired; Computer vision algorithms; Data mining and knowledge discovery; Predictive modeling; Quantitative experiments; Visually impaired people; Wearable devices; Deep neural networks","F.S. Bashiri; Marshfield Clinic Research Institute, Marshfield, United States; email: bashiri.fereshteh@marshfieldresearch.org","Xu K.; Lin S.; Boyle R.; Alsallakh B.; Turek M.; Ramalingam S.; Bebis G.; Parvin B.; Yang J.; Ventura J.; Koracin D.; Cuervo E.","Springer Verlag","03029743","978-303003800-7","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","","Scopus","2-s2.0-85057187793"
"Marques G.H.M.; Einloft D.C.; Bergamin A.C.P.; Marek J.A.; Maidana R.G.; Campos M.B.; Manssour I.H.; Amory A.M.","Marques, Guilherme H. M. (57202467565); Einloft, Daniel C. (57202467180); Bergamin, Augusto C. P. (57200176429); Marek, Joice A. (57202463438); Maidana, Renan G. (57202464093); Campos, Marcia B. (55803627100); Manssour, Isabel H. (8525042500); Amory, Alexandre M. (8293027500)","57202467565; 57202467180; 57200176429; 57202463438; 57202464093; 55803627100; 8525042500; 8293027500","Donnie robot: Towards an accessible and educational robot for visually impaired people","2017","Proceedings - 2017 LARS 14th Latin American Robotics Symposium and 2017 5th SBR Brazilian Symposium on Robotics, LARS-SBR 2017 - Part of the Robotics Conference 2017","2017-December","","","1","6","5","6","10.1109/SBR-LARS-R.2017.8215273","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85048501154&doi=10.1109%2fSBR-LARS-R.2017.8215273&partnerID=40&md5=c608ecdd3ce6e45fa86b457c03cba8a8","PUCRS University, Faculdade de Informatica, Porto Alegre, RS, Brazil","Marques G.H.M., PUCRS University, Faculdade de Informatica, Porto Alegre, RS, Brazil; Einloft D.C., PUCRS University, Faculdade de Informatica, Porto Alegre, RS, Brazil; Bergamin A.C.P., PUCRS University, Faculdade de Informatica, Porto Alegre, RS, Brazil; Marek J.A., PUCRS University, Faculdade de Informatica, Porto Alegre, RS, Brazil; Maidana R.G., PUCRS University, Faculdade de Informatica, Porto Alegre, RS, Brazil; Campos M.B., PUCRS University, Faculdade de Informatica, Porto Alegre, RS, Brazil; Manssour I.H., PUCRS University, Faculdade de Informatica, Porto Alegre, RS, Brazil; Amory A.M., PUCRS University, Faculdade de Informatica, Porto Alegre, RS, Brazil","Robotics has been used to draw the attention of young students to computing and engineering. Unfortunately, most hardware and software resources used to teach programming and robotics are not adequate for students with visual impairment (VI). For instance, most programming environments for young students are highly visual and the commercial robotics kits were not built for people with VI. This paper describes the main design requirements so that both the proposed robot hardware and the software can also be used by people with VI. The hardware part is the focus of this paper, a robot called Donnie. It consists of open hardware, a design with low-cost and easy to find parts, and a 3D printed structure. Our main contribution is the presentation of a low-cost Arduino-based robot, compatible with Player robotic framework and integrated with sound feedback and text-to-speech. © 2017 IEEE.","Assistive Robotics; Audio-based navigation; Blind Programmers; Educational Robotics","Educational robots; Hardware; Machine design; Robot programming; Robotics; Students; Assistive robotics; Audio-based navigation; Blind Programmers; Educational robotics; Hardware and software; Printed structures; Programming environment; Visually impaired people; Robots","","Tonidandel F.; Todt E.","Institute of Electrical and Electronics Engineers Inc.","","978-153860956-9","","","English","Proc. - LARS Lat. Am. Robot. Symp. SBR Braz. Symp. Robot., LARS-SBR - Part Robot. Conf.","Conference paper","Final","","Scopus","2-s2.0-85048501154"
"Joe Louis Paul I.; Sasirekha S.; Mohanavalli S.; Jayashree C.; Moohana Priya P.; Monika K.","Joe Louis Paul, I. (56521563600); Sasirekha, S. (57212600875); Mohanavalli, S. (37075351500); Jayashree, C. (57215363051); Moohana Priya, P. (57215341995); Monika, K. (57215361897)","56521563600; 57212600875; 37075351500; 57215363051; 57215341995; 57215361897","Smart Eye for Visually Impaired-An aid to help the blind people","2019","ICCIDS 2019 - 2nd International Conference on Computational Intelligence in Data Science, Proceedings","","","8862066","","","","33","10.1109/ICCIDS.2019.8862066","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85074170965&doi=10.1109%2fICCIDS.2019.8862066&partnerID=40&md5=e26f328a9b70dd93383e5ca6ee215074","Department of Information Technology, SSN College of Engineering, Kalavakkam, Chennai, India","Joe Louis Paul I., Department of Information Technology, SSN College of Engineering, Kalavakkam, Chennai, India; Sasirekha S., Department of Information Technology, SSN College of Engineering, Kalavakkam, Chennai, India; Mohanavalli S., Department of Information Technology, SSN College of Engineering, Kalavakkam, Chennai, India; Jayashree C., Department of Information Technology, SSN College of Engineering, Kalavakkam, Chennai, India; Moohana Priya P., Department of Information Technology, SSN College of Engineering, Kalavakkam, Chennai, India; Monika K., Department of Information Technology, SSN College of Engineering, Kalavakkam, Chennai, India","This paper presents an idea of developing a smart system which can assist the visually impaired people in their daily activities. Actually, there are many challenges faced by visually impaired people. In most cases, they require constant support in almost all scenarios especially in their day to day activities. Some of the major challenges include difficulty in moving from one place to another without the assistance of someone. Other challenges include difficulty in recognizing people, detecting obstacles, etc. In order to count avert this situation, we propose a 'smart eye system' in this work. The device is a voice enabled system that would direct the visually challenged person in their day to day works. The device combines the various available technologies and integrates them into a single multipurpose device that can be used by the visually impaired. The paper discusses about the design of such a system and the challenges involved in designing the device. © 2019 IEEE.","Face recognition; Obstacle detection; Route navigation; Sensors; Visually impaired; Voice commands","Artificial intelligence; Obstacle detectors; Sensors; Blind people; Daily activity; Obstacle detection; Route navigation; Smart System; Visually impaired; Visually impaired people; Voice command; Face recognition","","","Institute of Electrical and Electronics Engineers Inc.","","978-153869471-8","","","English","ICCIDS - Int. Conf. Comput. Intell. Data Sci., Proc.","Conference paper","Final","","Scopus","2-s2.0-85074170965"
"Chen Q.; Wu L.; Chen Z.; Lin P.; Cheng S.; Wu Z.","Chen, Qiaoyu (57212223200); Wu, Lijun (56456879100); Chen, Zhicong (57194563180); Lin, Peijie (48261290300); Cheng, Shuying (8691666600); Wu, Zhenhui (57212194572)","57212223200; 56456879100; 57194563180; 48261290300; 8691666600; 57212194572","Smartphone Based Outdoor Navigation and Obstacle Avoidance System for the Visually Impaired","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11909 LNAI","","","26","37","11","10","10.1007/978-3-030-33709-4_3","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85076254840&doi=10.1007%2f978-3-030-33709-4_3&partnerID=40&md5=5dfa1ef3420e2f49e5a135da2a1faef8","College of Physics and Information Engineering, Fuzhou University, Fuzhou, 350116, China; State Grid Fuzhou Electric Power Supply Company, Fuzhou, 350116, China","Chen Q., College of Physics and Information Engineering, Fuzhou University, Fuzhou, 350116, China; Wu L., College of Physics and Information Engineering, Fuzhou University, Fuzhou, 350116, China; Chen Z., College of Physics and Information Engineering, Fuzhou University, Fuzhou, 350116, China; Lin P., College of Physics and Information Engineering, Fuzhou University, Fuzhou, 350116, China; Cheng S., College of Physics and Information Engineering, Fuzhou University, Fuzhou, 350116, China; Wu Z., State Grid Fuzhou Electric Power Supply Company, Fuzhou, 350116, China","Interlaced roads and unexpected obstacles restrict the blind from traveling. Existing outdoor blind auxiliary systems are bulky or costly, and some of them cannot even feedback the type or distance of obstacles. It is important for auxiliary blind systems to provide navigation, obstacle detection and ranging functions with affordable price and portable size. This paper presents an outdoor navigation system based on smartphone for the visually impaired, which can also help them avoid multi-type dangerous obstacles. Geographic information obtained from GPS receiving module is processed by professional navigation API to provide directional guidance. In order to help the visually impaired avoid obstacle, SSD-MobileNetV2 is retrained by a self-collected dataset with 4500 images, for better detecting the typical obstacles on the road, i.e. car, motorcycle, electric bicycle, bicycle, and pedestrian. Then, a light-weight monocular ranging method is employed to estimate the obstacle’s distance. Based on category and distance, the risk level of obstacle is evaluated, which is timely conveyed to the blind via different tunes. Field tests show that the retrained SSD-MobileNetV2 model can detect obstacles with considerable precision, and the vision-based ranging method can effectively estimate distance. © Springer Nature Switzerland AG 2019.","Blind auxiliary system; Ranging; Smartphone; SSD-MobileNetV2; Walking navigation","Artificial intelligence; Auxiliary equipment; Navigation systems; Obstacle detectors; Range finding; Auxiliary systems; Electric bicycles; Geographic information; Obstacle detection; Obstacle-avoidance system; Outdoor navigation; Outdoor navigation systems; SSD-MobileNetV2; Smartphones","L. Wu; College of Physics and Information Engineering, Fuzhou University, Fuzhou, 350116, China; email: lijun.wu@fzu.edu.cn","Chamchong R.; Wong K.W.","Springer","03029743","978-303033708-7","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","","Scopus","2-s2.0-85076254840"
"Baskaran H.; Leng R.L.M.; Rahim F.A.; Rusli M.E.","Baskaran, Hasventhran (57211207528); Leng, Rachel Lum Mei (57211209685); Rahim, Fiza Abdul (57350579500); Rusli, Mohd Ezanee (16246214600)","57211207528; 57211209685; 57350579500; 16246214600","Smart vision: Assistive device for the visually impaired community using online computer vision service","2019","2019 IEEE 4th International Conference on Computer and Communication Systems, ICCCS 2019","","","8821635","730","734","4","5","10.1109/CCOMS.2019.8821635","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85072985146&doi=10.1109%2fCCOMS.2019.8821635&partnerID=40&md5=83824f0f2724b1897465b928b346593a","College of Computing and Informatics, Universiti Tenaga Nasional, Kajang, Malaysia; Institute of Informatics and Computing in Energy (IICE), Universiti Tenaga Nasional, Kajang, Malaysia","Baskaran H., College of Computing and Informatics, Universiti Tenaga Nasional, Kajang, Malaysia; Leng R.L.M., College of Computing and Informatics, Universiti Tenaga Nasional, Kajang, Malaysia; Rahim F.A., College of Computing and Informatics, Universiti Tenaga Nasional, Kajang, Malaysia, Institute of Informatics and Computing in Energy (IICE), Universiti Tenaga Nasional, Kajang, Malaysia; Rusli M.E., College of Computing and Informatics, Universiti Tenaga Nasional, Kajang, Malaysia, Institute of Informatics and Computing in Energy (IICE), Universiti Tenaga Nasional, Kajang, Malaysia","The visually impaired community face several difficulties in their daily life. With the increase of commercial assistive devices, it can greatly improve their lives, especially for navigation and orientation, which are their main obstacles. Recently, it has been introduced many powerful online image processing services, based on Machine learning and deep learning. Microsoft is one of the main players in online image processing services. In this paper, we discussed a prototype development by utilizing the online image processing service, Microsoft Cognitive Service. We expect to further evaluate the accuracy and the usage analytics in order to understand long term system behavior. © 2019 IEEE.","Blind assistive; Vision cognitive services; Visually impaired","Deep learning; Assistive; Assistive devices; Daily lives; On-machines; Online image processing; Prototype development; System behaviors; Visually impaired; Computer vision","","","Institute of Electrical and Electronics Engineers Inc.","","978-172811322-7","","","English","IEEE Int. Conf. Comput. Commun. Syst., ICCCS","Conference paper","Final","","Scopus","2-s2.0-85072985146"
"Nair V.; Budhai M.; Olmschenk G.; Seiple W.H.; Zhu Z.","Nair, Vishnu (57213507933); Budhai, Manjekar (57206472003); Olmschenk, Greg (55360998300); Seiple, William H. (7005218017); Zhu, Zhigang (7404803571)","57213507933; 57206472003; 55360998300; 7005218017; 7404803571","ASSIST: Personalized indoor navigation via multimodal sensors and high-level semantic information","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11134 LNCS","","","128","143","15","8","10.1007/978-3-030-11024-6_9","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85061754935&doi=10.1007%2f978-3-030-11024-6_9&partnerID=40&md5=dc13ff5651655f7daab7848f03bab207","Department of Computer Science, The City College of New York, New York, NY, United States; Department of Computer Science, CUNY Graduate Center, New York, NY, United States; Lighthouse Guild, New York, NY, United States","Nair V., Department of Computer Science, The City College of New York, New York, NY, United States; Budhai M., Department of Computer Science, The City College of New York, New York, NY, United States; Olmschenk G., Department of Computer Science, The City College of New York, New York, NY, United States, Department of Computer Science, CUNY Graduate Center, New York, NY, United States; Seiple W.H., Lighthouse Guild, New York, NY, United States; Zhu Z., Department of Computer Science, The City College of New York, New York, NY, United States, Department of Computer Science, CUNY Graduate Center, New York, NY, United States","Blind & visually impaired (BVI) individuals and those with Autism Spectrum Disorder (ASD) each face unique challenges in navigating unfamiliar indoor environments. In this paper, we propose an indoor positioning and navigation system that guides a user from point A to point B indoors with high accuracy while augmenting their situational awareness. This system has three major components: location recognition (a hybrid indoor localization app that uses Bluetooth Low Energy beacons and Google Tango to provide high accuracy), object recognition (a body-mounted camera to provide the user momentary situational awareness of objects and people), and semantic recognition (map-based annotations to alert the user of static environmental characteristics). This system also features personalized interfaces built upon the unique experiences that both BVI and ASD individuals have in indoor wayfinding and tailors its multimodal feedback to their needs. Here, the technical approach and implementation of this system are discussed, and the results of human subject tests with both BVI and ASD individuals are presented. In addition, we discuss and show the system’s user-centric interface and present points for future work and expansion. © Springer Nature Switzerland AG 2019.","Bluetooth beacons; Environmental & situational awareness; Google Tango; Indoor positioning","Bluetooth; Computer vision; Navigation systems; Object recognition; Semantics; Autism spectrum disorders; Bluetooth low energies (BTLE); Environmental characteristic; Google Tango; Indoor positioning; Personalized interface; Situational awareness; User centric interface; Indoor positioning systems","V. Nair; Department of Computer Science, The City College of New York, New York, United States; email: vnair000@citymail.cuny.edu","Leal-Taixé L.; Roth S.","Springer Verlag","03029743","978-303011023-9","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","","Scopus","2-s2.0-85061754935"
"Utaminingrum F.; Sari Y.A.; Komang Somawirata I.","Utaminingrum, Fitri (55488741100); Sari, Yuita Arum (57189058237); Komang Somawirata, I. (55646804200)","55488741100; 57189058237; 55646804200","Obstacle detection for assisting navigation of visually impaired people based on segmentation process","2019","ACM International Conference Proceeding Series","","","","27","32","5","2","10.1145/3351180.3351202","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85073230439&doi=10.1145%2f3351180.3351202&partnerID=40&md5=54691a3c056ab5fdf3c96c82f5dbe482","Computer Vision Research Group, Faculty of Computer Science, Brawijaya University, Indonesia; Department of Electrical Engineering, National Institute of Technology (ITN Malang), Indonesia","Utaminingrum F., Computer Vision Research Group, Faculty of Computer Science, Brawijaya University, Indonesia; Sari Y.A., Computer Vision Research Group, Faculty of Computer Science, Brawijaya University, Indonesia; Komang Somawirata I., Department of Electrical Engineering, National Institute of Technology (ITN Malang), Indonesia","Blindness is a condition when a person's sense of sight experiences a disturbance. Hence generally, it requires a tool for help him. One of the assistive devices for people with blind disabilities in carrying out their mobility is a stick or known as “The White Cane. In general, the blind stick has a permanent shape or cannot be folded. This stick works by tapping or sliding in all directions around the blind person standing, so it is very possible for the surrounding environment to feel disturbed. Based on these problems, we propose a device that can help the blind in walking or doing mobility based on computer vision analysis without having to disturb the surrounding environment. Connected Component Labeling is used to get a blob from an image. The blob that has been detected, then analyzed using segmentation based on the threshold process. The experimental results show that our proposed method using Min-max threshold is able to detect obstacles with an accuracy rate of 82.89 %. © 2019 Association for Computing Machinery.","Bind; Blob; Image; Obstacle; Segmentation; Threshold","Image segmentation; Obstacle detectors; Robotics; Robots; Bind; Blob; Image; Obstacle; Threshold; Computer vision","","","Association for Computing Machinery","","978-145037183-4","","","English","ACM Int. Conf. Proc. Ser.","Conference paper","Final","","Scopus","2-s2.0-85073230439"
"Weiss M.; Chamorro S.; Girgis R.; Luck M.; Kahou S.E.; Cohen J.P.; Nowrouzezahrai D.; Precup D.; Golemo F.; Pal C.","Weiss, Martin (57219523294); Chamorro, Simon (57274974400); Girgis, Roger (57201857825); Luck, Margaux (56800188500); Kahou, Samira E. (36600560600); Cohen, Joseph P. (55475172900); Nowrouzezahrai, Derek (24462364200); Precup, Doina (6603288659); Golemo, Florian (55683066700); Pal, Chris (15056677600)","57219523294; 57274974400; 57201857825; 56800188500; 36600560600; 55475172900; 24462364200; 6603288659; 55683066700; 15056677600","Navigation Agents for the Visually Impaired: A Sidewalk Simulator and Experiments","2019","Proceedings of Machine Learning Research","100","","","1314","1327","13","5","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85160839513&partnerID=40&md5=cbcfc397f7ea8a259a139a3fe2d6d50f","Université de Montréal, Canada; Université de Sherbrooke, Canada; Polytechnique Montréal, Canada; McGill University, Canada; ElementAI, Canada","Weiss M., Université de Montréal, Canada; Chamorro S., Université de Sherbrooke, Canada; Girgis R., Polytechnique Montréal, Canada; Luck M., Université de Montréal, Canada; Kahou S.E., McGill University, Canada; Cohen J.P., Université de Montréal, Canada; Nowrouzezahrai D., McGill University, Canada; Precup D., McGill University, Canada; Golemo F., Université de Montréal, Canada, ElementAI, Canada; Pal C., Polytechnique Montréal, Canada, ElementAI, Canada","Millions of blind and visually-impaired (BVI) people navigate urban environments everyday, using smartphones for high-level path-planning and white canes or guide dogs for local information. However, many BVI people still struggle to travel to new places. In our endeavour to create a navigation assistant for the BVI, we found that existing Reinforcement Learning (RL) environments were unsuitable for the task. This work introduces SEVN, a sidewalk simulation environment and a neural network-based approach to creating a navigation agent. SEVN contains panoramic images with labels for house numbers, doors, and street name signs, and formulations for several navigation tasks. We study the performance of an RL algorithm (PPO) in this setting. Our policy model fuses multi-modal observations in the form of variable resolution images, visible text, and simulated GPS data to navigate to a goal door. We hope that this dataset, simulator, and experimental results will provide a foundation for further research into the creation of agents that can assist members of the BVI community with outdoor navigation. © 2019 CoRL. All Rights Reserved.","computer vision; dataset; outdoor navigation; reinforcement learning","Air navigation; Computer vision; Global positioning system; Motion planning; Pavements; Blind and visually impaired; Dataset; Guide dogs; Outdoor navigation; Reinforcement learnings; Smart phones; Urban environments; Visually impaired; Visually impaired people; White cane; Reinforcement learning","M. Weiss; Université de Montréal, Canada; email: martin.clyde.weiss@gmail.com","Kaelbling L.P.; Kragic D.; Sugiura K.","ML Research Press","26403498","","","","English","Proc. Mach. Learn. Res.","Conference paper","Final","","Scopus","2-s2.0-85160839513"
"Kaur P.; Garg R.","Kaur, Preetjot (57213470646); Garg, Roopali (56126832400)","57213470646; 56126832400","Camera and Sensors-Based Assistive Devices For Visually Impaired Persons: A Systematic Review","2019","International Journal of Scientific and Technology Research","8","8","","622","641","19","2","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85073320761&partnerID=40&md5=019fc25e2f234bd69163780ad6b35ecd","UIET Panjab University, Chandigarh, India; Panjab University, Chandigarh, India; Panjab University, Chandigarh, India","Kaur P., UIET Panjab University, Chandigarh, India; Garg R., Panjab University, Chandigarh, India, Panjab University, Chandigarh, India","Assistive Technology has led to the removal of numerous navigation barriers for visually impaired individuals. It promotes more freedom by empowering such people to perform tasks that were formerly challenging, such as Obstacle Detection, indoor/outdoor Navigation, finding lost objects etc., with more ease. This paper provides a wider scope for researchers in the field of Obstacle detection for blind/partially sighted persons. This paper discusses several techniques contributed by numerous researchers to serve this purpose. These techniques are reviewed and categorized according to the criteria of taking visual information and then the research gaps in those techniques have been detailed. The critical challenges faced by visually impaired users in using Assistive systems based on smartphones, IoT devices, sensors, etc have been discussed along with future directions. In this paper the advancements and research done in this field is surveyed. Further, the various research gaps are included. © 2019, International Journal of Scientific and Technology Research. All rights reserved.","Assistive systems; Camera-Based; Computer Vision; Obstacle Detection; Sensors-Based; Visually Impaired","","","","International Journal of Scientific and Technology Research","22778616","","","","English","Int. J. Sci. Technol. Res.","Review","Final","","Scopus","2-s2.0-85073320761"
"","","","9th International Conference on Soft Computing and Pattern Recognition, SoCPaR 2017","2018","Advances in Intelligent Systems and Computing","737","","","","","193","0","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85043989565&partnerID=40&md5=a91644407061711ddf83fcd76faf1c94","","","The proceedings contain 18 papers. The special focus in this conference is on Soft Computing and Pattern Recognition. The topics include: Using multiple minimum support to auto-adjust the threshold of support in apriori algorithm; prediction of the degree of Parkinson’s condition using recordings of patients’ voices; an overview of a distributional word representation for an arabic named entity recognition system; discrete particle swarm optimization for travelling salesman problems: new combinatorial operators; on maximal frequent itemsets enumeration; particle swarm optimization as a new measure of machine translation efficiency; Experimental investigation of ant supervised by simplified PSO with local search mechanism (SAS-PSO-2Opt); A human identification technique through dorsal hand vein texture analysis based on NSCT decomposition; obstacle detection algorithm by stereoscopic image processing: navigation assistance for the blind and visually impaired; a simultaneous topic and sentiment classification of tweets; local directional multi radius binary pattern: novel descriptor for face recognition application; content-based image retrieval approach using color and texture applied to two databases (Coil-100 and wang); shape classification using B-spline approximation and contour based shape descriptors; an image processing based framework using crowdsourcing for a successful suspect investigation; recognition and classification of arabic fricative consonants; cancer classification using gene expression profiling: Application of the filter approach with the clustering algorithm.","","","","Muda A.K.; Abraham A.; Gandhi N.; Haqiq A.","Springer Verlag","21945357","978-331976356-9","","","English","Adv. Intell. Sys. Comput.","Conference review","Final","","Scopus","2-s2.0-85043989565"
"Shadi S.; Hadi S.; Nazari M.A.; Hardt W.","Shadi, Saleh (57209975218); Hadi, Saleh (57198496534); Nazari, Mohammad Amin (57212537784); Hardt, Wolfram (57206834211)","57209975218; 57198496534; 57212537784; 57206834211","Outdoor navigation for visually impaired based on deep learning","2019","CEUR Workshop Proceedings","2514","","","397","406","9","18","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85076975790&partnerID=40&md5=9e9b320b462266e8ff18f90fff5aeddb","Chemnitz University of Technology, Chemnitz, D-09111, Germany; Vladimir State University Named After Alexander and Nikolay Stoletov, Moscow, 125319, Russian Federation; National Research University, Higher School of Economics, Moscow, 125319, Russian Federation","Shadi S., Chemnitz University of Technology, Chemnitz, D-09111, Germany; Hadi S., Vladimir State University Named After Alexander and Nikolay Stoletov, Moscow, 125319, Russian Federation, National Research University, Higher School of Economics, Moscow, 125319, Russian Federation; Nazari M.A., National Research University, Higher School of Economics, Moscow, 125319, Russian Federation; Hardt W., Chemnitz University of Technology, Chemnitz, D-09111, Germany","Visually impaired and blind people frequently have no knowledge of outdoor obstacles. and need guidance in order to avoid colliding risks. The aim of this research is to develop a mobile-based navigation system for helping visually impaired people in outdoor navigation. The proposed system will be able to reduce the obstacle collision risks by enabling users to walk outside smoothly with voice awareness. The current used systems for navigating visually impaired have several drawbacks such as cost, dependency, and usability. The suggested solution includes a mobile-based camera vision system to build an independent application for outdoor navigation. Moreover, the system has high usability to navigate visually impaired people in unfamiliar environments such as a park, roads and so on. In the presented work, the deep learning algorithms are employed for recognizing and detecting different objects and it is implemented as a mobile navigation application. The suggested smartphone-based system is not restricted to the defined outdoor environments and does not depend on any other positioning system. Therefore, the proposed solution is not limited to any specific environment and provides the voice aids about surrounding obstacles for users. © 2019 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).","Blind people; Deep learning; Machine learning; Mobile-based system; Navigation application; Object recognition; Obstacle recognition; Visually impaired people","Learning algorithms; Learning systems; Navigation systems; Object detection; Object recognition; Smartphones; Software engineering; Blind people; Mobile-based system; Obstacle collision; Obstacle recognition; Outdoor environment; Positioning system; Surrounding obstacles; Visually impaired people; Deep learning","","Cavalli A.R.; Petrenko A.K.; Pozin B.A.","CEUR-WS","16130073","","","","English","CEUR Workshop Proc.","Conference paper","Final","","Scopus","2-s2.0-85076975790"
"Real S.; Araujo A.","Real, Santiago (57210642558); Araujo, Alvaro (16554123800)","57210642558; 16554123800","Navigation systems for the blind and visually impaired: Past work, challenges, and open problems","2019","Sensors (Switzerland)","19","15","3404","","","","125","10.3390/s19153404","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85071151993&doi=10.3390%2fs19153404&partnerID=40&md5=6228d0b61af2240feb33a7d7b0987d83","B105 Electronic Systems Lab, ETSI Telecomunicación, Universidad Politécnica de Madrid, Avenida Complutense 30, Madrid, 28040, Spain","Real S., B105 Electronic Systems Lab, ETSI Telecomunicación, Universidad Politécnica de Madrid, Avenida Complutense 30, Madrid, 28040, Spain; Araujo A., B105 Electronic Systems Lab, ETSI Telecomunicación, Universidad Politécnica de Madrid, Avenida Complutense 30, Madrid, 28040, Spain","Over the last decades, the development of navigation devices capable of guiding the blind through indoor and/or outdoor scenarios has remained a challenge. In this context, this paper’s objective is to provide an updated, holistic view of this research, in order to enable developers to exploit the different aspects of its multidisciplinary nature. To that end, previous solutions will be briefly described and analyzed from a historical perspective, from the first “Electronic Travel Aids” and early research on sensory substitution or indoor/outdoor positioning, to recent systems based on artificial vision. Thereafter, user-centered design fundamentals are addressed, including the main points of criticism of previous approaches. Finally, several technological achievements are highlighted as they could underpin future feasible designs. In line with this, smartphones and wearables with built-in cameras will then be indicated as potentially feasible options with which to support state-of-art computer vision solutions, thus allowing for both the positioning and monitoring of the user’s surrounding area. These functionalities could then be further boosted by means of remote resources, leading to cloud computing schemas or even remote sensing via urban infrastructure. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Assisting systems; Navigation systems; Perception; Situation awareness; Visually impaired","Blindness; Geographic Information Systems; Humans; Radio Frequency Identification Device; Sensory Aids; Smartphone; User-Computer Interface; Vision Disorders; Wearable Electronic Devices; Arts computing; Navigation systems; Remote sensing; Sensory perception; Vision aids; Electronic travel aidss; Historical perspective; Navigation devices; Sensory substitution; Situation awareness; Systems for the blinds; Urban infrastructure; Visually impaired; blindness; computer interface; electronic device; geographic information system; human; pathology; radio frequency identification device; sensory aid; smartphone; visual disorder; User centered design","S. Real; B105 Electronic Systems Lab, ETSI Telecomunicación, Universidad Politécnica de Madrid, Madrid, Avenida Complutense 30, 28040, Spain; email: sreal@b105.upm.es","","MDPI AG","14248220","","","31382536","English","Sensors","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85071151993"
"Chaudhari G.; Deshpande A.","Chaudhari, Gaurao (57212815295); Deshpande, Asmita (57210478154)","57212815295; 57210478154","Robotic assistant for visually impaired using sensor fusion","2018","2017 IEEE SmartWorld Ubiquitous Intelligence and Computing, Advanced and Trusted Computed, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People and Smart City Innovation, SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI 2017 - Conference Proceedings","","","","1","4","3","4","10.1109/UIC-ATC.2017.8397579","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85050186886&doi=10.1109%2fUIC-ATC.2017.8397579&partnerID=40&md5=4be7ec7ee481365d54967d8ce5f50289","Department of Computer Engineering, Charles Davidson College of Engineering, SJSU, San Jose, CA, United States","Chaudhari G., Department of Computer Engineering, Charles Davidson College of Engineering, SJSU, San Jose, CA, United States; Deshpande A., Department of Computer Engineering, Charles Davidson College of Engineering, SJSU, San Jose, CA, United States","Disabled people are in a dire need of electronic assistance. There are a couple of electronic assisting devices that bring their life to an easier turn. In this paper, we describe the design and implementation of a personal assistant robot for blind people. Visually impaired people need such personal assistant devices for they provide a real-Time assistance regarding any necessary problem that blind people face. Some of those main problems are navigation in the indoors, identifying objects around unless getting a physical sense of those objects and sensing the surrounding with the distance of multiple objects. Our paper discusses the various application targeting features like using the LIDAR for local mapping, using a 3D camera for understanding the depth of the surrounding so that the person understands the distance and other information of the objects around. This design has been experimentally validated and required observations are posted in this paper. © 2017 IEEE.","3D camera; audio; create2; localization; mapping; roomba; ROS","Big data; Cameras; Machine design; Mapping; Robots; Smart city; 3-D cameras; audio; create2; localization; roomba; Ubiquitous computing","","","Institute of Electrical and Electronics Engineers Inc.","","978-153860434-2","","","English","IEEE SmartWorld Ubiquitous Intell. Comput., Adv. Trust. Comput., Scalable Comput. Commun., Cloud Big Data Comput., Internet People Smart City Innov., SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI - Conf. Proc.","Conference paper","Final","","Scopus","2-s2.0-85050186886"
"","","","16th International Conference on Computers Helping People with Special Needs, ICCHP 2018","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10896 LNCS","","","","","1209","0","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85049773214&partnerID=40&md5=31be4091ebaa39b61f4580f39395a11c","","","The proceedings contain 99 papers. The special focus in this conference is on Computers Helping People with Special Needs. The topics include: Stereo vision based distance estimation and logo recognition for the visually impaired; intersection navigation for people with visual impairment; indoor localization using computer vision and visual-inertial odometry; hapticrein: Design and development of an interactive haptic rein for a guidance robot; echoVis: Training echolocation using binaural recordings – initial benchmark results; tactiBelt: Integrating spatial cognition and mobility theories into the design of a novel orientation and mobility assistive device for the blind; virtual navigation environment for blind and low vision people; visual shoreline detection for blind and partially sighted people; 3D-printing of personalized assistive technology; camassia: Monocular interactive mobile way sonification; a proposed method for producing embossed dots graphics with a 3D printer; accessibility as prerequisite for the production of individualized aids through inclusive maker spaces; Hackability: A methodology to encourage the development of DIY assistive devices; Universal design tactile graphics production system BPLOT4 for blind teachers and blind staffs to produce tactile graphics and ink print graphics of high quality; a user study to evaluate tactile charts with blind and visually impaired people; concept-building in blind readers with thematic tactile volumes; augmented reality for people with visual impairments: Designing and creating audio-tactile content from existing objects; recording of fingertip position on tactile picture by the visually impaired and analysis of tactile information; designing an interactive tactile relief of the meissen table fountain; one-handed braille in the air.","","","","Miesenberger K.; Kouroupetroglou G.","Springer Verlag","03029743","978-331994276-6","","","English","Lect. Notes Comput. Sci.","Conference review","Final","","Scopus","2-s2.0-85049773214"
"Deepthi Jain B.; Thakur S.M.; Suresh K.V.","Deepthi Jain, B (57204898846); Thakur, Shwetha M (57204903589); Suresh, K.V. (57202055453)","57204898846; 57204903589; 57202055453","Visual Assistance for Blind Using Image Processing","2018","Proceedings of the 2018 IEEE International Conference on Communication and Signal Processing, ICCSP 2018","","","8524251","499","503","4","24","10.1109/ICCSP.2018.8524251","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85057734400&doi=10.1109%2fICCSP.2018.8524251&partnerID=40&md5=0ad0df1091fa6d7578755244825930a3","Siddaganga Institute of Technology, Tumakuru, Karnataka, India; Electronics Communication Department, Siddaganaga Institute of Technology, Tumakuru, Karnataka, India","Deepthi Jain B., Siddaganga Institute of Technology, Tumakuru, Karnataka, India; Thakur S.M., Siddaganga Institute of Technology, Tumakuru, Karnataka, India; Suresh K.V., Electronics Communication Department, Siddaganaga Institute of Technology, Tumakuru, Karnataka, India","Visually impaired people face lot of difficulties in their daily life. Many a times they rely on others for help. Several technologies for assistance of visually impaired people have been developed. Among the various technologies being utilized to assist the blind, Computer Vision based solutions are emerging as one of the most promising options due to their affordability and accessibility. This paper proposes a system for visually impaired people. The proposed system aims to create a wearable visual aid for visually impaired people in which speech commands are accepted from the user. Its functionality addresses identification of objects and sign boards. This will help the visually impaired person to manage day-to-day activities and to navigate through his/her surroundings. Raspberry Pi is used to implement artificial vision using python language on the Open CV platform. © 2018 IEEE.","Haar-like Features; Image Processing; Indoor Navigation; Open CV; Python; Raspberry Pi; speech commands; Video capturing","High level languages; Haar-like features; In-door navigations; Open CV; Python; Raspberry pi; Speech commands; Video capturing; Image processing","","","Institute of Electrical and Electronics Engineers Inc.","","978-153863521-6","","","English","Proc. IEEE Int. Conf. Commun. Signal Process., ICCSP","Conference paper","Final","","Scopus","2-s2.0-85057734400"
"Biberci M.; Bayazit U.","Biberci, Mehmet (56246614600); Bayazit, Ulug (6602855781)","56246614600; 6602855781","Stereo vision based distance estimation and logo recognition for the visually impaired","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10897 LNCS","","","70","77","7","0","10.1007/978-3-319-94274-2_11","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85049778502&doi=10.1007%2f978-3-319-94274-2_11&partnerID=40&md5=04211c00a788565690b593f1d4e855ec","Department of Computer Engineering, Istanbul Technical University, Istanbul, Turkey","Biberci M., Department of Computer Engineering, Istanbul Technical University, Istanbul, Turkey; Bayazit U., Department of Computer Engineering, Istanbul Technical University, Istanbul, Turkey","Interpreting images to compute properties of the 3D world is a significant matter of computer vision. Therefore computer vision applications can help people requiring assistance. This paper presents a novel stereo-vision-based perception and navigation approach to assist visually impaired people. Frontal view images of stores in a shopping mall are first searched for logo recognition. Distances to the found logos (store signboards) are estimated by stereo matching. Both logo recognition and stereo matching are based on local image features (keypoint descriptors) calculated via Speeded Up Robust Features (SURF) algorithm. Final refined distances are calculated via statistical filtering and averaging of the individual keypoint distances found by matched keypoint pairs. Experimental results on our self generated stereo dataset of 28 storefront images from various distances and viewpoints demonstrate the performance of the proposed approach. © Springer International Publishing AG, part of Springer Nature 2018.","Blind navigation; Distance estimation; Logo recognition; Stereo vision; SURF","Air navigation; Computer vision; Stereo image processing; Blind navigation; Computer vision applications; Distance estimation; Logo recognition; Speeded Up Robust Features (SURF); SURF; Vision-based perception; Visually impaired people; Stereo vision","M. Biberci; Department of Computer Engineering, Istanbul Technical University, Istanbul, Turkey; email: biberci@itu.edu.tr","Miesenberger K.; Kouroupetroglou G.","Springer Verlag","03029743","978-331994273-5","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","","Scopus","2-s2.0-85049778502"
"Reis A.; Paulino D.; Filipe V.; Barroso J.","Reis, Arsénio (55803825800); Paulino, Dennis (57194002835); Filipe, Vitor (6507061487); Barroso, João (20435746800)","55803825800; 57194002835; 6507061487; 20435746800","Using online artificial vision services to assist the blind - An assessment of microsoft cognitive services and google cloud vision","2018","Advances in Intelligent Systems and Computing","746","","","174","184","10","12","10.1007/978-3-319-77712-2_17","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85045312502&doi=10.1007%2f978-3-319-77712-2_17&partnerID=40&md5=3964e7380433ba344a167c788aaa2182","INESC TEC and University of Trás-os-Montes e Alto Douro, Vila Real, Portugal","Reis A., INESC TEC and University of Trás-os-Montes e Alto Douro, Vila Real, Portugal; Paulino D., INESC TEC and University of Trás-os-Montes e Alto Douro, Vila Real, Portugal; Filipe V., INESC TEC and University of Trás-os-Montes e Alto Douro, Vila Real, Portugal; Barroso J., INESC TEC and University of Trás-os-Montes e Alto Douro, Vila Real, Portugal","The visually impaired must face several well-known difficulties on their daily life. The use of technology in assistive systems can greatly improve their lives by helping with navigation and orientation, for which several approaches and technologies have been proposed. Lately, it has been introduced powerful online image processing services, based on machine learning and deep learning, promising truly cognitive assessment capacities. Google and Microsoft are two of these main players. In this work we built a device to be used by the blind in order to test the usage of the Google and Microsoft services to assist the blind. The online services were tested by researchers in a laboratory environment and by blind users on a large meeting room, familiar to them. This work reports on our findings regarding the online services effectiveness, the user interface and system latency. © Springer International Publishing AG, part of Springer Nature 2018.","Assistive systems; Cognitive services; Human-computer interaction; Visually impaired","Deep learning; Human computer interaction; Image processing; Information systems; Information use; Online systems; User interfaces; Vision aids; Web services; Assistive system; Cognitive assessments; Cognitive services; Laboratory environment; On-line service; Online image processing; System latency; Visually impaired; Cognitive systems","A. Reis; INESC TEC and University of Trás-os-Montes e Alto Douro, Vila Real, Portugal; email: ars@utad.pt","Reis L.P.; Rocha A.; Costanzo S.; Adeli H.","Springer Verlag","21945357","978-331977711-5","","","English","Adv. Intell. Sys. Comput.","Conference paper","Final","","Scopus","2-s2.0-85045312502"
"Árvai L.","Árvai, László (57203000931)","57203000931","Mobile phone based indoor navigation system for blind and visually impaired people: VUK - Visionless supporting frameworK","2018","Proceedings of the 2018 19th International Carpathian Control Conference, ICCC 2018","","","","383","388","5","14","10.1109/CarpathianCC.2018.8399660","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85050198350&doi=10.1109%2fCarpathianCC.2018.8399660&partnerID=40&md5=75e1d8cee2c2f1c172e214972e64cf1b","Department of Infocommunication Technologies, Bay Zoltán Nonprofit Ltd. for Applied Research, Miskolc, Hungary","Árvai L., Department of Infocommunication Technologies, Bay Zoltán Nonprofit Ltd. for Applied Research, Miskolc, Hungary","The urban mobility for blind and visually impaired people is a challenging task. Using public transportation, subways or visiting complex buildings like shopping malls is virtually impossible without some kind of support. Beside the conventional helpers the advanced sensory and computational power of a recent smart phone make it ideal candidate as a navigation aid for blind people. To take the full advantage of this device a carefully designed and implemented software system is required where the special user needs, man-machine interfaces and state of the art indoor localization method needs to be combined. This paper introduces the architecture of an indoor navigation system specially designed for blind and visually impaired people, explains main components, interfaces and briefly presents important algorithms and methods. © 2018 IEEE.","indoor navigation; indoor navigation aid for blind people; indoor positioning","Cellular telephone systems; Global system for mobile communications; Interface states; Mass transportation; Navigation systems; Smartphones; Telephone sets; Blind and visually impaired; Blind people; In-door navigations; Indoor localization; Indoor navigation system; Indoor positioning; Man machine interface; Public transportation; Indoor positioning systems","L. Árvai; Department of Infocommunication Technologies, Bay Zoltán Nonprofit Ltd. for Applied Research, Miskolc, Hungary; email: laszlo.arvai@bayzoltan.hu","Petras I.; Vasarhelyi J.; Czap L.; Drotos D.","Institute of Electrical and Electronics Engineers Inc.","","978-153864762-2","","","English","Proc. Int. Carpathian Control Conf., ICCC","Conference paper","Final","","Scopus","2-s2.0-85050198350"
"Katzschmann R.K.; Araki B.; Rus D.","Katzschmann, Robert K. (56031680900); Araki, Brandon (56641871000); Rus, Daniela (57218886083)","56031680900; 56641871000; 57218886083","Safe local navigation for visually impaired users with a time-of-flight and haptic feedback device","2018","IEEE Transactions on Neural Systems and Rehabilitation Engineering","26","3","","583","593","10","137","10.1109/TNSRE.2018.2800665","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85041422648&doi=10.1109%2fTNSRE.2018.2800665&partnerID=40&md5=bcedfc82b93d997d36963ea5b5d4bc55","Distributed Robotics Laboratory, Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, 02139, MA, United States","Katzschmann R.K., Distributed Robotics Laboratory, Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, 02139, MA, United States; Araki B., Distributed Robotics Laboratory, Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, 02139, MA, United States; Rus D., Distributed Robotics Laboratory, Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, 02139, MA, United States","This paper presents ALVU (Array of Lidars and Vibrotactile Units), a contactless, intuitive, hands-free, and discreet wearable device that allows visually impaired users to detect low- and high-hanging obstacles, as well as physical boundaries in their immediate environment. The solution allows for safe local navigation in both confined and open spaces by enabling the user to distinguish free space from obstacles. The device presented is composed of two parts: a sensor belt and a haptic strap. The sensor belt is an array of time-of-flight distance sensors worn around the front of a user's waist, and the pulses of infrared light provide reliable and accurate measurements of the distances between the user and surrounding obstacles or surfaces. The haptic strap communicates the measured distances through an array of vibratory motors worn around the user's upper abdomen, providing haptic feedback. The linear vibration motors are combined with a point-loaded pretensioned applicator to transmit isolated vibrations to the user. We validated the device's capability in an extensive user study entailing 162 trials with 12 blind users. Users wearing the device successfully walked through hallways, avoided obstacles, and detected staircases. © 2001-2011 IEEE.","Assistive device; haptic feedback array; human-robot interaction; perception; sightless navigation","Adult; Aged; Blindness; Equipment Design; Feedback, Psychological; Female; Humans; Male; Middle Aged; Sensory Aids; Vibration; Visually Impaired Persons; Human rehabilitation equipment; Human robot interaction; Navigation; Sensory perception; Accurate measurement; Assistive devices; Haptic feedback devices; Haptic feedbacks; Immediate environment; Linear vibrations; Surrounding obstacles; Visually-impaired users; abdomen; Article; blindness; distance perception; echolocation; human; infrared radiation; Pacini corpuscle; patient care; physical mobility; staircase test; tactile feedback; vibration sense; visual orientation; visually impaired person; walking; wireless communication; adult; aged; blindness; equipment design; female; male; middle aged; psychological feedback; sensory aid; vibration; visually impaired person; Feedback","R.K. Katzschmann; Distributed Robotics Laboratory, Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, 02139, United States; email: rkk@csail.mit.edu","","Institute of Electrical and Electronics Engineers Inc.","15344320","","ITNSB","29522402","English","IEEE Trans. Neural Syst. Rehabil. Eng.","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85041422648"
"Grewe L.; Hu C.; Tank K.; Jaiswal A.; Martin T.; Sutaria S.; Huynh T.; Bustos F.D.","Grewe, Lynne (56133164200); Hu, Chengzhi (57211116508); Tank, Krishna (57201064578); Jaiswal, Aditya (57218160425); Martin, Thomas (57220565611); Sutaria, Sahil (57218170847); Huynh, Tran (57218160503); Bustos, Francis David (57218164269)","56133164200; 57211116508; 57201064578; 57218160425; 57220565611; 57218170847; 57218160503; 57218164269","First person perspective video activity recognition","2020","Proceedings of SPIE - The International Society for Optical Engineering","11423","","1142310","","","","0","10.1117/12.2557922","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85088105641&doi=10.1117%2f12.2557922&partnerID=40&md5=6bdcbb61c975198b70469d2c9316d2ee","Computer Science, California State University East Bay, 25800 Carlos Bee Boulevard, Hayward, 94542, CA, United States","Grewe L., Computer Science, California State University East Bay, 25800 Carlos Bee Boulevard, Hayward, 94542, CA, United States; Hu C., Computer Science, California State University East Bay, 25800 Carlos Bee Boulevard, Hayward, 94542, CA, United States; Tank K., Computer Science, California State University East Bay, 25800 Carlos Bee Boulevard, Hayward, 94542, CA, United States; Jaiswal A., Computer Science, California State University East Bay, 25800 Carlos Bee Boulevard, Hayward, 94542, CA, United States; Martin T., Computer Science, California State University East Bay, 25800 Carlos Bee Boulevard, Hayward, 94542, CA, United States; Sutaria S., Computer Science, California State University East Bay, 25800 Carlos Bee Boulevard, Hayward, 94542, CA, United States; Huynh T., Computer Science, California State University East Bay, 25800 Carlos Bee Boulevard, Hayward, 94542, CA, United States; Bustos F.D., Computer Science, California State University East Bay, 25800 Carlos Bee Boulevard, Hayward, 94542, CA, United States","The initial development of two First-Person Perspective Video Activity Recognition Systems is discussed. The first system, the First Person Fall Detection or UFall, can be used to recognize when a person wearing or holding the mobile vision system has fallen. The problem of fall detection is tackled from the unique first-person perspective. The second system, the directed CrossWalk System (UCross), involves detection of the user movement across a crosswalk and is intended for use in helping a low vision person navigate. In both cases, the user is wearing or holding the camera device for purposes of monitoring or inspection of the environment. This first-person perspective yields unusual fall data and this is captured and used for the creation of a fall detection system. For both systems Machine Learning is employed using video input to trained Long-Term Short-Term (LSTM) Networks. These first-perspective video activity recognition systems use the Tensorflow framework [1] and is deployed using mobile phones for proof of concept. These applications could be useful for low vision people and in the case of fall detection for senior citizens, police, construction and other inspection-oriented jobs to help users who have fallen. The success and challenges faced with this unique first-person perspective data are presented along with future avenues of work.  © 2020 SPIE.","Blind assistance; Deep Learning; Drone; Vision","Crosswalks; Pattern recognition; Signal processing; Wear of materials; Camera devices; Fall detection; First-person perspectives; Initial development; Mobile vision systems; Proof of concept; Senior citizens; Video activity; Long short-term memory","","Kadar I.; Blasch E.P.; Grewe L.L.","SPIE","0277786X","978-151063623-1","PSISD","","English","Proc SPIE Int Soc Opt Eng","Conference paper","Final","","Scopus","2-s2.0-85088105641"
"Gui W.; Li B.; Yuan S.; Rizzo J.-R.; Sharma L.; Feng C.; Tzes A.; Fang Y.","Gui, Wenjun (57215560547); Li, Bingyu (57215557853); Yuan, Shuaihang (57215559933); Rizzo, John-Ross (56527876700); Sharma, Lakshay (57219459325); Feng, Chen (51461205400); Tzes, Anthony (7005121367); Fang, Yi (55469293700)","57215560547; 57215557853; 57215559933; 56527876700; 57219459325; 51461205400; 7005121367; 55469293700","An assistive low-vision platform that augments spatial cognition through proprioceptive guidance: Point-to-Tell-and-Touch","2019","IEEE International Conference on Intelligent Robots and Systems","","","8967647","3817","3822","5","3","10.1109/IROS40897.2019.8967647","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85081162504&doi=10.1109%2fIROS40897.2019.8967647&partnerID=40&md5=a7d711122cab25a315933e292756d005","NYU, NYU Multimedia and Visual Computing Lab, Tandon, United States; NYU, Langone Medical Center, United States; NYU, Abu Dhabi, United Arab Emirates","Gui W., NYU, NYU Multimedia and Visual Computing Lab, Tandon, United States; Li B., NYU, NYU Multimedia and Visual Computing Lab, Tandon, United States; Yuan S., NYU, NYU Multimedia and Visual Computing Lab, Tandon, United States; Rizzo J.-R., NYU, Langone Medical Center, United States; Sharma L., NYU, NYU Multimedia and Visual Computing Lab, Tandon, United States; Feng C., NYU, Langone Medical Center, United States; Tzes A., NYU, Abu Dhabi, United Arab Emirates; Fang Y., NYU, Abu Dhabi, United Arab Emirates","Spatial cognition, as gained through the sense of vision, is one of the most important capabilities of human beings. However, for the visually impaired (VI), lack of this perceptual capability poses great challenges in their life. Therefore, we have designed Point-to-Tell-and-Touch, a wearable system with an ergonomic human-machine interface, for assisting the VI with active environmental exploration, with a particular focus on spatial intelligence and navigation to objects of interest in an alien environment. Our key idea is to link visual signals, as decoded synthetically, to the VI's proprioception for more intelligible guidance, in addition to vision-to-audio assistance, i.e., finger pose, as indicated by pointing, is used as 'proprioceptive laser pointer' to target an object in that line of sight. The whole system consists of two features, Point-to-Tell and Point-to-Touch, both of which can work independently or cooperatively. The Point-to-Tell feature contains a camera with a novel one-stage neural network tailored for blind-centered object detection and recognition, and a headphone telling the VI the semantic label and distance from the pointed object. the Point-to-Touch, the second feature, leverages a vibrating wrist band to create a haptic feedback tool that supplements the initial vectorial guidance provided by the first stage (hand pose being direction and the distance being the extent, offered through audio cues). Both platform features utilize proprioception or joint position sense. Through hand pose, the VI end user knows where he or she is pointing relative to their egocentric coordinate system and we are able to use this foundation to build spatial intelligence. Our successful indoor experiments demonstrate the proposed system to be effective and reliable in helping the VI gain spatial cognition and explore the world in a more intuitive way. © 2019 IEEE.","","Object detection; Object recognition; Semantics; Co-ordinate system; Environmental exploration; Human Machine Interface; Indoor experiment; Object detection and recognition; Spatial cognition; Spatial intelligence; Visually impaired; Intelligent robots","Y. Fang; NYU, Abu Dhabi, United Arab Emirates; email: yfang@nyu.edu","","Institute of Electrical and Electronics Engineers Inc.","21530858","978-172814004-9","85RBA","","English","IEEE Int Conf Intell Rob Syst","Conference paper","Final","","Scopus","2-s2.0-85081162504"
"Chen R.; Tian Z.; Liu H.; Zhao F.; Zhang S.; Liu H.","Chen, Runze (57203001921); Tian, Zhanhong (57203002223); Liu, Hailun (57203003546); Zhao, Fang (55087079700); Zhang, Shuai (57203165289); Liu, Haobo (57203003520)","57203001921; 57203002223; 57203003546; 55087079700; 57203165289; 57203003520","Construction of a voice driven life assistant system for visually impaired people","2018","2018 International Conference on Artificial Intelligence and Big Data, ICAIBD 2018","","","","87","92","5","17","10.1109/ICAIBD.2018.8396172","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85050193334&doi=10.1109%2fICAIBD.2018.8396172&partnerID=40&md5=6bb47b0824c63807ea862e117e2984c7","School of Software Engineering, Beijing University of Posts and Telecommunications, Beijing, China","Chen R., School of Software Engineering, Beijing University of Posts and Telecommunications, Beijing, China; Tian Z., School of Software Engineering, Beijing University of Posts and Telecommunications, Beijing, China; Liu H., School of Software Engineering, Beijing University of Posts and Telecommunications, Beijing, China; Zhao F., School of Software Engineering, Beijing University of Posts and Telecommunications, Beijing, China; Zhang S., School of Software Engineering, Beijing University of Posts and Telecommunications, Beijing, China; Liu H., School of Software Engineering, Beijing University of Posts and Telecommunications, Beijing, China","The rapid development of artificial intelligence and mobile computation brings more convenient life to the blind and visually impaired people. This paper presents a prototype of a voice assistant specially designed for them. The system mainly contains fundamental services including falling detection, safety care, accessibility of mobile phone, daily information broadcasting and view description to make life easier for them. Natural language understanding, voice recognition and synthesis have been integrated to enable users operate majority of mobile phones' functions. Also, the built-in falling detection algorithm based on tri-axis accelerometer and object detection algorithm based on Mask R-CNN can enrich sense of users and at the same time keep the safety of users. © 2018 IEEE.","Accessibility; Mobile computing; Natural language understanding; Navigation; Visually impaired; Voice assistant","Artificial intelligence; Big data; Cellular telephones; Mobile computing; Mobile phones; Navigation; Object detection; Signal detection; Speech recognition; Telephone sets; Accessibility; Blind and visually impaired; Information broadcasting; Natural language understanding; Object detection algorithms; Tri-axis accelerometers; Visually impaired; Visually impaired people; Cellular telephone systems","R. Chen; School of Software Engineering, Beijing University of Posts and Telecommunications, Beijing, China; email: chenrz925@bupt.edu.cn","","Institute of Electrical and Electronics Engineers Inc.","","978-153866987-7","","","English","Int. Conf. Artif. Intell. Big Data, ICAIBD","Conference paper","Final","","Scopus","2-s2.0-85050193334"
"Anwar A.; Aljahdali S.","Anwar, Ashraf (58254407900); Aljahdali, Sultan (25640715200)","58254407900; 25640715200","A smart stick for assisting visually impaired people","2018","Journal of Theoretical and Applied Information Technology","96","14","","4405","4416","11","2","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85051115149&partnerID=40&md5=861c9bc8e3fe6dce33c916c31b9e575a","Department of Computer Engineering, College of Computers and Information Technology, Taif University, Saudi Arabia; Department of Computer Science, College of Computers and Information Technology, Taif University, Saudi Arabia","Anwar A., Department of Computer Engineering, College of Computers and Information Technology, Taif University, Saudi Arabia; Aljahdali S., Department of Computer Science, College of Computers and Information Technology, Taif University, Saudi Arabia","This paper presents a smart stick system for assisting blind or visually impaired people. The smart stick acts as a vision assistant to enable visually impaired people to find difficulties in obstacles detection and dangers in front of them for autonomous navigation. Unlike existing papers, related to obstacles detection means: ultrasonic sensor, infrared sensor, and water sensor, alarm modules: buzzer, sound, and voice statement, and a location finder like the GPS/GSM system, the proposed system integrates all these technologies for the benefits of the blind. The system is designed to act like an artificial vision associated with an alarm unit, and a location finder of the blind. The Global Positioning System (GPS) and Global System for Mobile communications (GSM) are interfaced to the microcontroller to detect the blind person location. The feedback from the real test was positive. The average of avoidance accuracy is 88.75%. © 2005 – on going JATIT & LLS.","GPS; Infrared sensor; Microcontroller; Obstacle detection; Ultrasonic sensor","","","","Little Lion Scientific","19928645","","","","English","J. Theor. Appl. Inf. Technol.","Article","Final","","Scopus","2-s2.0-85051115149"
"Nishajith A.; Nivedha J.; Nair S.S.; Mohammed Shaffi J.","Nishajith, A. (57205763056); Nivedha, J. (57164046700); Nair, Shilpa. S. (57638321800); Mohammed Shaffi, J. (57200761631)","57205763056; 57164046700; 57638321800; 57200761631","Smart Cap - Wearable Visual Guidance System for Blind","2018","Proceedings of the International Conference on Inventive Research in Computing Applications, ICIRCA 2018","","","8597327","275","278","3","31","10.1109/ICIRCA.2018.8597327","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85061510407&doi=10.1109%2fICIRCA.2018.8597327&partnerID=40&md5=00040154b4469e9aa84471adef579002","Department of ECE, LBS Institute of Technology for Women, Thiruvananthapuram, India","Nishajith A., Department of ECE, LBS Institute of Technology for Women, Thiruvananthapuram, India; Nivedha J., Department of ECE, LBS Institute of Technology for Women, Thiruvananthapuram, India; Nair S.S., Department of ECE, LBS Institute of Technology for Women, Thiruvananthapuram, India; Mohammed Shaffi J., Department of ECE, LBS Institute of Technology for Women, Thiruvananthapuram, India","Science and technology always try to make human life easier. The people who are having complete blindness or low vision faces many difficulties during their navigation. Blindness can occur due to many reasons including disease, injury or other conditions that limit vision. The main purpose of this paper is to develop a navigation aid for the blind and the visually impaired people. In this paper, we design and implement a smart cap which helps the blind and the visually impaired people to navigate freely by experiencing their surroundings. The scene around the person will be captured by using a NoIR camera and the objects in the scene will be detected. The earphones will give a voice output describing the detected objects. The architecture of the system includes the processor Raspberry Pi 3, NoIR camera, earphones and a power source. The processor collects the frames of the surroundings and convert it to voice output. The device uses TensorFlow API, open-source machine learning library developed by the Google Brain Team for the object detection and classification. TensorFlow helps in creating machine learning models capable of identifying and classifying multiple objects in a single image. Thus, details corresponding to various objects present within a single frame are obtained using TensorFlow API. A Text to Speech Synthesiser (TTS) software called eSpeak is used for converting the details of the detected object (in text format) to speech output. So the video captured by using the NoIR camera is finally converted to speech signals and thus narration of the scene describing various objects is done. Objects which come under 90 different classes like cell phone, vase, person, couch etc are detected. © 2018 IEEE.","eSpeak; NoIR camera; Raspberry Pi 3; TensorFlow API; TTS","Cameras; Earphones; Eye protection; Machine learning; Open source software; Speech recognition; Wearable computers; Design and implements; eSpeak; Machine learning models; Multiple objects; Raspberry Pi 3; Science and Technology; TensorFlow API; Visually impaired people; Object detection","","","Institute of Electrical and Electronics Engineers Inc.","","978-153862456-2","","","English","Proc. Int. Conf. Inven. Res. Comput. Appl., ICIRCA","Conference paper","Final","","Scopus","2-s2.0-85061510407"
"Ritterbusch S.; Jaworek G.","Ritterbusch, Sebastian (36195853100); Jaworek, Gerhard (24724417200)","36195853100; 24724417200","Camassia: Monocular interactive mobile way sonification","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10897 LNCS","","","12","18","6","2","10.1007/978-3-319-94274-2_2","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85049789238&doi=10.1007%2f978-3-319-94274-2_2&partnerID=40&md5=d608b39d30f4dd16437e980dfdf68853","iXpoint Informationssysteme GmbH, Ettlingen, Germany; VWA-Hochschule, Stuttgart, Germany; Study Centre for the Visually Impaired, Karlsruhe Institute of Technology, Karlsruhe, Germany","Ritterbusch S., iXpoint Informationssysteme GmbH, Ettlingen, Germany, VWA-Hochschule, Stuttgart, Germany; Jaworek G., Study Centre for the Visually Impaired, Karlsruhe Institute of Technology, Karlsruhe, Germany","Real-time camera image analysis informs the walking person about the way in front by sonification. For visually impaired people, this opens a new way to experience their surroundings and move more safely outdoors. We extended an image analysis workflow from autonomous robots to human interaction, adaption to alternating way appearance, and various sonification options. The solution is available on off-the-shelf smartphones. © Springer International Publishing AG, part of Springer Nature 2018.","Assistive system; Blind; Free way detection; Navigation; Sonification","Human robot interaction; Navigation; Assistive system; Blind; Camera images; Human interactions; Real time; Sonifications; Visually impaired people; Image analysis","S. Ritterbusch; iXpoint Informationssysteme GmbH, Ettlingen, Germany; email: sebastian.ritterbusch@ixpoint.de","Miesenberger K.; Kouroupetroglou G.","Springer Verlag","03029743","978-331994273-5","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","","Scopus","2-s2.0-85049789238"
"Meza-de-Luna M.E.; Terven J.R.; Raducanu B.; Salas J.","Meza-de-Luna, María Elena (55607349300); Terven, Juan R. (59013477200); Raducanu, Bogdan (22235547800); Salas, Joaquín (58713274100)","55607349300; 59013477200; 22235547800; 58713274100","A Social-Aware Assistant to support individuals with visual impairments during social interaction: A systematic requirements analysis","2019","International Journal of Human Computer Studies","122","","","50","60","10","21","10.1016/j.ijhcs.2018.08.007","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85052849179&doi=10.1016%2fj.ijhcs.2018.08.007&partnerID=40&md5=f1e3b4b8ab86ce3737e37cc528799ce4","Universidad Autónoma de Querétaro, Mexico; AiFi Inc., United States; Centro de Visión por Computadora, Spain; Instituto Politécnico Nacional, Mexico","Meza-de-Luna M.E., Universidad Autónoma de Querétaro, Mexico; Terven J.R., AiFi Inc., United States; Raducanu B., Centro de Visión por Computadora, Spain; Salas J., Instituto Politécnico Nacional, Mexico","Visual impairment affects the normal course of activities in everyday life including mobility, education, employment, and social interaction. Most of the existing technical solutions devoted to empowering the visually impaired people are in the areas of navigation (obstacle avoidance), access to printed information and object recognition. Less effort has been dedicated so far in developing solutions to support social interactions. In this paper, we introduce a Social-Aware Assistant (SAA) that provides visually impaired people with cues to enhance their face-to-face conversations. The system consists of a perceptive component (represented by smartglasses with an embedded video camera) and a feedback component (represented by a haptic belt). When the vision system detects a head nodding, the belt vibrates, thus suggesting the user to replicate (mirror) the gesture. In our experiments, sighted persons interacted with blind people wearing the SAA. We instructed the former to mirror the noddings according to the vibratory signal, while the latter interacted naturally. After the face-to-face conversation, the participants had an interview to express their experience regarding the use of this new technological assistant. With the data collected during the experiment, we have assessed quantitatively and qualitatively the device usefulness and user satisfaction. © 2018 Elsevier Ltd","Assistive technology; Computer vision; Social interaction; Visually impaired","Computer vision; Mirrors; Object recognition; Video cameras; Assistive technology; Developing solutions; Face-to-face conversation; Requirements analysis; Social interactions; Technical solutions; Visually impaired; Visually impaired people; Behavioral research","J. Salas; Joaquín Salas, Querétaro, Cerro Blanco 141, Colinas del Cimatario, 76090, Mexico; email: jsalasr@ipn.mx","","Academic Press","10715819","","IHSTE","","English","Int J Hum Comput Stud","Article","Final","","Scopus","2-s2.0-85052849179"
"Martinez M.; Roitberg A.; Koester D.; Stiefelhagen R.; Schauerte B.","Martinez, Manuel (7404594284); Roitberg, Alina (56622647500); Koester, Daniel (7004367115); Stiefelhagen, Rainer (6602180348); Schauerte, Boris (35234793300)","7404594284; 56622647500; 7004367115; 6602180348; 35234793300","Using Technology Developed for Autonomous Cars to Help Navigate Blind People","2017","Proceedings - 2017 IEEE International Conference on Computer Vision Workshops, ICCVW 2017","2018-January","","","1424","1432","8","41","10.1109/ICCVW.2017.169","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85046247968&doi=10.1109%2fICCVW.2017.169&partnerID=40&md5=84ee0ab3c9b52127778aca23e4cfc60b","Karlsruhe Institute of Technology, Institute for Anthropomatics and Robotics, Karlsruhe, 76131, Germany","Martinez M., Karlsruhe Institute of Technology, Institute for Anthropomatics and Robotics, Karlsruhe, 76131, Germany; Roitberg A., Karlsruhe Institute of Technology, Institute for Anthropomatics and Robotics, Karlsruhe, 76131, Germany; Koester D., Karlsruhe Institute of Technology, Institute for Anthropomatics and Robotics, Karlsruhe, 76131, Germany; Stiefelhagen R., Karlsruhe Institute of Technology, Institute for Anthropomatics and Robotics, Karlsruhe, 76131, Germany; Schauerte B., Karlsruhe Institute of Technology, Institute for Anthropomatics and Robotics, Karlsruhe, 76131, Germany","Autonomous driving is currently a very active research area with virtually all automotive manufacturers competing to bring the first autonomous car to the market. This race leads to billions of dollars being invested in the development of novel sensors, processing platforms, and algorithms. In this paper, we explore the synergies between the challenges in self-driving technology and development of navigation aids for blind people. We aim to leverage the recently emerged methods for self-driving cars, and use it to develop assistive technology for the visually impaired. In particular we focus on the task of perceiving the environment in realtime from cameras. First, we review current developments in embedded platforms for real-time computation as well as current algorithms for image processing, obstacle segmentation and classification. Then, as a proof-of-concept, we build an obstacle avoidance system for blind people that is based on a hardware platform used in the automotive industry. To perceive the environment, we adapt an implementation of the stixels algorithm, designed for self-driving cars. We discuss the challenges and modifications required for such an application domain transfer. Finally, to show its usability in practice, we conduct and evaluate a user study with six blindfolded people. © 2017 IEEE.","","Automobile manufacture; Automotive industry; Computer vision; Image segmentation; Assistive technology; Automotive manufacturers; Autonomous driving; Embedded platforms; Obstacle segmentation; Obstacle-avoidance system; Processing platform; Real-time computations; Autonomous vehicles","","","Institute of Electrical and Electronics Engineers Inc.","","978-153861034-3","","","English","Proc. - IEEE Int. Conf. Comput. Vis. Workshops, ICCVW","Conference paper","Final","","Scopus","2-s2.0-85046247968"
"Lock J.; Cielniak G.; Bellotto N.","Lock, Jacobus (57195542066); Cielniak, Grzegorz (6508366670); Bellotto, Nicola (23059304100)","57195542066; 6508366670; 23059304100","A portable navigation system with an adaptive multimodal interface for the blind","2017","AAAI Spring Symposium - Technical Report","SS-17-01 - SS-17-08","","","395","400","5","14","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85028734746&partnerID=40&md5=d75f7f516dd75fbc2edeecad0a614008","Lincoln Centre for Autonomous Systems (L-CAS), School of Computer Science, University of Lincoln, Lincoln, LN6 7TS, United Kingdom","Lock J., Lincoln Centre for Autonomous Systems (L-CAS), School of Computer Science, University of Lincoln, Lincoln, LN6 7TS, United Kingdom; Cielniak G., Lincoln Centre for Autonomous Systems (L-CAS), School of Computer Science, University of Lincoln, Lincoln, LN6 7TS, United Kingdom; Bellotto N., Lincoln Centre for Autonomous Systems (L-CAS), School of Computer Science, University of Lincoln, Lincoln, LN6 7TS, United Kingdom","Recent advances in mobile technology have the potential to radically change the quality of tools available for people with sensory impairments, in particular the blind and partially sighted. Nowadays almost every smart-phone and tablet is equipped with high-resolution cameras, typically used for photos, videos, games and virtual reality applications. Very little has been proposed to exploit these sensors for user localisation and navigation instead. To this end, the ""Active Vision with Human-in-the-Loop for the Visually Impaired"" (ActiVis) project aims to develop a novel electronic travel aid to tackle the ""last 10 yards problem"" and enable blind users to independently navigate in unknown environments, ultimately enhancing or replacing existing solutions such as guide dogs and white canes. This paper describes some of the project's key challenges, in particular with respect to the design of a user interface (UI) that translates visual information from the camera to guidance instructions for the blind person, taking into account the limitations introduced by visual impairment. In this paper we also propose a multimodal UI that caters to the needs of the visually impaired that exploits human-machine progressive co-adaptation to enhance the user's experience and improve navigation performance. © Copyright 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.","","Artificial intelligence; Cameras; Learning systems; Machine oriented languages; Navigation systems; Smartphones; User interfaces; Virtual reality; Electronic travel aidss; High resolution camera; Multi-modal interfaces; Navigation performance; Portable navigation system; Sensory impairment; Visual information; Visually impaired; Vision aids","","","AI Access Foundation","","978-157735779-7","","","English","AAAI Spring Symp. Tech. Rep.","Conference paper","Final","","Scopus","2-s2.0-85028734746"
"Petrausch V.; Schwarz T.; Stiefelhagen R.","Petrausch, Vanessa (57190192828); Schwarz, Thorsten (56266788700); Stiefelhagen, Rainer (6602180348)","57190192828; 56266788700; 6602180348","Prototype development of a low-cost vibro-tactile navigation aid for the visually impaired","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10897 LNCS","","","63","69","6","2","10.1007/978-3-319-94274-2_10","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85049778736&doi=10.1007%2f978-3-319-94274-2_10&partnerID=40&md5=ef3f54168e405a1810e881d209ec70b2","Study Centre for the Visually Impaired, Karlsruhe Institute of Technology, Engesserstr. 4, Karlsruhe, 76131, Germany","Petrausch V., Study Centre for the Visually Impaired, Karlsruhe Institute of Technology, Engesserstr. 4, Karlsruhe, 76131, Germany; Schwarz T., Study Centre for the Visually Impaired, Karlsruhe Institute of Technology, Engesserstr. 4, Karlsruhe, 76131, Germany; Stiefelhagen R., Study Centre for the Visually Impaired, Karlsruhe Institute of Technology, Engesserstr. 4, Karlsruhe, 76131, Germany","Vibro-tactile support for navigation tasks is helpful, not only for visually impaired people. However many different prototypes exist which are not available for experimental usage or commercial ones are expensive. We developed a low-cost prototype of a wristlet and anklet that can easily be rebuild. We evaluated them in a two step procedure with 11 and 10 participants. Both prototypes are easy to use and comfortable to wear. However, the wristlet had a high error rate so that it was not used in the second test. Comparisons between the anklet and voice guidance during navigation in the second test showed the potential of vibration for navigation tasks, but implied a refinement of the design, which will be tested in further studies. © Springer International Publishing AG, part of Springer Nature 2018.","Blind; Navigation; Prototype; User study; Vibro-tactile wristlet/anklet; Visual impaired","Artificial intelligence; Computer science; Computers; Blind; Prototype; User study; Vibro-tactile wristlet/anklet; Visual impaired; Navigation","V. Petrausch; Study Centre for the Visually Impaired, Karlsruhe Institute of Technology, Karlsruhe, Engesserstr. 4, 76131, Germany; email: vanessa.petrausch@kit.edu","Miesenberger K.; Kouroupetroglou G.","Springer Verlag","03029743","978-331994273-5","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","","Scopus","2-s2.0-85049778736"
"Damasio Oliveira J.; Campos M.B.; Stangherlin Machado Paixão-Cortes V.","Damasio Oliveira, Juliana (57190282486); Campos, Márcia de Borba (55803627100); Stangherlin Machado Paixão-Cortes, Vanessa (57203126735)","57190282486; 55803627100; 57203126735","Usable and accessible robot programming system for people who are visually impaired","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12188 LNCS","","","445","464","19","2","10.1007/978-3-030-49282-3_32","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85088746815&doi=10.1007%2f978-3-030-49282-3_32&partnerID=40&md5=a56bc5a530e98cd187bebb884f4ceca2","Pontifical Catholic University of Rio Grande do Sul (PUCRS), School of Technology, Porto Alegre, Brazil; Inedi College – CESUCA, Cachoeirinha, Brazil","Damasio Oliveira J., Pontifical Catholic University of Rio Grande do Sul (PUCRS), School of Technology, Porto Alegre, Brazil; Campos M.B., Inedi College – CESUCA, Cachoeirinha, Brazil; Stangherlin Machado Paixão-Cortes V., Pontifical Catholic University of Rio Grande do Sul (PUCRS), School of Technology, Porto Alegre, Brazil","In the 1960s, Papert introduced Logo language that commanded a graphic turtle. These were the first advances in the field of educational robotics. Many environments were created in this theme; however, most programming environments are highly graphical, so they do not provide accessible interfaces for visually impaired users. Thus, our research presents challenges and recommendations in terms of usability and accessibility in designing a robot programming environment for people who are visually impaired. We conjectured that by using a programming language to guide a robot in a virtual environment, the person who is visually impaired could better understand orientation and mobility skills. Hence, we created the GoDonnie programming language based on the Logo language. GoDonnie runs in a programming environment called Donnie. We show all the design process of Donnie, that was based on interactive design. Participates in this study, people who are visually impaired and sighted programming professors. Results indicate that GoDonnie and Donnie have good usability, supports the development of orientation and mobility in visually impaired people, and meets the expectations regarding the programming environment. Finally, we created a set of design guidelines for developing robot programming environments for people who are visually impaired. © Springer Nature Switzerland AG 2020.","Accessibility; Blind programmer; Orientation and mobility; Robotic education; Usability; Visually impaired","Computer programming languages; Educational robots; Human computer interaction; Machine design; Robots; Educational robotics; Interactive design; Mobility skills; Programming environment; Programming system; Visually impaired; Visually impaired people; Visually-impaired users; Robot programming","J. Damasio Oliveira; Pontifical Catholic University of Rio Grande do Sul (PUCRS), School of Technology, Porto Alegre, Brazil; email: juliana.damasio@acad.pucrs.br","Antona M.; Stephanidis C.","Springer","03029743","978-303049281-6","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","","Scopus","2-s2.0-85088746815"
"Guo F.; He H.-D.; Xiao Y.-R.; Fu T.-M.; Liao C.; Tan P.","Guo, Fan (55417282100); He, Han-Dong (57202744828); Xiao, Yan-Ru (57225866326); Fu, Teng-Mu (57202752832); Liao, Cheng (57202741759); Tan, Ping (55726699900)","55417282100; 57202744828; 57225866326; 57202752832; 57202741759; 55726699900","BlindReader: An assistant system to support text reading for the visually impaired people","2018","Journal of Information Hiding and Multimedia Signal Processing","9","4","","938","948","10","0","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85049211523&partnerID=40&md5=f68bafcc52acb5ffd3fc75f226321d3d","School of Information Science and Engineering, Central South University, Changsha, 410083, China; Key Laboratory of Hunan Province for New Retail Virtual Reality Technology, School of Computer and Information Engineering, Hunan University of Commerce, Changsha, 410205, China","Guo F., School of Information Science and Engineering, Central South University, Changsha, 410083, China; He H.-D., School of Information Science and Engineering, Central South University, Changsha, 410083, China; Xiao Y.-R., School of Information Science and Engineering, Central South University, Changsha, 410083, China; Fu T.-M., School of Information Science and Engineering, Central South University, Changsha, 410083, China; Liao C., School of Information Science and Engineering, Central South University, Changsha, 410083, China; Tan P., Key Laboratory of Hunan Province for New Retail Virtual Reality Technology, School of Computer and Information Engineering, Hunan University of Commerce, Changsha, 410205, China","Accessing printed text is a great challenge for the visually impaired people. Therefore, in this paper, we introduce a visual assistant system, BlindReader, that assists visually impaired people to read printed text. Some computer vision algorithms are introduced to adjust the intensity and orientation of captured document images in preprocessing stage. Using the system, the low-vision users can read the text more clearly just like the way people use the magnifying glass and the blind users can hear the recognized words synthesized to audio speech for the whole text or the single text line. Finally, the system is evaluated in terms of text recognition accuracy and user feedback to determine the usability of the BlindReader. © 2018, Ubiquitous International. All rights reserved.","Assistant system; Layout analysis; Text reading; Text recognition; Visual impaired people","Human rehabilitation equipment; Vision aids; Assistant system; Layout analysis; Text reading; Text recognition; Visual impaired peoples; Character recognition","P. Tan; Key Laboratory of Hunan Province for New Retail Virtual Reality Technology, School of Computer and Information Engineering, Hunan University of Commerce, Changsha, 410205, China; email: tp2008@foxmail.com","","Ubiquitous International","20734212","","","","English","J. Inf. Hiding Multimedia Signal Proces.","Article","Final","","Scopus","2-s2.0-85049211523"
"Mocanu B.; Tapu R.; Zaharia T.","Mocanu, Bogdan (24822846400); Tapu, Ruxandra (26424843800); Zaharia, Titus (6601999900)","24822846400; 26424843800; 6601999900","Seeing Without Sight - An Automatic Cognition System Dedicated to Blind and Visually Impaired People","2017","Proceedings - 2017 IEEE International Conference on Computer Vision Workshops, ICCVW 2017","2018-January","","","1452","1459","7","15","10.1109/ICCVW.2017.172","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85046295445&doi=10.1109%2fICCVW.2017.172&partnerID=40&md5=b8057281fff12f12cf3bc024b8232e89","ARTEMIS Department, Institute Mines - Télécom/Télécom SudParis, UMR CNRS 8145, MAP5, 5157 SAMOVAR, Evry, France; Department of Telecommunications, Faculty of ETTI, University 'Politehnica' of Bucharest, Romania","Mocanu B., ARTEMIS Department, Institute Mines - Télécom/Télécom SudParis, UMR CNRS 8145, MAP5, 5157 SAMOVAR, Evry, France, Department of Telecommunications, Faculty of ETTI, University 'Politehnica' of Bucharest, Romania; Tapu R., ARTEMIS Department, Institute Mines - Télécom/Télécom SudParis, UMR CNRS 8145, MAP5, 5157 SAMOVAR, Evry, France, Department of Telecommunications, Faculty of ETTI, University 'Politehnica' of Bucharest, Romania; Zaharia T., ARTEMIS Department, Institute Mines - Télécom/Télécom SudParis, UMR CNRS 8145, MAP5, 5157 SAMOVAR, Evry, France","In this paper we present an automatic cognition system, based on computer vision algorithms and deep convolutional neural networks, designed to assist the visually impaired (VI) users during navigation in highly dynamic urban scenes. A first feature concerns the realtime detection of various types of objects existent in the outdoor environment relevant from the perspective of a VI person. The objects are followed between successive frames using a novel tracker, which exploits an offline trained neural-network and is able to track generic objects using motion patterns and visual attention models. The system is able to handle occlusions, sudden camera/object movements, rotation or various complex changes. Finally, an object classification module is proposed that exploits the YOLO algorithm and extends it with new categories specific to assistive devices applications. The feedback to VI users is transmitted as a set of acoustic warning messages through bone conducting headphones. The experimental evaluation, performed on the VOT 2016 dataset and on a set of videos acquired with the help of VI users, demonstrates the effectiveness and efficiency of the proposed method. © 2017 IEEE.","","Behavioral research; Deep neural networks; Neural networks; Blind and visually impaired; Computer vision algorithms; Convolutional neural network; Effectiveness and efficiencies; Experimental evaluation; Object classification; Trained neural networks; Visual attention model; Computer vision","","","Institute of Electrical and Electronics Engineers Inc.","","978-153861034-3","","","English","Proc. - IEEE Int. Conf. Comput. Vis. Workshops, ICCVW","Conference paper","Final","","Scopus","2-s2.0-85046295445"
"Grewe L.; Stevenson G.","Grewe, Lynne (56133164200); Stevenson, Garrett (57202743800)","56133164200; 57202743800","Seeing eye drone: A Deep Learning, Vision-based UAV for Assisting the Visually Impaired with Mobility","2019","ACM International Conference Proceeding Series","","","110","","","","11","10.1145/3321408.3321414","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85072827713&doi=10.1145%2f3321408.3321414&partnerID=40&md5=2ba7aa857bf0fe7fcca4f65a2ac4aa42","Computer Science California State Univ., East Bay, Hayward, CA, United States","Grewe L., Computer Science California State Univ., East Bay, Hayward, CA, United States; Stevenson G., Computer Science California State Univ., East Bay, Hayward, CA, United States","Seeing Eye Drone assists low-vision persons with environment awareness performing exploration and obstacle detection. The modalities of 3D (stereo) and 2D vision on a drone are compared for this task. Different deep-learning systems are developed including 2D only and 3D+2D networks. Comparisons of retrained networks versus training from scratch are also made and approximately 34,000 samples were collected for training and the resulting SSD CNN architecture is used to determine a user's location and direction of travel. A second network identifies locations of common objects in the scene. The object locations are then compared with the user location/heading and depth data to determine whether they represent obstacles. Obstacles determined to be in the user's region of interest are communicated to the visually-impaired user via Text-to-Speech. Real data from outdoor drone flights that communicate with an Android based application are shown. © 2019 Association for Computing Machinery. All rights reserved.","Assistive Technology; Blind/ Low Vision; Computer Vision; Drone; Machine Learning","Aircraft detection; Computer vision; Drones; Image segmentation; Learning systems; Location; Obstacle detectors; Stereo image processing; Stereo vision; Assistive technology; Blind low vision; Environment awareness; Object location; Obstacle detection; Region of interest; Visually impaired; Visually-impaired users; Deep learning","","","Association for Computing Machinery","","978-145037158-2","","","English","ACM Int. Conf. Proc. Ser.","Conference paper","Final","","Scopus","2-s2.0-85072827713"
"Wu T.-F.; Tsai P.-S.; Hu N.-T.; Chen J.-Y.","Wu, Ter-Feng (56982194300); Tsai, Pu-Sheng (7202064450); Hu, Nien-Tsu (36903695600); Chen, Jen-Yang (35975769700)","56982194300; 7202064450; 36903695600; 35975769700","Intelligent wheeled mobile robots for blind navigation application","2017","Engineering Computations (Swansea, Wales)","34","2","","214","238","24","5","10.1108/EC-08-2015-0256","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85020422726&doi=10.1108%2fEC-08-2015-0256&partnerID=40&md5=7dd80a9d7edcdfc423479d1d56d519bc","Department of Electrical Engineering, National Ilan University, Yilan, Taiwan; Department of Electronic Engineering, China University of Science and Technology, Taipei, Taiwan; Department of Chemical Systems Research Division, National Chung-Shan Institute of Science and Technology, Taoyuan, Taiwan; Department of Electronic Engineering, Ming Chuan University, Taoyuan, Taiwan","Wu T.-F., Department of Electrical Engineering, National Ilan University, Yilan, Taiwan; Tsai P.-S., Department of Electronic Engineering, China University of Science and Technology, Taipei, Taiwan; Hu N.-T., Department of Chemical Systems Research Division, National Chung-Shan Institute of Science and Technology, Taoyuan, Taiwan; Chen J.-Y., Department of Electronic Engineering, Ming Chuan University, Taoyuan, Taiwan","Purpose: Visually impaired people have long been living in the dark. They cannot realize the colorful world with their vision, so they rely on hearing, touch and smell to feel the space they live in. Lacking image information, they face challenges in the external environment and barrier spaces. They face danger that is hundreds of times higher than that faced by normal people. Especially during outdoor activities, they can only explore the surrounding environment aided by their hearing and crutches and then based on a vague impression speculate where they are located. To let the blind confidently take each step, this paper proposes sticking the electronic tag of the radio-frequency identification (RFID) system on the back of guide bricks. Design/methodology/approach: Thus, the RFID reader, ultrasonic sensor and voice chip on a wheeled mobile robot link the front end to the crutch. Once the blind person nears a guide brick, the RFID will read the message on the tag through the voice broadcast system, and a voice will inform the visually impaired person of the direction to walk and information of the surrounding environment. In addition, the CMOS image sensor set up in the wheeled mobile robot is used to detect the black marking on the guide brick and to guide the blind to walk forward or turn around between the two markings. Finally, the lithium battery charging control unit was installed on the wheeled mobile robot. The ATtiny25 microcontroller conducts the battery charge and discharge control and monitoring of the current battery capacity. Findings: The development of this system will let visually impaired people acquire environmental information, road guidance function and nearby traffic information. Originality/value: Through rich spatial environment messages, the blind can have the confidence and courage to go outside. © Emerald Publishing Limited.","CMOS image sensor; Embedding microcontroller unit (EMCU); Radio-frequency identification module; Ultrasonic sensor; Wheeled mobile robot","Advanced traffic management systems; Audition; Brick; CMOS integrated circuits; Controllers; Digital cameras; Electric batteries; Image sensors; Intelligent robots; Lithium batteries; Machine design; Microcontrollers; Radio frequency identification (RFID); Radio waves; Robots; Secondary batteries; Ultrasonic applications; Ultrasonic sensors; Vision aids; Battery charge and discharge; CMOS image sensor; Design/methodology/approach; Environmental information; Microcontroller unit; Visually impaired people; Visually impaired persons; Wheeled mobile robot; Mobile robots","P.-S. Tsai; Department of Electronic Engineering, China University of Science and Technology, Taipei, Taiwan; email: tps@ee2.cust.edu.tw","","Emerald Group Publishing Ltd.","02644401","","ENCOE","","English","Eng. Comput. (Swansea Wales)","Article","Final","","Scopus","2-s2.0-85020422726"
"Sridhar Chakravarthy G.; Anupam K.; Harish Varma P.N.S.; Teja G.H.; Rodda S.","Sridhar Chakravarthy, G. (57215611177); Anupam, K. (57215610566); Harish Varma, P.N.S. (57215614034); Teja, G. Harsha (57215600244); Rodda, Sireesha (23986052300)","57215611177; 57215610566; 57215614034; 57215600244; 23986052300","Face Recognition with Voice Assistance for the Visually Challenged","2020","Advances in Intelligent Systems and Computing","1034","","","701","709","8","1","10.1007/978-981-15-1084-7_68","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85081336702&doi=10.1007%2f978-981-15-1084-7_68&partnerID=40&md5=464f0c69ccd9e9ed56e3203786a253cf","Department of Computer Science and Engineering, GITAM Institute of Technology, GITAM (Deemed to Be University), Visakhapatnam, India","Sridhar Chakravarthy G., Department of Computer Science and Engineering, GITAM Institute of Technology, GITAM (Deemed to Be University), Visakhapatnam, India; Anupam K., Department of Computer Science and Engineering, GITAM Institute of Technology, GITAM (Deemed to Be University), Visakhapatnam, India; Harish Varma P.N.S., Department of Computer Science and Engineering, GITAM Institute of Technology, GITAM (Deemed to Be University), Visakhapatnam, India; Teja G.H., Department of Computer Science and Engineering, GITAM Institute of Technology, GITAM (Deemed to Be University), Visakhapatnam, India; Rodda S., Department of Computer Science and Engineering, GITAM Institute of Technology, GITAM (Deemed to Be University), Visakhapatnam, India","Visually impaired people face a lot of challenges in day-to-day life. Having seen the difficulties faced by them, our primary objective is to facilitate confidence and to empower them to lead a life free from threats related to their safety and well-being. The lack of ability to identify known individuals in the absence of auditory or physical interaction cues drastically limits the visually challenged in their social interactions and poses a threat to their security. Over the past few years many prototype models have been developed to aid this population with the task of face recognition. This application will reduce the inherent difficulty for recognition of a person. It will present a facial recognition application with an intuitive user interface that enables the blind to recognise people and interact socially. The carefully designed interface lets the visually challenged to be able to access and use it without any requirement for visual cues as the users are acquainted by a voice assistant to navigate through the application. The entire build is designed to run efficiently on a Raspberry Pi 3 model B module using the Android Things platform. The Open CV library has been used for the detection and recognition of people in this project. This enables the scope for the software to be run on a multitude of devices such as camera embedded glasses to warn users of their surroundings and identify people to interact safely. Since everything in the application is done in real time with no requirement for prior datasets to be hardcoded it drastically improves the versatility of the software. We hope to make the visually impaired feel closer, comfortable and more secure with the world surrounding them through our application. © 2020, Springer Nature Singapore Pte Ltd.","Face recognition; Open CV; Pyttsx; Speech driven; Voice-based assistance","Application programs; Artificial life; Intelligent computing; Speech recognition; User interfaces; Facial recognition; Intuitive user interface; Open CV; Physical interactions; Primary objective; Pyttsx; Social interactions; Visually impaired people; Face recognition","G. Sridhar Chakravarthy; Department of Computer Science and Engineering, GITAM Institute of Technology, GITAM (Deemed to Be University), Visakhapatnam, India; email: sridharchakravarthyg@gmail.com","Bhateja V.; Bhateja V.; Satapathy S.C.; Zhang Y.-D.; Aradhya V.N.M.","Springer","21945357","978-981151083-0","","","English","Adv. Intell. Sys. Comput.","Conference paper","Final","","Scopus","2-s2.0-85081336702"
"Malūkas U.; Maskeliūnas R.; Damaševičius R.; Woźniak M.","Malūkas, Ugnius (57203098357); Maskeliūnas, Rytis (27467587600); Damaševičius, Robertas (6603451290); Woźniak, Marcin (36195020800)","57203098357; 27467587600; 6603451290; 36195020800","Real time path finding for assisted living using deep learning","2018","Journal of Universal Computer Science","24","4","","475","487","12","24","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85050496123&partnerID=40&md5=c27146ba2406c38a5db0c89ef18d53d3","Rubedo sistemos, Kaunas, Lithuania; Kaunas University of Technology, Kaunas, Lithuania; Silesian University of Technology, Gliwice, Poland","Malūkas U., Rubedo sistemos, Kaunas, Lithuania; Maskeliūnas R., Kaunas University of Technology, Kaunas, Lithuania; Damaševičius R., Kaunas University of Technology, Kaunas, Lithuania; Woźniak M., Silesian University of Technology, Gliwice, Poland","The paper presents a computer vision based system, which performs real time path finding for visually impaired or blind people. The semantic segmentation of camera images is performed using deep convolutional neural network (CNN), which able to recognize patterns across image feature space. Out of three different CNN architectures (AlexNet, GoogLeNet and VGG) analysed, the fully connected VGG16 neural network is shown to perform best in the semantic segmentation task. The algorithm for extracting and finding paths, obstacles and path boundaries is presented. The experiments performed using own dataset (300 images extracted from two hours of video recording walking in outdoors environment) show that the developed system is able to find paths, path objects and path boundaries with an accuracy of 96.1 ± 2.6%. © J.UCS.","Assisted Living; Deep Learning; Image processing; Neural networks; Object recognition; Outdoor navigation; Path finding; Semantic segmentation","","","","IICM","0948695X","","","","English","J. Univers. Comput. Sci.","Article","Final","","Scopus","2-s2.0-85050496123"
"Akbar I.; Misman A.F.","Akbar, Israh (57212146032); Misman, Ahmad Fatzilah (57193340082)","57212146032; 57193340082","Research on semantics used in GPS based mobile phone applications for blind pedestrian navigation in an outdoor environment","2018","Proceedings - International Conference on Information and Communication Technology for the Muslim World 2018, ICT4M 2018","","","8567120","196","201","5","4","10.1109/ICT4M.2018.00044","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85060444683&doi=10.1109%2fICT4M.2018.00044&partnerID=40&md5=0dd80ce2f3079835a37b007019b0d1c6","Department of Information Systems (DIS), Kulliyah of Information and Communication Tech (KICT), International Islamic University Malaysia (IIUM), Kuala Lumpur, Malaysia","Akbar I., Department of Information Systems (DIS), Kulliyah of Information and Communication Tech (KICT), International Islamic University Malaysia (IIUM), Kuala Lumpur, Malaysia; Misman A.F., Department of Information Systems (DIS), Kulliyah of Information and Communication Tech (KICT), International Islamic University Malaysia (IIUM), Kuala Lumpur, Malaysia","The research is based on studies on semantics used in GPS guided mobile navigation applications in particular for blind pedestrians. The scope is in information of words or narrative use in guiding them and how to improve the effectiveness in the semantics via machines learning. These are tested in this research. The long term goal is to create a mobile technology, self-contained system that allows blind users to navigate through unfamiliar environments without the assistance of guides but mobile application. The experiments took place at MFB (Malaysian Foundation for the Blind) using common existing GPS based mobile applications and the results were used to prove the hypothesis that the blind are not supposedly at a substantial disadvantage in independent navigation because of the insufficiency and abnormality in providing information or semantic for them with the GPS navigation technology of today. The outcome derived from the research can further help in creating and improving the semantics of the GPS based navigation technology for the blind pedestrians in an unknown environment. © 2018 IEEE.","Blind; GPS; Navigation; Pedestrians; Semantics; Visually impaired","Mobile computing; Navigation; Semantics; Blind; GPS-based navigation; Mobile navigation applications; Mobile phone applications; Pedestrian navigation; Pedestrians; Self-contained systems; Visually impaired; Global positioning system","","","Institute of Electrical and Electronics Engineers Inc.","","978-153867525-0","","","English","Proc. - Int. Conf. Inf. Commun. Technol. Muslim World , ICT4M","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85060444683"
"Wei C.-Y.; Li B.-Y.; Wei H.-W.; Lee W.-T.","Wei, Chi-You (57215223423); Li, Bo-Yi (57215223658); Wei, Hsin-Wen (14632869100); Lee, Wei-Tsong (57216140609)","57215223423; 57215223658; 14632869100; 57216140609","The Design Concept of Intelligent Guide Cane","2019","2019 IEEE International Conference on Consumer Electronics - Taiwan, ICCE-TW 2019","","","8991709","","","","2","10.1109/ICCE-TW46550.2019.8991709","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85080130380&doi=10.1109%2fICCE-TW46550.2019.8991709&partnerID=40&md5=bf6e1c1bbe37b918ef964207b4827f7d","Tamkang University, Taipei, Taiwan","Wei C.-Y., Tamkang University, Taipei, Taiwan; Li B.-Y., Tamkang University, Taipei, Taiwan; Wei H.-W., Tamkang University, Taipei, Taiwan; Lee W.-T., Tamkang University, Taipei, Taiwan","Some people are congenial blindness or visually impaired while the others are caused by acquired factors. One reason that results in poor vision or blindness is age. As people get old, the eyesight of people is destined to deteriorate. Normally, people with poor vision can only live in a familiar environment to ensure their own safety. The mobility of people with impaired vision is restricted and they may live in inconvenient life. Therefore, in this study, we aim to provide a design concept of novel intelligent guide cane to enhance the mobility of visually impaired people. The proposed design is to add some functions to the original guide cane and connect the intelligent guide cane with hand-held device. Since the hand-held device that is widely used in today and has a considerable amount of computing, the proposed design can act as an auxiliary machine similar to guide dogs or better than guide dogs. The proposed design aims to provide the functions such as route planning, navigation, obstacle detection, and object recognition. With the proposed design, the mobility of visually impaired people can be greatly improved. © 2019 IEEE.","accessibility; guide cane; smart navigation","Eye protection; Object detection; Object recognition; Obstacle detectors; accessibility; guide cane; Hand held device; Impaired vision; Obstacle detection; Route planning; Visually impaired; Visually impaired people; Hand held computers","","","Institute of Electrical and Electronics Engineers Inc.","","978-172813279-2","","","English","IEEE Int. Conf. Consum. Electron. - Taiwan, ICCE-TW","Conference paper","Final","","Scopus","2-s2.0-85080130380"
"Elmannai W.M.; Elleithy K.M.","Elmannai, Wafa M. (55545871900); Elleithy, Khaled M. (35576221100)","55545871900; 35576221100","A Highly Accurate and Reliable Data Fusion Framework for Guiding the Visually Impaired","2018","IEEE Access","6","","","33029","33054","25","52","10.1109/ACCESS.2018.2817164","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85044092453&doi=10.1109%2fACCESS.2018.2817164&partnerID=40&md5=60b50c8ebd53c422097d197f2d8c0ced","Department of Computer Science and Engineering, University of Bridgeport, Bridgeport, 06604, CT, United States","Elmannai W.M., Department of Computer Science and Engineering, University of Bridgeport, Bridgeport, 06604, CT, United States; Elleithy K.M., Department of Computer Science and Engineering, University of Bridgeport, Bridgeport, 06604, CT, United States","The world has approximately 253 million visually impaired (VI) people according to a report by the world health organization (WHO) in 2014. Thirty-six million people are estimated to be blind. According to WHO, 217 million people are estimated to have moderate to severe visual impairment. An important factor that motivated this research is the fact that 90% of VI people live in developing countries. Several systems were designed to improve the quality of the life of VI people and support their mobility. Unfortunately, none of these systems are considered to be a complete solution for VI people and these systems are very expensive. We present in this paper an intelligent framework for supporting VI people. The proposed work integrates sensor-based and computer vision-based techniques to provide an accurate and economical solution. These techniques allow us to detect multiple objects and enhance the accuracy of the collision avoidance system. In addition, we introduce a novel obstacle avoidance algorithm based on the image depth information and fuzzy logic. By using the fuzzy logic, we were able to provide precise information to help the VI user in avoiding front obstacles. The system has been deployed and tested in real-time scenarios. An accuracy of 98% was obtained for detecting objects and 100% accuracy in avoiding the detected objects. © 2013 IEEE.","Assistive wearable devices; blindness; computer vision systems; data fusion algorithm; mobility limitation; obstacle detection and obstacle collision avoidance; sensor-based networks; visual impairment","Acoustics; Collision avoidance; Computer circuits; Computer vision; Data visualization; Developing countries; Flow visualization; Fuzzy logic; Interactive computer systems; Navigation; Object detection; Obstacle detectors; Ophthalmology; Sensor data fusion; Sensors; Wearable sensors; and mobility limitation; Assistive technology; blindness; Computer vision system; Data fusion algorithm; Obstacle collision; Visual impairment; Wearable devices; Real time systems","W.M. Elmannai; Department of Computer Science and Engineering, University of Bridgeport, Bridgeport, 06604, United States; email: welmanna@my.bridgeport.edu","","Institute of Electrical and Electronics Engineers Inc.","21693536","","","","English","IEEE Access","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85044092453"
"Bai J.; Liu Z.; Lin Y.; Li Y.; Lian S.; Liu D.","Bai, Jinqiang (55889538000); Liu, Zhaoxiang (57211257370); Lin, Yimin (36651057300); Li, Ye (57210435908); Lian, Shiguo (7005702391); Liu, Dijun (57195592394)","55889538000; 57211257370; 36651057300; 57210435908; 7005702391; 57195592394","Wearable travel aid for environment perception and navigation of visually impaired people","2019","Electronics (Switzerland)","8","6","697","","","","70","10.3390/electronics8060697","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85070695254&doi=10.3390%2felectronics8060697&partnerID=40&md5=4bfc487fba8912a0fc56e7438e3ca659","School of Electronic Information Engineering, Beihang University, No. 37, Xueyuan Rd., Haidian Distrct, Beijing, 10083, China; Department of AI, CloudMinds Technologies Inc, Beijing, 100102, China; China Academy of Telecommunication Technology, Beijing, 10083, China","Bai J., School of Electronic Information Engineering, Beihang University, No. 37, Xueyuan Rd., Haidian Distrct, Beijing, 10083, China; Liu Z., Department of AI, CloudMinds Technologies Inc, Beijing, 100102, China; Lin Y., Department of AI, CloudMinds Technologies Inc, Beijing, 100102, China; Li Y., Department of AI, CloudMinds Technologies Inc, Beijing, 100102, China; Lian S., Department of AI, CloudMinds Technologies Inc, Beijing, 100102, China; Liu D., China Academy of Telecommunication Technology, Beijing, 10083, China","Assistive devices for visually impaired people (VIP) which support daily traveling and improve social inclusion are developing fast. Most of them try to solve the problem of navigation or obstacle avoidance, and other works focus on helping VIP to recognize their surrounding objects. However, very few of them couple both capabilities (i.e., navigation and recognition). Aiming at the above needs, this paper presents a wearable assistive device that allows VIP to (i) navigate safely and quickly in unfamiliar environment, and (ii) to recognize the objects in both indoor and outdoor environments. The device consists of a consumer Red, Green, Blue and Depth (RGB-D) camera and an Inertial Measurement Unit (IMU), which are mounted on a pair of eyeglasses, and a smartphone. The device leverages the ground height continuity among adjacent image frames to segment the ground accurately and rapidly, and then search the moving direction according to the ground. A lightweight Convolutional Neural Network (CNN)-based object recognition system is developed and deployed on the smartphone to increase the perception ability of VIP and promote the navigation system. It can provide the semantic information of surroundings, such as the categories, locations, and orientations of objects. Human–machine interaction is performed through audio module (a beeping sound for obstacle alert, speech recognition for understanding the user commands, and speech synthesis for expressing semantic information of surroundings). We evaluated the performance of the proposed system through many experiments conducted in both indoor and outdoor scenarios, demonstrating the efficiency and safety of the proposed assistive system. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Blind navigation; Ground segmentation; Object recognition; Visually impaired people; Wearable assistive device","","J. Bai; School of Electronic Information Engineering, Beihang University, Beijing, No. 37, Xueyuan Rd., Haidian Distrct, 10083, China; email: baijinqiang@buaa.edu.cn","","MDPI AG","20799292","","","","English","Electronics (Switzerland)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85070695254"
"Yu H.; Ohn-Bar E.; Yoo D.; Kitani K.M.","Yu, Heng (57200930972); Ohn-Bar, Eshed (55511352100); Yoo, Donghyun (57203286118); Kitani, Kris M. (15835267300)","57200930972; 55511352100; 57203286118; 15835267300","SmartPartNet: Part-Informed Person Detection for Body-Worn Smartphones","2018","Proceedings - 2018 IEEE Winter Conference on Applications of Computer Vision, WACV 2018","2018-January","","8354230","1103","1112","9","2","10.1109/WACV.2018.00126","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85051130774&doi=10.1109%2fWACV.2018.00126&partnerID=40&md5=4183d6566d8b60b27812081d1c3d2b0b","Tsinghua University, China; Carnegie Mellon University, United States","Yu H., Tsinghua University, China; Ohn-Bar E., Carnegie Mellon University, United States; Yoo D., Carnegie Mellon University, United States; Kitani K.M., Carnegie Mellon University, United States","We are interested in the development of image-based person detection algorithms for wearable computing using commodity smartphones. We focus on the use of smartphones as a wearable device because it is a practical means of augmenting human sensing for applications such as navigation for the blind or assisting social interaction. We identify two unique features of developing a vision-based person detector for body-worn smartphones: (1) the detector must take into account the strong bias in the size of people in the images taken with a wearable device and (2) the detector must consider the low image quality due to dim lighting and rapid ego-motion which leads to motion blur. In order to account for the unique distribution over the visibility of body parts when using a wearable camera, we propose a part-based person detector specialized for chestmounted smartphones. We perform extensive ablative analysis on the usefulness of part information, providing several insights regarding the design of the optimal person detector across different application domains. To account for the frequent occurrence of motion blur in our target domain, we introduce a data augmentation technique to generate synthetic motion-blurred images during training. In addition to addressing the aforementioned features, the final detector must also run in real-time using only smartphone resources. We leverage recent progress in deep neural networks for mobile devices and show that our proposed person detector, SmartPartNet, obtains performance similar to state-of-the-art pedestrian detection networks, while being 3X smaller and 5X faster. © 2018 IEEE.","","Computer vision; Deep neural networks; Smartphones; Wearable technology; Data augmentation; Motion blurred image; Pedestrian detection; Social interactions; State of the art; Wearable cameras; Wearable computing; Wearable devices; Feature extraction","","","Institute of Electrical and Electronics Engineers Inc.","","978-153864886-5","","","English","Proc. - IEEE Winter Conf. Appl. Comput. Vis., WACV","Conference paper","Final","","Scopus","2-s2.0-85051130774"
"Abobeah R.; Hussein M.; Abdelwahab M.; Shoukry A.","Abobeah, Reham (57202310406); Hussein, Mohamed (56066640000); Abdelwahab, Moataz (16642214100); Shoukry, Amin (7003338982)","57202310406; 56066640000; 16642214100; 7003338982","Wearable RGB camera-based navigation system for the visually impaired","2018","VISIGRAPP 2018 - Proceedings of the 13th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications","5","","","555","562","7","9","10.5220/0006617505550562","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85047840253&doi=10.5220%2f0006617505550562&partnerID=40&md5=dd597c83837fe23a2cc0c19404e79fdb","CSE Department, Egypt-Japan University of Science and Technology, New Borg El-Arab City, Alexandria, Egypt; Information Sciences Institute, Arlington, VA, United States; CSE Department, Faculty of Engineering, Alexandria University, Alexandria, Alexandria, Egypt; ECE Department, Egypt-Japan University of Science and Technology, New Borg El-Arab City, Alexandria, Egypt","Abobeah R., CSE Department, Egypt-Japan University of Science and Technology, New Borg El-Arab City, Alexandria, Egypt; Hussein M., Information Sciences Institute, Arlington, VA, United States, CSE Department, Faculty of Engineering, Alexandria University, Alexandria, Alexandria, Egypt; Abdelwahab M., ECE Department, Egypt-Japan University of Science and Technology, New Borg El-Arab City, Alexandria, Egypt; Shoukry A., CSE Department, Egypt-Japan University of Science and Technology, New Borg El-Arab City, Alexandria, Egypt, CSE Department, Faculty of Engineering, Alexandria University, Alexandria, Alexandria, Egypt","This paper proposes a wearable RGB camera-based system for sightless people through which they can easily and independently navigate their surrounding environment. The system uses a single head or chest mounted RGB camera to capture the visual information from the current user’s path, and an auditory system to inform the user about the right direction to follow. This information is obtained through a novel alignment technique which takes as input a visual snippet from the current user’s path and responds with the corresponding location on the training path. Then, assuming that the wearable camera pose reflects the user’s pose, the system corrects the current user’s pose to align with the corresponding pose in the training location. As a result, the user receives periodically an acoustic instruction to assist him in reaching his destination safely. The experiments conducted to test the system, in various collected indoor and outdoor paths, have shown that it satisfies its design specifications in terms of correctly generating the instructions for guiding the visually impaired along these paths, in addition to its ability to detect and correct deviations from the predefined paths. © 2018 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.","Blind navigation; Deviation detection; Indoor/ outdoor navigation; Mono camera; Path following; Pose estimation; Temporal alignment","Cameras; Computer graphics; Computer vision; Navigation systems; Blind navigation; Deviation detection; Outdoor navigation; Path following; Pose estimation; Temporal alignment; Wearable technology","","Imai F.; Tremeau A.; Braz J.","SciTePress","","978-989758290-5","","","English","VISIGRAPP - Proc. Int. Jt. Conf. Comput. Vis., Imaging Comput. Graph. Theory Appl.","Conference paper","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85047840253"
"Rocha T.; Fernandes H.; Reis A.; Paredes H.; Barroso J.","Rocha, Tânia (53364126200); Fernandes, Hugo (35172835800); Reis, Arsénio (55803825800); Paredes, Hugo (23398113700); Barroso, João (20435746800)","53364126200; 35172835800; 55803825800; 23398113700; 20435746800","Assistive platforms for the visual impaired: Bridging the gap with the general public","2017","Advances in Intelligent Systems and Computing","570","","","602","608","6","11","10.1007/978-3-319-56538-5_61","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85018564055&doi=10.1007%2f978-3-319-56538-5_61&partnerID=40&md5=7c083de233b56cf95a0fa1c67b53ecdc","INESC TEC and Universidade de Trás-os-Montes e Alto Douro, Vila Real, Portugal","Rocha T., INESC TEC and Universidade de Trás-os-Montes e Alto Douro, Vila Real, Portugal; Fernandes H., INESC TEC and Universidade de Trás-os-Montes e Alto Douro, Vila Real, Portugal; Reis A., INESC TEC and Universidade de Trás-os-Montes e Alto Douro, Vila Real, Portugal; Paredes H., INESC TEC and Universidade de Trás-os-Montes e Alto Douro, Vila Real, Portugal; Barroso J., INESC TEC and Universidade de Trás-os-Montes e Alto Douro, Vila Real, Portugal","The visual impaired are a specific minority group that can benefit from specific assistive systems in order to mitigate their mobility and accessibility constrains. In the last decade, our research group has been integrating and developing assistive technologies, focused in human-computer interaction, artificial vision, assisted navigation, pervasive computing, among others. Several projects and prototypes have been developed with the main objective of improving the blind’s autonomy, mobility, and quality of life. Currently the technology has reached a maturation point that allows the development of systems based on video capturing, image recognition and location referencing, which are key for providing features of artificial vision, assisted navigation and spatial perception. The miniaturization of electronics can be used to create devices such as electronic canes that equipped with sensors can provide so much more contextual information to a blind user. The adoption of these systems is dependent of an information catalogue regarding points of interest and their physical location reference. In this paper we describe the current work on assistive systems for the blind and propose a new perspective on using the base information of those systems to provide new services to the general public. By bridging the gap between the two groups, we expect to further advance the development of the current systems and contribute to their economic sustainability. © Springer International Publishing AG 2017.","3D mapping; Assistive technologies; Blind users; Navigation; NFC","Human rehabilitation equipment; Image recognition; Information systems; Navigation; Ubiquitous computing; Vision; 3-D mapping; Assisted navigations; Assistive technology; Blind users; Contextual information; Economic sustainability; Physical locations; Recognition and locations; Human computer interaction","J. Barroso; INESC TEC and Universidade de Trás-os-Montes e Alto Douro, Vila Real, Portugal; email: jbarroso@utad.pt","Adeli H.; Correia A.M.; Costanzo S.; Reis L.P.; Rocha A.","Springer Verlag","21945357","978-331956537-8","","","English","Adv. Intell. Sys. Comput.","Conference paper","Final","","Scopus","2-s2.0-85018564055"
"Khan W.; Hussain A.; Khan B.; Nawaz R.; Baker T.","Khan, Wasiq (55319932200); Hussain, Abir (56212648400); Khan, Bilal (57216733975); Nawaz, Raheel (52164201600); Baker, Thar (34267505000)","55319932200; 56212648400; 57216733975; 52164201600; 34267505000","Novel framework for outdoor mobility assistance and auditory display for visually impaired people","2019","Proceedings - International Conference on Developments in eSystems Engineering, DeSE","October-2019","","9073198","984","989","5","8","10.1109/DeSE.2019.00183","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85084409638&doi=10.1109%2fDeSE.2019.00183&partnerID=40&md5=42cd26d5c713064150e642576df305a7","Department of Comouter Science, Liverpool John Moores University, Liverpoorl, United Kingdom; Center for Environmental Implications of Nanotechnology, University of California, Los Angeles, United States; School of Bussiness and Mangement Science, Manchester Metropolitan University, Manchester, United Kingdom","Khan W., Department of Comouter Science, Liverpool John Moores University, Liverpoorl, United Kingdom; Hussain A., Department of Comouter Science, Liverpool John Moores University, Liverpoorl, United Kingdom; Khan B., Center for Environmental Implications of Nanotechnology, University of California, Los Angeles, United States; Nawaz R., School of Bussiness and Mangement Science, Manchester Metropolitan University, Manchester, United Kingdom; Baker T., Department of Comouter Science, Liverpool John Moores University, Liverpoorl, United Kingdom","Outdoor mobility of Visually Impaired People (VIPs) has always been challenging due to the dynamically varying scenes and environmental states. Variety of systems have been introduced to assist VIPs' mobility that include sensor mounted canes and use of machine intelligence. However, these systems are not reliable when used to navigate the VIPs in dynamically changing environments. The associated challenges are the robust sensing and avoiding diverse types of obstacles, dynamically modelling the changing environmental states (e.g. moving objects, road-works), and effective communication to interpret the environmental states and hazards. In this paper, we propose an intelligent wearable auditory display framework that will process real-time video and multi-sensor data streams to: a) identify the type of obstacles, b) recognize the surrounding scene/objects and corresponding attributes (e.g. geometry, size, shape, distance from user), c) automatically generate the descriptive information about the recognized obstacle/objects and attributes, d) produce accurate, precise and reliable spatial information and corresponding instructions in audio-visual form to assist and navigate VIPs safely with or without the assistance of traditional means. © 2019 IEEE.","Blind people navigation; Data fusion; Healthcare aid; Healthcare technology framework; Object recognition; Obstacle detection; Outdoor mobility assistance; Scene recognition; Visual impairment","Data streams; Navigation; Changing environment; Descriptive information; Effective communication; Environmental state; Machine intelligence; Mobility assistance; Spatial informations; Visually impaired people; Wearable sensors","","Al-Jumeily D.; Hind J.; Mustafina J.; Al-Hajj A.; Hussain A.; Magid E.; Tawfik H.","Institute of Electrical and Electronics Engineers Inc.","21611343","978-172813021-7","","","English","Proc. - Int. Conf. Dev. eSystems Eng., DeSE","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85084409638"
"Leong K.Y.; Egerton S.; Chan C.K.Y.","Leong, K.Y. (57201859110); Egerton, S. (8252012400); Chan, Carina K.Y. (37261026700)","57201859110; 8252012400; 37261026700","A wearable technology to negotiate surface discontinuities for the blind and low vision","2017","2017 IEEE Life Sciences Conference, LSC 2017","2018-January","","","115","120","5","3","10.1109/LSC.2017.8268157","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85046253574&doi=10.1109%2fLSC.2017.8268157&partnerID=40&md5=5d372297db9cc59ab478fe3c4ff0762c","School of Information Technology (Caulfield), Monash University, Melbourne, Australia; Department of Computer Science and Information Technology, La Trobe University, Australia; School of Psychology, Faculty of Health Sciences, Australian Catholic University, Brisbane, Australia","Leong K.Y., School of Information Technology (Caulfield), Monash University, Melbourne, Australia; Egerton S., Department of Computer Science and Information Technology, La Trobe University, Australia; Chan C.K.Y., School of Psychology, Faculty of Health Sciences, Australian Catholic University, Brisbane, Australia","One of the greatest challenges to independence for the blind and low vision (BLV) individuals are difficulties in self-navigation. This paper presents a wearable assistive technology to augment the perception of the BLVs along their navigational pathway. The research addresses specifically the challenge of negotiating surface discontinuities typically found within urban areas. A lightweight, small and unobtrusive wearable prototype is developed, and it utilizes a stereo camera to collect depth data from the navigational pathway. A depth profile is proposed to reduce the dimension of depth map computed from the image pair, and a deep belief network is then assembled to classify the depth profiles such that appropriate haptic feedback can be communicated to the user. © 2017 IEEE.","","Stereo image processing; Wearable technology; Assistive technology; Deep belief networks; Depth-profile; Depthmap; Image pairs; Low vision; Stereo cameras; Surface discontinuities; Urban areas; Wearable prototypes; Navigation","","","Institute of Electrical and Electronics Engineers Inc.","","978-153861030-5","","","English","IEEE Life Sci. Conf., LSC","Conference paper","Final","","Scopus","2-s2.0-85046253574"
"Panchal A.A.; Varde S.; Panse M.S.","Panchal, Akhilesh A. (57520109400); Varde, Shrugal (57193572097); Panse, M.S. (59259323200)","57520109400; 57193572097; 59259323200","Character detection and recognition system for visually impaired people","2017","2016 IEEE International Conference on Recent Trends in Electronics, Information and Communication Technology, RTEICT 2016 - Proceedings","","","7808080","1492","1496","4","14","10.1109/RTEICT.2016.7808080","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85015078622&doi=10.1109%2fRTEICT.2016.7808080&partnerID=40&md5=1b9f8b617ded82a06aad7891a64beb31","Department of Electrical Engineering, Veermata Jijabai Technological Institute, Mumbai, India","Panchal A.A., Department of Electrical Engineering, Veermata Jijabai Technological Institute, Mumbai, India; Varde S., Department of Electrical Engineering, Veermata Jijabai Technological Institute, Mumbai, India; Panse M.S., Department of Electrical Engineering, Veermata Jijabai Technological Institute, Mumbai, India","Nowadays, increasing use of digital technology, availability of economical image capturing devices like mobile phones, digital cameras, etc. and need of powerful technology to aid blind or visually impaired people, attracting researchers to the problem of recognizing text in images. Detecting text from scene image is more difficult as compared to that from printed documents. Lots of research has been done on detecting scene text to overcome certain challenges like perspective distortion, aspect ratio, font size, etc. Speed, complexity, cost and accuracy are important parameters must be taken into consideration while designing such systems. Computer vision is one of the emerging technologies that can be used to aid visually impaired people for navigation (both indoor and outdoor), accessing printed material, etc. This paper describes an approach to extract and recognize text from scene images effectively using computer vision technology and to convert recognized text into speech so that it can be incorporated with hardware to develop Electronic travel aid for visually impaired people in future. © 2016 IEEE.","Character detection and extraction; Computer Vision; Image Acquisition; Image Enhancement; Pattern recognition; Text to Speech conversion","Air navigation; Aspect ratio; Cellular telephone systems; Character recognition; Computer hardware; Computer vision; Digital devices; Image acquisition; Image enhancement; Image processing; Pattern recognition; Computer vision technology; Digital technologies; Electronic travel aidss; Emerging technologies; Image capturing devices; Perspective distortion; Text-to-speech conversion; Visually impaired people; Speech recognition","","","Institute of Electrical and Electronics Engineers Inc.","","978-150900774-5","","","English","IEEE Int. Conf. Recent Trends Electron., Inf. Commun. Technol., RTEICT - Proc.","Conference paper","Final","","Scopus","2-s2.0-85015078622"
"Shajahan S.; Kalpana M.; Ali M.S.","Shajahan, S. (56405456200); Kalpana, Mamillapalli (57212557983); Ali, Mohammad Sadiq (57204561465)","56405456200; 57212557983; 57204561465","Designing a flexible device to assist visually challenged people","2018","Indian Journal of Public Health Research and Development","9","10","","1048","1051","3","0","10.5958/0976-5506.2018.01272.X","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85056178944&doi=10.5958%2f0976-5506.2018.01272.X&partnerID=40&md5=f90ead3277c06c074794c8bbf264b3b3","Dept. of ECE, Sri Venkateswara College of Engineering and Technology, Chittoor, A.P, India","Shajahan S., Dept. of ECE, Sri Venkateswara College of Engineering and Technology, Chittoor, A.P, India; Kalpana M., Dept. of ECE, Sri Venkateswara College of Engineering and Technology, Chittoor, A.P, India; Ali M.S., Dept. of ECE, Sri Venkateswara College of Engineering and Technology, Chittoor, A.P, India","Several people have slowly lost a vision over years; on the other hand some people are blind from birth. People who advise that the blind have a little vision and may be sensitive to light, May be blurred vision, and also having a limited field of vision like side vision, central vision or can see only half of the view. Some visually impaired peoples may not come out any different from other peoples; other people use cane. Visually impaired people find difficulty in identifying detail or in reading. However, when the output vision is affected it can decrease the visual area and hence the mobility a mobility difficult. By using Raspberry pi along with camera module a progressive work is done for developing an aid for visually impaired, which will help them in Reading, face Recognition, GPS location detection and controlling home appliances. © 2018, Indian Journal of Public Health Research and Development. All rights reserved.","Camera; Face recognition; GPS; Raspberry pi; Zigbee","Article; classifier; equipment design; facial recognition; flexible assistive device; global positioning system; human; image processing; reading; support vector machine; visual disorder","","","Institute of Medico-Legal Publications","09760245","","","","English","Indian J. Public Health Res. Dev.","Article","Final","","Scopus","2-s2.0-85056178944"
"Zhang X.; Yao X.; Zhu Y.; Hu F.","Zhang, Xiaochen (54390539800); Yao, Xiaoyu (57208053106); Zhu, Yi (57208055690); Hu, Fei (35366272600)","54390539800; 57208053106; 57208055690; 35366272600","An ARCore based user centric assistive navigation system for visually impaired people","2019","Applied Sciences (Switzerland)","9","5","989","","","","47","10.3390/app9050989","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85063669844&doi=10.3390%2fapp9050989&partnerID=40&md5=cfbbffeb14a943e453e7ee9d1e966222","Department of Industrial Design, Guangdong University of Technology, Guangzhou, 510006, China; School of Industrial Design, Georgia Institute of Technology, 30332, GA, United States","Zhang X., Department of Industrial Design, Guangdong University of Technology, Guangzhou, 510006, China; Yao X., Department of Industrial Design, Guangdong University of Technology, Guangzhou, 510006, China; Zhu Y., Department of Industrial Design, Guangdong University of Technology, Guangzhou, 510006, China, School of Industrial Design, Georgia Institute of Technology, 30332, GA, United States; Hu F., Department of Industrial Design, Guangdong University of Technology, Guangzhou, 510006, China","In this work, we propose an assistive navigation system for visually impaired people (ANSVIP) that takes advantage of ARCore to acquire robust computer vision-based localization. To complete the system, we propose adaptive artificial potential field (AAPF) path planning that considers both efficiency and safety. We also propose a dual-channel human-machine interaction mechanism, which delivers accurate and continuous directional micro-instruction via a haptic interface and macro-long-term planning and situational awareness via audio. Our system user-centrically incorporates haptic interfaces to provide fluent and continuous guidance superior to the conventional turn-by-turn audio-guiding method; moreover, the continuous guidance makes the path under complete control in avoiding obstacles and risky places. The system prototype is implemented with full functionality. Unit tests and simulations are conducted to evaluate the localization, path planning, and human-machine interactions, and the results show that the proposed solutions are superior to those of the present state-of-the-art solutions. Finally, integrated tests are carried out with low-vision and blind subjects to verify the proposed system. © 2019 by the authors.","Adaptive path planning; ARCore; Assistive technology; Haptic interaction; Navigation aids; SLAM; User centric design; Visual impairment","","F. Hu; Department of Industrial Design, Guangdong University of Technology, Guangzhou, 510006, China; email: hufei@gdut.edu.cn","","MDPI AG","20763417","","","","English","Appl. Sci.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85063669844"
"Vergnieux V.; Macé M.J.-M.; Jouffrais C.","Vergnieux, Victor (56648170000); Macé, Marc J.-M. (7005499567); Jouffrais, Christophe (7801364165)","56648170000; 7005499567; 7801364165","Simplification of Visual Rendering in Simulated Prosthetic Vision Facilitates Navigation","2017","Artificial Organs","41","9","","852","861","9","20","10.1111/aor.12868","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85016041632&doi=10.1111%2faor.12868&partnerID=40&md5=93e5ce500f3437a10781f1ecee64717a","Université de Toulouse and CNRS, IRIT, UMR5505, Toulouse, France","Vergnieux V., Université de Toulouse and CNRS, IRIT, UMR5505, Toulouse, France; Macé M.J.-M., Université de Toulouse and CNRS, IRIT, UMR5505, Toulouse, France; Jouffrais C., Université de Toulouse and CNRS, IRIT, UMR5505, Toulouse, France","Visual neuroprostheses are still limited and simulated prosthetic vision (SPV) is used to evaluate potential and forthcoming functionality of these implants. SPV has been used to evaluate the minimum requirement on visual neuroprosthetic characteristics to restore various functions such as reading, objects and face recognition, object grasping, etc. Some of these studies focused on obstacle avoidance but only a few investigated orientation or navigation abilities with prosthetic vision. The resolution of current arrays of electrodes is not sufficient to allow navigation tasks without additional processing of the visual input. In this study, we simulated a low resolution array (15 × 18 electrodes, similar to a forthcoming generation of arrays) and evaluated the navigation abilities restored when visual information was processed with various computer vision algorithms to enhance the visual rendering. Three main visual rendering strategies were compared to a control rendering in a wayfinding task within an unknown environment. The control rendering corresponded to a resizing of the original image onto the electrode array size, according to the average brightness of the pixels. In the first rendering strategy, vision distance was limited to 3, 6, or 9 m, respectively. In the second strategy, the rendering was not based on the brightness of the image pixels, but on the distance between the user and the elements in the field of view. In the last rendering strategy, only the edges of the environments were displayed, similar to a wireframe rendering. All the tested renderings, except the 3 m limitation of the viewing distance, improved navigation performance and decreased cognitive load. Interestingly, the distance-based and wireframe renderings also improved the cognitive mapping of the unknown environment. These results show that low resolution implants are usable for wayfinding if specific computer vision algorithms are used to select and display appropriate information regarding the environment. © 2017 International Center for Artificial Organs and Transplantation and Wiley Periodicals, Inc.","Blind; Computer vision; Navigation; Retinal implant; Spatial cognition; Visual neuroprostheses; Wayfinding","Adult; Algorithms; Comprehension; Electrodes; Female; Healthy Volunteers; Humans; Image Processing, Computer-Assisted; Male; Phosphenes; Prosthesis Design; Spatial Navigation; Vision Disorders; Vision, Ocular; Visual Prosthesis; Young Adult; Brain computer interface; Computer vision; Electrodes; Face recognition; Luminance; Navigation; Neural prostheses; Ophthalmology; Pixels; Prosthetics; Rendering (computer graphics); Vision; Blind; Computer vision algorithms; Lower resolution; Minimum requirements; Neuroprosthetic; Prosthetic vision; Retinal implants; Spatial cognition; Visual neuroprosthesis; Way finding; adult; algorithm; Article; brightness; computer; computer vision; controlled study; distance perception; female; human; human experiment; male; mental load; normal human; priority journal; retinal implant; simulation; spatial orientation; visual information; comprehension; electrode; image processing; physiology; prosthesis design; vision; visual disorder; visual prosthesis; young adult; Restoration","M.J.-M. Macé; Université de Toulouse and CNRS, IRIT, UMR5505, Toulouse, France; email: Marc.Mace@irit.fr","","","0160564X","","ARORD","28321887","English","Artif. Organs","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85016041632"
"Wu J.; Zeng J.; Liu Y.; Shi G.; Lin W.","Wu, Jinjian (35076764600); Zeng, Jichen (57201857467); Liu, Yongxu (57200439587); Shi, Guangming (55536676300); Lin, Weisi (8574872000)","35076764600; 57201857467; 57200439587; 55536676300; 8574872000","Hierarchical Feature Degradation Based Blind Image Quality Assessment","2017","Proceedings - 2017 IEEE International Conference on Computer Vision Workshops, ICCVW 2017","2018-January","","","510","517","7","20","10.1109/ICCVW.2017.67","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85046286598&doi=10.1109%2fICCVW.2017.67&partnerID=40&md5=1de066d1787c6aec2d9df018ec88b392","Xidian University, China; Nanyang Technological University, Singapore, Singapore","Wu J., Xidian University, China; Zeng J., Xidian University, China; Liu Y., Xidian University, China; Shi G., Xidian University, China; Lin W., Nanyang Technological University, Singapore, Singapore","Though blind image quality assessment (BIQA) is highly demanded for many image processing systems, it is extremely difficult for BIQA to accurately predict the quality without the guide of the reference image. In this paper, we introduce a novel BIQA method with hierarchical feature degradation (HFD). Since the human brain presents hierarchical procedure for visual recognition, we suggest that different levels of distortion generate different degradations on hierarchical features, and propose to consider the degradations on both the low and high level features for quality assessment. Inspired by the orientation selectivity (OS) mechanism in the primary visual cortex, an OS based local visual structure is designed for low-level visual content extraction. Meanwhile, according to the feature integration function of deep neural networks, the deep semantics is extracted with the residual network for high-level visual content representation. Next, by analyzing the degradation on both the local structure and the deep semantics, a HFD based memory (prior knowledge) is learned to represent the generalized quality degradation. Finally, with the guidance of the HFD based memory, a novel HFD-BIQA model is built. Experimental results on the publicly available databases demonstrate the quality prediction accuracy of the proposed HFD-BIQA, and verify that the HFD-BIQA performs highly consistent with the subjective perception1. © 2017 IEEE.","","Computer vision; Deep neural networks; Semantics; Hierarchical features; High-level features; Image processing system; Image quality assessment; Orientation selectivity; Primary visual cortex; Quality degradation; Visual content representations; Image quality","","","Institute of Electrical and Electronics Engineers Inc.","","978-153861034-3","","","English","Proc. - IEEE Int. Conf. Comput. Vis. Workshops, ICCVW","Conference paper","Final","","Scopus","2-s2.0-85046286598"
"Zhang X.; Zhang H.; Zhang L.; Zhu Y.; Hu F.","Zhang, Xiaochen (54390539800); Zhang, Hui (57211520696); Zhang, Linyue (57211518852); Zhu, Yi (57208055690); Hu, Fei (35366272600)","54390539800; 57211520696; 57211518852; 57208055690; 35366272600","Double-diamond model-based orientation guidance in wearable human-machine navigation systems for blind and visually impaired people","2019","Sensors (Switzerland)","19","21","4670","","","","18","10.3390/s19214670","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85074269517&doi=10.3390%2fs19214670&partnerID=40&md5=22caac18832fb2e2dfe20206872490c5","Department of Industrial Design, Guangdong University of Technology, Guangzhou, 510006, China; School of Communication and Design, Sun Yat-Sen University, Guangzhou, 510275, China; School of Industrial Design, Georgia Institute of Technology, Atlanta, 30332, GA, United States","Zhang X., Department of Industrial Design, Guangdong University of Technology, Guangzhou, 510006, China; Zhang H., Department of Industrial Design, Guangdong University of Technology, Guangzhou, 510006, China; Zhang L., School of Communication and Design, Sun Yat-Sen University, Guangzhou, 510275, China; Zhu Y., Department of Industrial Design, Guangdong University of Technology, Guangzhou, 510006, China, School of Industrial Design, Georgia Institute of Technology, Atlanta, 30332, GA, United States; Hu F., Department of Industrial Design, Guangdong University of Technology, Guangzhou, 510006, China","This paper presents the analysis and design of a new, wearable orientation guidance device in modern travel aid systems for blind and visually impaired people. The four-stage double-diamond design model was applied in the design process to achieve human-centric innovation and to ensure technical feasibility and economic viability. Consequently, a sliding tactile feedback wristband was designed and prototyped. Furthermore, a Bezier curve-based adaptive path planner is proposed to guarantee collision-free planned motion. Proof-of-concept experiments on both virtual and real-world scenarios are conducted. The evaluation results confirmed the efficiency and feasibility of the design and imply the design’s remarkable potential in spatial perception rehabilitation. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Blind and visually impaired people; Double diamond; Navigation aids; Tactile feedback; User-centric design","Blindness; Humans; Man-Machine Systems; Models, Theoretical; Sensory Aids; Visually Impaired Persons; Wearable Electronic Devices; Diamonds; Navigation systems; Blind and visually impaired; Economic viability; Evaluation results; Navigation aids; Real-world scenario; Spatial perception; Tactile feedback; User-centric designs; blindness; electronic device; human; man machine interaction; sensory aid; theoretical model; visually impaired person; Wearable technology","F. Hu; Department of Industrial Design, Guangdong University of Technology, Guangzhou, 510006, China; email: hufei@gdut.edu.cn","","MDPI AG","14248220","","","31661798","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85074269517"
"Zubov D.","Zubov, Dmytro (57195415653)","57195415653","A case study on the spatial cognition of surrounding objects by the b&vi people using sound patterns and ultrasonic sensing","2017","Emerging Trends and Applications of the Internet of Things","","","","105","116","11","1","10.4018/978-1-5225-2437-3.ch004","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85028017386&doi=10.4018%2f978-1-5225-2437-3.ch004&partnerID=40&md5=57929ff502f1251934996a2f404109cd","Universidad Politécnica de San Luis Potosí, Mexico","Zubov D., Universidad Politécnica de San Luis Potosí, Mexico","In this paper, two assistive projects on the spatial cognition by blind and visually impaired (B&VI) people are presented using the sound patterns and ultrasonic sensing. The first device supports the sport activities of B&VI, the golf game specifically. Every golf flagstick has the sound marking device with the active buzzer and WiFi remote control by the person with good vision. The NodeMcu Lua ESP8266 ESP-12 WiFi boards in devices are controlled by the cross-platform HTML web-sites, and hence any WiFi smartphone and/or computer can be in use to start the HTML web-page. Mini portable WiFi router links all devices in the network. End-users are securely connected using the password to wireless router. Ten assistive devices were handed in Instituto para Ciegos y Débiles Visuales ""Ezequiel Hernández Romo"" together with WiFi router. The second device supports the orientation of B&VI by measuring the distance to the obstacle based on the ultrasonic sensor HC-SR04 and Arduino Uno. The distance is pronounced to the B&VI using headphone and MP3 player with SD card. Nowadays, Universidad Politécnica de San Luis Potosí is negotiating with several organizations to create a production line. All devices are of the budget price up to USD 10. All devices were tested successfully. This isjoint work of Instituto para Ciegos y Débiles Visuales ""Ezequiel Hernández Romo"", Universidad Politécnica de San Luis Potosí, and Tecnológico de Monterrey with ongoing project ""Artificial Eyes"" based on Raspberry Pi 3 Model B board with an ultrasonic sensor and camera for the image and/or video processing of the surrounding environment, as well as the friendly integration into the local networks using onboard WiFi and Bluetooth. © 2017, IGI Global. All rights reserved.","","Behavioral research; Body fluids; Budget control; HTML; Remote control; Routers; Sports; Ultrasonic sensors; Video signal processing; Websites; Wireless local area networks (WLAN); Assistive devices; Blind and visually impaired; Spatial cognition; Sport activities; Surrounding environment; Ultrasonic sensing; Video processing; Wireless routers; Wi-Fi","","","IGI Global","","978-152252438-0; 1522524371; 978-152252437-3","","","English","Emerg. Trends and Appl. of the Internet of Things","Book chapter","Final","","Scopus","2-s2.0-85028017386"
"Meshram V.V.; Patil K.; Meshram V.A.; Shu F.C.","Meshram, Vidula V. (57211062704); Patil, Kailas (49862163900); Meshram, Vishal A. (57211062703); Shu, Felix Che (26421430800)","57211062704; 49862163900; 57211062703; 26421430800","An Astute Assistive Device for Mobility and Object Recognition for Visually Impaired People","2019","IEEE Transactions on Human-Machine Systems","49","5","8801898","449","460","11","88","10.1109/THMS.2019.2931745","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85072534228&doi=10.1109%2fTHMS.2019.2931745&partnerID=40&md5=6ea1fabac56a5122f3c973cc1f7616f3","Department of Computer Engineering, Vishwakarma University, Pune, 411048, India; Department of Mathematics and Computer Science, University of Bamenda, Bamenda, Cameroon","Meshram V.V., Department of Computer Engineering, Vishwakarma University, Pune, 411048, India; Patil K., Department of Computer Engineering, Vishwakarma University, Pune, 411048, India; Meshram V.A., Department of Computer Engineering, Vishwakarma University, Pune, 411048, India; Shu F.C., Department of Mathematics and Computer Science, University of Bamenda, Bamenda, Cameroon","To provide autonomous navigation and orientation to visually impaired people, this article proposes a new electronic assistive device called the NavCane. The device helps people find obstacle-free paths in both indoor and outdoor settings. The NavCane also aids in the recognition of objects in an indoor setting. The advantage of the NavCane device is that it provides priority information about obstacles in the path without causing information overload. The priority information deduced by the system is transmitted to the user using tactile and auditory communication methods. Unlike existing electronic travel assistance systems which are limited to obstacle detection and path finding, the NavCane also helps users by recognizing objects in known indoor settings. The developed prototype is low cost and as a low power embedded device for obstacle detection and obstacle identification, it is an alternative to machine vision systems. It has a radio-frequency identification reader, ultrasonic sensors, a global system for mobile communication module, a global positioning system module, vibration motors, a gyroscope, a wet floor sensor, and a battery. To test the usefulness of the NavCane in mundane commuting, object recognition, and rehabilitation for visually impaired people, we assessed it with the help of 80 visually impaired people from a blind school and a home for elderly people. All the assessments were executed in controlled indoor and outdoor test environments with both a NavCane and a white cane. The experimental results show that the NavCane is an effective device for detecting of obstacles, ascending and descending staircases, navigating wet floors, and object recognition in environments that are known and unknown to the user. In addition, our evaluation results indicate that the NavCane improves the performance of obstacle-free navigation compared to a white cane. © 2013 IEEE.","Assistive technology; distance measurement; feedback communications; indoor navigation; man-machine systems; object detection; object recognition; radio-frequency identification (RFID) tags; rehabilitation; tactile sensors; ultrasonic transducers","Computer vision; Distance measurement; Embedded systems; Floors; Global system for mobile communications; Interactive computer systems; Machinery; Man machine systems; Navigation systems; Object detection; Obstacle detectors; Patient rehabilitation; Radio frequency identification (RFID); Radio navigation; Radio waves; Space optics; Ultrasonic applications; Ultrasonic transducers; Visual servoing; Assistive technology; Feedback communications; In-door navigations; Radio-frequency-identification tags (RFID); Tactile sensors; Object recognition","K. Patil; Department of Computer Engineering, Vishwakarma University, Pune, 411048, India; email: kailas.patil@vupune.ac.in","","Institute of Electrical and Electronics Engineers Inc.","21682291","","","","English","IEEE Trans. Human Mach. Syst.","Article","Final","","Scopus","2-s2.0-85072534228"
"Tapu R.; Mocanu B.; Zaharia T.","Tapu, Ruxandra (26424843800); Mocanu, Bogdan (24822846400); Zaharia, Titus (6601999900)","26424843800; 24822846400; 6601999900","A computer vision system that ensure the autonomous navigation of blind people","2013","2013 E-Health and Bioengineering Conference, EHB 2013","","","6707267","","","","38","10.1109/EHB.2013.6707267","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84893799662&doi=10.1109%2fEHB.2013.6707267&partnerID=40&md5=69b44050e4c445e424037930372167ff","ARTEMIS Department, UMR CNRS 8145 MAP5, Télécom SudParis, Evry, France","Tapu R., ARTEMIS Department, UMR CNRS 8145 MAP5, Télécom SudParis, Evry, France; Mocanu B., ARTEMIS Department, UMR CNRS 8145 MAP5, Télécom SudParis, Evry, France; Zaharia T., ARTEMIS Department, UMR CNRS 8145 MAP5, Télécom SudParis, Evry, France","In this paper we introduce a real-time obstacle recognition framework designed to alert the visually impaired people/blind of their presence and to assist humans to navigate safely, in indoor and outdoor environments, by handling a Smartphone device. Static and dynamic objects are detected using interest points selected based on an image grid and tracked using the multiscale Lucas-Kanade algorithm. Next, we activated an object classification methodology. We incorporate HOG (Histogram of Oriented Gradients) descriptor into the BoVW (Bag of Visual Words) retrieval framework and demonstrate how this combination may be used for obstacle classification in video streams. The experimental results performed on various challenging scenes demonstrate that our approach is effective in image sequence with important camera movement, including noise and low resolution data and achieves high accuracy, while being computational efficient. © 2013 IEEE.","Bag of Visual Words (BoVW); Histogram of Oriented Gradients (HOG); object classification","Graphic methods; Handicapped persons; Video streaming; Bag-of-visual-words; Computer vision system; Histogram of oriented gradients; Histogram of oriented gradients (HOG); Lucas-kanade algorithms; Object classification; Obstacle classification; Visually impaired people; Computer vision","","","","","978-147992373-1","","","English","E-Health Bioeng. Conf., EHB","Conference paper","Final","","Scopus","2-s2.0-84893799662"
"Joseph R.F.; Godbole A.A.","Joseph, Richard F. (57202087279); Godbole, Anand A. (56266711400)","57202087279; 56266711400","An intelligent traveling companion for visually impaired pedestrian","2014","2014 International Conference on Circuits, Systems, Communication and Information Technology Applications, CSCITA 2014","","","6839273","283","288","5","4","10.1109/CSCITA.2014.6839273","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84904179320&doi=10.1109%2fCSCITA.2014.6839273&partnerID=40&md5=6af1601db95a7d32c277dc307ffde9c0","Dept. of Computer Engineering, Sardar Patel Institute of Technology, Mumbai, India","Joseph R.F., Dept. of Computer Engineering, Sardar Patel Institute of Technology, Mumbai, India; Godbole A.A., Dept. of Computer Engineering, Sardar Patel Institute of Technology, Mumbai, India","A navigation system that will guide the blind and the visually impaired pedestrian with ease. The system will adapt to the user's behavior and will also provide the user with shortest path from the source to the user's chosen destination in the building. RFID technology is used to track the user's current location. The user carries his own PDA with the application installed in it and a RFID reader with him. RFID tags are deployed in the building. On detection of the RFID tag by the RFID reader as the reader comes in vicinity of the tag, the reader sends the tag information to the PDA using Bluetooth Technology. The PDA based user device provides the user navigation instructions in an audio form and the user can also select the preferences provides by giving input in form of speech. The system will be a blend of Optimal Routing and Users Preference. The PDA consist of the ACO algorithm that will help in the optimization of A* algorithm which will work as the prediction algorithm. © 2014 IEEE.","A; ACO; Adaptive; Algorithm; Evaporation; RFID; Weight Modifier","Artificial intelligence; Behavioral research; Communication; Evaporation; Information technology; Internet of things; Navigation systems; Radio frequency identification (RFID); Vision aids; A; ACO; Adaptive; Bluetooth technology; Prediction algorithms; User navigation; Visually impaired pedestrians; Weight Modifier; Algorithms","","","IEEE Computer Society","","978-147992494-3","","","English","Int. Conf. Circuits, Syst., Commun. Inf. Technol. Appl., CSCITA","Conference paper","Final","","Scopus","2-s2.0-84904179320"
"Asad M.; Ikram W.","Asad, Muhammad (57210681680); Ikram, Waseem (57213273227)","57210681680; 57213273227","Smartphone based guidance system for visually impaired person","2012","2012 3rd International Conference on Image Processing Theory, Tools and Applications, IPTA 2012","","","6469553","442","447","5","9","10.1109/IPTA.2012.6469553","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84875830175&doi=10.1109%2fIPTA.2012.6469553&partnerID=40&md5=95de2e0fa8435d7da666ed1b21d683be","Department of Electrical and Electronics Engineering, University of Sheffield, United Kingdom; Department of Electrical Engineering, National University of Computer and Emerging Sciences, Pakistan","Asad M., Department of Electrical and Electronics Engineering, University of Sheffield, United Kingdom; Ikram W., Department of Electrical Engineering, National University of Computer and Emerging Sciences, Pakistan","In order to facilitate the visually impaired person in navigation, we have developed a prototype guidance system. The main assumption of this guidance system is that there are many straight paths in different real world scenarios. These straight paths have parallel edges, which when captured as an image seem to converge to a single point called the vanishing point. Proper feature extraction and mathematical modelling of the captured frame leads to the detection of these parallel edges. The vanishing point is then calculated and a decision system is formed which notifies the blind person about his/her deviation from a straight path. The scope of this system is limited to a straight path and has been tested in different lighting conditions and with different level of occlusion. A laptop mounted on a 2D robotic platform is used to develop and verify the robustness of the algorithm. Finally, a smartphone based real-time application has been implemented for this visual guidance system, in which the decision system returns an audio output to guide the visually impaired person. This application has an average execution rate of 20 frames per second, with each frame being of 320 by 240 pixel size. The system has an accuracy of 84.1% in a scenario with pedestrians and other objects, while without pedestrians it produces an accuracy of over 90%. © 2012 IEEE.","Guidance System; Hough Transform; Navigation; Real-time System; Vanishing Point; Visually Impaired","Feature extraction; Handicapped persons; Hough transforms; Image processing; Laptop computers; Navigation; Real time systems; Signal encoding; Smartphones; Vision aids; Guidance system; Lighting conditions; Prototype guidance systems; Real-time application; Real-world scenario; Vanishing point; Visually impaired; Visually impaired persons; Remote control","M. Asad; Department of Electrical and Electronics Engineering, University of Sheffield, United Kingdom; email: masad.801@gmail.com","","","","978-146732583-7","","","English","Int. Conf. Image Process. Theory, Tools Appl., IPTA","Conference paper","Final","","Scopus","2-s2.0-84875830175"
"Chaccour K.; Badr G.","Chaccour, Kabalan (56979407700); Badr, Georges (35101305600)","56979407700; 35101305600","Computer vision guidance system for indoor navigation of visually impaired people","2016","2016 IEEE 8th International Conference on Intelligent Systems, IS 2016 - Proceedings","","","7737460","449","454","5","43","10.1109/IS.2016.7737460","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85005996196&doi=10.1109%2fIS.2016.7737460&partnerID=40&md5=0b8792f2a9984d02218fbfe47ef18149","TICKET Lab, Antonine University, Hadat-Baabda, 40016, Lebanon","Chaccour K., TICKET Lab, Antonine University, Hadat-Baabda, 40016, Lebanon; Badr G., TICKET Lab, Antonine University, Hadat-Baabda, 40016, Lebanon","Visually Impaired (VI) and blind people suffer from reduced mobility, as they cannot detect the terrain and their environment. They always need assistance and walking support systems in their daily life. Solutions have been proposed many decades ago and are rapidly improving nowadays due to the technology evolution and integration. A large number of assistance aids have been deployed in real life situations whereas other concepts remained as research ideas. This paper describes a new approach of an ambient navigation system that would help the visually impaired or blind person to move freely indoor (house, office, etc.) without the assistance of anyone. The system is composed of IP cameras attached to the ceiling of each room and the smart phone of the subject is used as human machine interface (HMI). Frames are sent to a computer that analyzes the environment, detects and recognizes objects. A computer vision guidance algorithm is designed to help the user reach his destination (or his personal item) with obstacle detection. The system is commanded by voice messages via a simple mobile application. Feedbacks (alerts, route) are voice messages returns by the application to the user. This system provides a reliable solution to assist those users in their indoor navigation providing them a correct route with obstacle avoidance. © 2016 IEEE.","computer vision; image processing; indoor navigation; mobile application; Visually impaired","Air navigation; Computer vision; Image processing; Intelligent systems; Mobile computing; Mobile telecommunication systems; Navigation systems; Obstacle detectors; Smartphones; Vision aids; Voice/data communication systems; Walking aids; Human Machine Interface; In-door navigations; Mobile applications; Technology evolution; Vision guidance systems; Visually impaired; Visually impaired people; Walking support systems; Indoor positioning systems","","Sgurev V.; Yager R.; Hadjiski M.; Jotsov V.","Institute of Electrical and Electronics Engineers Inc.","","978-150901353-1","","","English","IEEE Int. Conf. Intell. Syst., IS - Proc.","Conference paper","Final","","Scopus","2-s2.0-85005996196"
"Alhmiedat T.; Taleb A.A.; Samaraz G.","Alhmiedat, Tareq (35955680500); Taleb, Anas Abu (24831286900); Samaraz, Ghassan (36549085000)","35955680500; 24831286900; 36549085000","A prototype navigation system for guiding blind people indoors using NXT mindstorms","2013","International Journal of Online Engineering","9","5","","52","58","6","21","10.3991/ijoe.v9i5.2848","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84885130524&doi=10.3991%2fijoe.v9i5.2848&partnerID=40&md5=ff3f4c09852a9eccaa53e44f05c029cf","Department of Computer Science, Tabuk University, Saudi Arabia; Department of Computer Science, Isra University, Amman, Jordan; Department of Computer Science, Zarqa University, Zarqa, Jordan","Alhmiedat T., Department of Computer Science, Tabuk University, Saudi Arabia; Taleb A.A., Department of Computer Science, Isra University, Amman, Jordan; Samaraz G., Department of Computer Science, Zarqa University, Zarqa, Jordan","People with visual impairment face enormous difficulties in terms of their mobility as they do not have enough information about their location and orientation with respect to traffic and obstacles on their route. Visually impaired people can navigate unknown areas by relying on the assistance of canes, other people, or specially trained guide dogs. The traditional ways of using guide dogs and long-cane only help to avoid obstacles, not to know where they are. The research presented in this paper introduces a mobile assistant navigation prototype to locate and direct blind people indoors. Since most of the existing navigation systems developed so far for blind people employ a complex conjunction of positioning systems, video cameras, location-based and image processing algorithms, we designed an affordable low-cost prototype navigation system for orienting and tracking the position of blind people in complex environments. The prototype system is based on the inertial navigation system and experiments have been performed on NXT Mindstorms platform.","Localization; Navigation; NXT; Robotics","Image processing; Navigation; Navigation systems; Robotics; Robots; Complex environments; Image processing algorithm; Localization; NXT; Positioning system; Prototype system; Visual impairment; Visually impaired people; Handicapped persons","T. Alhmiedat; Department of Computer Science, Tabuk University, Saudi Arabia; email: t.alhmiedat@ut.edu.sa","","","18612121","","","","English","Int. J. Online Eng.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-84885130524"
"Wang S.; Tian Y.","Wang, Shuihua (36544650700); Tian, Yingli (16556710700)","36544650700; 16556710700","Detecting stairs and pedestrian crosswalks for the blind by RGBD camera","2012","Proceedings - 2012 IEEE International Conference on Bioinformatics and Biomedicine Workshops, BIBMW 2012","","","6470227","732","739","7","57","10.1109/BIBMW.2012.6470227","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84875579660&doi=10.1109%2fBIBMW.2012.6470227&partnerID=40&md5=39e3e63ca2855b0768c75494184950ed","Department of Electrical Engineering, City College of New York, NY, 10031, United States","Wang S., Department of Electrical Engineering, City College of New York, NY, 10031, United States; Tian Y., Department of Electrical Engineering, City College of New York, NY, 10031, United States","A computer vision-based wayfinding and navigation aid can improve the mobility of blind and visually impaired people to travel independently. In this paper, we develop a new framework to detect and recognize stairs and pedestrian crosswalks using a RGBD camera. Since both stairs and pedestrian crosswalks are featured by a group of parallel lines, we first apply Hough transform to extract the concurrent parallel lines based on the RGB channels. Then, the Depth channel is employed to further recognize pedestrian crosswalks, upstairs, and downstairs using support vector machine (SVM) classifiers. Furthermore, we estimate the distance between the camera and stairs for the blind users. The detection and recognition results on our collected dataset demonstrate that the effectiveness and efficiency of our proposed framework. © 2012 IEEE.","","Bioinformatics; Cameras; Hough transforms; Human rehabilitation equipment; Stairs; Support vector machines; Blind and visually impaired; Blind users; Effectiveness and efficiencies; Parallel line; Rgb-d cameras; Vision based; Wayfinding and navigations; Crosswalks","S. Wang; Department of Electrical Engineering, City College of New York, NY, 10031, United States; email: swang15@ccny.cuny.edu","","","","978-146732746-6","","","English","Proc. - IEEE Int. Conf. Bioinformatics Biomed. Workshops, BIBMW","Conference paper","Final","","Scopus","2-s2.0-84875579660"
"Klaus H.; Marano D.; Neuschmid J.; Schrenk M.; Wasserburger W.","Klaus, Höckner (57195971977); Marano, Daniele (57214297615); Neuschmid, Julia (42962021600); Schrenk, Manfred (42962327200); Wasserburger, Wolfgang (55308061600)","57195971977; 57214297615; 42962021600; 42962327200; 55308061600","AccessibleMap: Web-based city maps for blind and visually impaired","2012","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","7383 LNCS","PART 2","","536","543","7","5","10.1007/978-3-642-31534-3_79","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84864815524&doi=10.1007%2f978-3-642-31534-3_79&partnerID=40&md5=9236e29e23fc699bcb0011dba3321d66","Hilfsgemeinschaft der Blinden und Sehschwachen Österreichs, 1200 Wien, Jägerstrasse 36, Austria; CEIT ALANOVA - Institute of Urbanism, Transport, Environment and Information, Austria","Klaus H., Hilfsgemeinschaft der Blinden und Sehschwachen Österreichs, 1200 Wien, Jägerstrasse 36, Austria; Marano D., Hilfsgemeinschaft der Blinden und Sehschwachen Österreichs, 1200 Wien, Jägerstrasse 36, Austria; Neuschmid J., CEIT ALANOVA - Institute of Urbanism, Transport, Environment and Information, Austria; Schrenk M., CEIT ALANOVA - Institute of Urbanism, Transport, Environment and Information, Austria; Wasserburger W., CEIT ALANOVA - Institute of Urbanism, Transport, Environment and Information, Austria","Today cities can be discovered easily with the help of web-based maps. They assist to discover streets, squares and districts by supporting orientation, mobility and feeling of safety. Nevertheless do online maps still belong to those elements of the web which are hardly or even not accessible for partially sighted people. Therefore the main objective of the AccessibleMap project is to develop methods to design web-based city maps in a way that they can be better used by people affected with limited sight or blindness in several application areas of daily life. © 2012 Springer-Verlag.","Accessible Maps; Semantic description of maps; Styled Layer Description; Web Map Services","Artificial intelligence; Application area; City map; Daily lives; Online maps; Semantic descriptions; Styled Layer Description; Visually impaired; Web map services; Websites","H. Klaus; Hilfsgemeinschaft der Blinden und Sehschwachen Österreichs, 1200 Wien, Jägerstrasse 36, Austria; email: hoeckner@hilfsgemeinschaft.at","","","16113349","978-364231533-6","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","","Scopus","2-s2.0-84864815524"
"Fazli S.; Dehnavi H.M.; Moallem P.","Fazli, Saeid (9235800200); Dehnavi, Hajar Mohammadi (35758461600); Moallem, Payman (8504588700)","9235800200; 35758461600; 8504588700","A robust negative obstacle detection method using seed-growing and dynamic programming for visually-impaired/blind persons","2011","Optical Review","18","6","","415","422","7","8","10.1007/s10043-011-0079-y","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-82055207037&doi=10.1007%2fs10043-011-0079-y&partnerID=40&md5=7fad03fd410a1deab34f5dac539e3fc5","Electrical Department, Engineering Faculty, University of Isfahan, Isfahan 45156-14669, Iran; Electrical Department, Engineering Faculty, Zanjan University, Zanjan 45371-38111, Iran","Fazli S., Electrical Department, Engineering Faculty, Zanjan University, Zanjan 45371-38111, Iran; Dehnavi H.M., Electrical Department, Engineering Faculty, Zanjan University, Zanjan 45371-38111, Iran; Moallem P., Electrical Department, Engineering Faculty, University of Isfahan, Isfahan 45156-14669, Iran","Though a significant amount of work has been done on detecting obstacles, not much attention has been given to the detection of drop offs, e. g., sidewalk curbs, downward stairs, and other hazards. In this paper, we propose algorithms for detecting negative obstacles in an urban setting using stereo vision and two-stage dynamic programming (TSDP) technique. We are developing computer vision algorithms for sensing important terrain features as an aid to blind navigation, which interpret visual information obtained from images collected by cameras mounted on camera legs nearly as high as young person. This paper focuses specifically on a novel computer vision algorithm for detecting negative obstacles (i. e. anything below the level of the ground, such as holes and drop-offs), which are important and ubiquitous features on and near sidewalks and other walkways. The proposed algorithm is compared to other algorithms such as belief propagation and random growing correspondence seeds (GCS). According to the results, the proposed method achieves higher speed, more accurate disparity map and lower RMS errors. The speed of the proposed algorithm is about 28% higher than the random GCS algorithm. We demonstrate experimental results on typical sidewalk scenes to show the effectiveness of the proposed method. © 2011 The Japan Society of Applied Physics.","blind navigation; negative obstacle; obstacle detection; stereo matching; visually impaired","","S. Fazli; Electrical Department, Engineering Faculty, Zanjan University, Zanjan 45371-38111, Iran; email: fazli@znu.ac.ir","","","13499432","","","","English","Opt. Rev.","Article","Final","","Scopus","2-s2.0-82055207037"
"Rao A.S.; Gubbi J.; Palaniswami M.; Wong E.","Rao, Aravinda S. (35796100000); Gubbi, Jayavardhana (23090806600); Palaniswami, Marimuthu (7005048570); Wong, Elaine (56397542600)","35796100000; 23090806600; 7005048570; 56397542600","A vision-based system to detect potholes and uneven surfaces for assisting blind people","2016","2016 IEEE International Conference on Communications, ICC 2016","","","7510832","","","","26","10.1109/ICC.2016.7510832","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84981313210&doi=10.1109%2fICC.2016.7510832&partnerID=40&md5=14e70648507a9326ed968a1d5c4b8a94","Department of Electrical and Electronic Engineering, University of Melbourne, Parkville Campus, 3010, VIC, Australia; Tata Innovation Labs, Tata Consultancy Services, Bengaluru, Karnataka, 560066, India","Rao A.S., Department of Electrical and Electronic Engineering, University of Melbourne, Parkville Campus, 3010, VIC, Australia; Gubbi J., Tata Innovation Labs, Tata Consultancy Services, Bengaluru, Karnataka, 560066, India; Palaniswami M., Department of Electrical and Electronic Engineering, University of Melbourne, Parkville Campus, 3010, VIC, Australia; Wong E., Department of Electrical and Electronic Engineering, University of Melbourne, Parkville Campus, 3010, VIC, Australia","Vision is one of the most advanced and important sensory input in humans. However, many people have vision problems due to birth defects, uncorrected errors, work nature, accidents, and aging. The white cane and guide dog are the most widely used means of navigation for the vision-impaired. With advancements in technology, electronic devices have been created using different sensors and technologies to help navigate the blind. Electronic Travel AIDS (ETAs) assist in navigating a person by collecting information about the environment and relaying this information in a form that allows a blind or vision-impaired person to understand the nature of the environment. However, there is still a lack of devices to detect potholes and uneven pavements, which inhibits mobility after dark. This pilot study proposes a computer vision based pothole and uneven surface detection approach to assist blind people in meeting their mobility needs. The system includes projecting laser patterns, recording the patterns through a monocular video, analyzing the patterns to extract features and then providing path cues for the blind user. With over 90% accuracy in detecting potholes, the proposed system aims to assist blind people in real-time navigation. © 2016 IEEE.","","Computer vision; Landforms; Electronic device; Electronic travel aidss; Monocular video; Real-time navigation; Uneven surfaces; Vision based system; Vision impaired; Vision problems; Vision aids","","","Institute of Electrical and Electronics Engineers Inc.","","978-147996664-6","","","English","IEEE Int. Conf. Commun., ICC","Conference paper","Final","","Scopus","2-s2.0-84981313210"
"Bhatlawande S.; Mahadevappa M.; Mukherjee J.; Biswas M.; Das D.; Gupta S.","Bhatlawande, Shripad (55212307900); Mahadevappa, Manjunatha (6604071152); Mukherjee, Jayanta (58722066100); Biswas, Mukul (57197998279); Das, Debabrata (56994099900); Gupta, Somedeb (57213337983)","55212307900; 6604071152; 58722066100; 57197998279; 56994099900; 57213337983","Design, development, and clinical evaluation of the electronic mobility cane for vision rehabilitation","2014","IEEE Transactions on Neural Systems and Rehabilitation Engineering","22","6","6818393","1148","1159","11","82","10.1109/TNSRE.2014.2324974","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84912136417&doi=10.1109%2fTNSRE.2014.2324974&partnerID=40&md5=15393924d9ddfd3f59129ffe42e6ce80","School of Medical Science and Technology, Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, 721302, India; Department of Ophthalmology, Midnapore Medical College and Hospital, Medinipur, 721101, India","Bhatlawande S., School of Medical Science and Technology, Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, 721302, India; Mahadevappa M., School of Medical Science and Technology, Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, 721302, India; Mukherjee J., School of Medical Science and Technology, Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, 721302, India; Biswas M., Department of Ophthalmology, Midnapore Medical College and Hospital, Medinipur, 721101, India; Das D., Department of Ophthalmology, Midnapore Medical College and Hospital, Medinipur, 721101, India; Gupta S., Department of Ophthalmology, Midnapore Medical College and Hospital, Medinipur, 721101, India","This paper proposes a new electronic mobility cane (EMC) for providing obstacle detection and way-finding assistance to the visually impaired people. The main feature of this cane is that it constructs the logical map of the surrounding environment to deduce the priority information. It provides a simplified representation of the surrounding environment without causing any information overload. It conveys this priority information to the subject by using intuitive vibration, audio or voice feedback. The other novel features of the EMC are staircase detection and nonformal distance scaling scheme. It also provides information about the floor status. It consists of a low power embedded system with ultrasonic sensors and safety indicators. The EMC was subjected to series of clinical evaluations in order to verify its design and to assess its ability to assist the subjects in their daily-life mobility. Clinical evaluations were performed with 16 totally blind and four low vision subjects. All subjects walked controlled and the real-world test environments with the EMC and the traditional white cane. The evaluation results and significant scores of subjective measurements have shown the usefulness of the EMC in vision rehabilitation services. © 2014 IEEE.","Electronic travel aid; ultrasonic obstacle detector; vision rehabilitation; visually impaired","Adult; Blindness; Canes; Electronics, Medical; Equipment Design; Equipment Failure Analysis; Humans; Man-Machine Systems; Middle Aged; Orientation; Pattern Recognition, Automated; Self-Help Devices; Sensory Aids; Ultrasonography; Wireless Technology; Obstacle detectors; Ultrasonic applications; Electronic travel aidss; Low power embedded systems; Subjective measurements; Surrounding environment; Ultrasonic obstacle detector; Vision rehabilitation; Visually impaired; Visually impaired people; adult; automated pattern recognition; blindness; cane; device failure analysis; devices; echography; electronics; equipment design; human; man machine interaction; middle aged; orientation; pathophysiology; procedures; self help device; sensory aid; wireless communication; Electromagnetic compatibility","","","Institute of Electrical and Electronics Engineers Inc.","15344320","","ITNSB","24860035","English","IEEE Trans. Neural Syst. Rehabil. Eng.","Article","Final","","Scopus","2-s2.0-84912136417"
"Zhu Y.; Mei S.L.; Le J.","Zhu, Yu (56335930900); Mei, Sun Liang (7102846294); Le, Jian (56336287200)","56335930900; 7102846294; 56336287200","Co-processing strategies for feature detecting in smart video sensor","2014","Applied Mechanics and Materials","610","","","320","324","4","0","10.4028/www.scientific.net/AMM.610.320","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84906504656&doi=10.4028%2fwww.scientific.net%2fAMM.610.320&partnerID=40&md5=b8e19fe26e50f4fa89b2be29dfe969b4","Tsinghua University, Beijing, 100084, Rohm Building, China","Zhu Y., Tsinghua University, Beijing, 100084, Rohm Building, China; Mei S.L., Tsinghua University, Beijing, 100084, Rohm Building, China; Le J., Tsinghua University, Beijing, 100084, Rohm Building, China","To assist man who is blind or visually impaired on the navigation, a vision system depending upon the DM3730 as the core is developed, which can provide accurate information regarding the environment surrounding the patients. As one part of the project, the feature detection module must be suitable for an embedded implementation, hardware parallelisms and algorithmic modifications need. The Harris detection is chosen as the main algorithm of this module. The experimental results show that the precision and speed of feature extraction are both better than of the PC platform. © (2014) Trans Tech Publications, Switzerland.","DM3730; Feature point detecting; Harris algorithm; SBC8140","Computer vision; Feature extraction; Intelligent systems; DM3730; Embedded implementation; Feature detecting; Feature point detecting; Hardware parallelisms; Harris algorithm; SBC8140; Visually impaired; Video signal processing","Y. Zhu; Tsinghua University, Beijing, 100084, Rohm Building, China; email: zhuiyulianzhu@qq.com","","Trans Tech Publications Ltd","16609336","978-303835175-7","","","English","Appl. Mech. Mater.","Conference paper","Final","","Scopus","2-s2.0-84906504656"
"Plaza Torres M.; Aperador W.; Cifuentes Bernal A.","Plaza Torres, Mauricio (55578707200); Aperador, William (24398551800); Cifuentes Bernal, Andres (57223999911)","55578707200; 24398551800; 57223999911","Thermal vision system as a walking aid for blind and mobility-impaired persons; [Sistema de visión térmica para desplazamiento en personas invidentes y con problemas de movilidad]","2016","Revista Cubana de Investigaciones Biomedicas","35","1","","102","111","9","1","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84966477982&partnerID=40&md5=a336ce66b36f59b320a42df3e68908b3","Universidad Militar Nueva Granada, Bogotá, Colombia","Plaza Torres M., Universidad Militar Nueva Granada, Bogotá, Colombia; Aperador W., Universidad Militar Nueva Granada, Bogotá, Colombia; Cifuentes Bernal A., Universidad Militar Nueva Granada, Bogotá, Colombia","The project describes the invention patent (Resolution 78032) of a thermal system informing a blind person of the obstacles lying in their way. The prototype was designed as an aid for the mobility of blind persons based on a wheel chair which may be scaled onto by people with impaired mobility due to an injured spine. The camera on the peripheral captures the environment and indicates the direction to be taken, avoiding the obstacles by means of a thermal panel conveying information to a hand-held device. Aids for vision and mobility impaired persons provide users with support to improve their quality of life, as well as some degree of independence in controlled environments. The development of devices comprising a wide range of disabilities has not been thoroughly explored. The project designed a device allowing a visually impaired person to control a wheel chair and interact with a controlled environment. Additionally, the design solved the mobility and autonomy problems of persons with limited lower limb mobility. © 2016, Editorial Ciencias Medicas. All rights reserved.","Artificial vision; Autonomy; Domotic control; Mobility impairment; Thermal panel","","M. Plaza Torres; Universidad Militar Nueva Granada, Bogotá, Colombia; email: mauricio.plaza@unimilitar.edu.co","","Editorial Ciencias Medicas","08640300","","RCIBE","","Spanish","Rev. Cuba. Invest. Biomed.","Article","Final","","Scopus","2-s2.0-84966477982"
"Wever J.W.M.; Gosselin C.; Herder J.L.","Wever, Jacobus W.M. (56084639600); Gosselin, Clement (57210720538); Herder, Just L. (7004307869)","56084639600; 57210720538; 7004307869","On the design of a portable force illusion device for navigation aids","2013","Proceedings of the ASME Design Engineering Technical Conference","6 A","","V06AT07A004","","","","3","10.1115/DETC2013-12374","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84897000976&doi=10.1115%2fDETC2013-12374&partnerID=40&md5=14e8e5d4c880627df98f427543f92101","Department of Mechanical Engineering, Delft University of Technology, 2628 CD Delft, Mekelweg 2, Netherlands; Department of Mechanical Engineering, Université Laval, Québec, QC G1V0A6, 1065 avenue de la médecine, Canada","Wever J.W.M., Department of Mechanical Engineering, Delft University of Technology, 2628 CD Delft, Mekelweg 2, Netherlands; Gosselin C., Department of Mechanical Engineering, Université Laval, Québec, QC G1V0A6, 1065 avenue de la médecine, Canada; Herder J.L., Department of Mechanical Engineering, Delft University of Technology, 2628 CD Delft, Mekelweg 2, Netherlands","Navigation aids rely mostly on (audio)visual cues when it comes to communication with the user. An alternative and more intuitive communication modality may be provided by means of haptic guidance generated by a portable mechatronic device. Especially visually impaired and blind people may benefit from a device that generates the illusion of an external force; it may possibly eliminate the need for a guide dog. This paper investigates constant-velocity crank-driven mechanisms which are able to generate such a force illusion by means of a reciprocating mass. The focus of this paper is on the generation of the illusion itself rather than manipulating the direction of this force. The force illusion is a result of successive positive and negative reaction forces with unequal amplitude, generated by a reciprocating mass. The acceleration ratio of the mass is selected as the main evaluation criterion for comparing different types of candidate mechanisms. Because the input is a simple motor rotating at a constant velocity, the synthesis of the mechanism is key to generating proper acceleration profiles. A brute-force approach is used for the synthesis procedure, i.e., characteristic distances and link lengths are varied with steps of 1mm for each of the candidate mechanisms, thereby generating very large numbers of variants. Kinematic performance reveal typical acceleration ratios in the range of 1 to 19; where a ratio of one does not result in a force illusion while a ratio of 19 might be demanding on the physical design. An objective evaluation leads to selecting the Square Recti-Linear mechanism as the overall most promising candidate mechanism. A prototype of this mechanism is then presented to demonstrate the working principle. The shape of the prototype's force profile over time is measured experimentally and is shown to be very similar to the profile obtained by simulation. The reciprocating mass accounts for almost one fifth of the total mass of the prototype, resulting in a strong force illusion in comparison with gravitational forces. Copyright © 2013 by ASME.","","Communication; Design; Gravitation; Handicapped persons; Synthesis (chemical); Acceleration profiles; Brute-force approach; Candidate mechanisms; Characteristic distance; Communication modalities; Gravitational forces; Kinematic performance; Objective evaluation; Robots","","","American Society of Mechanical Engineers","","978-079185593-5","","","English","Proc. ASME Des. Eng. Tech. Conf.","Conference paper","Final","","Scopus","2-s2.0-84897000976"
"Hairuman I.F.Bt.; Foong O.-M.","Hairuman, Intan Fariza Bt. (54960900500); Foong, Oi-Mean (35848525300)","54960900500; 35848525300","OCR signage recognition with skew & slant correction for visually impaired people","2011","Proceedings of the 2011 11th International Conference on Hybrid Intelligent Systems, HIS 2011","","","6122123","306","310","4","12","10.1109/HIS.2011.6122123","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84856723727&doi=10.1109%2fHIS.2011.6122123&partnerID=40&md5=5c7199da3b192d8c7f254a61980a192e","Computer and Information Sciences Department, Universiti Teknologi PETRONAS, Tronoh 31750, Perak, Malaysia","Hairuman I.F.Bt., Computer and Information Sciences Department, Universiti Teknologi PETRONAS, Tronoh 31750, Perak, Malaysia; Foong O.-M., Computer and Information Sciences Department, Universiti Teknologi PETRONAS, Tronoh 31750, Perak, Malaysia","It is a challenge for visually impaired people (VIPs) to navigate independently whenever they attempt to find their way in unfamiliar buildings searching for amenities (i.e. exits, ladies/gents toilets) even with a walking stick or a guide dog. Camera-based computer vision systems have the potential to assist VIPs in independent navigation or way finding in unfamiliar places. To leverage on previous research of Signage Recognition Framework which could only recognize public signage with slanted angle less than30°, an improved OCR signage recognition model with skew and slant correction in public signage is presented. The proposed OCR method consists of Canny edge detection algorithm, Hough Transformation and Shearing Transformation were used to detect and correct skewed and slanted images. The proposed model would capture a public signage image, compare the image in the database using template matching algorithm and convert to machine readable text in a text file. The text will then be processed by Microsoft Speech Application Program Interface (SAPI) speech synthesizer and translated to voice as output. Experiments were conducted on 5 blind folded subjects to test the performance of the model. The proposed OCR recognition model has achieved satisfactory recognition rate of 82.7%. © 2011 IEEE.","Hough Transformation; Optical Character Recognition; Shearing Transformation; slant correction; visually impaired","Edge detection; Ferry boats; Intelligent systems; Optical character recognition; Shearing; Template matching; Canny edge detection; Computer vision system; Hough Transformation; MicroSoft; Recognition models; Recognition rates; slant correction; Speech applications; Speech synthesizer; Template-matching algorithms; Text file; Visually impaired; Visually impaired people; Way finding; Algorithms","I.F.Bt. Hairuman; Computer and Information Sciences Department, Universiti Teknologi PETRONAS, Tronoh 31750, Perak, Malaysia; email: intanfariza@gmail.com","","","","978-145772150-2","","","English","Proc. Int. Conf. Hybrid Intelligent Syst., HIS","Conference paper","Final","","Scopus","2-s2.0-84856723727"
"Kulkarni A.; Wang A.; Urbina L.; Steinfeld A.; Dias B.","Kulkarni, Aditi (57189035997); Wang, Allan (57204114397); Urbina, Lynn (57189046722); Steinfeld, Aaron (7103257672); Dias, Bernardine (57189040447)","57189035997; 57204114397; 57189046722; 7103257672; 57189040447","Robotic assistance in indoor navigation for people who are blind","2016","ACM/IEEE International Conference on Human-Robot Interaction","2016-April","","7451806","461","462","1","27","10.1109/HRI.2016.7451806","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84964915787&doi=10.1109%2fHRI.2016.7451806&partnerID=40&md5=605ae144472c60cf81847ba0fc69e394","Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States; Swanson School of Engineering, University of Pittsburgh, Pittsburgh, PA, United States","Kulkarni A., Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States; Wang A., Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States; Urbina L., Swanson School of Engineering, University of Pittsburgh, Pittsburgh, PA, United States; Steinfeld A., Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States; Dias B., Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States","In this paper, we describe the process of making a robot useful as a guide robot for people who are blind or visually impaired. For this group, the interactive audio feature of a robot assumes a very high level of importance. We have introduced some features that will help to make the robot sound natural and be more comfortable. We first addressed the question of the speaker placement to help the user determine the size and distance of the robot. After the initial meeting, user data will be retained by the robot so that their communication evolves with every interaction. The robot will also ask the users if they need to take a rest after a specified interval depending upon the user's age and the distance they need to cover. The next time they visit, all this information will be used to make the interaction more natural and customized for each individual user. © 2016 IEEE.","Audio interaction; Blind; Low vision; Mobile robot; Navigational assistance","Computer vision; Indoor positioning systems; Man machine systems; Mobile robots; Robots; Audio interaction; Blind; Guide robots; In-door navigations; Interactive audio; Low vision; Navigational assistance; Visually impaired; Human robot interaction","","","IEEE Computer Society","21672148","978-146738370-7","","","English","ACM/IEEE Int. Conf. Hum.-Rob. Interact.","Conference paper","Final","","Scopus","2-s2.0-84964915787"
"Pan H.; Yi C.; Tian Y.","Pan, Hangrong (55938230100); Yi, Chucai (36192382300); Tian, Yingli (16556710700)","55938230100; 36192382300; 16556710700","A primary travelling assistant system of bus detection and recognition for visually impaired people","2013","Electronic Proceedings of the 2013 IEEE International Conference on Multimedia and Expo Workshops, ICMEW 2013","","","6618346","","","","32","10.1109/ICMEW.2013.6618346","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84888259001&doi=10.1109%2fICMEW.2013.6618346&partnerID=40&md5=e2cf33cd1d63fae07e752d258afb5668","Dept. of Electrical Engineering, City College of New York, United States; Dept. of Computer Science, Graduate Center, City University of New York, United States","Pan H., Dept. of Electrical Engineering, City College of New York, United States; Yi C., Dept. of Computer Science, Graduate Center, City University of New York, United States; Tian Y., Dept. of Electrical Engineering, City College of New York, United States, Dept. of Computer Science, Graduate Center, City University of New York, United States","It is a challenging task for many blind and visually impaired passengers travelling by bus. To assist visually impaired passengers to travel more independently, we design a computer vision-based system to detect and recognize bus information from images captured by a camera at a bus stop. Our system is able to notify the visually impaired people in speech the information of the coming bus, and detect the route number and other related information which is depicted in the form of text. In bus detection, histogram of the oriented gradient (HOG) descriptor is employed to extract the image-based features of bus facade. Cascade SVM model is applied to train a bus classifier to detect the existence of bus facade in sliding windows. In bus route number recognition, we design a text detection algorithm on the basis of layout analysis and text feature learning, and then recognize the text codes from detected text regions for audio notification. This algorithm is able to compute the image regions containing text information. Experimental results demonstrate the effectiveness of our proposed algorithm for bus detection and route number recognition. © 2013 IEEE.","cascade classifiers; linear SVM; sliding windows; text detection and recognition; travelling assistance","Algorithms; Bus transportation; Buses; Classification (of information); Exhibitions; Cascade classifiers; Linear SVM; Sliding Window; Text detection; travelling assistance; Character recognition","","","","","978-147991604-7","","","English","Electron. Proc. IEEE Int. Conf. Multimedia Expo Workshops, ICMEW","Conference paper","Final","","Scopus","2-s2.0-84888259001"
"Nguyen Q.-H.; Vu H.; Tran T.-H.; Nguyen Q.-H.","Nguyen, Quoc-Hung (57212637876); Vu, Hai (56104446200); Tran, Thanh-Hai (56024813000); Nguyen, Quang-Hoan (56419354100)","57212637876; 56104446200; 56024813000; 56419354100","A vision-based system supports mapping services for visually impaired people in indoor environments","2014","2014 13th International Conference on Control Automation Robotics and Vision, ICARCV 2014","","","7064541","1518","1523","5","4","10.1109/ICARCV.2014.7064541","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84988268416&doi=10.1109%2fICARCV.2014.7064541&partnerID=40&md5=bd9b5b5d3b6d2ff29e61362552507cba","International Research Institute MICA, Hanoi University of Science and Technology, Viet Nam; Faculty of Information Technology, Hung Yen University of Technology and Education, Viet Nam","Nguyen Q.-H., International Research Institute MICA, Hanoi University of Science and Technology, Viet Nam; Vu H., International Research Institute MICA, Hanoi University of Science and Technology, Viet Nam; Tran T.-H., International Research Institute MICA, Hanoi University of Science and Technology, Viet Nam; Nguyen Q.-H., Faculty of Information Technology, Hung Yen University of Technology and Education, Viet Nam","This paper describes and extensively evaluates a visual-based system that autonomously operators for both building a map and localization tasks. The proposed system is to assist mapping services to the visually impaired/blind people in small or mid-scale environments such as inside a building or campus of school, hospital. Toward this end, the proposed approaches solely rely on visual data thanks to a self-designed image acquisition system. On one hand, a robust visual odometry method is utilized to create a map of the environments. On the other hand, the proposed approaches utilize FAB-MAP algorithm that is maybe the most successful for learning places in the environments. Map building and learning places in an environment are processed in an off-line phase. Through a matching place procedure, online captured images are continuously positioned on the map. Furthermore, we utilize a Kaiman Filter that combines the matching results of current observation and the estimation of robot states based on its kinematic model. We evaluate performances of the proposed system through experimental schemes. The results show that the constructed map coincides with ground truth, and matching image-to-map is high confidence. The evaluations also contain scenarios which the blind pupils move following Robot. The experimental results confirmed that proposed system feasibly navigating blind pupils in indoor environments. © 2014 IEEE.","FAB-MAP algorithms; Navigations; Place Recognition; Visual Odometry","Computer vision; Image acquisition; Kinematics; Mapping; Navigation; Robotics; Vision; Vision aids; Experimental scheme; Image acquisition systems; Indoor environment; MAP algorithms; Place recognition; Vision based system; Visual odometry; Visually impaired people; Image matching","","","Institute of Electrical and Electronics Engineers Inc.","","978-147995199-4","","","English","Int. Conf. Control Autom. Rob. Vis., ICARCV","Conference paper","Final","","Scopus","2-s2.0-84988268416"
"Hempel R.; Stahl C.; Stockinger B.; Kemeth F.; Vaupel T.","Hempel, René (57252267600); Stahl, Christoph (35811385200); Stockinger, Birgit (57252516800); Kemeth, Ferdinand (36497164400); Vaupel, Thorsten (36706906900)","57252267600; 35811385200; 57252516800; 36497164400; 36706906900","Outdoor mobility assistance-technologies helping on the way","2016","Active and Assisted Living","","","","241","259","18","1","10.1049/PBHE006E_ch13","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85064961842&doi=10.1049%2fPBHE006E_ch13&partnerID=40&md5=8f0b3fbcfe43dd8dcf66cbdabd7c8604","Institut f. Automation u. Kommunikation e.V., Werner-Heisenberg-Str. 1, Magdeburg, 39106, Germany; Fraunhofer Institute for Integrated Circuits IIS, Nordostpark 84, Nuremberg, 90411, Germany; German Research Center for Artificial Intelligence (DFKI), Enrique-Schmidt-Str. 5, Bremen, 28359, Germany","Hempel R., Institut f. Automation u. Kommunikation e.V., Werner-Heisenberg-Str. 1, Magdeburg, 39106, Germany; Stahl C., German Research Center for Artificial Intelligence (DFKI), Enrique-Schmidt-Str. 5, Bremen, 28359, Germany; Stockinger B., Fraunhofer Institute for Integrated Circuits IIS, Nordostpark 84, Nuremberg, 90411, Germany; Kemeth F., Fraunhofer Institute for Integrated Circuits IIS, Nordostpark 84, Nuremberg, 90411, Germany; Vaupel T., Fraunhofer Institute for Integrated Circuits IIS, Nordostpark 84, Nuremberg, 90411, Germany","This chapter on outdoor mobility showed that the realisation of seamless and intermodal mobility requires not only a lot of technologies but also the systematic integration and combination of different navigational methods. First of all the positioning of pedestrians is constrained by a high inaccuracy of GNSS in urban areas. By entering public or private buildings, there is a need for a seamless change to indoor positioning technologies, which in turn are only available in selected areas. Additionally, both-the indoor as well as the outdoor environment-need to be modelled precisely and with a high level of detail. At last, the lack of interoperability still remains. Although the use of crowdsourcing concepts could lead to a higher data quality in the meantime, there are only just a few standardisation approaches. Even efficient and intuitive human machine interfaces for pedestrian navigation are still subject to research. Chapter Contents: • 13.1 Introduction • 13.1.1 Scenario • 13.1.2 Current challenges • 13.2 Towards seamless mobility • 13.2.1 Localisation and positioning • 13.2.2 Environment model and routing • 13.2.3 Human-computer interaction • 13.3 Products • 13.3.1 Pedestrian navigation • 13.3.2 Wearable GPS tracker • 13.3.3 Mobility aids • 13.3.4 Solutions for blind and visually impaired people • 13.4 Research activities • 13.5 Summary • References. © The Institution of Engineering and Technology 2016.","Crowdsourcing concepts; Data quality; GNSS; Human machine interfaces; Indoor positioning technologies; Intermodal mobility; Interoperability; Mobile computing; Mobility management (mobile radio); Navigational methods; Outdoor mobility assistance technologies; Pedestrian navigation; Pedestrians; Private buildings; Public buildings; Satellite navigation; Systematic integration; User interfaces","","","","Institution of Engineering and Technology","","978-184919987-2","","","English","Active and Assisted Living","Book chapter","Final","","Scopus","2-s2.0-85064961842"
"BǍlan O.; Moldoveanu A.; Moldoveanu F.","BǍlan, Oana (57189327901); Moldoveanu, Alin (24438151800); Moldoveanu, Florica (6507837390)","57189327901; 24438151800; 6507837390","Binaural sound analysis and spatial localization for the visually impaired people","2015","Proceedings of the International Conferences on Interfaces and Human Computer Interaction 2015, IHCI 2015, Game and Entertainment Technologies 2015, GET 2015 and Computer Graphics, Visualization, Computer Vision and Image Processing 2015, CGVCVIP 2015 - Part of the Multi Conference on Computer Science and Information Systems 2015","","","","331","335","4","1","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84969262866&partnerID=40&md5=bd936c0d090cd9a40eb12e0ffdcfc565","University Politehnica of Bucharest, Faculty of Automatic Control and Computers, Splaiul independentei, 313, Bucharest, Romania","BǍlan O., University Politehnica of Bucharest, Faculty of Automatic Control and Computers, Splaiul independentei, 313, Bucharest, Romania; Moldoveanu A., University Politehnica of Bucharest, Faculty of Automatic Control and Computers, Splaiul independentei, 313, Bucharest, Romania; Moldoveanu F., University Politehnica of Bucharest, Faculty of Automatic Control and Computers, Splaiul independentei, 313, Bucharest, Romania","The blind people face serious difficulties concerning exclusion from working activities, lack of social involvement and having a sedentary lifestyle. The blind individuals can, however, enhance their life quality by using powerful and effective assistive devices that would help them to perform navigational and orientation tasks and to build a rich mental representation of the environment. An assistive system for the visually impaired people needs to fulfill certain usability requirements, such as to be wearable and affordable and to provide a large quantity of information as efficiently as possible in order to give the user a natural-like perception of the settings. The purpose of our research is to develop an assistive device for the blind people, based on alternative sensory modalities, such as hearing (by encoding the visual information into sound) and touch (by providing additional information through vibrations and other haptic cues). This paper presents the results achieved so far in our research, concerning the sonification and training techniques that will be applied for the development of the proposed assistive system.","3D sound; Blind people; Haptic; HRTF; Training; Virtual auditory display","Audition; Computer games; Computer graphics; Computer vision; Image processing; Information systems; Personnel training; Vision aids; Visualization; 3D sound; Auditory display; Blind people; Haptic; HRTF; Human computer interaction","","Blashki K.; Isaias P.; Rodrigues L.; Betancort H.F.; Kommers P.; Xiao Y.","IADIS","","978-989853338-8","","","English","Proc. Int. Conf. Interfaces Hum. Comput. Interact., IHCI, Game Entertain. Technol., GET Comput. Graph., Vis., Comput. Vis. Image Process., CGVCVIP - Part Multi Conf. Comput. Sci. Inf. Syst.","Conference paper","Final","","Scopus","2-s2.0-84969262866"
"Aladren A.; Lopez-Nicolas G.; Puig L.; Guerrero J.J.","Aladren, A. (56174238300); Lopez-Nicolas, G. (14016076700); Puig, Luis (56423333100); Guerrero, Josechu J. (7203082458)","56174238300; 14016076700; 56423333100; 7203082458","Navigation Assistance for the Visually Impaired Using RGB-D Sensor with Range Expansion","2016","IEEE Systems Journal","10","3","6819807","922","932","10","163","10.1109/JSYST.2014.2320639","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84901083808&doi=10.1109%2fJSYST.2014.2320639&partnerID=40&md5=3b6c8ed8db0aa6b5979a22cca331a04d","Instituto Universitario de Investigación en Ingenieriá de Aragón, Universidad de Zaragoza, Zaragoza, 50009, Spain; General Robotics, Automation, Sensing and Perception Laboratory, University of Pennsylvania, Philadelphia, 19104-6228, PA, United States","Aladren A., Instituto Universitario de Investigación en Ingenieriá de Aragón, Universidad de Zaragoza, Zaragoza, 50009, Spain; Lopez-Nicolas G., Instituto Universitario de Investigación en Ingenieriá de Aragón, Universidad de Zaragoza, Zaragoza, 50009, Spain; Puig L., General Robotics, Automation, Sensing and Perception Laboratory, University of Pennsylvania, Philadelphia, 19104-6228, PA, United States; Guerrero J.J., Instituto Universitario de Investigación en Ingenieriá de Aragón, Universidad de Zaragoza, Zaragoza, 50009, Spain","Navigation assistance for visually impaired (NAVI) refers to systems that are able to assist or guide people with vision loss, ranging from partially sighted to totally blind, by means of sound commands. In this paper, a new system for NAVI is presented based on visual and range information. Instead of using several sensors, we choose one device, a consumer RGB-D camera, and take advantage of both range and visual information. In particular, the main contribution is the combination of depth information with image intensities, resulting in the robust expansion of the range-based floor segmentation. On one hand, depth information, which is reliable but limited to a short range, is enhanced with the long-range visual information. On the other hand, the difficult and prone-to-error image processing is eased and improved with depth information. The proposed system detects and classifies the main structural elements of the scene providing the user with obstacle-free paths in order to navigate safely across unknown scenarios. The proposed system has been tested on a wide variety of scenarios and data sets, giving successful results and showing that the system is robust and works in challenging indoor environments. © 2014 IEEE.","Navigation assistance for visually impaired (NAVI); range and vision; RGB-D camera; visually impaired assistance; wearable system","Cameras; Image processing; Image segmentation; Image intensities; Indoor environment; Navigation assistance for visually impaired; Rgb-d cameras; Structural elements; Visual information; Visually impaired; Wearable systems; Navigation","","","Institute of Electrical and Electronics Engineers Inc.","19328184","","","","English","IEEE Syst. J.","Article","Final","","Scopus","2-s2.0-84901083808"
"Costa P.; Fernandes H.; Barroso J.; Paredes H.; Hadjileontiadis L.J.","Costa, Paulo (7201895673); Fernandes, Hugo (35172835800); Barroso, Joao (20435746800); Paredes, Hugo (23398113700); Hadjileontiadis, Leontios J. (7004037926)","7201895673; 35172835800; 20435746800; 23398113700; 7004037926","Obstacle detection and avoidance module for the blind","2016","World Automation Congress Proceedings","2016-October","","7582990","","","","12","10.1109/WAC.2016.7582990","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84993940434&doi=10.1109%2fWAC.2016.7582990&partnerID=40&md5=b464d5e8230f5c35de1b7933dcf6baa3","School of Technology and Management Computer Graphics and Sound Research, CCIC, Polytechnic Institute of Leiria, Leiria, Portugal; INESC TEC, INESC Porto, University of Trásos-Montes e Alto Douro, Vila Real, Portugal; Department of Electrical and Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki, Greece","Costa P., School of Technology and Management Computer Graphics and Sound Research, CCIC, Polytechnic Institute of Leiria, Leiria, Portugal; Fernandes H., INESC TEC, INESC Porto, University of Trásos-Montes e Alto Douro, Vila Real, Portugal; Barroso J., INESC TEC, INESC Porto, University of Trásos-Montes e Alto Douro, Vila Real, Portugal; Paredes H., INESC TEC, INESC Porto, University of Trásos-Montes e Alto Douro, Vila Real, Portugal; Hadjileontiadis L.J., Department of Electrical and Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki, Greece","Assistive technology enables people to achieve independence when performing daily tasks and it enhances their overall quality of life. Visual information is the basis for most navigational tasks, so visually impaired individuals are at disadvantage due to the lack of sufficient information about their surrounding environment. With recent advances in inclusive technology it is possible to extend the support given to people with visual disabilities in terms of their mobility. In this context we present and describe a wearable system (Blavigator project), whose global objective is to assist visually impaired people in their navigation on indoor and outdoor environments. This paper is focused mainly on the Computer Vision module of the Blavigator prototype. We propose an object collision detection algorithm based on stereo vision. The proposed algorithm uses Peano-Hilbert Ensemble Empirical Mode Decomposition (PH-EEMD) for disparity image processing and a two layer disparity image segmentation to detect nearby objects. Using the adaptive ensemble empirical mode decomposition (EEMD) image analysis real time is not achieved, with PH-EEMD results on a fast implementation suitable for real time applications. © 2016 TSI Enterprise Inc (TSI Press).","Assistive technologies; Empirical Mode Decomposition; Obstacle detection; Peano Hilbert curves; Stereo vision","Computer vision; Image processing; Image segmentation; Object detection; Obstacle detectors; Stereo vision; Assistive technology; Empirical Mode Decomposition; Ensemble empirical mode decomposition; Ensemble empirical mode decompositions (EEMD); Hilbert curve; Obstacle detection; Surrounding environment; Visually impaired people; Stereo image processing","","","IEEE Computer Society","21544824","978-188933551-3","","","English","World Autom. Congress Proc.","Conference paper","Final","","Scopus","2-s2.0-84993940434"
"Tian Y.; Yang X.; Yi C.; Arditi A.","Tian, Yingli (16556710700); Yang, Xiaodong (59108799200); Yi, Chucai (36192382300); Arditi, Aries (57207554001)","16556710700; 59108799200; 36192382300; 57207554001","Toward a computer vision-based wayfinding aid for blind persons to access unfamiliar indoor environments","2013","Machine Vision and Applications","24","3","","521","535","14","78","10.1007/s00138-012-0431-7","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84879688003&doi=10.1007%2fs00138-012-0431-7&partnerID=40&md5=2d08a9c386fe79646a181e1f10a05cc8","Electrical Engineering Department, City College, City University of New York, New York, NY 10031, United States; Graduate Center, City University of New York, New York, NY 10036, United States; Visibility Metrics LLC, Chappaqua, NY 10514, United States","Tian Y., Electrical Engineering Department, City College, City University of New York, New York, NY 10031, United States; Yang X., Electrical Engineering Department, City College, City University of New York, New York, NY 10031, United States; Yi C., Graduate Center, City University of New York, New York, NY 10036, United States; Arditi A., Visibility Metrics LLC, Chappaqua, NY 10514, United States","Independent travel is a well-known challenge for blind and visually impaired persons. In this paper, we propose a proof-of-concept computer vision-based wayfinding aid for blind people to independently access unfamiliar indoor environments. In order to find different rooms (e.g. an office, a laboratory, or a bathroom) and other building amenities (e.g. an exit or an elevator), we incorporate object detection with text recognition. First, we develop a robust and efficient algorithm to detect doors, elevators, and cabinets based on their general geometric shape, by combining edges and corners. The algorithm is general enough to handle large intra-class variations of objects with different appearances among different indoor environments, as well as small inter-class differences between different objects such as doors and door-like cabinets. Next, to distinguish intra-class objects (e.g. an office door from a bathroom door), we extract and recognize text information associated with the detected objects. For text recognition, we first extract text regions from signs with multiple colors and possibly complex backgrounds, and then apply character localization and topological analysis to filter out background interference. The extracted text is recognized using off-the-shelf optical character recognition software products. The object type, orientation, location, and text information are presented to the blind traveler as speech. © 2012 Springer-Verlag.","Blind/visually impaired persons; Computer vision; Indoor wayfinding; Object detection; Optical character recognition (OCR); Text extraction","Algorithms; Computer vision; Edge detection; Elevators; Object recognition; Optical character recognition; Topology; Blind/visually impaired persons; Object Detection; Optical character recognition (OCR); Text extraction; Way-finding; Handicapped persons","Y. Tian; Electrical Engineering Department, City College, City University of New York, New York, NY 10031, United States; email: ytian@ccny.cuny.edu","","","14321769","","MVAPE","","English","Mach Vision Appl","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-84879688003"
"Dunai L.D.; Lengua I.L.; Tortajada I.; Simon F.B.","Dunai, Larisa Dunai (24767311200); Lengua, Ismael Lengua (36809976100); Tortajada, Ignacio (24768021400); Simon, Fernando Brusola (57197366063)","24767311200; 36809976100; 24768021400; 57197366063","Obstacle detectors for visually impaired people","2014","2014 International Conference on Optimization of Electrical and Electronic Equipment, OPTIM 2014","","","6850903","809","816","7","30","10.1109/OPTIM.2014.6850903","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84904891548&doi=10.1109%2fOPTIM.2014.6850903&partnerID=40&md5=31979dd6864310b78cb6864e387aa661","Universitat Politècnica de València, Research Center in Graphic Technology, Spain","Dunai L.D., Universitat Politècnica de València, Research Center in Graphic Technology, Spain; Lengua I.L., Universitat Politècnica de València, Research Center in Graphic Technology, Spain; Tortajada I., Universitat Politècnica de València, Research Center in Graphic Technology, Spain; Simon F.B., Universitat Politècnica de València, Research Center in Graphic Technology, Spain","This paper carries out a review on Electronic Travel Aid Systems (ETAS) for visually impaired people and describes a new wearable Cognitive Aid System for Blind People (CASBliP) developed within the frame of European CASBliP project, in which the authors are taking part. Information on the environment enables humans and vertebrates to know about sources that are in many different directions, particularly signals that are outside the detection range of other senses. Sound source localization is inherently important for safety-survival and navigation. In addition to the acoustical cues, the visual cues such as object detection, tracking and distance measurement play an important role in the navigation not only for robots, but also for blind people, since they are often dependent on artificial intelligence. Due to the fact that blind people make maximum use of sound not only to know the obstacle presence, but also how dangerous it is, in order to avoid it effectively, the CASBliP devices use acoustical sounds in order to represent the visual information detected by the sensors and artificial vision systems. © 2014 IEEE.","","Artificial intelligence; Cognitive systems; Computer vision; Obstacle detectors; Robots; Artificial vision system; Blind people; Detection range; Electronic travel aidss; Object Detection; Sound source localization; Visual information; Visually impaired people; Target tracking","","","IEEE Computer Society","","978-147995183-3","","","English","Int. Conf. Optim. Electr. Electron. Equip., OPTIM","Conference paper","Final","","Scopus","2-s2.0-84904891548"
"Fontanesi S.; Fanucci L.; Li W.","Fontanesi, Simone (57023649000); Fanucci, Luca (7004093241); Li, William (57022387800)","57023649000; 7004093241; 57022387800","Real-Time Pedestrian Crossing Recognition for Assistive Outdoor Navigation","2015","Studies in Health Technology and Informatics","217","","","963","968","5","2","10.3233/978-1-61499-566-1-963","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84952006291&doi=10.3233%2f978-1-61499-566-1-963&partnerID=40&md5=bb0cd6afe9d4dc69c737efe6456466a2","University of Pisa, Italy; Massachusetts Institute of Technology, United States","Fontanesi S., University of Pisa, Italy; Fanucci L., University of Pisa, Italy; Li W., Massachusetts Institute of Technology, United States","Navigation in urban environments can be difficult for people who are blind or visually impaired. In this project, we present a system and algorithms for recognizing pedestrian crossings in outdoor environments. Our goal is to provide navigation cues for crossing the street and reaching an island or sidewalk safely. Using a state-of-The-Art Multisense S7S sensor, we collected 3D pointcloud data for real-Time detection of pedestrian crossing and generation of directional guidance. We demonstrate improvements to a baseline, monocular-camera-based system by integrating 3D spatial prior information extracted from the pointcloud. Our system's parameters can be set to the actual dimensions of real-world settings, which enables robustness of occlusion and perspective transformation. The system works especially well in non-occlusion situations, and is reasonably accurate under different kind of conditions. As well, our large dataset of pedestrian crossings, organized by different types and situations of pedestrian crossings in order to reflect real-word environments, is publicly available in a commonly used format (ROS bagfiles) for further research. © 2015 The authors and IOS Press. All rights reserved.","Assistive navigation; Computer vision; Dataset; Multisense; Pointcloud; Robotics; Visual impairments; Wearable Technology","Algorithms; Artificial Intelligence; Blindness; Cloud Computing; Computer Systems; Cues; Datasets as Topic; Humans; Imaging, Three-Dimensional; Pedestrians; Robotics; Software Design; Urban Population; Vision Disorders; Wearable Electronic Devices; Computer vision; Crosswalks; Footbridges; Navigation; Robotics; Wearable technology; Assistive navigations; Dataset; Multisense; Pointcloud; Visual impairment; algorithm; artificial intelligence; association; blindness; cloud computing; computer system; electronic device; human; information processing; pedestrian; robotics; software design; three dimensional imaging; urban population; visual disorder; Robots","","Sik-Lanyi C.; Cudd P.; Miesenberger K.; Hoogerwerf E.J.","IOS Press","09269630","978-161499565-4","","26294593","English","Stud. Health Technol. Informatics","Conference paper","Final","","Scopus","2-s2.0-84952006291"
"Monekosso D.N.; Flórez-Revuelta F.; Remagnino P.","Monekosso, Dorothy N. (23091283600); Flórez-Revuelta, Francisco (13106226300); Remagnino, Paolo (6602806859)","23091283600; 13106226300; 6602806859","Guest Editorial Special Issue on Ambient-Assisted Living: Sensors, Methods, and Applications","2015","IEEE Transactions on Human-Machine Systems","45","5","7265158","545","549","4","8","10.1109/THMS.2015.2458019","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84942233535&doi=10.1109%2fTHMS.2015.2458019&partnerID=40&md5=ad30edfabac741257b24402ec617f33b","School of Computing, Creative Technologies, and Engineering, Leeds Beckett University, Leeds, LS6 3QL, United Kingdom; Digital Imaging Research Centre, Faculty of Science, Engineering, and Computing, Kingston University, Surrey, KT1 2EE, United Kingdom; School of Computing and Information Systems, Faculty of Science, Engineering, and Computing, Kingston University, Surrey, KT1 2EE, United Kingdom","Monekosso D.N., School of Computing, Creative Technologies, and Engineering, Leeds Beckett University, Leeds, LS6 3QL, United Kingdom; Flórez-Revuelta F., Digital Imaging Research Centre, Faculty of Science, Engineering, and Computing, Kingston University, Surrey, KT1 2EE, United Kingdom; Remagnino P., School of Computing and Information Systems, Faculty of Science, Engineering, and Computing, Kingston University, Surrey, KT1 2EE, United Kingdom","The Special Issue of IEEE Transactions on Human-Machine Systems, deals with papers on ambient-assisted living (AAL), sensors, methods, and applications. In the paper by Tao and colleagues, a technology is proposed, based on infrared (IR) sensors, to track multiple people in a closed environment. The proposed system makes possible the description of movement in a scene with nonintrusive technology. Gaglio and colleagues present a method for recognizing human activities from data acquired with an RGB-D sensor. This sensor allows obtaining both the RGB video and the skeleton of a person. Zhang and colleagues investigate the problem of transitioning between activities. They recognize that humans frequently perform activities sequentially and that the transition between activities can be gradual. Xiao and colleagues contribution is an intelligent assistive navigation system for blind and visually impaired people. The basis for this paper is to enhance accessibility and situational awareness in unmodified environments integrating existing technologies. Abascal and colleagues present the design, implementation, and evaluation of tailored user interfaces for people with physical and cognitive impairments.","","Navigation systems; Ambient assisted living; Ambient assisted living (AAL); Assistive navigations; Blind and visually impaired; Closed environment; Cognitive impairment; Human activities; Situational awareness; User interfaces","","","Institute of Electrical and Electronics Engineers Inc.","21682291","","","","English","IEEE Trans. Human Mach. Syst.","Review","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-84942233535"
"Ahmetovic D.","Ahmetovic, Dragan (53867884600)","53867884600","Smartphone-assisted mobility in urban environments for visually impaired users through computer vision and sensor fusion","2013","Proceedings - IEEE International Conference on Mobile Data Management","2","","6569055","15","18","3","13","10.1109/MDM.2013.58","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84883543056&doi=10.1109%2fMDM.2013.58&partnerID=40&md5=3795f81f89309ac0ea93663b67674f90","Dipartimento di Informatica, Università Degli Studi di Milano, Italy","Ahmetovic D., Dipartimento di Informatica, Università Degli Studi di Milano, Italy","For visually impaired users one of the major challenges is unassisted orientation and way-finding, especially in unexplored and potentially dangerous environments. The following work analyzes the issues stemmed from this problem and summarizes the merits and flaws of solutions available in literature. Afterwards, the research methodology is briefly described and the already achieved results are listed. Finally, a roadmap for the future contributions is proposed. © 2013 IEEE.","Accelerometers; Accessibility; Assistive technologies; Blind; Computer Vision; Disabilities; Gyroscopes; Mobile; Mobility; Navigation; Object recognition; Orientation; Sensor fusion; Smartphones; Visual Impairments","Accelerometers; Carrier mobility; Computer vision; Crystal orientation; Gyroscopes; Inertial navigation systems; Information management; Navigation; Object recognition; Smartphones; Accessibility; Assistive technology; Blind; Disabilities; Mobile; Sensor fusion; Visual impairment; Sensors","","","","15516245","","","","English","Proc. IEEE Int. Conf. Mobile Data Manage.","Conference paper","Final","","Scopus","2-s2.0-84883543056"
"Sövény B.; Kovács G.; Kardkovács Z.T.","Sövény, Bálint (56528514400); Kovács, Gábor (56527728900); Kardkovács, Zsolt T. (8881974300)","56528514400; 56527728900; 8881974300","Blind guide: A virtual eye for guiding indoor and outdoor movement","2015","Journal on Multimodal User Interfaces","9","4","","287","297","10","13","10.1007/s12193-015-0191-6","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84947862529&doi=10.1007%2fs12193-015-0191-6&partnerID=40&md5=35fb7ca3c48f76c9aa2584504ef8ad09","U1 Research Ltd., Gabor Denes 2, Budapest, Hungary","Sövény B., U1 Research Ltd., Gabor Denes 2, Budapest, Hungary; Kovács G., U1 Research Ltd., Gabor Denes 2, Budapest, Hungary; Kardkovács Z.T., U1 Research Ltd., Gabor Denes 2, Budapest, Hungary","In this paper, we present a design of a wearable equipment that helps with the perception of the environment for visually impaired people in both indoor and outdoor mobility and navigation. Our prototype can detect and identify traffic situations such as street crossings, traffic lamps, cars, cyclists, other people and obstacles hanging down from above or placed on the ground. The detection takes place in real time based on input data of sensors and cameras, the mobility of the user is aided with audio signals. © 2015, OpenInterface Association.","Augmented reality; Stereo vision; Virtual eye","Augmented reality; Stereo image processing; Stereo vision; Wearable technology; Audio signal; Input datas; Real time; Street crossing; Traffic situations; Virtual eye; Visually impaired people; Eye movements","G. Kovács; U1 Research Ltd., Budapest, Gabor Denes 2, Hungary; email: kovacsg@u1research.org","","Springer Verlag","17837677","","","","English","J. Multimodal User Interfaces","Article","Final","","Scopus","2-s2.0-84947862529"
"Hicks S.L.; Wilson I.; Muhammed L.; Worsfold J.; Downes S.M.; Kennard C.","Hicks, Stephen L. (23100097900); Wilson, Iain (56789479700); Muhammed, Louwai (55386910500); Worsfold, John (55746318800); Downes, Susan M. (7006919830); Kennard, Christopher (57203958600)","23100097900; 56789479700; 55386910500; 55746318800; 7006919830; 57203958600","A Depth-Based Head-Mounted Visual Display to Aid Navigation in Partially Sighted Individuals","2013","PLoS ONE","8","7","e67695","","","","88","10.1371/journal.pone.0067695","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84879763466&doi=10.1371%2fjournal.pone.0067695&partnerID=40&md5=22169ee3c62625662dfacc65934923d9","The Nuffield Department of Clinical Neurosciences, University of Oxford, Oxford, United Kingdom; The Royal National Institute for Blind People, London, United Kingdom; Oxford University Hospitals NHS Trust, Oxford, United Kingdom; Nuffield Laboratory of Ophthalmology, University of Oxford, Oxford, United Kingdom; NIHR Biomedical Research Centre, Oxford, United Kingdom","Hicks S.L., The Nuffield Department of Clinical Neurosciences, University of Oxford, Oxford, United Kingdom; Wilson I., The Nuffield Department of Clinical Neurosciences, University of Oxford, Oxford, United Kingdom; Muhammed L., The Nuffield Department of Clinical Neurosciences, University of Oxford, Oxford, United Kingdom; Worsfold J., The Royal National Institute for Blind People, London, United Kingdom; Downes S.M., Oxford University Hospitals NHS Trust, Oxford, United Kingdom, Nuffield Laboratory of Ophthalmology, University of Oxford, Oxford, United Kingdom, NIHR Biomedical Research Centre, Oxford, United Kingdom; Kennard C., The Nuffield Department of Clinical Neurosciences, University of Oxford, Oxford, United Kingdom, NIHR Biomedical Research Centre, Oxford, United Kingdom","Independent navigation for blind individuals can be extremely difficult due to the inability to recognise and avoid obstacles. Assistive techniques such as white canes, guide dogs, and sensory substitution provide a degree of situational awareness by relying on touch or hearing but as yet there are no techniques that attempt to make use of any residual vision that the individual is likely to retain. Residual vision can restricted to the awareness of the orientation of a light source, and hence any information presented on a wearable display would have to limited and unambiguous. For improved situational awareness, i.e. for the detection of obstacles, displaying the size and position of nearby objects, rather than including finer surface details may be sufficient. To test whether a depth-based display could be used to navigate a small obstacle course, we built a real-time head-mounted display with a depth camera and software to detect the distance to nearby objects. Distance was represented as brightness on a low-resolution display positioned close to the eyes without the benefit focussing optics. A set of sighted participants were monitored as they learned to use this display to navigate the course. All were able to do so, and time and velocity rapidly improved with practise with no increase in the number of collisions. In a second experiment a cohort of severely sight-impaired individuals of varying aetiologies performed a search task using a similar low-resolution head-mounted display. The majority of participants were able to use the display to respond to objects in their central and peripheral fields at a similar rate to sighted controls. We conclude that the skill to use a depth-based display for obstacle avoidance can be rapidly acquired and the simplified nature of the display may appropriate for the development of an aid for sight-impaired individuals. © 2013 Hicks et al.","","Adult; Aged; Aged, 80 and over; Audiovisual Aids; Blindness; Computer Terminals; Female; Humans; Male; Middle Aged; Vision, Ocular; Visually Impaired Persons; Canis familiaris; adult; aged; article; brightness; camera; clinical article; cohort analysis; computer program; controlled study; depth perception; disease severity; distance perception; female; human; image display; image processing; male; optics; vision; visual aid; visual display unit; visual field; visual impairment","S. L. Hicks; The Nuffield Department of Clinical Neurosciences, University of Oxford, Oxford, United Kingdom; email: stephen.hicks@ndcn.ox.ac.uk","","","19326203","","POLNC","23844067","English","PLoS ONE","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-84879763466"
"Peiris H.; Kulasekara C.; Wijesinghe H.; Kothalawala B.; Walgampaya N.; Kasthurirathna D.","Peiris, Hiranya (57215356641); Kulasekara, Charitha (57215361839); Wijesinghe, Hashan (57215351118); Kothalawala, Basiru (57215346963); Walgampaya, Namalie (57215359614); Kasthurirathna, Dharshana (54420261000)","57215356641; 57215361839; 57215351118; 57215346963; 57215359614; 54420261000","EyeVista: An assistive wearable device for visually impaired sprint athletes","2016","2016 IEEE International Conference on Information and Automation for Sustainability: Interoperable Sustainable Smart Systems for Next Generation, ICIAfS 2016","","","7946558","","","","8","10.1109/ICIAFS.2016.7946558","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85021950046&doi=10.1109%2fICIAFS.2016.7946558&partnerID=40&md5=2824b77a1abf961411ef983afd3f02ee","Faculty of Information Technology, Sri Lanka Institute of Information Technology, Malabe, 10115, Sri Lanka","Peiris H., Faculty of Information Technology, Sri Lanka Institute of Information Technology, Malabe, 10115, Sri Lanka; Kulasekara C., Faculty of Information Technology, Sri Lanka Institute of Information Technology, Malabe, 10115, Sri Lanka; Wijesinghe H., Faculty of Information Technology, Sri Lanka Institute of Information Technology, Malabe, 10115, Sri Lanka; Kothalawala B., Faculty of Information Technology, Sri Lanka Institute of Information Technology, Malabe, 10115, Sri Lanka; Walgampaya N., Faculty of Information Technology, Sri Lanka Institute of Information Technology, Malabe, 10115, Sri Lanka; Kasthurirathna D., Faculty of Information Technology, Sri Lanka Institute of Information Technology, Malabe, 10115, Sri Lanka","On-going progressions of Information Technology increase the scope for computer vision-based interventions to facilitate efficient and promising technology for people with disabilities. This project aims to develop a wearable navigational assistive device, titled EyeVista, to facilitate visually impaired sprint athletes. It is a lightweight, easy-To-use, customizable and low-cost wearable jacket built-in with off-The-shelf based on computer vision techniques. Synthesis of research initially reflects the impact of the main barriers of a human guide and how to break down such barriers. In doing so, we hope to introduce an alternative to the current practice of having a human guide for blind athletes, overcoming the shortcomings of it. The designed system uses Raspberry Pi single board computer to process the real-Time image captured by Raspberry Pi camera module to navigate the athletes within the assigned track and to avoid collisions. As a result, we believe the project EyeVista will empower the visually impaired sprint athletes to enhance their performance by easing their mobility by allowing the user to move within their relevant track lanes and avoid collisions without the support of a human guide and enhance the independence, safety along with the quality of life. © 2016 IEEE.","Assistive Technologies; Computer Vision; Obstacle Detection; Wearable Devices","Computer vision; Interoperability; Obstacle detectors; Sustainable development; Assistive devices; Assistive technology; Computer vision techniques; Current practices; Obstacle detection; People with disabilities; Single board computers; Wearable devices; Wearable technology","","","Institute of Electrical and Electronics Engineers Inc.","","978-150906132-7","","","English","IEEE Int. Conf. Inf. Auto. Sustain.: Interoper. Sustain. Smart Syst. Next Gener., ICIAfS","Conference paper","Final","","Scopus","2-s2.0-85021950046"
"Haraszy Z.; Cristea D.-G.; Micut S.; Bogdanov I.","Haraszy, Zoltan (24476948300); Cristea, David-George (57195451009); Micut, Sebastian (39861754900); Bogdanov, Ivan (24172337800)","24476948300; 57195451009; 39861754900; 24172337800","Efficient algorithm for extracting essential Head Related Impulse Response data for Acoustic Virtual Reality development","2011","Recent Researches in System Science - Proceedings of the 15th WSEAS International Conference on Systems, Part of the 15th WSEAS CSCC Multiconference","","","","315","320","5","2","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-82955190447&partnerID=40&md5=dffa14baaa061d22d3602cd95a7e5fc6","Department of Applied Electronics, POLITEHNICA University of Timisoara, Timisoara 300223, Bvd. Vasile Parvan 2, Romania","Haraszy Z., Department of Applied Electronics, POLITEHNICA University of Timisoara, Timisoara 300223, Bvd. Vasile Parvan 2, Romania; Cristea D.-G., Department of Applied Electronics, POLITEHNICA University of Timisoara, Timisoara 300223, Bvd. Vasile Parvan 2, Romania; Micut S., Department of Applied Electronics, POLITEHNICA University of Timisoara, Timisoara 300223, Bvd. Vasile Parvan 2, Romania; Bogdanov I., Department of Applied Electronics, POLITEHNICA University of Timisoara, Timisoara 300223, Bvd. Vasile Parvan 2, Romania","The new Acoustic Virtual Reality (AVR) concept is often used as a man-machine interface in electronic travel aid (ETA), that help blind and visually impaired individuals to navigate in real outdoor environments. According to this concept, the presence of obstacles in the surrounding environment and the path to the desired target will be signalized to the blind subject by burst of sounds, whose virtual source position suggests the position of the real obstacles and the direction of movement, respectively. The practical implementation of the AVR concept requires the so-called Head Related Transfer Functions (HRTFs) to be known in every point of the 3D space and for each involved individual. In the present paper, we describe an efficient algorithm for extracting essential Head Related Impulse Response (HRIR) data, we apply it for one person's HRIRs from the Listen Ircam HRTF database. To verify its applicability, after describing the experimental setup, listening tests are also conducted and results are compared with the results of listening tests without using the proposed algorithm. Finally, conclusions and future research ideas are also presented.","Acoustic Virtual Reality; Artificial neural networks; Head Related Transfer Functions; Localization experiment; Man-machine interface; Visually impaired","Data visualization; Impulse response; Neural networks; Systems science; Transfer functions; Virtual reality; 3-D space; Artificial Neural Network; Efficient algorithm; Electronic travel aidss; Experimental setup; Head related transfer function; Head-related impulse response; Listening tests; Man-machine interface; Outdoor environment; Practical implementation; Surrounding environment; Virtual sources; Visually impaired; Algorithms","Z. Haraszy; Department of Applied Electronics, POLITEHNICA University of Timisoara, Timisoara 300223, Bvd. Vasile Parvan 2, Romania; email: zoltan.haraszy@etc.upt.ro","","","","978-161804023-7","","","English","Recent Res. Syst. Sci. - Proc. WSEAS Int. Conf. Syst., Part WSEAS CSCC Multiconference","Conference paper","Final","","Scopus","2-s2.0-82955190447"
"Karthikeyan M.; Henry J.; Ramakrishnan; Rajan K.","Karthikeyan, M. (56417442600); Henry, Joseph (7403671573); Ramakrishnan (56708777500); Rajan, K. (56392535200)","56417442600; 7403671573; 56708777500; 56392535200","Obstacle detection and guidence for blind and partially sighted people with digital image processing","2015","International Journal of Applied Engineering Research","10","9","","24385","24395","10","0","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84934989124&partnerID=40&md5=152fc98a1a00aa21bc6ce5198b5a25fd","Vel Tech Dr. Rr and Dr. Sr Technical University, Chennai, India; School of Electrical Engineering, Veltech DR RR and DR SR Technical University, Avadi, Chennai 62, India; Veltech, Avadi, Chennai, India","Karthikeyan M., Vel Tech Dr. Rr and Dr. Sr Technical University, Chennai, India; Henry J., School of Electrical Engineering, Veltech DR RR and DR SR Technical University, Avadi, Chennai 62, India; Ramakrishnan, School of Electrical Engineering, Veltech DR RR and DR SR Technical University, Avadi, Chennai 62, India; Rajan K., Veltech, Avadi, Chennai, India","Blind navigator to navigate the blind and visually impaired people with embedded based system is proposed in this paper. Blind Navigator would detect an object or obstacle and guide the blind with the use of audio instructions and vibrations. The main concept behind the system is to operate when the disabled is in need. The embedded system is dedicated to specific tasks, which can optimize a embedded designed system to reduce the size and cost of the product and increase its reliability and performance. The blind navigator mainly uses camera and Infrared sensor (IR sensor). A microcontroller (or MCU) is a computer-on-a-chip used to control electronic devices. A typical microcontroller contains all the memory and interfaces needed for a simple application. The APR sound system is used for audio instruction and relay controlled vibration motor is used for vibration. The importance of our system is, it is used for both indoor and outdoor navigation. © Research India Publications.","4x4 keypad; APR sound system vibration motor; Camera; Infrared sensor","","","","Research India Publications","09734562","","","","English","Int. J. Appl. Eng. Res.","Article","Final","","Scopus","2-s2.0-84934989124"
"Ghaderi V.S.; Mulas M.; Pereira V.F.S.; Everding L.; Weikersdorfer D.; Conradt J.","Ghaderi, Viviane S. (54683841200); Mulas, Marcello (56960073800); Pereira, Vinicius Felisberto Santos (57038144900); Everding, Lukas (57039137900); Weikersdorfer, David (54929869700); Conradt, Jorg (13005140400)","54683841200; 56960073800; 57038144900; 57039137900; 54929869700; 13005140400","A wearable mobility device for the blind using retina-inspired dynamic vision sensors","2015","Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS","2015-November","","7319115","3371","3374","3","11","10.1109/EMBC.2015.7319115","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84953326012&doi=10.1109%2fEMBC.2015.7319115&partnerID=40&md5=ff30704de46a4c8f4d5cb9856f48250b","Electrical and Computer Engineering Department, Technische Universitärt Mürnchen, Mürnchen, 80333, Germany; Google, Inc., Mountain View, United States","Ghaderi V.S., Electrical and Computer Engineering Department, Technische Universitärt Mürnchen, Mürnchen, 80333, Germany; Mulas M., Electrical and Computer Engineering Department, Technische Universitärt Mürnchen, Mürnchen, 80333, Germany; Pereira V.F.S., Electrical and Computer Engineering Department, Technische Universitärt Mürnchen, Mürnchen, 80333, Germany; Everding L., Electrical and Computer Engineering Department, Technische Universitärt Mürnchen, Mürnchen, 80333, Germany; Weikersdorfer D., Google, Inc., Mountain View, United States; Conradt J., Electrical and Computer Engineering Department, Technische Universitärt Mürnchen, Mürnchen, 80333, Germany","Proposed is a prototype of a wearable mobility device which aims to assist the blind with navigation and object avoidance via auditory-vision-substitution. The described system uses two dynamic vision sensors and event-based information processing techniques to extract depth information. The 3D visual input is then processed using three different strategies, and converted to a 3D output sound using an individualized head-related transfer function. The performance of the device with different processing strategies is evaluated via initial tests with ten subjects. The outcome of these tests demonstrate promising performance of the system after only very short training times of a few minutes due to the minimal encoding of outputs from the vision sensors which are translated into simple sound patterns easily interpretable for the user. The envisioned system will allow for efficient real-time algorithms on a hands-free and lightweight device with exceptional battery life-time. © 2015 IEEE.","","Adult; Algorithms; Blindness; Equipment Design; Humans; Image Processing, Computer-Assisted; Male; Retina; Sound; Vision, Ocular; Visually Impaired Persons; Young Adult; Engineering; Industrial engineering; Battery life time; Depth information; Dynamic vision sensors; Head related transfer function; Information processing technique; Lightweight devices; Processing strategies; Real time algorithms; adult; algorithm; blindness; equipment design; human; image processing; male; physiology; retina; sound; vision; visually impaired person; young adult; Wearable sensors","","","Institute of Electrical and Electronics Engineers Inc.","1557170X","978-142449271-8","","26737015","English","Proc. Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. EMBS","Conference paper","Final","","Scopus","2-s2.0-84953326012"
"He H.; Tan J.","He, Hongsheng (56231174200); Tan, Jindong (7402302766)","56231174200; 7402302766","Demonstration paper: Adaptive ego-motion tracking using visual-inertial sensors for wearable blind navigation","2014","Proceedings - Wireless Health 2014, WH 2014","","","2669590","","","","1","10.1145/2668883.2669590","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84919359843&doi=10.1145%2f2668883.2669590&partnerID=40&md5=0450c2ebddfdf0f82d5d3198d9425349","Department of Mechanical, Aerospace and Biomedical Engineering, University of Tennessee, Knoxville, TN, United States","He H., Department of Mechanical, Aerospace and Biomedical Engineering, University of Tennessee, Knoxville, TN, United States; Tan J., Department of Mechanical, Aerospace and Biomedical Engineering, University of Tennessee, Knoxville, TN, United States","This paper presents an ego-motion tracking method using visual-inertial sensors to assist the visually impaired and blind (VIB) people to travel in unknown dynamic environments. We focus on the ego-motion tracking functionality to inform the wearers of their relative position with respect to the environment. A traveled trajectory is recovered by concatenating the transformation estimated from visual correspondences and instantaneous movements captured by inertial sensors. Therefore, we introduce an adaptive mechanism to judge the reliability of visual tracking by comparing the estimated rotation with the gyroscopic measurement. The measuring frequencies of visual and inertial sensors are different because of different physical sampling rates and the introduced adaptive mechanism. We adopt the multi-rate extended Kalman filter (EKF) to fuse the visual estimation and inertial measurement. In the experiment, we wear the navigation system to follow a path in an indoor environment, and the results show the effectiveness and precision of the proposed methods in ego-motion tracking. Copyright © 2014 ACM.","Assistive device; Blind navigation; Wearable robotics","Extended Kalman filters; Inertial navigation systems; Robots; Wearable sensors; Assistive devices; Blind navigation; Dynamic environments; Indoor environment; Inertial measurements; Instantaneous movement; Measuring frequency; Wearable robotics; Motion tracking","","","Association for Computing Machinery","","978-145033160-9","","","English","Proc. - Wirel. Health, WH","Conference paper","Final","","Scopus","2-s2.0-84919359843"
"Lee M.-F.R.; Chiu F.H.S.; Zhuo C.","Lee, Min-Fan Ricky (35409746200); Chiu, Fu Hsin Steven (56067815500); Zhuo, Chen (57227626900)","35409746200; 56067815500; 57227626900","Novel design of a social mobile robot for the blind disabilities","2013","2013 IEEE/SICE International Symposium on System Integration, SII 2013","","","6776710","161","166","5","8","10.1109/sii.2013.6776710","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84902515717&doi=10.1109%2fsii.2013.6776710&partnerID=40&md5=ba5702d00b2b67110a9091cb46a6f9e2","Graduate Institute of Automation and Control, National Taiwan University of Science and Technology, Taipei, Taiwan; Department of Mechanical Engineering, University of British Columbia, Vancouver, BC, Canada","Lee M.-F.R., Graduate Institute of Automation and Control, National Taiwan University of Science and Technology, Taipei, Taiwan; Chiu F.H.S., Graduate Institute of Automation and Control, National Taiwan University of Science and Technology, Taipei, Taiwan; Zhuo C., Department of Mechanical Engineering, University of British Columbia, Vancouver, BC, Canada","World Health Organization estimated that over 1 billion people are living with some form of disability. Some 110 to 190 million people live with significant disabilities. The number of guide dogs is far lower than the demand and only few visually impaired have access to qualified guidance dogs. This paper present an autonomous mobile robot system aimed for the service to the blind. The system includes a proof-of-concept appearance design as the human machine interface that consists of a stick and main body enveloping the mobile robot platform. The main body contains four modules as sensing, navigation, remote control and social platform. The appearance has a high quality texture demonstrated human factors engineering concepts emphasizing user-friendly interface. The stick has rotation joint design fitting the average human height. The length and angle of the stick is adjustable to user's height and can be used in separate from or combination with the main body. The system combines artificial intelligence, sensing, and navigation system that allows blind people to move in a free state. © 2013 IEEE.","","Human engineering; Mobile robots; Navigation systems; Remote control; Textures; Appearance design; Autonomous Mobile Robot; Human Machine Interface; Mobile robot platforms; Proof of concept; User friendly interface; Visually impaired; World Health Organization; Machine design","","","IEEE Computer Society","","978-147992626-8","","","English","IEEE/SICE Int. Symp. Syst. Integr., SII","Conference paper","Final","","Scopus","2-s2.0-84902515717"
"Coughlan J.M.; Shen H.","Coughlan, James M. (7006567201); Shen, Huiying (14523706200)","7006567201; 14523706200","Crosswatch: A system for providing guidance to visually impaired travelers at traffic intersection","2013","Journal of Assistive Technologies","7","2","","131","142","11","32","10.1108/17549451311328808","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84879711592&doi=10.1108%2f17549451311328808&partnerID=40&md5=19c1443e521a13d64094acaaaa50dcf1","The Smith-Kettlewell Eye Research Institute, San Francisco, CA, United States","Coughlan J.M., The Smith-Kettlewell Eye Research Institute, San Francisco, CA, United States; Shen H., The Smith-Kettlewell Eye Research Institute, San Francisco, CA, United States","Purpose: The purpose of this paper is to describe recent progress on the ""Crosswatch"" project, a smartphone-based system developed for providing guidance to blind and visually impaired travelers at traffic intersections. Building on past work on Crosswatch functionality to help the user achieve proper alignment with the crosswalk and read the status of walk lights to know when it is time to cross, the authors outline the directions Crosswatch is now taking to help realize its potential for becoming a practical system: namely, augmenting computer vision with other information sources, including geographic information systems (GIS) and sensor data, and inferring the user's location much more precisely than is possible through GPS alone, to provide a much larger range of information about traffic intersections to the pedestrian. Design/methodology/approach: The paper summarizes past progress on Crosswatch and describes details about the development of new Crosswatch functionalities. One such functionality, which is required for determination of the user's precise location, is studied in detail, including the design of a suitable user interface to support this functionality and preliminary tests of this interface with visually impaired volunteer subjects. Findings: The results of the tests of the new Crosswatch functionality demonstrate that the functionality is feasible in that it is usable by visually impaired persons. Research limitations/implications: While the tests that were conducted of the new Crosswatch functionality are preliminary, the results of the tests have suggested several possible improvements, to be explored in the future. Practical implications: The results described in this paper suggest that the necessary technologies used by the Crosswatch system are rapidly maturing, implying that the system has an excellent chance of becoming practical in the near future. Originality/value: The paper addresses an innovative solution to a key problem faced by blind and visually impaired travelers, which has the potential to greatly improve independent travel for these individuals. © Emerald Group Publishing Limited.","Assistive technologies; Blind people; Blindness; Mobile communication systems; Pedestrian safety; Roads; Traffic intersections; Visual impairment","","J. M. Coughlan; The Smith-Kettlewell Eye Research Institute, San Francisco, CA, United States; email: coughlan@ski.org","","","20428723","","","","English","J. Assistive Technol.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-84879711592"
"Mascetti S.; Ahmetovic D.; Gerino A.; Bernareggi C.; Busso M.; Rizzi A.","Mascetti, Sergio (8919148700); Ahmetovic, Dragan (53867884600); Gerino, Andrea (55845429900); Bernareggi, Cristian (56266643600); Busso, Mario (57008852300); Rizzi, Alessandro (56962787100)","8919148700; 53867884600; 55845429900; 56266643600; 57008852300; 56962787100","Robust traffic lights detection on mobile devices for pedestrians with visual impairment","2016","Computer Vision and Image Understanding","148","","","123","135","12","54","10.1016/j.cviu.2015.11.017","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84950326974&doi=10.1016%2fj.cviu.2015.11.017&partnerID=40&md5=5dc744b3eb46290b91f9c592620baef3","Università degli Studi di Milano, Deptartment of Computer Science, Milan, Italy; EveryWare Technologies, Milan, Italy","Mascetti S., Università degli Studi di Milano, Deptartment of Computer Science, Milan, Italy, EveryWare Technologies, Milan, Italy; Ahmetovic D., Università degli Studi di Milano, Deptartment of Computer Science, Milan, Italy; Gerino A., Università degli Studi di Milano, Deptartment of Computer Science, Milan, Italy, EveryWare Technologies, Milan, Italy; Bernareggi C., Università degli Studi di Milano, Deptartment of Computer Science, Milan, Italy, EveryWare Technologies, Milan, Italy; Busso M., Università degli Studi di Milano, Deptartment of Computer Science, Milan, Italy; Rizzi A., Università degli Studi di Milano, Deptartment of Computer Science, Milan, Italy","Independent mobility involves a number of challenges for people with visual impairment or blindness. In particular, in many countries the majority of traffic lights are still not equipped with acoustic signals. Recognizing traffic lights through the analysis of images acquired by a mobile device camera is a viable solution already experimented in scientific literature. However, there is a major issue: the recognition techniques should be robust under different illumination conditions. This contribution addresses the above problem with an effective solution: besides image processing and recognition, it proposes a robust setup for image capture that makes it possible to acquire clearly visible traffic light images regardless of daylight variability due to time and weather. The proposed recognition technique that adopts this approach is reliable (full precision and high recall), robust (works in different illumination conditions) and efficient (it can run several times a second on commercial smartphones). The experimental evaluation conducted with visual impaired subjects shows that the technique is also practical in supporting road crossing. © 2015 Elsevier Inc. All rights reserved.","Assistive technologies; Computer vision; Mobile devices; Traffic lights; Visual impairment","Computer vision; Image acquisition; Image processing; Ophthalmology; Signal processing; Assistive technology; Effective solution; Experimental evaluation; Illumination conditions; Scientific literature; Traffic light; Viable solutions; Visual impairment; Mobile devices","S. Mascetti; Università degli Studi di Milano, Deptartment of Computer Science, Milan, Italy; email: sergio.mascetti@unimi.it","","Academic Press Inc.","10773142","","CVIUF","","English","Comput Vision Image Understanding","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-84950326974"
"Praveen R.G.; Paily R.P.","Praveen, R. Gnana (55425508800); Paily, Roy P. (18438148100)","55425508800; 18438148100","Blind navigation assistance for visually impaired based on local depth hypothesis from a single image","2013","Procedia Engineering","64","","","351","360","9","24","10.1016/j.proeng.2013.09.107","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84891789183&doi=10.1016%2fj.proeng.2013.09.107&partnerID=40&md5=c80c5cd6a75fdc363be3415a3a3ec1c4","Dept of Electronics and Electrical Engineering, IIT Guwahati, Guwahati, Assam - 781039, India","Praveen R.G., Dept of Electronics and Electrical Engineering, IIT Guwahati, Guwahati, Assam - 781039, India; Paily R.P., Dept of Electronics and Electrical Engineering, IIT Guwahati, Guwahati, Assam - 781039, India","Assisting the visually impaired along their navigation path is a challenging task which drew the attention of several researchers. A lot of techniques based on RFID, GPS and computer vision modules are available for blind navigation assistance. In this paper, we proposed a depth estimation technique from a single image based on local depth hypothesis devoid of any user intervention and its application to assist the visually impaired people. The ambient space ahead of the user is captured by a camera and the captured image is resized for computational efficiency. The obstacles in the foreground of the image are segregated using edge detection followed by morphological operations. Then depth is estimated for each obstacle based on local depth hypothesis. The estimated depth map is then compared with the reference depth map of the corresponding depth hypothesis and the deviation of the estimated depth map from the reference depth map is used to retrieve the spatial information about the obstacles ahead of the user. © 2013 The Authors. Published by Elsevier Ltd.","Blind Navigation Assistance; Canny Edge Detection; Depth Hypothesis; Obstacle Detection; Vanishing Point Estimation","","R.G. Praveen; Dept of Electronics and Electrical Engineering, IIT Guwahati, Guwahati, Assam - 781039, India; email: r.praveen@iitg.ernet.in","","Elsevier Ltd","18777058","","","","English","Procedia Eng.","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-84891789183"
"Khade S.; Dandawate Y.H.","Khade, Sanket (57188749941); Dandawate, Yogesh H. (23476715900)","57188749941; 23476715900","Hardware implementation of obstacle detection for assisting visually impaired people in an unfamiliar environment by using Raspberry Pi","2016","Communications in Computer and Information Science","628 CCIS","","","889","895","6","9","10.1007/978-981-10-3433-6_106","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85009476100&doi=10.1007%2f978-981-10-3433-6_106&partnerID=40&md5=eaac5dac46789af70b0f5ca789fcf18b","Department of E&TC, Vishwakarma Institute of Information Technology, Pune, India","Khade S., Department of E&TC, Vishwakarma Institute of Information Technology, Pune, India; Dandawate Y.H., Department of E&TC, Vishwakarma Institute of Information Technology, Pune, India","For assisting blind or visually impaired persons, many computer vision technology has been developed. Some camera based systems were developed to help those people in way finding, navigation and finding daily necessities. The motion of the observer causes all scene object stationary or non-stationary in motion. And hence it is very much important to detect moving object with the moving observer. In this context we have proposed a camera based prototype system for assisting blind person in detection of obstacles by using motion vectors. We have collected dataset of their indoor and outdoor environment and estimated the optical flow to perform object detection. Furthermore we have detected the objects in the region of interest without using costly Depth cameras and sensors. The hardware used in the proposed work is ‘Raspberry Pi 2-B’ and the algorithms used for object detection is performed using MATLAB (for simulation purpose) and Python language. © Springer Nature Singapore Pte Ltd. 2016.","Blind people; Computer vision; Object detection; Optical flow; Raspberry Pi","Cameras; Computer hardware; Computer simulation languages; Computer software; Computer vision; Hardware; Image segmentation; MATLAB; Object recognition; Obstacle detectors; Optical flows; Blind people; Computer vision technology; Hardware implementations; Obstacle detection; Outdoor environment; Region of interest; Visually impaired people; Visually impaired persons; Object detection","S. Khade; Department of E&TC, Vishwakarma Institute of Information Technology, Pune, India; email: sanket.khade5@gmail.com","Nayak M.; Singh D.; Mishra D.K.; Unal A.; Joshi A.","Springer Verlag","18650929","978-981103432-9","","","English","Commun. Comput. Info. Sci.","Conference paper","Final","","Scopus","2-s2.0-85009476100"
"Amin N.; Borschbach M.","Amin, Navya (55522303300); Borschbach, Markus (6508264103)","55522303300; 6508264103","Classification criteria for local navigation digital assistance techniques for the visually impaired","2014","2014 13th International Conference on Control Automation Robotics and Vision, ICARCV 2014","","","7064576","1724","1728","4","5","10.1109/ICARCV.2014.7064576","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84949924458&doi=10.1109%2fICARCV.2014.7064576&partnerID=40&md5=0bff984439983258532a708aa1912906","Competence Center Optimized Systems, University of Applied Sciences (FHDW), Hauptstr. 2, Bergisch Gladbach, 51465, Germany","Amin N., Competence Center Optimized Systems, University of Applied Sciences (FHDW), Hauptstr. 2, Bergisch Gladbach, 51465, Germany; Borschbach M., Competence Center Optimized Systems, University of Applied Sciences (FHDW), Hauptstr. 2, Bergisch Gladbach, 51465, Germany","This paper presents the criteria for classification of the existing techniques for the development of navigation aid for the visually impaired people. Digital assistance for the visually impaired people to navigate easily even during the presence of static and moving obstacles has been a food for research over the years. The motivation for the current work is the development of a smart hearing aid for people with hearing loss. The filtering of essential information to enable a better sensing of the surrounding environment for people with diminished vision is the main aim of developing a navigation system for people with such impairment. The current study gives an overview of the techniques used for obstacle detection, detection of path for navigation and system for alerting the users about existing obstacles in their path. It also presents the selected algorithms used for detecting the obstacles in the path and alarming the user about its presence which offers as a criteria for the comparison of the techniques for the development of a navigation system for the blind. © 2014 IEEE.","Digital Navigation; Motion Sensing; Object Segmentation; Obstacle Avoidance; Obstacle Detection; Visual Impairment","Audition; Collision avoidance; Computer vision; Hearing aids; Information filtering; Navigation systems; Obstacle detectors; Robotics; Vision aids; Digital navigation; Motion sensing; Object segmentation; Obstacle detection; Visual impairment; Robots","","","Institute of Electrical and Electronics Engineers Inc.","","978-147995199-4","","","English","Int. Conf. Control Autom. Rob. Vis., ICARCV","Conference paper","Final","","Scopus","2-s2.0-84949924458"
"Sourab B.S.; Ranganatha Chakravarthy H.S.; D'Souza S.","Sourab, B.S. (56641618800); Ranganatha Chakravarthy, H.S. (56641896200); D'Souza, Sachith (56640796600)","56641618800; 56641896200; 56640796600","Design and implementation of mobility aid for blind people","2015","Proceedings of the 2015 IEEE International Conference on Power and Advanced Control Engineering, ICPACE 2015","","","7274960","290","294","4","13","10.1109/ICPACE.2015.7274960","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84961864395&doi=10.1109%2fICPACE.2015.7274960&partnerID=40&md5=0c7abc947933ddf6094a33530fc1d272","EEE Department, BNM Institute of Technology, Bangalore, India","Sourab B.S., EEE Department, BNM Institute of Technology, Bangalore, India; Ranganatha Chakravarthy H.S., EEE Department, BNM Institute of Technology, Bangalore, India; D'Souza S., EEE Department, BNM Institute of Technology, Bangalore, India","With the scope of electronics increasing day by day, the need for utilizing these advanced technologies to make human lives simpler is becoming more and more necessary. The demand for using these technologies to make lives easier for disabled people is also increasing. This has prompted many new areas of research and one of the areas is electronic mobility aid for blind people. According to the World Health Organization, approximately 285 million people of all ages are blind, which is significantly a large number [1]. Traditional mobility aids include the white stick and the guide dogs which take a lot of time getting used to. There are a few smart systems available in the market which uses electronic sensors mounted on the cane but those systems also have certain disadvantages. This paper analyses the available solutions and proposes an entire new approach to solve the above problem. The new approach not only eliminates the disadvantages of the existing solutions, but it also is reliable, cost effective and most importantly easier to use. © 2015 IEEE.","Blind; Electronics; Mobility Aid; Navigation; Obstacle Detection; Smart System; Visually Impaired","Artificial intelligence; Cost effectiveness; Electronic equipment; Navigation; Obstacle detectors; Blind; Mobility aids; Obstacle detection; Smart System; Visually impaired; Power control","","","Institute of Electrical and Electronics Engineers Inc.","","978-147998370-4","","","English","Proc. IEEE Int. Conf. Power Adv. Control Eng., ICPACE","Conference paper","Final","","Scopus","2-s2.0-84961864395"
"Min B.-C.; Steinfeld A.; Dias M.B.","Min, Byung-Cheol (39161762500); Steinfeld, Aaron (7103257672); Dias, M. Bernardine (55664937300)","39161762500; 7103257672; 55664937300","How Would You Describe Assistive Robots to People Who are Blind or Low Vision?","2015","ACM/IEEE International Conference on Human-Robot Interaction","02-05-March-2015","","","91","92","1","3","10.1145/2701973.2702002","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84969134823&doi=10.1145%2f2701973.2702002&partnerID=40&md5=c929ac62fe19659827c8f1b625d13d26","Robotics Institute, Carnegie Mellon University, Pittsburgh, 15213, PA, United States","Min B.-C., Robotics Institute, Carnegie Mellon University, Pittsburgh, 15213, PA, United States; Steinfeld A., Robotics Institute, Carnegie Mellon University, Pittsburgh, 15213, PA, United States; Dias M.B., Robotics Institute, Carnegie Mellon University, Pittsburgh, 15213, PA, United States","Assistive robots can enhance the safety, efficiency, and independence of people who are blind or low vision (B/LV) during urban travel. However, a clear understanding is still lacking in how best to introduce and describe an assistive robot to B/LV persons in a way that facilitates effective human-robot interaction. The goal of this study was to understand how different people would describe an assistive robot to a B/LV traveler. Our preliminary results showed that participants described the robot in a similar order (i.e. robot's appearance, function, and capability in order); however, they had different focuses on their descriptions. This pilot study will lead to better descriptions of assistive robots to B/LV users, supporting more effective interaction in our future real-world deployment works. © 2015 Authors.","Assistive robotics; blind; human-robot interaction; low vision","Human computer interaction; Man machine systems; Robots; Assistive robotics; Assistive robots; blind; Effective interactions; Low vision; Pilot studies; Real world deployment; Urban travels; Human robot interaction","","","IEEE Computer Society","21672148","978-145033318-4","","","English","ACM/IEEE Int. Conf. Hum.-Rob. Interact.","Conference paper","Final","","Scopus","2-s2.0-84969134823"
"Lam T.; Pauhl K.; Ferguson A.; Malik R.N.; Krassioukov A.; Janice J.","Lam, Tania (7202523193); Pauhl, Katherine (37005902700); Ferguson, Amanda (57225442885); Malik, Raza N (56189482900); Krassioukov, Andrei (7003542604); Janice, J. (7102461610)","7202523193; 37005902700; 57225442885; 56189482900; 7003542604; 7102461610","Training with robot-applied resistance in people with motor-incomplete spinal cord injury: Pilot study","2015","Journal of Rehabilitation Research and Development","52","1","","113","130","17","49","10.1682/JRRD.2014.03.0090","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84929317281&doi=10.1682%2fJRRD.2014.03.0090&partnerID=40&md5=98a61d4e183592675fbbaa453f3bbcc4","School of Kinesiology, University of British Columbia, Vancouver, BC, Canada; International Collaboration on Repair Discoveries, University of British Columbia, Vancouver, BC, Canada; NeuroMotion Physical Therapy, Vancouver, BC, Canada; Department of Medicine, Division of Physical Medicine and Rehabilitation, University of British Columbia, Vancouver, BC, Canada; GF Strong Rehabilitation Centre, Vancouver, BC, Canada; Department of Physical Therapy, University of British Columbia, Vancouver, BC, Canada","Lam T., School of Kinesiology, University of British Columbia, Vancouver, BC, Canada, International Collaboration on Repair Discoveries, University of British Columbia, Vancouver, BC, Canada; Pauhl K., School of Kinesiology, University of British Columbia, Vancouver, BC, Canada, International Collaboration on Repair Discoveries, University of British Columbia, Vancouver, BC, Canada; Ferguson A., NeuroMotion Physical Therapy, Vancouver, BC, Canada; Malik R.N., School of Kinesiology, University of British Columbia, Vancouver, BC, Canada, International Collaboration on Repair Discoveries, University of British Columbia, Vancouver, BC, Canada; Krassioukov A., International Collaboration on Repair Discoveries, University of British Columbia, Vancouver, BC, Canada, Department of Medicine, Division of Physical Medicine and Rehabilitation, University of British Columbia, Vancouver, BC, Canada, GF Strong Rehabilitation Centre, Vancouver, BC, Canada; Janice J., International Collaboration on Repair Discoveries, University of British Columbia, Vancouver, BC, Canada, Department of Medicine, Division of Physical Medicine and Rehabilitation, University of British Columbia, Vancouver, BC, Canada, GF Strong Rehabilitation Centre, Vancouver, BC, Canada, Department of Physical Therapy, University of British Columbia, Vancouver, BC, Canada","People with motor-incomplete spinal cord injury (m-iSCI) can recover basic walking function but still have difficulty performing the skilled walking required for everyday environments. We hypothesized that a robotic-based gait rehabilitation strategy founded on principles of motor learning would be a feasible and potentially effective approach for improving skilled walking in people with m-iSCI. Fifteen individuals with chronic (>1 yr) m-iSCI were randomly allocated to body weight-supported treadmill training (BWSTT) with Lokomat-applied resistance (Loko-R) or conventional Lokomat-assisted BWSTT (Control). Training sessions were 45 min, 3 times/week for 3 mo. Tolerance to training was assessed by ratings of perceived exertion and reports of pain/soreness. Overground skilled walking capacity (Spinal Cord Injury-Functional Ambulation Profile [SCI-FAP]), as well as walking speed and distance, were measured at baseline, posttraining, and 1 and 6 mo follow-up. Our results indicate that Loko-R training could be feasibly applied for people with m-iSCI, although participants in Loko-R tended to report higher levels of perceived exertion during training. Participants in the Loko-R group performed significantly better in the SCI-FAP than Control at posttraining and in follow-up assessments. This study provides evidence that Loko-R training is feasible in people with m-iSCI. Furthermore, there is preliminary evidence suggesting that Loko-R may help improve performance in skilled overground walking tasks. © 2015, Rehabilitation Research and Development Service. All rights reserved.","Body weight support; Functional ambulation; Gait training; Lokomat resistance; Motor learning; Motor-incomplete SCI; Robotics; Skilled walking; Spinal cord injury","Adult; Cervical Vertebrae; Double-Blind Method; Exercise Test; Exercise Tolerance; Female; Gait; Humans; Locomotion; Male; Middle Aged; Muscle Strength; Physical Exertion; Pilot Projects; Resistance Training; Robotics; Spinal Cord Injuries; Thoracic Vertebrae; Walking; Anthropometry; Exercise equipment; Patient rehabilitation; Robotics; Body weight supports; Functional ambulation; Gait training; Lokomat; Motor learning; Skilled walking; Spinal cord injuries (SCI); 10 Metre Walk Test; adult; Article; assessment of humans; blurred vision; body weight; clinical article; controlled study; dizziness; double blind procedure; fatigue; female; Functional Ambulation Profile; headache; human; hyperhidrosis; leg pain; low back pain; male; middle aged; motor incomplete spinal cord injury; priority journal; randomized controlled trial; robotics; Six Minute Walk Test; spinal cord injury; training; treadmill exercise; walking speed; Wheeled Mobility Scale; cervical vertebra; classification; devices; exercise; exercise test; exercise tolerance; gait; locomotion; muscle strength; physiology; pilot study; procedures; resistance training; robotics; Spinal Cord Injuries; thoracic vertebra; walking; Personnel training","","","Rehabilitation Research and Development Service","07487711","","JRRDD","26230667","English","J. Rehabil. Res. Dev.","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-84929317281"
"Leung T.-S.; Medioni G.","Leung, Tung-Sing (55613387500); Medioni, Gerard (7006497957)","55613387500; 7006497957","Visual navigation aid for the blind in dynamic environments","2014","IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","","","6910038","579","586","7","51","10.1109/CVPRW.2014.89","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84908510825&doi=10.1109%2fCVPRW.2014.89&partnerID=40&md5=e3e3df97afb4bd59c8c7a48af8d9057f","Computer Science Department, University of Southern California, Los Angeles, 90089, CA, United States","Leung T.-S., Computer Science Department, University of Southern California, Los Angeles, 90089, CA, United States; Medioni G., Computer Science Department, University of Southern California, Los Angeles, 90089, CA, United States","We describe a robust method to estimate egomotion in highly dynamic environments. Our application is a head mounted stereo system designed to help the visually impaired navigate. Instead of computing egomotion from 3D point correspondences in consecutive frames, we propose to find the ground plane, then decompose the 6DoF egomotion into the motion of the ground plane, and a planar motion on the ground plane. The ground plane is estimated at each frame by analysis of the disparity array. Next, we estimate the normal to the ground plane. This is done either from the visual data, or from the IMU reading. We evaluate the results on both synthetic and real scenes, and compare the results of the direct, 6 DoF estimate with our plane-based approach, with and without the IMU. We conclude that the egomotion estimation using this new approach produces significantly better results, both in simulation and on real data sets. © 2014 IEEE.","dynamic environment; visual odometry; visually impaired","Stereo image processing; Dynamic environments; Ego-motion estimation; Point correspondence; Real data sets; Robust methods; Visual Navigation; Visual odometry; Visually impaired; Computer vision","","","IEEE Computer Society","21607508","978-147994309-8; 978-147994309-8","","","English","IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recogn. Workshops","Conference paper","Final","","Scopus","2-s2.0-84908510825"
"Rituerto A.; Fusco G.; James M.C.","Rituerto, Alejandro (36622408100); Fusco, Giovanni (57196680565); James, M. Coughlan (7006567201)","36622408100; 57196680565; 7006567201","Towards a sign-based indoor navigation system for people with visual impairments","2016","ASSETS 2016 - Proceedings of the 18th International ACM SIGACCESS Conference on Computers and Accessibility","","","","287","288","1","23","10.1145/2982142.2982202","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85006791240&doi=10.1145%2f2982142.2982202&partnerID=40&md5=123cb8594b7bec998fa8a759a59f7532","Smith-Kettlewell Eye Research Institute, 2318 Fillmore Street, San Francisco, 94115, CA, United States","Rituerto A., Smith-Kettlewell Eye Research Institute, 2318 Fillmore Street, San Francisco, 94115, CA, United States; Fusco G., Smith-Kettlewell Eye Research Institute, 2318 Fillmore Street, San Francisco, 94115, CA, United States; James M.C., Smith-Kettlewell Eye Research Institute, 2318 Fillmore Street, San Francisco, 94115, CA, United States","Navigation is a challenging task for many travelers with visual impairments. While a variety of GPS-enabled tools can provide wayfinding assistance in outdoor settings, GPS provides no useful localization information indoors. A variety of indoor navigation tools are being developed, but most of them require potentially costly physical infrastructure to be installed and maintained, or else the creation of detailed visual models of the environment. We report development of a new smartphone-based navigation aid, which combines inertial sensing, computer vision and floor plan information to estimate the user's location with no additional physical infrastructure and requiring only the locations of signs relative to the floor plan. A formative study was conducted with three blind volunteer participants demonstrating the feasibility of the approach and highlighting the areas needing improvement.","Blindness; Low vision; Navigation; Wayfinding","Computer vision; Floors; Global positioning system; Navigation; Navigation systems; Smartphones; Transportation; Blindness; Indoor navigation system; Indoor navigation tools; Localization information; Low vision; Visual impairment; Way-finding; Wayfinding assistance; Indoor positioning systems","","","Association for Computing Machinery, Inc","","978-145034124-0","","","English","ASSETS - Proc. Int. ACM SIGACCESS Conf. Comput. Access.","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85006791240"
"Haraszy Z.","Haraszy, Zoltan (24476948300)","24476948300","Hardware and software design of a prototype electronic travel aid for visually impaired","2011","Journal of Electrical and Electronics Engineering","4","2","","41","46","5","0","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-82555188376&partnerID=40&md5=c05b1400843cd1a48519f3e6beca6cf6","Politehnica University of Timisoara, Department of Applied Electronics, Faculty of Electronics and Telecommunications, 300223 Timisoara, Bd. Vasile Parvan 2, Romania","Haraszy Z., Politehnica University of Timisoara, Department of Applied Electronics, Faculty of Electronics and Telecommunications, 300223 Timisoara, Bd. Vasile Parvan 2, Romania","The current paper presents a prototype of an electronic travel aid (ETA) designed for visually impaired people to help them navigate in real outdoor environments. The prototype proposes to guide blind people by implementing a simple Acoustic Virtual Reality (AVR), which uses acoustic signals intended for headphone listening, whose virtual source position suggests the direction of the movement to the desired target The AVR concept is implemented using artificial neural networks on a microcontroller evaluation kit connected to an experimental board, designed and implemented by the author. The experimental board is able to generate the acoustic signals for headphone listening using class D amplifiers. The whole design is validated on a predefined path, where the prototype is able to successfully guide the visually impaired subject from start to destination. The current shortcomings of the ETA are also presented.","Acoustic virtual reality; Artificial neural networks; Electronic travel aid; Man-machine interface; Visually impaired","","Z. Haraszy; Politehnica University of Timisoara, Department of Applied Electronics, Faculty of Electronics and Telecommunications, 300223 Timisoara, Bd. Vasile Parvan 2, Romania; email: zoltan.haraszv@etc.upt.ro","","","20672128","","","","English","J. Electr. Electron. Eng.","Article","Final","","Scopus","2-s2.0-82555188376"
"Kozik R.","Kozik, Rafał (26654240400)","26654240400","Rapid threat detection for stereovision mobility aid system","2011","Advances in Intelligent and Soft Computing","103","","","115","123","8","1","10.1007/978-3-642-23169-8_13","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-80052948045&doi=10.1007%2f978-3-642-23169-8_13&partnerID=40&md5=4286546396f8a316debf53e6e5052aae","Institute of Telecommunications, University of Technology and Life Sciences, Bydgoszcz 85-796, Kaliskiego 7, Poland","Kozik R., Institute of Telecommunications, University of Technology and Life Sciences, Bydgoszcz 85-796, Kaliskiego 7, Poland","The majority of solutions, dedicated for visually impaired persons, typically utilize obstacle detection mechanism without object recognition, therefore in this paper an approach for rapid object/threat detection and identification for the SMAS system (Stereo Vision Mobility Aid System) is proposed. The results obtained during the experiments are promising and proving that this method is suitable to be used as an real-time solution even for relatively slow portable devices. © 2011 Springer-Verlag Berlin Heidelberg.","blind support; computer vision; object detection; sterovision","Computer vision; Handicapped persons; Object recognition; Obstacle detectors; Mobility aids; Object Detection; Obstacle detection; Portable device; Real-time solutions; sterovision; Threat detection; Visually impaired persons; Stereo vision","R. Kozik; Institute of Telecommunications, University of Technology and Life Sciences, Bydgoszcz 85-796, Kaliskiego 7, Poland; email: rafal.kozik@upt.edu.pl","Czachorski T.; Polish Academy of Sciences, Institute of Theoretical and Applied, Informatics, Gliwice, 44-100; Kozielski S.; Stanczyk U.; Silesian University of Technology, Institute of Informatics, Gliwice, 44-100","","18675662","978-364223168-1","","","English","Adv. Intell. Soft Comput.","Article","Final","","Scopus","2-s2.0-80052948045"
"Mattoccia S.; Macrı P.","Mattoccia, Stefano (6505990778); Macrı, Paolo (56426884100)","6505990778; 56426884100","3D glasses as mobility aid for visually impaired people","2015","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8927","","","539","554","15","15","10.1007/978-3-319-16199-0_38","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84928820817&doi=10.1007%2f978-3-319-16199-0_38&partnerID=40&md5=628ff28eb3119715ac88ecb117fb9d89","Department of Computer Science and Engineering (DISI), University of Bologna, Viale Risorgimento 2, Bologna, 40136, Italy","Mattoccia S., Department of Computer Science and Engineering (DISI), University of Bologna, Viale Risorgimento 2, Bologna, 40136, Italy; Macrı P., Department of Computer Science and Engineering (DISI), University of Bologna, Viale Risorgimento 2, Bologna, 40136, Italy","This paper proposes an effective and wearable mobility aid aimed at improving the quality of life of people suffering for visual disabilities by enabling autonomous and safe navigation in unknown environments. Our system relies on dense and accurate depth maps, provided in real-time by a compact stereo vision system mapped into an FPGA, in order to detect obstacles in front of the user and to provide accordingly vibration feedbacks as well as audio information by means of a bone-conductive speakers. Compared to most approaches with similar purposes, even in the current prototype arrangement deployed for testing, our system is extremely compact, lightweight and energy efficient thus enabling hours of safe and autonomous navigation with standard batteries avoiding the need to carry cumbersome devices. Moreover, by conceiving the 3D sensing device as a replacement of standard glasses typically worn by visually impaired people and by using intuitive feedbacks provided by means of lightweight actuators, our system provides an ergonomic and comfortable user interface with a fast learning curve for its effective deployment. This fact has been extensively verified on the field by means of an experimental evaluation, in indoor as well as in outdoor environments, with different users simulating visual impairment including a blind person. © Springer International Publishing Switzerland 2015.","3D; Obstacle Detection; Stereo vision; Visually impaired; Wearable","Computer vision; Energy efficiency; Glass; Obstacle detectors; Stereo image processing; User interfaces; Wearable technology; Autonomous navigation; Experimental evaluation; Lightweight actuators; Obstacle detection; Stereo vision system; Visually impaired; Visually impaired people; Wearable; Stereo vision","","Rother C.; Agapito L.; Bronstein M.M.","Springer Verlag","03029743","978-331916198-3","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-84928820817"
"Manoharan S.; Sankaran P.","Manoharan, Sudha (57217697631); Sankaran, Palani (57191957922)","57217697631; 57191957922","An audio based real time text detection and recognition approach for visually impaired people","2016","Journal of Computational and Theoretical Nanoscience","13","8","","4895","4905","10","1","10.1166/jctn.2016.5363","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84995527340&doi=10.1166%2fjctn.2016.5363&partnerID=40&md5=158a96f9edb45bc404eacf6c61c8673d","Department of Electronics and Communication Engineering, Sudharsan Engineering College, Pudukkottai, Tamilnadu, 622501, India","Manoharan S., Department of Electronics and Communication Engineering, Sudharsan Engineering College, Pudukkottai, Tamilnadu, 622501, India; Sankaran P., Department of Electronics and Communication Engineering, Sudharsan Engineering College, Pudukkottai, Tamilnadu, 622501, India","Text detection and character recognition in natural images are widespread, yet unsolved issues are available in computer vision. In most of the existing techniques, the texts were extracted from restored word image, which depends on the height difference between texts for text recognition. However, the text recognition accuracy using such techniques was low. Moreover, some existing techniques may not produce an audio-based output, so that it is a most challenging task to the visually impaired people. Therefore, this paper proposed an audio based real time text detection and recognition approach with the aim to provide guidance to the visually impaired people. This paper defines the system design and utilized five steps to recognize the characters. In the case of blind people, finding the text region is an essential problem that must be addressed. Because, it cannot be assumed that the learned image includes only characters. At first, the proposed system tries to recognize the characters and provide the audio solutions to the visually impaired people. Initially, image preprocessing is done based on the Gaussian filtering algorithm and histogram equalization. This paper introduces the Gradient Based Superpixel Segmentation (GBSS) algorithm to segment the text portions alone. The texture and shape features of the image are collected based on Convoluted Local Tetra Patterns (CLTrP), Histogram of Oriented Gradient (HOG) with the set of shape features. Then, the optimal features are extracted using the Fuzzy based Particle Swarm Optimization (FPSO) algorithm. Moreover, this paper introduces the Distance based Relevance Vector Machine (DRVM) algorithm to accurately recognize the characters. At last, the system provides an audio output to assist the blind persons for their day-To-day activities. The performance of the proposed method is compared with the existing SVM and RVM approaches, the results show that the proposed method yields higher precision, recall and accuracy than the existing algorithms. © Copyright 2016 American Scientific Publishers All rights reserved.","Convoluted local tetra patterns (CLTrP); Distance based relevance vector machine (DRVM); Fuzzy based particle swarm optimization (FPSO); Support vector machine (SVM); Text detection and character recognition","Air navigation; Computer vision; Convolution; Face recognition; Graphic methods; Image processing; Image segmentation; Motion compensation; Optimization; Particle swarm optimization (PSO); Support vector machines; Vision aids; Convoluted local tetra patterns (CLTrP); Fuzzy based particle swarm optimization (FPSO); Histogram equalizations; Histogram of oriented gradients (HOG); Relevance Vector Machine; Superpixel segmentations; Text detection; Visually impaired people; Character recognition","","","American Scientific Publishers","15461955","","","","English","J. Comput. Theor. Nanosci.","Article","Final","","Scopus","2-s2.0-84995527340"
"Viswanathan K.; Sengupta S.","Viswanathan, Kavitha (57189247305); Sengupta, Sharmila (57189233062)","57189247305; 57189233062","Blind navigation proposal using SONAR","2016","2015 IEEE International Conference on Computer Graphics, Vision and Information Security, CGVIS 2015","","","7449912","151","156","5","3","10.1109/CGVIS.2015.7449912","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84966551152&doi=10.1109%2fCGVIS.2015.7449912&partnerID=40&md5=11d7e60158786d6f4b5f162acfe50c40","Department of Electronics and Telecommunication, Vivekanand Education Society's Institute of Technology, Mumbai, Maharashtra, 400074, India; Department of Computer Engineering, Vivekanand Education Society's Institute of Technology, Mumbai, Maharashtra, 400074, India","Viswanathan K., Department of Electronics and Telecommunication, Vivekanand Education Society's Institute of Technology, Mumbai, Maharashtra, 400074, India; Sengupta S., Department of Computer Engineering, Vivekanand Education Society's Institute of Technology, Mumbai, Maharashtra, 400074, India","We present here the results of our effort to develop a navigation system for the visually impaired using Sound Navigation and Ranging (SONAR). SONAR system has widespread applications in underwater scenarios. Here, we have focused on its use in real-Time detection of an obstacle. Our goal is to create a portable, cost-effective, lightweight, unobtrusive, and unprecedented system for the blind, to improve their movement without human assistance. Satellite navigation based on Global Positioning System (GPS) is not accurate enough to guide pedestrians, especially around cities. The performance of the obstacle avoidance strategy described in the system depends primarily on the SONAR sensors used. The effect of their limitation on the proposed system is presented in detail. Our results indicate that a group of sonar devices, appropriately positioned with respect to one another, together make up a system that is suitable as a navigation aide for the visually impaired. © 2015 IEEE.","obstacle avoidance; SONAR; Visually impaired","Collision avoidance; Computer graphics; Computer vision; Cost effectiveness; Navigation systems; Security of data; Sonar; Tracking (position); Underwater acoustics; Vision aids; Blind navigation; Cost effective; Human assistance; Real-time detection; Satellite navigation; Sonar sensor; Sonar system; Visually impaired; Global positioning system","","","Institute of Electrical and Electronics Engineers Inc.","","978-146737437-8","","","English","IEEE Int. Conf. Comput. Graphics, Vis. Inf. Secur., CGVIS","Conference paper","Final","","Scopus","2-s2.0-84966551152"
"Villamizar L.H.; Gualdron M.; Gonzalez F.; Aceros J.; Rizzo-Sierra C.V.","Villamizar, Luz H. (57193164652); Gualdron, Mauricio (55903390700); Gonzalez, Fabio (56499519700); Aceros, Juan (36796610400); Rizzo-Sierra, Carlos V. (37075794800)","57193164652; 55903390700; 56499519700; 36796610400; 37075794800","A necklace sonar with adjustable scope range for assisting the visually impaired","2013","Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS","","","6609784","1450","1453","3","14","10.1109/EMBC.2013.6609784","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84886514144&doi=10.1109%2fEMBC.2013.6609784&partnerID=40&md5=3442c84c6c86e57fe0f15f4e3d9029b4","Colombian Administrative Department of Science, Technology, and Innovation (Colciencias), Colombia; University of Santander, Bucaramanga, Colombia; Technology Unit of Santander, Bucaramanga, Colombia; School of Engineering, University of North Florida, Jacksonville, FL 32224, United States; School of Industrial Engineering, SIGMMA, St. Thomas University, Bucaramanga, Colombia","Villamizar L.H., Colombian Administrative Department of Science, Technology, and Innovation (Colciencias), Colombia; Gualdron M., University of Santander, Bucaramanga, Colombia; Gonzalez F., Technology Unit of Santander, Bucaramanga, Colombia; Aceros J., School of Engineering, University of North Florida, Jacksonville, FL 32224, United States; Rizzo-Sierra C.V., School of Industrial Engineering, SIGMMA, St. Thomas University, Bucaramanga, Colombia","A sonar based device with tactile feedback was developed to improve the mobility and independence of visually impaired individuals. It features a transceiver/receiver, a potentiometer, a microcontroller, a rechargeable polymer lithium ion battery, and a Nokia Cell phone vibrator. All components are commercially available and housed in a custom acrylic package with 86 mm × 34 mm × 12 mm in dimension, and 120 grms in weight. Additionally, the device features an adjustable detection scheme for user customization of distance range, and a tactile feedback system that avoids interference with auditory sensory information. The device was tested for its navigational efficacy in an artificial indoor environment, and in a live outdoor setting. Ten subjects (9 males and 1 female), with a mean age of 35 years-old (range: 17 to 52) were presented with a series of navigational tasks resulting in considerable reduction of head, shoulder, chest, and arms collisions during their locomotion. We conclude that this device greatly improves the mobility and safety of visually impaired individuals. © 2013 IEEE.","Assistive Technology; Echolocation; Obstacles; Sonar; Visually Impaired","Adolescent; Adult; Blindness; Calibration; Electric Power Supplies; Equipment Design; Female; Glaucoma; Humans; Locomotion; Male; Middle Aged; Retinal Diseases; Sensory Aids; Sound; Ultrasonics; Visually Impaired Persons; Young Adult; Voltage dividers; Assistive technology; Echolocation; Indoor environment; Obstacles; Polymer lithium ion battery; Sensory information; User customizations; Visually impaired; adolescent; adult; blindness; calibration; equipment design; female; glaucoma; human; locomotion; male; middle aged; patient; power supply; Retinal Diseases; sensory aid; sound; ultrasound; young adult; Sonar","","","","1557170X","978-145770216-7","","24109971","English","Proc. Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. EMBS","Conference paper","Final","","Scopus","2-s2.0-84886514144"
"Bulat J.; Glowacz A.","Bulat, Jaroslaw (22033560900); Glowacz, Andrzej (24400778100)","22033560900; 24400778100","Vision-based navigation assistance for visually impaired individuals using general purpose mobile devices","2016","2016 International Conference on Signals and Electronic Systems, ICSES 2016 - Proceedings","","","7593849","189","194","5","6","10.1109/ICSES.2016.7593849","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84995482370&doi=10.1109%2fICSES.2016.7593849&partnerID=40&md5=5d0bf9301c808be68ba5114f07528d1e","AGH University of Science and Technology, Department of Telecommunications, Al. A. Mickiewicza 30, Krakow, 30-059, Poland","Bulat J., AGH University of Science and Technology, Department of Telecommunications, Al. A. Mickiewicza 30, Krakow, 30-059, Poland; Glowacz A., AGH University of Science and Technology, Department of Telecommunications, Al. A. Mickiewicza 30, Krakow, 30-059, Poland","At the present there are some systems offering pedestrian navigation, but not as many are designated for people with visual impairments. The aim of this study is to present a navigation aid which makes use of a stereoscopy video system, OpenCV library, and general purpose mobile devices. The system is based on algorithms for machine image recognition using a stereoscopy camera rig. It reconstructs the surface of the surrounding scene with high accuracy, and is thus able to provide information about potential obstacles. Our experiments show it is possible to create a real-Time solution using a contemporary Android-based mobile phone and two USB cameras. © 2016 IEEE.","3D visual based navigation; Android; navigation assistance for blind; OpenCV; stereovision","Cameras; Image recognition; Mobile devices; Navigation; Stereo image processing; Stereo vision; Android; Navigation aids; OpenCV; Pedestrian navigation; Real time solution; Video systems; Vision based navigation; Visual impairment; Android (operating system)","","AGH University of Science and Technology, Department of Electronics, al. Mickiewicza 30, Krakow; Machowski W.; Stepien J.; AGH University of Science and Technology, Department of Electronics, al. Mickiewicza 30, Krakow","Institute of Electrical and Electronics Engineers Inc.","","978-150902667-8","","","English","Int. Conf. Signals Electron. Syst., ICSES - Proc.","Conference paper","Final","","Scopus","2-s2.0-84995482370"
"Yi C.; Tian Y.","Yi, Chucai (36192382300); Tian, Yingli (16556710700)","36192382300; 16556710700","Assistive text reading from natural scene for blind persons","2015","Mobile Cloud Visual Media Computing: From Interaction to Service","","","","219","241","22","9","10.1007/978-3-319-24702-1_9","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84956768972&doi=10.1007%2f978-3-319-24702-1_9&partnerID=40&md5=b68fe5ff15d562e663ac94f9712476b0","HERE North America, 425 W Randolph St, Chicago, 60606, IL, United States; The City College of New York, Convent Avenue at 138th Street, New York, 10031, NY, United States","Yi C., HERE North America, 425 W Randolph St, Chicago, 60606, IL, United States; Tian Y., The City College of New York, Convent Avenue at 138th Street, New York, 10031, NY, United States","Text information serves as an understandable and comprehensive indicator, which plays a significant role in navigation and recognition in our daily lives. It is very difficult to access this valuable information for blind or visually impaired persons, in particular, in unfamiliar environments. With the development of computer vision technology and smart mobile applications, many assistive systems are developed to help blind or visually impaired persons in their daily lives. This chapter focuses on the methods of text reading from natural scene as well as their applications to assist people who are visually impaired.With the research work on accessibility for the disabled, the assistive text reading technique for the blind is implemented in mobile platform, such as smart phone, tablet, and other wearable device. The popularity and interconnection of mobile devices would provide more low-cost and convenient assistance for blind or visually impaired persons © Springer International Publishing Switzerland 2015.","","Computer programming; Computer science; Assistive system; Assistive text readings; Computer vision technology; Mobile applications; Mobile platform; Text information; Visually impaired persons; Wearable devices; Character recognition","","","Springer International Publishing","","978-331924702-1; 978-331924700-7","","","English","Mob. Cloud Visual Media Computing: From Interact. to Service","Book chapter","Final","","Scopus","2-s2.0-84956768972"
"Silva C.S.; Wimalaratne P.","Silva, Chathurika S. (57194770481); Wimalaratne, Prasad (6507207700)","57194770481; 6507207700","Sensor fusion for visually impaired navigation in constrained spaces","2016","2016 IEEE International Conference on Information and Automation for Sustainability: Interoperable Sustainable Smart Systems for Next Generation, ICIAfS 2016","","","7946537","","","","9","10.1109/ICIAFS.2016.7946537","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85021997397&doi=10.1109%2fICIAFS.2016.7946537&partnerID=40&md5=458b905009d409498ea9816794e3271c","University of Colombo School of Computing, 35 Reid Avenue, Colombo, 07, Sri Lanka","Silva C.S., University of Colombo School of Computing, 35 Reid Avenue, Colombo, 07, Sri Lanka; Wimalaratne P., University of Colombo School of Computing, 35 Reid Avenue, Colombo, 07, Sri Lanka","This work presents a multi-sensor fusion approach for an electronic navigation aid for the blind and visually impaired persons. This approach proposes to intelligently fuse the surrounding information senses via ultrasonic sensors and vision sensors. The intelligent component of the prototype serves in several facets including object detection and recognition. Extended Kalman filter is used to fuse the data emerging from homogeneous sensors and rule-based fusion is used to fuse the data from heterogeneous sensors. Feedback is provided via tactile and audio feedback. Critical obstacles of blind navigation like staircases are recognized by the Hough line detection in image processing. Rotations, which occur due to the body movements in the camera, correct using the fusion of data obtain by the inertial measurement unit which is connected to the camera. The results of the evaluations proved that the use of fusion of multiple homogeneous sensors improve the detection of a particular obstacle and fusion of vision and ultrasonic sensors improve the object detection identification of the obstacles. The current status of the work and the future developments are presented in this paper. © 2016 IEEE.","complementary and competitive sensors; homogeneous and heterogeneous sensor fusion; ultrasonic sensor; vision sensor","Air navigation; Biofeedback; Cameras; Interoperability; Kalman filters; Navigation; Object detection; Object recognition; Sustainable development; Ultrasonic applications; Ultrasonic sensors; Blind and visually impaired; Detection identifications; Electronic navigation; Heterogeneous sensors; Inertial measurement unit; Intelligent components; Object detection and recognition; Vision sensors; Feedback","","","Institute of Electrical and Electronics Engineers Inc.","","978-150906132-7","","","English","IEEE Int. Conf. Inf. Auto. Sustain.: Interoper. Sustain. Smart Syst. Next Gener., ICIAfS","Conference paper","Final","","Scopus","2-s2.0-85021997397"
"Tian Y.","Tian, Yingli (16556710700)","16556710700","RGB-D sensor-based computer vision assistive technology for visually impaired persons","2014","Advances in Computer Vision and Pattern Recognition","67","","","173","194","21","23","10.1007/978-3-319-08651-4_9","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84984878640&doi=10.1007%2f978-3-319-08651-4_9&partnerID=40&md5=8f92b57bb85d3bc9f278d49d9b18f610","Department of Electrical Engineering, The City College of New York, New York, 10031, NY, United States","Tian Y., Department of Electrical Engineering, The City College of New York, New York, 10031, NY, United States","A computer vision-based way finding and navigation aid can improve the mobility of blind and visually impaired people to travel independently. In this chapter, we focus on RGB-D sensor-based computer vision technologies in application to assist blind and visually impaired persons. We first briefly review the existing computer vision based assistive technology for the visually impaired. Then we provide a detailed description of the recent RGB-D sensor based assistive technology to help blind or visually impaired people. Next, we present the prototype system to detect and recognize stairs and pedestrian crosswalks based on RGB-D images. Since both stairs and pedestrian crosswalks are featured by a group of parallel lines, Hough transform is applied to extract the concurrent parallel lines based on the RGB (Red, Green, and Blue) channels. Then, the Depth channel is employed to recognize pedestrian crosswalks and stairs. The detected stairs are further identified as stairs going up (upstairs) and stairs going down (downstairs). The distance between the camera and stairs is also estimated for blind users. The detection and recognition results on our collected datasets demonstrate the effectiveness and efficiency of our developed prototype. We conclude the chapter by the discussion of the future directions. © Springer International Publishing Switzerland 2014.","","","Y. Tian; Department of Electrical Engineering, The City College of New York, New York, 10031, United States; email: ytian@ccny.cuny.edu","","Springer-Verlag London Ltd","21916586","","","","English","Adv. Comput. Vis. Pattern Recognit.","Book chapter","Final","","Scopus","2-s2.0-84984878640"
"Bologna G.; Gomez J.D.; Pun T.","Bologna, Guido (7005522450); Gomez, Juan Diego (55436470000); Pun, Thierry (7005509099)","7005522450; 55436470000; 7005509099","Vision substitution experiments with see ColOr","2013","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","7930 LNCS","PART 1","","83","93","10","1","10.1007/978-3-642-38637-4_9","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84885729935&doi=10.1007%2f978-3-642-38637-4_9&partnerID=40&md5=5970f239bf5a5c69d6d6492fae0b1eea","Computer Vision and Multimedia Lab., University of Geneva, Switzerland","Bologna G., Computer Vision and Multimedia Lab., University of Geneva, Switzerland; Gomez J.D., Computer Vision and Multimedia Lab., University of Geneva, Switzerland; Pun T., Computer Vision and Multimedia Lab., University of Geneva, Switzerland","See ColOr is a mobility aid for visually impaired people that uses the auditory channel to represent portions of captured images in real time. A distinctive feature of the See ColOr interface is the simultaneous coding of colour and depth. Four main modules were developed, in order to replicate a number of mechanisms present in the human visual systems. In this work, we first present the main experiments carried out in the first years of the project; among them : the avoidance of obstacles, the recognition and localization of objects, the detection of edges and the identification of coloured targets. Finally, we introduce new undergoing experiments in Colombia with blind persons, whose purpose is (1) to determine and to touch a target; (2) to navigate and to find a person; and (3) to find particular objects. Preliminary results illustrate encouraging results. © 2013 Springer-Verlag.","3D-vision; colour-depth sonification; human-computer interaction; vision substitution","Color; Computer vision; Experiments; Human computer interaction; Auditory channel; Blind person; First year; Human Visual System; Main module; Mobility aids; Sonifications; Visually impaired people; Handicapped persons","","","","16113349","978-364238636-7","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","","Scopus","2-s2.0-84885729935"
"Fusco G.; Shen H.; Murali V.; Coughlan J.M.","Fusco, Giovanni (57196680565); Shen, Huiying (14523706200); Murali, Vidya (24825244100); Coughlan, James M. (7006567201)","57196680565; 14523706200; 24825244100; 7006567201","Determining a blind pedestrian's location and orientation at traffic intersections","2014","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8547 LNCS","PART 1","","427","432","5","6","10.1007/978-3-319-08596-8_65","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84904162482&doi=10.1007%2f978-3-319-08596-8_65&partnerID=40&md5=a027e496e43fbbbf1746b9b94ef95b43","Smith-Kettlewell Eye Research Institute, San Francisco, CA, United States","Fusco G., Smith-Kettlewell Eye Research Institute, San Francisco, CA, United States; Shen H., Smith-Kettlewell Eye Research Institute, San Francisco, CA, United States; Murali V., Smith-Kettlewell Eye Research Institute, San Francisco, CA, United States; Coughlan J.M., Smith-Kettlewell Eye Research Institute, San Francisco, CA, United States","This paper describes recent progress on Crosswatch, a smartphone-based computer vision system developed by the authors for providing guidance to blind and visually impaired pedestrians at traffic intersections. One of Crosswatch's key capabilities is determining the user's location (with precision much better than what is obtainable by GPS) and orientation relative to the crosswalk markings in the intersection that he/she is currently standing at; this capability will be used to help him/her find important features in the intersection, such as walk lights, pushbuttons and crosswalks, and achieve proper alignment to these features. We report on two new contributions to Crosswatch: (a) experiments with a modified user interface, tested by blind volunteer participants, that makes it easier to acquire intersection images than with previous versions of Crosswatch; and (b) a demonstration of the system's ability to localize the user with precision better than what is obtainable by GPS, as well as an example of its ability to estimate the user's orientation. © 2014 Springer International Publishing.","Assistive Technology; Blindness; Smartphone; Traffic Intersection; Visual Impairment","Computer vision; Crosswalks; User interfaces; Assistive technology; Blind and visually impaired; Blindness; Computer vision system; Important features; Recent progress; Traffic intersections; Visual impairment; Smartphones","","","Springer Verlag","03029743","978-331908595-1","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-84904162482"
"Tapu R.; Mocanu B.; Zaharia T.","Tapu, R. (26424843800); Mocanu, B. (24822846400); Zaharia, T. (6601999900)","26424843800; 24822846400; 6601999900","Real time static/dynamic obstacle detection for visually impaired persons","2014","Digest of Technical Papers - IEEE International Conference on Consumer Electronics","","","6776055","394","395","1","20","10.1109/ICCE.2014.6776055","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84898687259&doi=10.1109%2fICCE.2014.6776055&partnerID=40&md5=8477c214a0f400bfb2afe99785e96dd2","IT/Télécom SudParis, ARTEMIS Department, UMR CNRS 8145 MAP5, Evry, France","Tapu R., IT/Télécom SudParis, ARTEMIS Department, UMR CNRS 8145 MAP5, Evry, France; Mocanu B., IT/Télécom SudParis, ARTEMIS Department, UMR CNRS 8145 MAP5, Evry, France; Zaharia T., IT/Télécom SudParis, ARTEMIS Department, UMR CNRS 8145 MAP5, Evry, France","In this paper we introduce a novel framework for detecting static/moving obstacles in order to assist visually impaired/blind persons to navigate safely. Firstly, a set of interest points is extracted base on an image grid and tracked using the multiscale Lucas - Kanade algorithm. Next, the camera/background motion is determined through a set of homographic transforms, estimated by recursively applying the RANSAC algorithm on the interest point correspondence while other types of movements are identified using an agglomerative clustering technique. Finally, obstacles are classified as urgent/normal based on their distance to the subject and motion vectors orientation. The experimental results performed on various challenging scenes demonstrate that our approach is effective in videos with important camera movement, including noise and low resolution data. © 2014 IEEE.","","Computer vision; Consumer electronics; Obstacle detectors; Agglomerative clustering; Camera movement; Motion Vectors; Obstacle detection; Point correspondence; RANSAC algorithm; Visually impaired; Visually impaired persons; Handicapped persons","","","Institute of Electrical and Electronics Engineers Inc.","0747668X","978-147991291-9","DTPEE","","English","Dig Tech Pap IEEE Int Conf Consum Electron","Conference paper","Final","","Scopus","2-s2.0-84898687259"
"Hans du Buf J.M.; Barroso J.; Rodrigues J.M.F.; Paredes H.; Farrajota M.; Fernandes H.; José J.; Teixeira V.; Saleiro M.","Hans du Buf, J.M. (6505528621); Barroso, João (20435746800); Rodrigues, João M F. (34973296100); Paredes, Hugo (23398113700); Farrajota, Miguel (41361172400); Fernandes, Hugo (35172835800); José, João (39261741400); Teixeira, Victor (57197542636); Saleiro, Mário (39262423000)","6505528621; 20435746800; 34973296100; 23398113700; 41361172400; 35172835800; 39261741400; 57197542636; 39262423000","The SmartVision navigation prototype for blind users","2011","International Journal of Digital Content Technology and its Applications","5","5","","351","361","10","44","10.4156/jdcta.vol5.issue5.39","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-79958050951&doi=10.4156%2fjdcta.vol5.issue5.39&partnerID=40&md5=a876f4c8c7922752ca0ef53dae416f8d","Vision Laboratory - Inst. for Systems and Robotics (ISR), University of the Algarve (FCT and ISE), Faro, Portugal; GECAD, Knowledge Eng. and Decision Support Research Group, Inst. of Engineering of Porto, Portugal; University of Trás-os-Montes and Alto Douro (UTAD), Vila Real, Portugal","Hans du Buf J.M., Vision Laboratory - Inst. for Systems and Robotics (ISR), University of the Algarve (FCT and ISE), Faro, Portugal; Barroso J., GECAD, Knowledge Eng. and Decision Support Research Group, Inst. of Engineering of Porto, Portugal, University of Trás-os-Montes and Alto Douro (UTAD), Vila Real, Portugal; Rodrigues J.M.F., Vision Laboratory - Inst. for Systems and Robotics (ISR), University of the Algarve (FCT and ISE), Faro, Portugal; Paredes H., GECAD, Knowledge Eng. and Decision Support Research Group, Inst. of Engineering of Porto, Portugal, University of Trás-os-Montes and Alto Douro (UTAD), Vila Real, Portugal; Farrajota M., Vision Laboratory - Inst. for Systems and Robotics (ISR), University of the Algarve (FCT and ISE), Faro, Portugal; Fernandes H., University of Trás-os-Montes and Alto Douro (UTAD), Vila Real, Portugal; José J., Vision Laboratory - Inst. for Systems and Robotics (ISR), University of the Algarve (FCT and ISE), Faro, Portugal; Teixeira V., University of Trás-os-Montes and Alto Douro (UTAD), Vila Real, Portugal; Saleiro M., Vision Laboratory - Inst. for Systems and Robotics (ISR), University of the Algarve (FCT and ISE), Faro, Portugal","The goal of the Portuguese project SmartVision: active vision for the blind is to develop a small, portable and cheap yet intelligent and reliable system for assisting the blind and visually impaired while navigating autonomously, both in- and outdoor. In this article we present an overview of the prototype, design issues, and its different modules which integrate GPS and Wi-Fi localisation with a GIS, passive RFID tags, and computer vision. The prototype addresses global navigation for going to some destiny, by following known landmarks stored in the GIS in combination with path optimisation, and local navigation with path and obstacle detection just beyond the reach of the white cane. The system does not replace the white cane but complements it, in order to alert the user to looming hazards. In addition, computer vision is used to identify objects on shelves, for example in a pantry or refrigerator. The user-friendly interface consists of a four-button hand-held box, a vibration actuator in the handle of the white cane, and speech synthesis. In the near future, passive RFID tags will be complemented by active tags for marking navigation landmarks, and speech recognition may complement or substitute the vibration actuator.","Autonomous navigation; Vision aid","Actuators; Computer vision; Cryptography; Obstacle detectors; Patient rehabilitation; Radio frequency identification (RFID); Radio navigation; Speech synthesis; Vision aids; Active tag; Active Vision; Autonomous navigation; Blind users; Design issues; Global navigation; Local navigation; Localisation; Obstacle detection; Optimisations; Passive RFID; Reliable systems; User friendly interface; Visually impaired; White cane; Speech recognition","J. M. Hans du Buf; Vision Laboratory - Inst. for Systems and Robotics (ISR), University of the Algarve (FCT and ISE), Faro, Portugal; email: dubuf@ualg.pt","","","19759339","","","","English","Int. J. Digit. Content Technol. Appl.","Article","Final","","Scopus","2-s2.0-79958050951"
"Costa P.; Fernandes H.; Vasconcelos V.; Coelho P.; Barroso J.; Hadjileontiadis L.","Costa, Paulo (7201895673); Fernandes, Hugo (35172835800); Vasconcelos, Verónica (35794011300); Coelho, Paulo (57128835100); Barroso, João (20435746800); Hadjileontiadis, Leontios (7004037926)","7201895673; 35172835800; 35794011300; 57128835100; 20435746800; 7004037926","Fiducials marks detection to assist visually impaired people navigation","2011","International Journal of Digital Content Technology and its Applications","5","5","","342","350","8","2","10.4156/jdcta.vol5.issue5.38","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-79958058723&doi=10.4156%2fjdcta.vol5.issue5.38&partnerID=40&md5=fc5ffb5fbfcf8a5bbc175099803a9673","Polytechnic Institute of Leiria, Portugal; University of Trás-os-Montes e Alto Douro, Portugal; Polytechnic Institute of Coimbra, Portugal; GECAD - Knowledge Engineering and Decision Support Research Center, Porto, Portugal; Department of Electrical and Computer Engineering, Aristotle University of Thessaloniki, Greece","Costa P., Polytechnic Institute of Leiria, Portugal; Fernandes H., University of Trás-os-Montes e Alto Douro, Portugal; Vasconcelos V., Polytechnic Institute of Coimbra, Portugal; Coelho P., Polytechnic Institute of Leiria, Portugal; Barroso J., University of Trás-os-Montes e Alto Douro, Portugal, GECAD - Knowledge Engineering and Decision Support Research Center, Porto, Portugal; Hadjileontiadis L., Department of Electrical and Computer Engineering, Aristotle University of Thessaloniki, Greece","Assistive technology enables people to achieve independence in the accomplishment of their daily tasks and enhance their quality of life. Visual information is the basis for most navigational tasks, so visually impaired individuals are at disadvantage due to the lack of information or given unsufficient information about their surrounding environment. With the recent advances in inclusive technology it is possible to extend the support given to people with visual disabilities during their mobility. In this context we propose and describe the SmartVision project, whose global objective is to assist visually impaired people in their navigation through unknown indoor and outdoor environments. This paper is focused mainly on the Computer Vision module of the SmartVision prototype, were we propose a new algorithm to recognise fiducials marks suitably placed on sidewalks, revealing to be a promising solution.","Accessibility; Blind navigation; Computer vision; Ensemble empirical mode decomposition","Computer vision; Navigation; Accessibility; Assistive technology; Blind navigation; Daily tasks; Ensemble empirical mode decomposition; Fiducials; Global objective; Outdoor environment; Quality of life; Surrounding environment; Visual disability; Visual information; Visually impaired; Visually impaired people; Handicapped persons","P. Costa; Polytechnic Institute of Leiria, Portugal; email: paulo.costa@ipleiria.pt","","","19759339","","","","English","Int. J. Digit. Content Technol. Appl.","Article","Final","","Scopus","2-s2.0-79958058723"
"Rajakaruna N.; Rathnayake C.; Chan K.Y.; Murray I.","Rajakaruna, Nimali (56245212100); Rathnayake, Chamila (56245309500); Chan, Kit Yan (55647800400); Murray, Iain (55605780042)","56245212100; 56245309500; 55647800400; 55605780042","Image deblurring for navigation systems of vision impaired people using sensor fusion data","2014","IEEE ISSNIP 2014 - 2014 IEEE 9th International Conference on Intelligent Sensors, Sensor Networks and Information Processing, Conference Proceedings","","","6827599","","","","3","10.1109/ISSNIP.2014.6827599","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84903715241&doi=10.1109%2fISSNIP.2014.6827599&partnerID=40&md5=44d8a08d5e3deb3a3592ef5cfd460e20","Department of Electrical and Computer Engineering, Curtin University, Perth, WA, Australia","Rajakaruna N., Department of Electrical and Computer Engineering, Curtin University, Perth, WA, Australia; Rathnayake C., Department of Electrical and Computer Engineering, Curtin University, Perth, WA, Australia; Chan K.Y., Department of Electrical and Computer Engineering, Curtin University, Perth, WA, Australia; Murray I., Department of Electrical and Computer Engineering, Curtin University, Perth, WA, Australia","Image deblurring is a key component in vision based indoor/outdoor navigation systems; as blurring is one of the main causes of poor image quality. When images with poor quality are used for analysis, navigation errors are likely to be generated. For navigation systems, camera movement mainly causes blurring, as the camera is continuously moving by the body movement. This paper proposes a deblurring methodology that takes advantage of the fact that most smartphones are equipped with 3-axis accelerometers and gyroscopes. It uses data of the accelerometer and gyroscope to derive a motion vector calculated from the motion of the smartphone during the image-capturing period. A heuristic method, namely particle swarm optimization, is developed to determine the optimal motion vector, in order to deblur the captured image by reversing the effect of motion. Experimental results indicated that deblurring can be successfully performed using the optimal motion vector and that the deblurred images can be used as a readily approach to object and path identification in vision based navigation systems, especially for blind and vision impaired indoor/outdoor navigation. Also, the performance of proposed method is compared with the commonly used deblurring methods. Better results in term of image quality can be achieved. This experiment aims to identify issues in image quality including low light conditions, low quality images due to movement of the capture device and static and moving obstacles in front of the user in both indoor and outdoor environments. From this information, image-processing techniques to will be identified to assist in object and path edge detection necessary to create a guidance system for those with low vision. © 2014 IEEE.","image deblurring; inertial sensors; particle swarm optimization; vision impaired navigation","Accelerometers; Air navigation; Cameras; Edge detection; Gyroscopes; Heuristic methods; Image quality; Indoor positioning systems; Intelligent control; Navigation systems; Object detection; Particle swarm optimization (PSO); Sensor data fusion; Sensor networks; Smart sensors; Smartphones; 3-axis accelerometer; Image deblurring; Image processing technique; Inertial sensor; Low light conditions; Path identifications; Vision impaired; Vision-based navigation system; Image enhancement","","","IEEE Computer Society","","978-147992843-9","","","English","IEEE ISSNIP - IEEE Int. Conf. Intelligent Sens., Sens. Networks Inf. Process., Conf. Proc.","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-84903715241"
"Weiland J.D.; Parikh N.; Pradeep V.; Medioni G.","Weiland, James D. (7006322846); Parikh, Neha (58387649500); Pradeep, Vivek (57217189069); Medioni, Gerard (7006497957)","7006322846; 58387649500; 57217189069; 7006497957","Smart image processing system for retinal prosthesis","2012","Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS","","","6345928","300","303","3","14","10.1109/EMBC.2012.6345928","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84881040672&doi=10.1109%2fEMBC.2012.6345928&partnerID=40&md5=984d524805f7f3f46fe99c5cf1edc47e","Doheny Eye Institute, University of Southern California, United States; Medtronic, Inc., United States; University of Southern California, United States","Weiland J.D., Doheny Eye Institute, University of Southern California, United States; Parikh N., Medtronic, Inc., United States; Pradeep V., Medtronic, Inc., United States; Medioni G., University of Southern California, United States","Retinal prostheses for the blind have demonstrated the ability to provide the sensation of light in otherwise blind individuals. However, visual task performance in these patients remains poor relative to someone with normal vision. Computer vision algorithms for navigation and object detection were evaluated for their ability to improve task performance. Blind subjects navigating a mobility course had fewer collisions when using a wearable camera system that guided them on a safe path. Subjects using a retinal prosthesis simulator could locate objects more quickly when an object detection algorithm assisted them. Computer vision algorithms can assist retinal prosthesis patients and low-vision patients in general. © 2012 IEEE.","","Algorithms; Blindness; Eye, Artificial; Female; Humans; Image Processing, Computer-Assisted; Male; Retina; Task Performance and Analysis; Visual Prosthesis; Algorithms; Computer vision; Object recognition; Ophthalmology; Blind individuals; Computer vision algorithms; Image processing system; Object Detection; Object detection algorithms; Retinal prosthesis; Task performance; Visual tasks; Wearable cameras; MLCS; MLOWN; algorithm; article; blindness; clinical trial; equipment; female; human; image processing; male; methodology; retina; task performance; visual prosthesis; Prosthetics","","","","1557170X","978-142444119-8","","23365889","English","Proc. Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. EMBS","Conference paper","Final","","Scopus","2-s2.0-84881040672"
"Markus N.; Malik S.; Juhasz Z.; Arató A.","Markus, Norbert (24724972700); Malik, Szabolcs (55331627400); Juhasz, Zoltan (56941610900); Arató, András (36342332200)","24724972700; 55331627400; 56941610900; 36342332200","Accessibility for the blind on an open-source mobile platform: MObile Slate Talker (MOST) for Android","2012","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","7383 LNCS","PART 2","","599","606","7","9","10.1007/978-3-642-31534-3_88","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84864825248&doi=10.1007%2f978-3-642-31534-3_88&partnerID=40&md5=0005109b0775386f507814733b19062f","Laboratory of Speech Technology for Rehabilitation, Institute for Particle and Nuclear Physics, Hungarian Academy of Sciences, Budapest, Hungary; Dept. of Electrical Engineering and Information Systems, University of Pannonia, Veszprem, Hungary","Markus N., Laboratory of Speech Technology for Rehabilitation, Institute for Particle and Nuclear Physics, Hungarian Academy of Sciences, Budapest, Hungary; Malik S., Laboratory of Speech Technology for Rehabilitation, Institute for Particle and Nuclear Physics, Hungarian Academy of Sciences, Budapest, Hungary; Juhasz Z., Dept. of Electrical Engineering and Information Systems, University of Pannonia, Veszprem, Hungary; Arató A., Laboratory of Speech Technology for Rehabilitation, Institute for Particle and Nuclear Physics, Hungarian Academy of Sciences, Budapest, Hungary","As Android handsets keep flooding the shops in a wide range of prices and capabilities, many of the blind community turn their attention to this emerging alternative, especially because of a plethora of cheaper models offered. Earlier, accessibility experts only recommended Android phones sporting an inbuilt QWERTY keyboard, as the touch-screen support had then been in an embryotic state. Since late 2011, with Android 4.X (ICS), this has changed. However, most handsets on the market today -especially the cheaper ones- ship with a pre-ICS Android version. This means that their visually impaired users won't be able to enjoy the latest accessibility innovations. Porting MObile SlateTalker to Android has been aimed at filling this accessibility gap with a low-cost solution, regarding the special needs of our target audience: the elderly, persons with minimal tech skills and active Braille users. © 2012 Springer-Verlag.","(e)Accessibility; (e)Aging and Gerontechnology; Android; Assistive Technology; Blind People; Braille; Mobility; Usability and Ergonomics","Carrier mobility; Ergonomics; Handicapped persons; Telephone sets; Touch screens; (e)Accessibility; Android; Assistive technology; Blind people; Braille; Gerontechnology; Robots","","","","16113349","978-364231533-6","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","","Scopus","2-s2.0-84864825248"
"De Sousa E Silva J.; Silva C.; Marcelino L.; Ferreira R.; Pereira A.","De Sousa E Silva, João (55481208600); Silva, Catarina (8770080300); Marcelino, Luís (6602919691); Ferreira, Rui (57199948941); Pereira, António (7402230199)","55481208600; 8770080300; 6602919691; 57199948941; 7402230199","Assistive mobile software for public transportation","2011","UBICOMM 2011 - 5th International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies; PECES 2011 - 3rd International Workshop on Pervasive Computing in Embedded Systems","","","","309","313","4","7","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84881650729&partnerID=40&md5=c09403beafb20aaf9635cf9a845406a2","Computer Science Communication and Research Centre, School of Technology and Management, Polytechnic Institute of Leiria, 2411-901 Leiria, Morro do Lena, Alto do Vieiro, Portugal","De Sousa E Silva J., Computer Science Communication and Research Centre, School of Technology and Management, Polytechnic Institute of Leiria, 2411-901 Leiria, Morro do Lena, Alto do Vieiro, Portugal; Silva C., Computer Science Communication and Research Centre, School of Technology and Management, Polytechnic Institute of Leiria, 2411-901 Leiria, Morro do Lena, Alto do Vieiro, Portugal; Marcelino L., Computer Science Communication and Research Centre, School of Technology and Management, Polytechnic Institute of Leiria, 2411-901 Leiria, Morro do Lena, Alto do Vieiro, Portugal; Ferreira R., Computer Science Communication and Research Centre, School of Technology and Management, Polytechnic Institute of Leiria, 2411-901 Leiria, Morro do Lena, Alto do Vieiro, Portugal; Pereira A., Computer Science Communication and Research Centre, School of Technology and Management, Polytechnic Institute of Leiria, 2411-901 Leiria, Morro do Lena, Alto do Vieiro, Portugal","The need of mobility on public transport for persons with visual impairment is mandatory. While traveling on a public transport, the simple ability to know the current location is almost impossible for such persons. To overcome this hurdle, we developed an assistive application that can alert its user to the proximity of all public transportation stops, giving emphasis to the chosen final stop. The application is adjustable to any transportation system and is particularly relevant to use in public transports that do not have any audio system available. The developed prototype runs on an Android OS device equipped with Global Positioning System (GPS). To ensure the highest possible level of reliability and to make it predictable to users, the application's architecture is free of as much dependencies as possible. Therefore, only GPS, or other localization mechanism, is required. The interface was designed to be suitable not only for talkback (Android's inbuilt screen-reader) aimed at blind users, but also for people with low vision that can still use their sight to check the screen. Thus, it was meant to be graphically simple and unobtrusive. It was tested by visual impaired persons leading to the conclusion that it demonstrates an existing need, and opens a new perspective in public transportation's accessibility.","Accessibility; Android; Assistive software; Mobility; Public transportation","Bus transportation; Carrier mobility; Embedded systems; Mass transportation; Robots; Ubiquitous computing; Accessibility; Android; Assistive applications; Assistive softwares; Public transportation; Transportation system; Visual impaired persons; Visual impairment; Handicapped persons","","","","","978-161208171-7","","","English","UBICOMM - Int. Conf. Mob. Ubiquitous Comput., Syst., Serv. Technol.; PECES - Int. Workshop Pervasive Comput. Embedded Syst.","Conference paper","Final","","Scopus","2-s2.0-84881650729"
"Fernandes H.; Costa P.; Paredes H.; Filipe V.; Barroso J.","Fernandes, Hugo (35172835800); Costa, Paulo (7201895673); Paredes, Hugo (23398113700); Filipe, Vítor (6507061487); Barroso, João (20435746800)","35172835800; 7201895673; 23398113700; 6507061487; 20435746800","Integrating computer vision object recognition with location based services for the blind","2014","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8515 LNCS","PART 3","","493","500","7","13","10.1007/978-3-319-07446-7_48","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84903473236&doi=10.1007%2f978-3-319-07446-7_48&partnerID=40&md5=61f95ad398bae3d11dfad1700ce77ea3","INESC-TEC, UTAD - University of Trás-os-Montes e Alto Douro, Vila Real, Portugal; School of Technology and Management, Polytechnic Institute of Leiria, Leiria, Portugal","Fernandes H., INESC-TEC, UTAD - University of Trás-os-Montes e Alto Douro, Vila Real, Portugal; Costa P., School of Technology and Management, Polytechnic Institute of Leiria, Leiria, Portugal; Paredes H., INESC-TEC, UTAD - University of Trás-os-Montes e Alto Douro, Vila Real, Portugal; Filipe V., INESC-TEC, UTAD - University of Trás-os-Montes e Alto Douro, Vila Real, Portugal; Barroso J., INESC-TEC, UTAD - University of Trás-os-Montes e Alto Douro, Vila Real, Portugal","The task of moving from one place to another is a difficult challenge that involves obstacle avoidance, staying on street walks, finding doors, knowing the current location and keeping on track through the desired path. Nowadays, navigation systems are widely used to find the correct path, or the quickest, between two places. While assistive technology has contributed to the improvement of the quality of life of people with disabilities, people with visual impairment still face enormous limitations in terms of their mobility. In recent years, several approaches have been made to create systems that allow seamless tracking and navigation both in indoor and outdoor environments. However there is still an enormous lack of availability of information that can be used to assist the navigation of users with visual impairments as well as a lack of sufficient precision in terms of the estimation of the user's location. Blavigator is a navigation system designed to help users with visual impairments. In a known location, the use of object recognition algorithms can provide contextual feedback to the user and even serve as a validator to the positioning module and geographic information system of a navigation system for the visually impaired. This paper proposes a method where the use of computer vision algorithms validate the outputs of the positioning system of the Blavigator prototype. © 2014 Springer International Publishing.","blind; computer vision; location-based; navigation; object recognition; services","Computer vision; Geographic information systems; Human computer interaction; Navigation; Navigation systems; Object recognition; Vision aids; Assistive technology; blind; Computer vision algorithms; Location based; Object recognition algorithm; Outdoor environment; People with disabilities; services; Location based services","","","Springer Verlag","03029743","978-331907445-0","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-84903473236"
"Gomez J.D.; Bologna G.; Pun T.","Gomez, Juan Diego (55436470000); Bologna, Guido (7005522450); Pun, Thierry (7005509099)","55436470000; 7005522450; 7005509099","Non-visual-cueing-based sensing and understanding of nearby entities in aided navigation","2012","ASSETS'12 - Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility","","","","213","214","1","2","10.1145/2384916.2384959","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84869748017&doi=10.1145%2f2384916.2384959&partnerID=40&md5=07d2aaf63d82fdc31f5aac0f9c39e6a2","Computer Science Department Geneva, University of Geneva, Switzerland","Gomez J.D., Computer Science Department Geneva, University of Geneva, Switzerland; Bologna G., Computer Science Department Geneva, University of Geneva, Switzerland; Pun T., Computer Science Department Geneva, University of Geneva, Switzerland","Exploring unfamiliar environments is a challenging task in which additionally, unsighted individuals frequently fail to gain perception of obstacles and make serendipitous discoveries. This is because the mental depiction of the context is drastically lessened due to the absence of visual information. It is still not clear in neuroscience, whether stimuli elicited by visual cueing can be replicated by other senses cross-model transfer). In the practice, however, everyone recognizes a key, whether it is felt in a pocket or seen on a table. We present a context-aware aid system for the blind that merges three levels of assistance enhancing the intelligibility of the nearby entities: an exploration module to help gain awareness of the surrounding context, an alerting method for warning the user when a stumble is likely, and, finally, a recognition engine that retrieves natural targets previously learned. Practical experiences with our system show that in the absence of visual cueing, the audio and haptic trajectory playback coupled with computer-vision methods is a promising approach to depict dynamic information of the immediate environment.","Assistance; Context-aware aid; Visual cueing; Visually impaired","Assistance; Context-Aware; Dynamic information; Immediate environment; Natural targets; Practical experience; Recognition engines; Serendipitous discovery; Visual cueing; Visual information; Visually impaired; Computer vision","J.D. Gomez; Computer Science Department Geneva, University of Geneva, Switzerland; email: juan.gomez@unige.ch","","","","978-145031321-6","","","English","ASSETS - Proc. Int. ACM SIGACCESS Conf. Comput. Accessibility","Conference paper","Final","","Scopus","2-s2.0-84869748017"
"","","","2013 International Conference on Advances in Materials Science and Manufacturing Technology, AMSMT 2013","2013","Advanced Materials Research","765-767","","","","","","0","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84885053329&partnerID=40&md5=8ffba02db999572ba07fdba109db429a","","","The proceedings contain 716 papers. The special focus in this conference is on Manufacturing Technology, Mechatronics, Measurement, Instruments, Electronics and Information Technology, Materials Science, Control and Monitoring, Environmental Engineering. The topics include: Research and design on intelligent decision support system frame for vehicle maintenance; research on gap analysis and compensation of rollers in printing machine; experimental research on the stiffness of air spring; risk assessment of sawing machine oriented to mechanical safety design; review of mechanical structure of force actuator for optical astronomical telescope; fuzzy reliability allocation of CNC grinding machine tools; the contrast for offset method of catmull-Clark subdivision surface; development of virtual test desk for dynamic wheel model; stepping experiment analysis of a biaxial piezoelectric actuated stage; the effect of surface defect on EHL point contact; fault analysis and modification of spray painting robot travelling system; the effects of medium and heavy plate manufacture plan on mill pacing; spindle thermal analysis of CNC milling machine; the study on circular tool of ultrasonic cutting; structure design of a novel biaxial piezoelectric actuated stage; research progress of the laser and low-pressure medium jet composite processing; development of adjustment and durability test machine based on automobile steering column; the application of virtual design on feeding device of precoated laminating machine; driving system design of lifting permanent magnet; innovative design of new type of mobility scooter; application of the CNC system based on PMAC in laser forming machine; web-based design of inject-molding tool and technical service system; development and application of virtual manufacturing technology; preparation of paper-based printing plate and its ink-water balance performance analysis; research on factors influencing the flexographic platemaking quality; study on structural complexity of transmission case production line; research of CAPP system based on the solid edge; structure optimization design for the bed of gear hobbing machine; experimental study of the bolted joint surface contact angle; research on visualization technology of mechanical virtual test system; study on normalizing process for 100mm E40 heavy plate; study on the influence of process parameters on curling springback; application of LED in marine fishery; study on principle of spacecraft surface charge and discharge; research on the design method of single rotor eddy current retarder; submarine maximum speed's influence on the effect of evading an incoming torpedo; application of operations research for production scheduling; study on lost circulation in an extended horizontal well in Cuba; drilling fluid technology for extended-reach horizontal wells in Cuba; research on safety management to promote mechanical safety; 3D CAD model retrieval based on feature fusion; simulation and test of ASAC dynamic characteristic based on ABAQUS; the vibration analysis of five degrees of freedom man-vehicle-road coupled model; finite element analysis of automobile transmission shaft; a cutting force model for face milling operation; a visual EFSM modeling system for protocol testing; dynamics simulation and analysis of planetary reducer; a fast adaptive transmit power and bit allocation in OFDM system; study on UAV path planning simulation; finite element optimization of electric field structure in electrospinning; new method of discernibility matrix formation; an improved BP algorithm and its application; delay-dependent stochastic stability analysis of singular hybrid system; visual modeling and simulation of plant leaves; research on roles modeling of intelligent force based on ontology; an error-reduced ADI-FDTD algorithm in Debye media; research on the interactive technology of reservoir 3D display; volume rendering of 3D borehole data based on GPU; modified iterative sphere decoding algorithm in LTE system; parallel ant system based on OpenMP; a new genetic algorithm using order coding and a novel genetic operator; application of UML to obviating operation modeling; image registration based on improved ant colony algorithm; improved ant colony algorithm for optimal solution TSP; a smoothing Newton method for solving absolute value equations; credibility evaluation of simulation models; a triangle division based point matching for image registration; cheeger cut model for the balanced data classification problem; existence of high energy solutions for Kirchhoff-type equations; indoor localization algorithm research in intelligent lighting; solutions for a class of the exponential Diophantine equation; solutions for a class of the higher Diophantine equation; an algorithm of global path planning applied for rapid prototyping; a blind watermarking algorithm for 2D CAD drawings based on SVD; modeling and proof of a file system based on micro-kernel; cloud computing environment data mining storage management design; teaching reform and practice of computer control system; efficiency of dynamic GOP length in video stream; analysis of the remote emergency care in wartime; network video retrieval technology analysis and research; the research and implementation of XBRL data service platform; research on PHP agile development framework; design for the township e-government system based on wap and web; zero-copy implementation in the Linux operating system; key technologies of cloud computing model; an implementation of ant colony optimization algorithm using java; a method for key updating of IBE with wildcards; IT-based framework for food security supervision; building quotient cube with mapreduce in hadoop; image retrieval based on color-statistic feature; anti-missile network edge importance research; study on meta-data of network information resources; node importance evaluation of anti-missile networks; migrating resources and rebuilding educational apps store in metro age; the medical infusion alarm system based on the smart phone platform; design of the examination inspect system based on the network; virtual reality and virtual reality system components; a semantic parsing model applied into search engine; user behavior analysis based on Gn interface of GPRS network; network attack characteristics of automatic data extraction technology; a modeling study of sensor data; a spatial data security model under the cloud environment; a development model of semantic web application; LSSVM-based social spam detection model; design of SSL VPN system based on RBAC access; video call traffic identification based on Bayesian model; critical operations selecting method; a survey of cloud workflow; analyzing of dynamic characteristics for discrete S-PCNN; hierarchy model of application framework; construct a credit evaluation framework of e-commerce; improving listening skills of ESL students through web-based technology; an adaptive topic crawler for electronic public opinion; information security risk assessment on complex information system; context-aware personalization recommendation of web services; component based method for usability testing of a website; application of data mining in the guidance of sports training; study on application of honeypot in campus net security; personalized search ranking based on semantic tag; keywords extraction based on text classification; analysis framework of freemodbus; a TPA based efficient non-repudiation scheme for cloud storage; function and database design of storage management system; multitask similarity cluster; research on affecting college graduates' employment selection based on AHP; SQL query algorithm based on restricted Chinese; a car locator system for indoor parking areas; a SDN-controlled ECMP QoS solution for data networks; Qos constraints-based energy-efficient model for IP networks; a new dynamic protocol analysis model; design and implementation of general spacecraft control language; automatic gain control of EDFAs using DSP; application of fuzzy control in sewage treatment system; research on a new access control technology; the design of servo loop bandwidth based on fuzzy control theory; SVM inverse control method to nonlinear systems; study on vibration controller based on adaptive inverse method; design of an automatic magnetic balance system; research on general scheme for curtain wall cleaning robot; the application of EEMD to fault diagnosis of rolling bearing; research on monitoring system for factory aquaculture; research on the ventilator noise sound intensity measure; a new method for heavy-haul train operation monitoring; cause analysis and diagnosis of unbalanced vibration of rotor; design of remote monitoring system based on configuration software; study on lane detection based on computer vision; research on cloud platform for wind turbine fault diagnosis; measuring technology and its application for an SI model; a study on space debris observation system using radio telescope; technology research on intelligent infrared gas analyzer; the research of blood glucose monitoring system based on biosensor; automatic analysis of mineral's abundance; analysis on the static features of flame images; research on fuel cell power generation system model with grid; a design of electronic laser harp based on SCM; realization of electronic scoring device on ball games; power system simulation data description and conversion; response status evaluation of sensitive equipment to voltage sag; a design of FPGA-based interpolator on digital integration; design of intelligent system for LED lighting based on PWM.","","","","","","10226680","978-303785798-4","","","English","Adv. Mater. Res.","Conference review","Final","","Scopus","2-s2.0-84885053329"
"","","","ASSETS 2016 - Proceedings of the 18th International ACM SIGACCESS Conference on Computers and Accessibility","2016","ASSETS 2016 - Proceedings of the 18th International ACM SIGACCESS Conference on Computers and Accessibility","","","","","","356","0","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85006711897&partnerID=40&md5=9dcca92c2545b54ace6ef3341c538c72","","","The proceedings contain 70 papers. The topics discussed include: a personalizable mobile sound detector app design for deaf and hard-of-hearing users; improving real-time captioning experiences for deaf and hard of hearing students; SlidePacer: a presentation delivery tool for instructors of deaf and hard of hearing students; comparing tactile, auditory, and visual assembly error-feedback for workers with cognitive impairments; gesture-based interaction for individuals with developmental disabilities in India; customizable 3d printed tactile maps as interactive overlays; LucentMaps: 3D printed audiovisual tactile maps for blind and visually impaired people; the cost of turning heads: a comparison of a head-worn display to a smartphone for supporting persons with aphasia in conversation; a computer vision-based system for stride length estimation using a mobile phone camera; uncovering challenges and opportunities for 3D printing assistive technology with physical therapists; WeAllWalk: an annotated data set of inertial sensor time series from blind walkers; supporting orientation of people with visual impairment: analysis of large scale usage data; and an evaluation of SingleTapBraille keyboard: a text entry method that utilizes Braille patterns on touchscreen devices.","","","","","Association for Computing Machinery, Inc","","978-145034124-0","","","English","ASSETS - Proc. Int. ACM SIGACCESS Conf. Comput. Access.","Conference review","Final","","Scopus","2-s2.0-85006711897"
"Hu F.; Zhu Z.; Zhang J.","Hu, Feng (56612688000); Zhu, Zhigang (7404803571); Zhang, Jianting (57203378062)","56612688000; 7404803571; 57203378062","Mobile panoramic vision for assisting the blind via indexing and localization","2015","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8927","","","600","614","14","18","10.1007/978-3-319-16199-0_42","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84928801781&doi=10.1007%2f978-3-319-16199-0_42&partnerID=40&md5=d054a28167125629c125da41ae7d4463","Department of Computer Science, The Graduate Center, CUNY, New York, 10016, NY, United States; Department of Computer Science, The City College of New York, New York, 10031, NY, United States","Hu F., Department of Computer Science, The Graduate Center, CUNY, New York, 10016, NY, United States; Zhu Z., Department of Computer Science, The Graduate Center, CUNY, New York, 10016, NY, United States, Department of Computer Science, The City College of New York, New York, 10031, NY, United States; Zhang J., Department of Computer Science, The Graduate Center, CUNY, New York, 10016, NY, United States, Department of Computer Science, The City College of New York, New York, 10031, NY, United States","In this paper, we propose a first-person localization and navigation system for helping blind and visually-impaired people navigate in indoor environments. The system consists of a mobile vision front end with a portable panoramic lens mounted on a smart phone, and a remote GPU-enabled server. Compact and effective omnidirectional video features are extracted and represented in the smart phone front end, and then transmitted to the server, where the features of an input image or a short video clip are used to search a database of an indoor environment via image-based indexing to find both the location and the orientation of the current view. To deal with the high computational cost in searching a large database for a realistic navigation application, data parallelism and task parallelism properties are identified in database indexing, and computation is accelerated by using multi-core CPUs and GPUs. Experiments on synthetic data and real data are carried out to demonstrate the capacity of the proposed system, with respect to real-time response and robustness. © Springer International Publishing Switzerland 2015.","Blind navigation; Cloud computing; Mobile computing; Panoramic vision","Cloud computing; Computer vision; Database systems; Indexing (of information); Mobile computing; Navigation systems; Program processors; Smartphones; Telephone sets; Blind and visually impaired; Blind navigation; Computational costs; Database indexing; Indoor environment; Localization and navigation systems; Panoramic vision; Real time response; Cellular telephone systems","","Rother C.; Agapito L.; Bronstein M.M.","Springer Verlag","03029743","978-331916198-3","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-84928801781"
"Nguyen J.S.; Nguyen T.N.; Tran Y.; Su S.W.; Craig A.; Nguyen H.T.","Nguyen, Jordan S. (23390217400); Nguyen, Tuan Nghia (35183742900); Tran, Yvonne (56283966500); Su, Steven W. (7402029883); Craig, Ashley (56962719100); Nguyen, Hung T. (7403322521)","23390217400; 35183742900; 56283966500; 7402029883; 56962719100; 7403322521","Real-time performance of a hands-free semi-autonomous wheelchair system using a combination of stereoscopic and spherical vision","2012","Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS","","","6346612","3069","3072","3","6","10.1109/EMBC.2012.6346612","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84882981858&doi=10.1109%2fEMBC.2012.6346612&partnerID=40&md5=76fca2263e06c17eb6b5f627e62f386d","Faculty of Engineering and Information Technology, University of Technology, Sydney, Broadway, NSW 2007, Australia; Rehabilitation Studies Unit, Faculty of Medicine at University of Sydney, Australia","Nguyen J.S., Faculty of Engineering and Information Technology, University of Technology, Sydney, Broadway, NSW 2007, Australia; Nguyen T.N., Faculty of Engineering and Information Technology, University of Technology, Sydney, Broadway, NSW 2007, Australia; Tran Y., Faculty of Engineering and Information Technology, University of Technology, Sydney, Broadway, NSW 2007, Australia; Su S.W., Faculty of Engineering and Information Technology, University of Technology, Sydney, Broadway, NSW 2007, Australia; Craig A., Rehabilitation Studies Unit, Faculty of Medicine at University of Sydney, Australia; Nguyen H.T., Faculty of Engineering and Information Technology, University of Technology, Sydney, Broadway, NSW 2007, Australia","This paper is concerned with the operational performance of a semi-autonomous wheelchair system named TIM (Thought-controlled Intelligent Machine), which uses cameras in a system configuration modeled on the vision system of a horse. This new camera configuration utilizes stereoscopic vision for 3-Dimensional (3D) depth perception and mapping ahead of the wheelchair, combined with a spherical camera system for 360-degrees of monocular vision. The unique combination allows for static components of an unknown environment to be mapped and any surrounding dynamic obstacles to be detected, during real-time autonomous navigation, minimizing blind-spots and preventing accidental collisions with people or obstacles. Combining this vision system with a shared control strategy provides intelligent assistive guidance during wheelchair navigation, and can accompany any hands-free wheelchair control technology for people with severe physical disability. Testing of this system in crowded dynamic environments has displayed the feasibility and real-time performance of this system when assisting hands-free control technologies, in this case being a proof-of-concept brain-computer interface (BCI). © 2012 IEEE.","","Algorithms; Brain-Computer Interfaces; Disabled Persons; Equipment Design; Humans; Vision Disparity; Vision, Ocular; Wheelchairs; Brain computer interface; Cameras; Collision avoidance; Computer vision; Depth perception; 3-dimensional; Assistive; Autonomous navigation; Brain-computer interfaces (BCI); Camera configuration; Camera systems; Control technologies; Dynamic environments; Hands-free; Intelligent machine; Monocular vision; Operational performance; Physical disability; Proof of concept; Real time performance; Shared control; Stereoscopic vision; System configurations; Unknown environments; Vision systems; Wheelchair control; Wheelchair navigation; algorithm; article; brain computer interface; depth perception; disabled person; equipment design; human; physiology; vision; wheelchair; Wheelchairs","J.S. Nguyen; Faculty of Engineering and Information Technology, University of Technology, Sydney, Broadway, NSW 2007, Australia; email: Jordan.Nguyen@uts.edu.au","","","1557170X","978-142444119-8","","23366573","English","Proc. Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. EMBS","Conference paper","Final","","Scopus","2-s2.0-84882981858"
"Rajakaruna N.; Rathnayake C.; Abhayasinghe N.; Murray I.","Rajakaruna, Nimali (56245212100); Rathnayake, Chamila (56245309500); Abhayasinghe, Nimsiri (55820519200); Murray, Iain (55605780042)","56245212100; 56245309500; 55820519200; 55605780042","Inertial data based deblurring for vision impaired navigation","2014","IPIN 2014 - 2014 International Conference on Indoor Positioning and Indoor Navigation","","","7275510","416","420","4","2","10.1109/IPIN.2014.7275510","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84988268683&doi=10.1109%2fIPIN.2014.7275510&partnerID=40&md5=8696d9edc17df1a89232ff499ece56c7","Department of Electrical and Computer Engineering, Curtin University, Perth, WA, Australia","Rajakaruna N., Department of Electrical and Computer Engineering, Curtin University, Perth, WA, Australia; Rathnayake C., Department of Electrical and Computer Engineering, Curtin University, Perth, WA, Australia; Abhayasinghe N., Department of Electrical and Computer Engineering, Curtin University, Perth, WA, Australia; Murray I., Department of Electrical and Computer Engineering, Curtin University, Perth, WA, Australia","Image stabilization is very important in vision based indoor/outdoor navigation systems. Blurring is one main cause of poor image quality, which can be caused by a movement of the camera at the time of taking the image, a movement of objects in front, atmospheric turbulence or out-of-focus. Out of these factors, camera movement is dominant in navigation systems as the camera is continuously moving. This paper presents the preliminary results of deblurring performed using point spread function (PSF) computed using synchronized inertial sensor data. It uses data of the accelerometer and gyroscope to derive a motion vector calculated from the motion of the smartphone during the image capturing period. This motion vector is applied to the captured image so that the effect of motion is reversed during the debrurring process. This work is a part of an indoor navigation project that aims to assist people with vision impairment. Image processing form a significant part of the proposed system and as such clearly defined edges are essential for path and obstruction identification. Different deblurring methods are compared for their performance in reversing the effect of camera movement. Results indicated that deblurring can be successfully performed using the motion vector and that the resulting images can be used as a readily approach to object and path identification in vision based navigation systems, especially for blind and vision impaired indoor/outdoor navigation. The paper also proposes a novel deblurring algorithm that uses PSF computed for different portions of the image to deblur that portion of the image. © 2014 IEEE.","deblurring; Image stabilization; inertial sensors; vision impaired navigation","Air navigation; Atmospheric turbulence; Cameras; Image compression; Image enhancement; Indoor positioning systems; Inertial navigation systems; Navigation systems; Optical transfer function; Stabilization; Deblurring; Deblurring algorithms; Image stabilization; In-door navigations; Inertial sensor; Path identifications; Vision impaired; Vision-based navigation system; Image processing","","","Institute of Electrical and Electronics Engineers Inc.","","978-146738054-6","","","English","IPIN - Int. Conf. Indoor Position. Indoor Navig.","Conference paper","Final","","Scopus","2-s2.0-84988268683"
"Yu X.; Ganz A.","Yu, Xunyi (35761615200); Ganz, Aura (7005921941)","35761615200; 7005921941","Audible vision for the blind and visually impaired in indoor open spaces","2012","Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS","","","6347143","5110","5113","3","5","10.1109/EMBC.2012.6347143","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84880950367&doi=10.1109%2fEMBC.2012.6347143&partnerID=40&md5=1f99ffda1d809b19698fec6b53c26da2","Electrical and Computer Engineering Department, University of Massachusetts, Amherst, United States","Yu X., Electrical and Computer Engineering Department, University of Massachusetts, Amherst, United States; Ganz A., Electrical and Computer Engineering Department, University of Massachusetts, Amherst, United States","In this paper we introduce Audible Vision, a system that can help blind and visually impaired users navigate in large indoor open spaces. The system uses computer vision to estimate the location and orientation of the user, and enables the user to perceive his/her relative position to a landmark through 3D audio. Testing shows that Audible Vision can work reliably in real-life ever-changing environment crowded with people. © 2012 IEEE.","","Acoustic Stimulation; Artificial Intelligence; Biofeedback, Psychology; Blindness; Equipment Design; Equipment Failure Analysis; Humans; Imaging, Three-Dimensional; Sensory Aids; Therapy, Computer-Assisted; Visually Impaired Persons; Engineering; Industrial engineering; 3D audio; Blind and visually impaired users; Relative positions; System use; Visually impaired; MLCS; MLOWN; article; artificial intelligence; auditory stimulation; blindness; computer assisted therapy; equipment; equipment design; equipment failure; human; patient; psychophysiology; sensory aid; three dimensional imaging; Computer vision","","","","1557170X","978-142444119-8","","23367078","English","Proc. Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. EMBS","Conference paper","Final","","Scopus","2-s2.0-84880950367"
"Yelamarthi K.; Laubhan K.","Yelamarthi, Kumar (19640818800); Laubhan, Kevin (56401818600)","19640818800; 56401818600","Navigation assistive system for the blind using a portable depth sensor","2015","IEEE International Conference on Electro Information Technology","2015-June","","7293328","112","116","4","5","10.1109/EIT.2015.7293328","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84975764571&doi=10.1109%2fEIT.2015.7293328&partnerID=40&md5=ba5fe1169f334ffcba32a7eeb70a3570","School of Engineering and Technology, Central Michigan University, Mt Pleasant, 48859, MI, United States","Yelamarthi K., School of Engineering and Technology, Central Michigan University, Mt Pleasant, 48859, MI, United States; Laubhan K., School of Engineering and Technology, Central Michigan University, Mt Pleasant, 48859, MI, United States","The lightweight and low-cost 3-dimensional depth sensors have gained much attention in the computer vision and robotics community. While its performance has been proven successful in the robotics community, these sensors have not been utilized successfully for many assistive devices. Leveraging on this gap, this paper presents the design, implementation, and preliminary evaluation of a haptic feedback system for the blind using 3-D depth sensors. The proposed portable system interprets the visual scene using the depth sensor, converts it into distance map, processes, and evaluates this information using a tablet computer. In addition, it provides haptic cues to the user through an array of vibration motors woven into the gloves. Throughout preliminary evaluation, this system has shown to successfully identify, track, and present closest objects, closest humans, multiple humans, and perform real-time distance measurements. © 2015 IEEE.","blind; depth sensor; haptic feedback; navigation assistance","Computer vision; Haptic interfaces; Robotics; Assistive devices; Assistive system; blind; Depth sensors; Haptic feedback systems; Haptic feedbacks; Tablet computer; Vibration motor; Robots","","","IEEE Computer Society","21540357","978-147998802-0","","","English","IEEE Int. Conf. Electro Inform. Technol.","Conference paper","Final","","Scopus","2-s2.0-84975764571"
"Ortigosa N.; Morillas S.","Ortigosa, Nuria (36171206500); Morillas, Samuel (8871892000)","36171206500; 8871892000","Fuzzy free path detection from disparity maps by using least-squares fitting to a plane","2014","Journal of Intelligent and Robotic Systems: Theory and Applications","75","2","","313","330","17","4","10.1007/s10846-013-9997-1","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84905593710&doi=10.1007%2fs10846-013-9997-1&partnerID=40&md5=383b803aa8c044f02cf0a2725f79b631","I.U. Matemática Pura y Aplicada, Universidad Politécnica de Valencia, 46022 Valencia, Camino de Vera s/n, Spain","Ortigosa N., I.U. Matemática Pura y Aplicada, Universidad Politécnica de Valencia, 46022 Valencia, Camino de Vera s/n, Spain; Morillas S., I.U. Matemática Pura y Aplicada, Universidad Politécnica de Valencia, 46022 Valencia, Camino de Vera s/n, Spain","A method to detect obstacle-free paths in real-time which works as part of a cognitive navigation aid system for visually impaired people is proposed. It is based on the analysis of disparity maps obtained from a stereo vision system which is carried by the blind user. The presented detection method consists of a fuzzy logic system that assigns a certainty to be part of a free path to each group of pixels, depending on the parameters of a planar-model fitting. We also present experimental results on different real outdoor scenarios showing that our method is the most reliable in the sense that it minimizes the false positives rate. © 2013 Springer Science+Business Media Dordrecht.","Assisted navigation; Free path detection; Fuzzy logic","Computer vision; Fuzzy logic; Stereo image processing; Stereo vision; Assisted navigations; Detection methods; Free path; Fuzzy logic system; Least-squares fittings; Navigation aids; Stereo vision system; Visually impaired people; Human rehabilitation equipment","N. Ortigosa; I.U. Matemática Pura y Aplicada, Universidad Politécnica de Valencia, 46022 Valencia, Camino de Vera s/n, Spain; email: nuorar@upvnet.upv.es","","Kluwer Academic Publishers","09210296","","JIRSE","","English","J Intell Rob Syst Theor Appl","Article","Final","","Scopus","2-s2.0-84905593710"
"Kumar A.; Patra R.; Mahadevappa M.; Mukhopadhyay J.; Majumdar A.K.","Kumar, Amit (57206266783); Patra, Rusha (36994018100); Mahadevappa, M. (6604071152); Mukhopadhyay, J. (58722066100); Majumdar, A.K. (36765250600)","57206266783; 36994018100; 6604071152; 58722066100; 36765250600","An embedded system for aiding navigation of visually impaired persons","2013","Current Science","104","3","","302","306","4","5","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84874705247&partnerID=40&md5=9b394e3f745b2629a9e6b4e1d88744c7","Checktronix India Pvt Ltd, Chennai 600 010, India; Department of Electrical Engineering, Indian Institute of Technology, Kharagpur 721 302, India; School of Medical Science and Technology, Indian Institute of Technology, Kharagpur 721 302, India; Department of Computer Science and Engineering, Indian Institute of Technology, Kharagpur 721 302, India","Kumar A., Checktronix India Pvt Ltd, Chennai 600 010, India; Patra R., Department of Electrical Engineering, Indian Institute of Technology, Kharagpur 721 302, India; Mahadevappa M., School of Medical Science and Technology, Indian Institute of Technology, Kharagpur 721 302, India; Mukhopadhyay J., Department of Computer Science and Engineering, Indian Institute of Technology, Kharagpur 721 302, India; Majumdar A.K., Department of Computer Science and Engineering, Indian Institute of Technology, Kharagpur 721 302, India","Visually impaired individuals find navigation difficult as they often lack the much needed information for bypassing obstacles and hazards in their path. In order to help blind people navigate safely and quickly, an obstacle detection system using ultrasonic sensors and USB camera-based visual navigation has been considered. The proposed system detects obstacles up to 300 cm via sonar and sends audio feedback to inform the person about their location. In addition, a USB webcam is connected with eBox 2300™ Embedded System for capturing the field-of-view of the user, for finding the properties of the obstacle in particular, in the context of this work, locating a human being. Identification of human presence is based on face detection and cloth texture analysis. The major constraints for these algorithms to run on the Embedded System are small image frame (160 × 120) having reduced faces, limited memory and very less processing time available to achieve real-time image-processing requirements. Prototype of an electronic travel aid device has been developed and experimentally verified on blind-folded persons to analyse the device performance in a laboratory set-up.","","","M. Mahadevappa; School of Medical Science and Technology, Indian Institute of Technology, Kharagpur 721 302, India; email: mmaha2@smst.iitkgp.ernet.in","","","00113891","","CUSCA","","English","Curr. Sci.","Article","Final","","Scopus","2-s2.0-84874705247"
"Wang S.; Pan H.; Zhang C.; Tian Y.","Wang, Shuihua (36544650700); Pan, Hangrong (55938230100); Zhang, Chenyang (56097483900); Tian, Yingli (16556710700)","36544650700; 55938230100; 56097483900; 16556710700","RGB-D image-based detection of stairs, pedestrian crosswalks and traffic signs","2014","Journal of Visual Communication and Image Representation","25","2","","263","272","9","137","10.1016/j.jvcir.2013.11.005","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84891295297&doi=10.1016%2fj.jvcir.2013.11.005&partnerID=40&md5=fbafbc0a8979a1f8a32e8e33f6aaa446","Department of Electrical Engineering, City College of New York, City University of New York, New York, NY 10016, United States","Wang S., Department of Electrical Engineering, City College of New York, City University of New York, New York, NY 10016, United States; Pan H., Department of Electrical Engineering, City College of New York, City University of New York, New York, NY 10016, United States; Zhang C., Department of Electrical Engineering, City College of New York, City University of New York, New York, NY 10016, United States; Tian Y., Department of Electrical Engineering, City College of New York, City University of New York, New York, NY 10016, United States","A computer vision-based wayfinding and navigation aid can improve the mobility of blind and visually impaired people to travel independently. In this paper, we develop a new framework to detect and recognize stairs, pedestrian crosswalks, and traffic signals based on RGB-D (Red, Green, Blue, and Depth) images. Since both stairs and pedestrian crosswalks are featured by a group of parallel lines, we first apply Hough transform to extract the concurrent parallel lines based on the RGB (Red, Green, and Blue) channels. Then, the Depth channel is employed to recognize pedestrian crosswalks and stairs. The detected stairs are further identified as stairs going up (upstairs) and stairs going down (downstairs). The distance between the camera and stairs is also estimated for blind users. Furthermore, the traffic signs of pedestrian crosswalks are recognized. The detection and recognition results on our collected datasets demonstrate the effectiveness and efficiency of our proposed framework. © 2013 Elsevier Inc. All rights reserved.","Blind; Computer vision; Object recognition; Portable assistance; RGB-D camera; Scene recognition; Visually impaired; Wayfinding and navigation","Cameras; Computer vision; Crosswalks; Hough transforms; Human rehabilitation equipment; Object recognition; Traffic signals; Blind; Portable assistance; Rgb-d cameras; Scene recognition; Visually impaired; Wayfinding and navigations; Stairs","Y. Tian; Department of Electrical Engineering, City College of New York, City University of New York, New York, NY 10016, United States; email: ytian@ccny.cuny.edu","","","10959076","","JVCRE","","English","J Visual Commun Image Represent","Article","Final","","Scopus","2-s2.0-84891295297"
"Sivan S.; Darsan G.","Sivan, Shankar (57191370271); Darsan, Gopu (56943377800)","57191370271; 56943377800","Computer vision based assistive technology for blind and visually impaired people","2016","ACM International Conference Proceeding Series","06-08-July-2016","","2967923","","","","21","10.1145/2967878.2967923","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84989223335&doi=10.1145%2f2967878.2967923&partnerID=40&md5=362e245fa974e56a14456c914d7bf4f4","Sree Buddha College of Engineering, Pattoor, Alappuzha, Kerala, India","Sivan S., Sree Buddha College of Engineering, Pattoor, Alappuzha, Kerala, India; Darsan G., Sree Buddha College of Engineering, Pattoor, Alappuzha, Kerala, India","The computer vision based assistive technology for the blind and visually impaired is a developing area. The assistive technology helps the visually impaired by providing them with a greater independence. By enabling them with their day-to-day activities like indoor and outdoor navigation, obstacle detection, locating the doors and lost objects, etc. Even though different assistive technologies are available for the blind, most of them have complex designs which are developed for a specific purpose and are expensive for the commercial production. Rather than depending on a traditional white cane, the blind and visually impaired people can make use of the cheaper assistive device proposed in this paper. The proposed system incorporates several assistance features in a device which will be an asset for them according to their needs. © 2016 ACM.","Assistive technology; Computer vision; Electronic travel aids; Image processing; Wearable systems","Computer vision; Image processing; Obstacle detectors; Vision aids; Assistive technology; Blind and visually impaired; Commercial productions; Electronic travel aidss; Obstacle detection; Outdoor navigation; Visually impaired; Wearable systems; Wearable technology","","","Association for Computing Machinery","","978-145034179-0","","","English","ACM Int. Conf. Proc. Ser.","Conference paper","Final","","Scopus","2-s2.0-84989223335"
"","","","IEEE SSCI 2014 - 2014 IEEE Symposium Series on Computational Intelligence - CIR2AT 2014: 2014 IEEE Symposium on Computational Intelligence in Robotic Rehabilitation and Assistive Technologies, Proceedings","2014","IEEE SSCI 2014 - 2014 IEEE Symposium Series on Computational Intelligence - CIR2AT 2014: 2014 IEEE Symposium on Computational Intelligence in Robotic Rehabilitation and Assistive Technologies, Proceedings","","","","62","","","0","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84949088404&partnerID=40&md5=5e73b72c8387fdd987d6e40394094169","","","The proceedings contain 8 papers. The topics discussed include: virtuNav: a virtual reality indoor navigation simulator with haptic and audio feedback for the visually impaired; a guidance robot for the visually impaired: system description and velocity reference generation; assistive mobile manipulation for self-care tasks around the head; a novel approach of prosthetic arm control using computer vision, biosignals, and motion capture; tactile pitch feedback system for deaf-blind or hearing impaired persons -singing accuracy of hearing persons under conditions of added noise; spasticity assessment system for elbow flexors/extensors: healthy pilot study; robotic agents used to help teach social skills to individuals with autism: the fourth generation; and encouraging specific intervention motions via a robotic system for rehabilitation of hand function.","","","","","Institute of Electrical and Electronics Engineers Inc.","","978-147994474-3","","","English","IEEE SSCI - IEEE Symp. Ser Comput. Intell. - CIRAT: IEEE Symp. Comput. Intell. Rob. Rehabil. Assist. Technol., Proc.","Conference review","Final","","Scopus","2-s2.0-84949088404"
"Mocanu B.; Tapu R.; Zaharia T.","Mocanu, Bogdan (24822846400); Tapu, Ruxandra (26424843800); Zaharia, Titus (6601999900)","24822846400; 26424843800; 6601999900","An outdoor cognition system integrated on a regular smartphone device","2016","2015 E-Health and Bioengineering Conference, EHB 2015","","","7391375","","","","1","10.1109/EHB.2015.7391375","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84978044481&doi=10.1109%2fEHB.2015.7391375&partnerID=40&md5=783790d1ece632ea8b0df0373a022752","Telecommunication Department, Faculty of ETTI, University Politehnica of Bucharest, Romania; ARTEMIS Department, Institut Mines-Télécom, TélécomSudParis, UMR CNRS MAP5 8145, Evry, France","Mocanu B., Telecommunication Department, Faculty of ETTI, University Politehnica of Bucharest, Romania, ARTEMIS Department, Institut Mines-Télécom, TélécomSudParis, UMR CNRS MAP5 8145, Evry, France; Tapu R., Telecommunication Department, Faculty of ETTI, University Politehnica of Bucharest, Romania, ARTEMIS Department, Institut Mines-Télécom, TélécomSudParis, UMR CNRS MAP5 8145, Evry, France; Zaharia T., ARTEMIS Department, Institut Mines-Télécom, TélécomSudParis, UMR CNRS MAP5 8145, Evry, France","In this paper we introduce an assistive device dedicated to visual impaired / blind people completely integrated on a regular smartphone. The framework is designed to detect and localize static and dynamic obstacle during user navigation. We start by selecting a reduced and relevant set of FAST interest points based on a regular grid and Harris-Laplacian operator. Then, we construct a global image representation using VLAD (Vector of Locally Aggregated Descriptor) that is further whitened using PCA (Principal Component Analysis). At the end the image patch is fed to a SVM (Support Vector Machine) system that uses a statistical procedure to distinguish between different types of obstacles. © 2015 IEEE.","obstacle classification; visually impaired assistive device; VLAD descriptor","Fire fighting equipment; Mathematical operators; Signal encoding; Smartphones; Support vector machines; Descriptors; Image representations; Laplacian operator; Obstacle classification; PCA (principal component analysis); Static and dynamic obstacles; SVM(support vector machine); Visually impaired assistive devices; Principal component analysis","","","Institute of Electrical and Electronics Engineers Inc.","","978-146737545-0","","","English","E-Health Bioeng. Conf., EHB","Conference paper","Final","","Scopus","2-s2.0-84978044481"
"Dong H.; Ganz A.","Dong, Hao (56594108700); Ganz, Aura (7005921941)","56594108700; 7005921941","Automatic generation of indoor navigation instructions for blind users using a user-centric graph","2014","2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC 2014","","","6943737","902","905","3","5","10.1109/EMBC.2014.6943737","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84929484014&doi=10.1109%2fEMBC.2014.6943737&partnerID=40&md5=1c1be3c7ca7ea1a528fe2ce227e35430","Electrical and Computer Engineering, University of Massachusetts, Amherst, 01003, MA, United States","Dong H., Electrical and Computer Engineering, University of Massachusetts, Amherst, 01003, MA, United States; Ganz A., Electrical and Computer Engineering, University of Massachusetts, Amherst, 01003, MA, United States","The complexity and diversity of indoor environments brings significant challenges to automatic generation of navigation instructions for blind and visually impaired users. Unlike generation of navigation instructions for robots, we need to take into account the blind users wayfinding ability. In this paper we introduce a user-centric graph based solution for cane users that takes into account the blind users cognitive ability as well as the user's mobility patterns. We introduce the principles of generating the graph and the algorithm used to automatically generate the navigation instructions using this graph. We successfully tested the efficiency of the instruction generation algorithm, the correctness of the generated paths, and the quality of the navigation instructions. Blindfolded sighted users were successful in navigating through a three-story building. © 2014 IEEE.","","Activities of Daily Living; Algorithms; Blindness; Dependent Ambulation; Humans; Sensory Aids; Visually Impaired Persons; Graphic methods; Human rehabilitation equipment; Indoor positioning systems; Navigation; Automatic Generation; Blind and visually impaired users; Cognitive ability; Graph-based solution; In-door navigations; Indoor environment; Instruction generations; Mobility pattern; algorithm; blindness; daily life activity; human; sensory aid; visually impaired person; walking difficulty; Robots","","","Institute of Electrical and Electronics Engineers Inc.","","978-142447929-0","","25570105","English","Annu. Int. Conf. IEEE Eng. Med. Biol. Soc., EMBC","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-84929484014"
"Sherine S.; Priya S.","Sherine, Sebastian (57196356412); Priya, S. (52664062300)","57196356412; 52664062300","Text detection and recognition from images as an aid to blind persons accessing unfamiliar environments","2015","ARPN Journal of Engineering and Applied Sciences","10","17","","7559","7564","5","4","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84943395196&partnerID=40&md5=49003f04057508e4d08b05cb6683aae0","Department of Computer Engineering, Model Engineering College, Thrikkakara, Ernakulam, Kerala, India","Sherine S., Department of Computer Engineering, Model Engineering College, Thrikkakara, Ernakulam, Kerala, India; Priya S., Department of Computer Engineering, Model Engineering College, Thrikkakara, Ernakulam, Kerala, India","Independent travel is a well known challenge for blind or visually impaired persons. The text reading algorithm has proved to be robust in many kinds of real-world scenarios, including indoor and outdoor places with a wide variety of text appearance due to different writing styles, fonts, colors, sizes, textures and layouts, as well as the presence of geometrical distortions, partial occlusions, and different shooting angles that may cause deformed text. In this paper, we propose a method to detect panels and to recognize the information inside them. The proposal extracts local descriptors at some interest key points after applying color segmentation. Then, images are represented as a bag of visual words (BOVW) and classified using support vector machines. Finally, text detection and recognition method is applied on those images where a panel has been detected, in order to automatically read and save the information depicted in the panels. A language model partly based on a dynamic dictionary is also used. © 2006-2015 Asian Research Publishing Network (ARPN).","Bag of visual words; Image processing; Segmentation; Text detection; Text recognition","","","","Asian Research Publishing Network","18196608","","","","English","ARPN J. Eng. Appl. Sci.","Article","Final","","Scopus","2-s2.0-84943395196"
"Alghamdi S.; Van Schyndel R.; Khalil I.","Alghamdi, Saleh (57527490200); Van Schyndel, Ron (24538197700); Khalil, Ibrahim (24830581000)","57527490200; 24538197700; 24830581000","Safe trajectory estimation at a pedestrian crossing to assist visually impaired people","2012","Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS","","","6347144","5114","5117","3","6","10.1109/EMBC.2012.6347144","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84880927295&doi=10.1109%2fEMBC.2012.6347144&partnerID=40&md5=f48d10a6ddd56dc31f86077ebc7118ae","School of Computer Science and Information Technology, RMIT University, Melbourne, Australia","Alghamdi S., School of Computer Science and Information Technology, RMIT University, Melbourne, Australia; Van Schyndel R., School of Computer Science and Information Technology, RMIT University, Melbourne, Australia; Khalil I., School of Computer Science and Information Technology, RMIT University, Melbourne, Australia","The aim of this paper is to present a service for blind and people with low vision to assist them to cross the street independently. The presented approach provides the user with significant information such as detection of pedestrian crossing signal from any point of view, when the pedestrian crossing signal light is green, the detection of dynamic and fixed obstacles, predictions of the movement of fellow pedestrians and information on objects which may intersect his path. Our approach is based on capturing multiple frames using a depth camera which is attached to a user's headgear. Currently a testbed system is built on a helmet and is connected to a laptop in the user's backpack. In this paper, we discussed efficiency of using Speeded-Up Robust Features (SURF) algorithm for object recognition for purposes of blind people assistance. The system predicts the movement of objects of interest to provide the user with information on the safest path to navigate and information on the surrounding area. Evaluation of this approach on real sequence video frames provides 90% of human detection and more than 80% for recognition of other related objects. © 2012 IEEE.","","Acoustic Stimulation; Artificial Intelligence; Biofeedback, Psychology; Blindness; Equipment Design; Equipment Failure Analysis; Humans; Imaging, Three-Dimensional; Sensory Aids; Therapy, Computer-Assisted; Visually Impaired Persons; Crosswalks; Footbridges; Laptop computers; Object recognition; Signal detection; Blind people; Depth camera; Fixed obstacles; Human detection; Low vision; Multiple-frame; Signal light; Test bed systems; Trajectory estimation; Video frame; Visually impaired people; MLCS; MLOWN; article; artificial intelligence; auditory stimulation; blindness; computer assisted therapy; equipment; equipment design; equipment failure; human; patient; psychophysiology; sensory aid; three dimensional imaging; Handicapped persons","S. Alghamdi; School of Computer Science and Information Technology, RMIT University, Melbourne, Australia; email: S3299407@student.rmit.edu.au","","","1557170X","978-142444119-8","","23367079","English","Proc. Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. EMBS","Conference paper","Final","","Scopus","2-s2.0-84880927295"
"Chen K.; Plaza-Leiva V.; Min B.-C.; Steinfeld A.; Dias M.B.","Chen, Kangwei (57189049048); Plaza-Leiva, Victoria (26654963300); Min, Byung-Cheol (39161762500); Steinfeld, Aaron (7103257672); Dias, Mary Bernardine (55664937300)","57189049048; 26654963300; 39161762500; 7103257672; 55664937300","NavCue: Context immersive navigation assistance for blind travelers","2016","ACM/IEEE International Conference on Human-Robot Interaction","2016-April","","7451855","559","","","3","10.1109/HRI.2016.7451855","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84964907398&doi=10.1109%2fHRI.2016.7451855&partnerID=40&md5=edebde5877d333992d03c96518ed25aa","The Robotics Institute, Carnegie Mellon University, Pittsburgh, 15213, PA, United States; Andalucia Tech Dpto., Universidad de Malaga, Ingenieria de Sistemas y Automatica, Malaga, 29071, Spain; Department of Computer and Information Technology, Purdue University, West Lafayette, 47907, IN, United States","Chen K., The Robotics Institute, Carnegie Mellon University, Pittsburgh, 15213, PA, United States; Plaza-Leiva V., The Robotics Institute, Carnegie Mellon University, Pittsburgh, 15213, PA, United States, Andalucia Tech Dpto., Universidad de Malaga, Ingenieria de Sistemas y Automatica, Malaga, 29071, Spain; Min B.-C., Department of Computer and Information Technology, Purdue University, West Lafayette, 47907, IN, United States; Steinfeld A., The Robotics Institute, Carnegie Mellon University, Pittsburgh, 15213, PA, United States; Dias M.B., The Robotics Institute, Carnegie Mellon University, Pittsburgh, 15213, PA, United States","Research in assistive systems for travelers who are blind/low vision (B/LV) has been largely focused on basic map information. We present NavCue, an intelligent system module for providing rich, multi-sensory, context-based information using speech guidance and robot physical gestures. This approach is motivated by our previous user studies with people who are blind or low vision. This rich information should enhance user location awareness and confidence when traveling through unfamiliar locations. © 2016 IEEE.","Assistive robots; Blind and low vision; Context immersive navigation; Human-robot interaction","Intelligent robots; Intelligent systems; Man machine systems; Robots; Assistive robots; Assistive system; Context-based information; Immersive; Low vision; Multi-Sensory; User location; User study; Human robot interaction","","","IEEE Computer Society","21672148","978-146738370-7","","","English","ACM/IEEE Int. Conf. Hum.-Rob. Interact.","Conference paper","Final","","Scopus","2-s2.0-84964907398"
"Duarte K.; Cecilio J.; Furtado P.","Duarte, Karen (56594754200); Cecilio, Jose (26530830100); Furtado, Pedro (35612898700)","56594754200; 26530830100; 35612898700","Overview of assistive technologies for the blind: Navigation and shopping","2014","2014 13th International Conference on Control Automation Robotics and Vision, ICARCV 2014","","","7064611","1929","1934","5","7","10.1109/ICARCV.2014.7064611","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84988292056&doi=10.1109%2fICARCV.2014.7064611&partnerID=40&md5=3843b7866fd75fc6886eed5f00b6f4b9","Department of Informatics Engineering, University of Coimbra, Portugal","Duarte K., Department of Informatics Engineering, University of Coimbra, Portugal; Cecilio J., Department of Informatics Engineering, University of Coimbra, Portugal; Furtado P., Department of Informatics Engineering, University of Coimbra, Portugal","One of the major driving forces behind technology development is the establishment of equality among everyone. It is intended that the strategic use of technology complements competencies or diminished capacities, improving self confidence and autonomy. Blind and partially sighted people often deal with this inequality in the performance of tasks or daily activities. The presented work reviews some of the proposed technologies to assist visually impaired people in common activities, specifically navigation and going shopping. © 2014 IEEE.","","Computer vision; Robotics; Assistive technology; Daily activity; Driving forces; Technology development; Visually impaired people; Robots","","","Institute of Electrical and Electronics Engineers Inc.","","978-147995199-4","","","English","Int. Conf. Control Autom. Rob. Vis., ICARCV","Conference paper","Final","","Scopus","2-s2.0-84988292056"
"Zeb A.; Ullah S.; Rabbi I.","Zeb, Aurang (57215357058); Ullah, Sehat (56071344100); Rabbi, Ihsan (24776443500)","57215357058; 56071344100; 24776443500","Indoor vision-based auditory assistance for blind people in semi controlled environments","2015","2014 4th International Conference on Image Processing Theory, Tools and Applications, IPTA 2014","","","7001996","","","","20","10.1109/IPTA.2014.7001996","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84921722496&doi=10.1109%2fIPTA.2014.7001996&partnerID=40&md5=afe3d4725903fd6d905775380a97defb","Department of Computer Science and IT, University of Malakand, Pakistan","Zeb A., Department of Computer Science and IT, University of Malakand, Pakistan; Ullah S., Department of Computer Science and IT, University of Malakand, Pakistan; Rabbi I., Department of Computer Science and IT, University of Malakand, Pakistan","This paper presents an indoor auditory navigation system for blind and visually impaired people using computer vision based approach. In our approach, fiducial markers augmented with audio information are placed in the environment. The blind user holds a webcam attached to the system in his/her hand and navigates through the environment. The audio assistance is provided to the user whenever a particular marker is detected by the camera and thus enables him/her to independently navigate in the environment. The proposed navigation system provides two types of guidance (1) free mode guidance (2) targeted mode guidance. In the free mode guidance, the user navigates freely in the prepared environment and the system informs him/her about his/her current position through the audio information about the surrounding based on the detected marker. In the targeted mode guidance the user is guided from a source (current) position to his/her desired destination in the environment through the shortest path. Five blind users evaluated the system (both modes) in a semi controlled environment. The results obtained from the experiments indicate that the proposed system is successful in assisting blind users inside a huge and complex building. In addition the solution is low cost and gives high accuracy rate. © 2014 IEEE.","Audio assistance; Computer vision; Fiducial markers; Indoor navigation; Visually impaired; Way finding","Computer vision; Indoor positioning systems; Navigation systems; Audio assistance; Fiducial marker; In-door navigations; Visually impaired; Way finding; Air navigation","","Djemal K.","Institute of Electrical and Electronics Engineers Inc.","","978-147996461-1","","","English","Int. Conf. Image Process. Theory, Tools Appl., IPTA","Conference paper","Final","","Scopus","2-s2.0-84921722496"
"Bhowmick A.; Prakash S.; Bhagat R.; Prasad V.; Hazarika S.M.","Bhowmick, Alexy (38360941000); Prakash, Saurabh (56424960600); Bhagat, Rukmani (56425621900); Prasad, Vijay (56905178700); Hazarika, Shyamanta M. (24343762400)","38360941000; 56424960600; 56425621900; 56905178700; 24343762400","IntelliNavi : Navigation for blind based on kinect and machine learning","2014","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8875","","","172","183","11","23","10.1007/978-3-319-13365-2_16","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84911942280&doi=10.1007%2f978-3-319-13365-2_16&partnerID=40&md5=002080202a5859cb46ec5e076998e467","School of Technology, Assam Don Bosco University, Guwahati, Assam, India; School of Engineering, Tezpur University, Tezpur, Assam, India","Bhowmick A., School of Technology, Assam Don Bosco University, Guwahati, Assam, India; Prakash S., School of Technology, Assam Don Bosco University, Guwahati, Assam, India; Bhagat R., School of Technology, Assam Don Bosco University, Guwahati, Assam, India; Prasad V., School of Technology, Assam Don Bosco University, Guwahati, Assam, India; Hazarika S.M., School of Engineering, Tezpur University, Tezpur, Assam, India","This paper presents a wearable navigation assistive system for the blind and the visually impaired built with off-the-shelf technology. Microsoft Kinect’s on board depth sensor is used to extract Red, Green, Blue and Depth (RGB-D) data of the indoor environment. Speeded-Up Robust Features (SURF) and Bag-of-Visual-Words (BOVW) model is used to extract features and reduce generic indoor object detection into a machine learning problem. A Support Vector Machine classifier is used to classify scene objects and obstacles to issue critical real-time information to the user through an external aid (earphone) for safe navigation. We performed a user-study with blind-fold users to measure the efficiency of the overall framework. ©Springer International Publishing Switzerland 2014.","Blind; Kinect; Machine learning; Navigation systems; Object recognition; RGB-D","Classification (of information); Learning systems; Machine learning; Navigation systems; Object detection; Support vector machines; Wearable technology; Bag-of-visual-words; Blind; Kinect; Machine learning problem; Off-the-shelf technologies; Real-time information; Speeded up robust features; Support vector machine classifiers; Object recognition","","","Springer Verlag","03029743","","","","English","Lect. Notes Comput. Sci.","Article","Final","","Scopus","2-s2.0-84911942280"
"Lee Y.H.; Medioni G.","Lee, Young Hoon (58839377100); Medioni, Gérard (7006497957)","58839377100; 7006497957","RGB-D camera based wearable navigation system for the visually impaired","2016","Computer Vision and Image Understanding","149","","","3","20","17","107","10.1016/j.cviu.2016.03.019","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84963963845&doi=10.1016%2fj.cviu.2016.03.019&partnerID=40&md5=283a1503063f09fb23f7cb27c238a8ed","Institute for Robotics and Intelligent Systems, University of Southern California, Los Angeles, CA, United States","Lee Y.H., Institute for Robotics and Intelligent Systems, University of Southern California, Los Angeles, CA, United States; Medioni G., Institute for Robotics and Intelligent Systems, University of Southern California, Los Angeles, CA, United States","In this paper, a novel wearable RGB-D camera based indoor navigation system for the visually impaired is presented. The system guides the visually impaired user from one location to another location without a prior map or GPS information. Accurate real-time egomotion estimation, mapping, and path planning in the presence of obstacles are essential for such a system. We perform real-time 6-DOF egomotion estimation using sparse visual features, dense point clouds, and the ground plane to reduce drift from a head-mounted RGB-D camera. The system also builds 2D probabilistic occupancy grid map for efficient traversability analysis which is a basis for dynamic path planning and obstacle avoidance. The system can store and reload maps generated by the system while traveling and continually expand the coverage area of navigation. Next, the shortest path between the start location to destination is generated. The system generates a safe and efficient way point based on the traversability analysis result and the shortest path and updates the way point while a user is constantly moving. Appropriate cues are generated and delivered to a tactile feedback system to guide the visually impaired user to the way point. The proposed wearable system prototype is composed of multiple modules including a head-mounted RGB-D camera, standard laptop that runs a navigation software, smart phone user interface, and haptic feedback vest. The proposed system achieves real-time navigation performance at 28.6Hz in average on a laptop, and helps the visually impaired extends the range of their activities and improve the orientation and mobility performance in a cluttered environment. We have evaluated the performance of the proposed system in mapping and localization with blind-folded and the visually impaired subjects. The mobility experiment results show that navigation in indoor environments with the proposed system avoids collisions successfully and improves mobility performance of the user compared to conventional and state-of-the-art mobility aid devices. © 2016 Elsevier Inc.","Assistive technologies for the visually impaired; Visual SLAM; Wearable navigation system","Cameras; Computer vision; Graph theory; Haptic interfaces; Indoor positioning systems; Laptop computers; Location; Mapping; Motion planning; Navigation systems; Smartphones; User interfaces; Cluttered environments; Dynamic path planning; Ego-motion estimation; Indoor navigation system; Mapping and localization; Visual SLAM; Visually impaired; Visually-impaired users; Wearable technology","Y.H. Lee; Institute for Robotics and Intelligent Systems, University of Southern California, Los Angeles, CA, United States; email: lee126@usc.edu","","Academic Press Inc.","10773142","","CVIUF","","English","Comput Vision Image Understanding","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-84963963845"
"Fusco G.; Shen H.; Coughlan J.M.","Fusco, Giovanni (57196680565); Shen, Huiying (14523706200); Coughlan, James M. (7006567201)","57196680565; 14523706200; 7006567201","Self-localization at street intersections","2014","Proceedings - Conference on Computer and Robot Vision, CRV 2014","","","6816822","40","47","7","6","10.1109/CRV.2014.14","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84902267429&doi=10.1109%2fCRV.2014.14&partnerID=40&md5=e53403599f1bfdbeb1d774fcec9fe2e6","Smith-Kettlewell Eye Research Institute, San Francisco, CA 94115, United States","Fusco G., Smith-Kettlewell Eye Research Institute, San Francisco, CA 94115, United States; Shen H., Smith-Kettlewell Eye Research Institute, San Francisco, CA 94115, United States; Coughlan J.M., Smith-Kettlewell Eye Research Institute, San Francisco, CA 94115, United States","There is growing interest among smartphone users in the ability to determine their precise location in their environment for a variety of applications related to way finding, travel and shopping. While GPS provides valuable self-localization estimates, its accuracy is limited to approximately 10 meters in most urban locations. This paper focuses on the self-localization needs of blind or visually impaired travelers, who are faced with the challenge of negotiating street intersections. These travelers need more precise self-localization to help them align themselves properly to crosswalks, signal lights and other features such as walk light pushbuttons. We demonstrate a novel computer vision-based localization approach that is tailored to the street intersection domain. Unlike most work on computer vision-based localization techniques, which typically assume the presence of detailed, high-quality 3D models of urban environments, our technique harnesses the availability of simple, ubiquitous satellite imagery (e.g., Google Maps) to create simple maps of each intersection. Not only does this technique scale naturally to the great majority of street intersections in urban areas, but it has the added advantage of incorporating the specific metric information that blind or visually impaired travelers need, namely, the locations of intersection features such as crosswalks. Key to our approach is the integration of IMU (inertial measurement unit) information with geometric information obtained from image panorama stitchings. Finally, we evaluate the localization performance of our algorithm on a dataset of intersection panoramas, demonstrating the feasibility of our approach. © 2014 IEEE.","Assistive technology; Blindness and low vision; Image stitching; IMU (inertial measurement unit); Mobile vision; Self-localization","Crosswalks; Data processing; Satellite imagery; Signaling; Units of measurement; Assistive technology; Image stitching; Inertial measurement unit; Low vision; Mobile vision; Self-localization; Computer vision","","","IEEE Computer Society","","978-147994338-8","","","English","Proc. - Conf. Comput. Robot Vis., CRV","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-84902267429"
"Khoo W.L.; Knapp J.; Palmer F.; Ro T.; Zhu Z.","Khoo, Wai Lun (34976842100); Knapp, Joey (55781358000); Palmer, Franklin (55331775300); Ro, Tony (57202873115); Zhu, Zhigang (7404803571)","34976842100; 55781358000; 55331775300; 57202873115; 7404803571","Designing and testing wearable range-vibrotactile devices","2013","Journal of Assistive Technologies","7","2","","102","117","15","14","10.1108/17549451311328781","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84879711279&doi=10.1108%2f17549451311328781&partnerID=40&md5=e553806b7631751b8bc2c440d3563c4f","Department of Computer Science, City College of New York - CUNY, New York, United States; Department of Psychology, City College of New York - CUNY, New York, United States","Khoo W.L., Department of Computer Science, City College of New York - CUNY, New York, United States; Knapp J., Department of Computer Science, City College of New York - CUNY, New York, United States; Palmer F., Department of Computer Science, City College of New York - CUNY, New York, United States; Ro T., Department of Psychology, City College of New York - CUNY, New York, United States; Zhu Z., Department of Computer Science, City College of New York - CUNY, New York, United States","Purpose: The purpose of this paper is to demonstrate how commercially-off-the-shelf sensors and stimulators, such as infrared rangers and vibrators, can be retrofitted as a useful assistive technology in real and virtual environments. Design/methodology/approach: The paper describes how a wearable range-vibrotactile device is designed and tested in the real-world setting, as well as thorough evaluations in a virtual environment for complicated navigation tasks and neuroscience studies. Findings: In the real-world setting, a person with normal vision who has to navigate their way around a room with their eyes closed will quickly rely on their arms and hands to explore the room. The authors' device allows a person to ""feel"" their environment without touching it. Due to inherent difficulties in testing human subjects when navigating a real environment, a virtual environment affords us an opportunity to scientifically and extensively test the prototype before deploying the device in the real-world. Research limitations/implications: This project serves as a starting-point for further research in benchmarking assistive technology for the visually impaired and to eventually develop a man-machine sensorimotor model that will improve current state-of-the-art technology, as well as a better understanding of neural coding in the human brain. Social implications: Based on 2012 World Health Organization, there are 39 million blind people. This project will have a direct impact on this community. Originality/value: The paper demonstrates a low cost design of assistive technology that has been tested and evaluated in real and virtual environments, as well as integration of sensor designs and neuroscience. © Emerald Group Publishing Limited.","Assistive technologies; Blind people; Infrared range sensor; Multimodal; Sensors; Vibrotactile; Virtual reality; Visually impaired; Wearable","","W. L. Khoo; Department of Computer Science, City College of New York - CUNY, New York, United States; email: khoo@cs.ccny.cuny.edu","","","20428723","","","","English","J. Assistive Technol.","Article","Final","","Scopus","2-s2.0-84879711279"
"Stronks H.C.; Dagnelie G.","Stronks, H Christiaan (26538137700); Dagnelie, Gislin (7003576503)","26538137700; 7003576503","The functional performance of the Argus II retinal prosthesis","2014","Expert Review of Medical Devices","11","1","","23","30","7","145","10.1586/17434440.2014.862494","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84893037341&doi=10.1586%2f17434440.2014.862494&partnerID=40&md5=1ce1b28b71a6751a7b77fca3600a22c3","NICTA Canberra Research Laboratory, Tower A, Canberra, 7 London Circuit, Australia; Department of Neuroscience, John Curtin School of Medical Research, Australian National University, Canberra, Australia; Department of Ophthalmology, Johns Hopkins University, Baltimore, MD, United States","Stronks H.C., NICTA Canberra Research Laboratory, Tower A, Canberra, 7 London Circuit, Australia, Department of Neuroscience, John Curtin School of Medical Research, Australian National University, Canberra, Australia; Dagnelie G., Department of Ophthalmology, Johns Hopkins University, Baltimore, MD, United States","Visual prostheses are devices to treat profound vision loss by stimulating nerve cells anywhere along the visual pathway, typically with electrical pulses. The Argus® II implant, developed by Second Sight Medical Products (SSMP, Sylmar, CA, USA), targets the retina and features 60 electrodes that electrically stimulate the surviving retinal neurons. Of the approximately 20 research groups that are actively developing visual prostheses, SSMP has the longest track record. The Argus II was the first visual prosthesis to become commercially available: it received the European conformity mark in 2011 and FDA approval was granted in early 2013 for humanitarian use in the USA. Meanwhile, the Argus II safety/benefit study has been extended for research purposes, and is still ongoing. In this review, we will discuss the performance of the Argus II in restoring sight to the blind, and we will shed light on its expected developments in the coming years. © 2014 Informa UK Ltd..","Argus II; electrical stimulation of the retina; epiretinal implant; low vision; retinal prosthesis; retinitis pigmentosa; spatial resolution; vision rehabilitation; visual acuity; visual prosthetics","Blindness; Electric Stimulation; Electrodes, Implanted; Humans; Image Processing, Computer-Assisted; Retina; Visual Acuity; Visual Prosthesis; Aldehydes; Neurons; Ophthalmology; Vision; Argus II; Electrical stimulations; Low vision; Retinal prosthesis; Retinitis pigmentosa; Spatial resolution; Vision rehabilitation; Visual acuity; clinical trial (topic); cochlea prosthesis; contrast enhancement; device safety; electrode; eye movement; image processing; multicenter study (topic); nerve cell stimulation; oculomotor system; phase 1 clinical trial (topic); phase 2 clinical trial (topic); phase 3 clinical trial (topic); retina degeneration; retina nerve cell; retinal implant; review; visual acuity; visual field; visual impairment; visual orientation; visual system; Prosthetics","H.C. Stronks; NICTA Canberra Research Laboratory, Tower A, Canberra, 7 London Circuit, Australia; email: christiaan.stronks@nicta.com.au","","","17452422","","","24308734","English","Expert Rev. Med. Devices","Review","Final","All Open Access; Green Open Access","Scopus","2-s2.0-84893037341"
"Shalaby M.M.; Salem M.A.-M.; Khamis A.; Melgani F.","Shalaby, Marwa M. (57191243001); Salem, Mohammed A.-Megeed (24587978300); Khamis, Alaa (6603919018); Melgani, Farid (35613488300)","57191243001; 24587978300; 6603919018; 35613488300","Geometric model for vision-based door detection","2014","Proceedings of 2014 9th IEEE International Conference on Computer Engineering and Systems, ICCES 2014","","","7030925","41","46","5","16","10.1109/ICCES.2014.7030925","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84988228122&doi=10.1109%2fICCES.2014.7030925&partnerID=40&md5=527b445fec6f16b553d445ead7912787","Dep. of Mechatronics, German University in Cairo, Egypt; Dep. of Scientific Computing, Ain Shams University, Cairo, Egypt; Engineering Science Dep., Suez University, Egypt; Information Engineering and Computer Science Dep., University of Trento, Italy","Shalaby M.M., Dep. of Mechatronics, German University in Cairo, Egypt; Salem M.A.-M., Dep. of Scientific Computing, Ain Shams University, Cairo, Egypt; Khamis A., Engineering Science Dep., Suez University, Egypt; Melgani F., Information Engineering and Computer Science Dep., University of Trento, Italy","Emerging assistive technologies, such as assistive domotics and socially assistive robots have considerable potential for enhancing the lives of many elderly and physically challenged people throughout the world. Blind and visually impaired people can use these technologies for many tasks recognizing objects, handling various household duties, navigation in indoor and outdoor environments. Door detection is one of the important issues in indoor navigation. This paper presents a novel vision-based door detection technique. It is based on the geometric properties of the 4-side polygon. The efficacy of the proposed method is tested using large database of images with different levels of complexity. The experimental results show the robustness of the proposed method against changes in colors, sizes, shapes, orientations, and textures of the door. Detection rate of 83% with relatively low false positive rate for simple images is achieved. This proposed algorithm is suitable for real-time, portable applications where it only requires one digital camera and low computational resources. © 2014 IEEE.","","Robots; Assistive technology; Blind and visually impaired; Computational resources; False positive rates; Geometric properties; In-door navigations; Portable applications; Socially assistive robots; Indoor positioning systems","M.M. Shalaby; Dep. of Mechatronics, German University in Cairo, Egypt; email: marwam.shalaby@gmail.com","Wahba A.M.; El-Kharashi M.W.; Taher M.; Bahaa El-Din A.M.; Zaki A.M.","Institute of Electrical and Electronics Engineers Inc.","","978-147996594-6","","","English","Proc. IEEE Int. Conf. Comput. Eng. Syst., ICCES","Conference paper","Final","","Scopus","2-s2.0-84988228122"
"Mocanu B.; Tapu R.; Zaharia T.","Mocanu, Bogdan (24822846400); Tapu, Ruxandra (26424843800); Zaharia, Titus (6601999900)","24822846400; 26424843800; 6601999900","When ultrasonic sensors and computer vision join forces for efficient obstacle detection and recognition","2016","Sensors (Switzerland)","16","11","1807","","","","79","10.3390/s16111807","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84994745665&doi=10.3390%2fs16111807&partnerID=40&md5=1ed054b24bd8e5f30f80d97a3c5a51cf","ARTEMIS Department, Institut Mines-Télécom/Télécom SudParis, UMR CNRS MAP5 8145, 9 rue Charles Fourier, Évry, 91000, France; Telecommunication Department, Faculty of ETTI, University Politehnica of Bucharest, Splaiul Independentei 313, Bucharest, 060042, Romania","Mocanu B., ARTEMIS Department, Institut Mines-Télécom/Télécom SudParis, UMR CNRS MAP5 8145, 9 rue Charles Fourier, Évry, 91000, France, Telecommunication Department, Faculty of ETTI, University Politehnica of Bucharest, Splaiul Independentei 313, Bucharest, 060042, Romania; Tapu R., ARTEMIS Department, Institut Mines-Télécom/Télécom SudParis, UMR CNRS MAP5 8145, 9 rue Charles Fourier, Évry, 91000, France, Telecommunication Department, Faculty of ETTI, University Politehnica of Bucharest, Splaiul Independentei 313, Bucharest, 060042, Romania; Zaharia T., ARTEMIS Department, Institut Mines-Télécom/Télécom SudParis, UMR CNRS MAP5 8145, 9 rue Charles Fourier, Évry, 91000, France","In the most recent report published by theWorld Health Organization concerning people with visual disabilities it is highlighted that by the year 2020, worldwide, the number of completely blind people will reach 75 million, while the number of visually impaired (VI) people will rise to 250 million. Within this context, the development of dedicated electronic travel aid (ETA) systems, able to increase the safe displacement of VI people in indoor/outdoor spaces, while providing additional cognition of the environment becomes of outmost importance. This paper introduces a novel wearable assistive device designed to facilitate the autonomous navigation of blind and VI people in highly dynamic urban scenes. The system exploits two independent sources of information: ultrasonic sensors and the video camera embedded in a regular smartphone. The underlying methodology exploits computer vision and machine learning techniques and makes it possible to identify accurately both static and highly dynamic objects existent in a scene, regardless on their location, size or shape. In addition, the proposed system is able to acquire information about the environment, semantically interpret it and alert users about possible dangerous situations through acoustic feedback. To determine the performance of the proposed methodology we have performed an extensive objective and subjective experimental evaluation with the help of 21 VI subjects from two blind associations. The users pointed out that our prototype is highly helpful in increasing the mobility, while being friendly and easy to learn. © 2016 by the authors; licensee MDPI, Basel, Switzerland.","Acoustic feedback; Computer vision techniques; Machine learning algorithms; Object recognition; Obstacle detection; Ultrasonic network; Wearable assistive device","Acoustics; Adult; Aged; Cell Phone; Humans; Image Processing, Computer-Assisted; Machine Learning; Middle Aged; Self-Help Devices; Visually Impaired Persons; Artificial intelligence; Feedback; Learning algorithms; Learning systems; Navigation systems; Object recognition; Obstacle detectors; Ultrasonic applications; Ultrasonic sensors; Video cameras; Wearable sensors; Acoustic feedback; Autonomous navigation; Computer vision techniques; Electronic travel aidss; Experimental evaluation; Machine learning techniques; Obstacle detection; Wearable assistive devices; acoustics; adult; aged; devices; human; image processing; machine learning; middle aged; mobile phone; procedures; self help device; visually impaired person; Computer vision","R. Tapu; ARTEMIS Department, Institut Mines-Télécom/Télécom SudParis, UMR CNRS MAP5 8145, Évry, 9 rue Charles Fourier, 91000, France; email: ruxandra.tapu@telecom-sudparis.eu","","MDPI AG","14248220","","","27801834","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-84994745665"
"Tapu R.; Mocanu B.; Bursuc A.; Zaharia T.","Tapu, Ruxandra (26424843800); Mocanu, Bogdan (24822846400); Bursuc, Andrei (36472975300); Zaharia, Titus (6601999900)","26424843800; 24822846400; 36472975300; 6601999900","A smartphone-based obstacle detection and classification system for assisting visually impaired people","2013","Proceedings of the IEEE International Conference on Computer Vision","","","6755931","444","451","7","119","10.1109/ICCVW.2013.65","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84897491638&doi=10.1109%2fICCVW.2013.65&partnerID=40&md5=caa0d28cda6b9d51887dd5915dfa52c9","IT/Télécom SudParis, ARTEMIS Department, UMR CNRS MAP5 8145, Evry, France","Tapu R., IT/Télécom SudParis, ARTEMIS Department, UMR CNRS MAP5 8145, Evry, France; Mocanu B., IT/Télécom SudParis, ARTEMIS Department, UMR CNRS MAP5 8145, Evry, France; Bursuc A., IT/Télécom SudParis, ARTEMIS Department, UMR CNRS MAP5 8145, Evry, France; Zaharia T., IT/Télécom SudParis, ARTEMIS Department, UMR CNRS MAP5 8145, Evry, France","In this paper we introduce a real-time obstacle detection and classification system designed to assist visually impaired people to navigate safely, in indoor and outdoor environments, by handling a smartphone device. We start by selecting a set of interest points extracted from an image grid and tracked using the multiscale Lucas - Kanade algorithm. Then, we estimate the camera and background motion through a set of homographic transforms. Other types of movements are identified using an agglomerative clustering technique. Obstacles are marked as urgent or normal based on their distance to the subject and the associated motion vector orientation. Following, the detected obstacles are fed/sent to an object classifier. We incorporate HOG descriptor into the Bag of Visual Words (BoVW) retrieval framework and demonstrate how this combination may be used for obstacle classification in video streams. The experimental results demonstrate that our approach is effective in image sequences with significant camera motion and achieves high accuracy rates, while being computational efficient. © 2013 IEEE.","Object classification; Obstacle/moving object detection; Visually impaired/blind persons","Cameras; Computer vision; Object recognition; Obstacle detectors; Video streaming; Agglomerative clustering; Classification system; Object classification; Object Detection; Obstacle classification; Retrieval frameworks; Visually impaired; Visually impaired people; Smartphones","","","Institute of Electrical and Electronics Engineers Inc.","","978-147993022-7","PICVE","","English","Proc IEEE Int Conf Comput Vision","Conference paper","Final","","Scopus","2-s2.0-84897491638"
"Legge G.E.; Beckmann P.J.; Tjan B.S.; Havey G.; Kramer K.; Rolkosky D.; Gage R.; Chen M.; Puchakayala S.; Rangarajan A.","Legge, Gordon E. (7005064208); Beckmann, Paul J. (7004915668); Tjan, Bosco S. (6602818317); Havey, Gary (6505979312); Kramer, Kevin (35774355300); Rolkosky, David (35774523100); Gage, Rachel (36628376900); Chen, Muzi (56718720100); Puchakayala, Sravan (55874676700); Rangarajan, Aravindhan (55874889700)","7005064208; 7004915668; 6602818317; 6505979312; 35774355300; 35774523100; 36628376900; 56718720100; 55874676700; 55874889700","Indoor Navigation by People with Visual Impairment Using a Digital Sign System","2013","PLoS ONE","8","10","e76783","","","","73","10.1371/journal.pone.0076783","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84885155056&doi=10.1371%2fjournal.pone.0076783&partnerID=40&md5=a39b5ee0b7c424e01e855043eb2d131f","Minnesota laboratory for Low-Vision Research, University of Minnesota, Twin Cities, MN, United States; University of St. Thomas, St. Paul, MN, United States; University of Southern California, Los Angeles, CA, United States; Advanced Medical Electronics Corporation (AME), Maple Grove, MN, United States","Legge G.E., Minnesota laboratory for Low-Vision Research, University of Minnesota, Twin Cities, MN, United States; Beckmann P.J., University of St. Thomas, St. Paul, MN, United States; Tjan B.S., University of Southern California, Los Angeles, CA, United States; Havey G., Advanced Medical Electronics Corporation (AME), Maple Grove, MN, United States; Kramer K., Advanced Medical Electronics Corporation (AME), Maple Grove, MN, United States; Rolkosky D., Advanced Medical Electronics Corporation (AME), Maple Grove, MN, United States; Gage R., Minnesota laboratory for Low-Vision Research, University of Minnesota, Twin Cities, MN, United States; Chen M., Minnesota laboratory for Low-Vision Research, University of Minnesota, Twin Cities, MN, United States; Puchakayala S., Minnesota laboratory for Low-Vision Research, University of Minnesota, Twin Cities, MN, United States; Rangarajan A., Minnesota laboratory for Low-Vision Research, University of Minnesota, Twin Cities, MN, United States","There is a need for adaptive technology to enhance indoor wayfinding by visually-impaired people. To address this need, we have developed and tested a Digital Sign System. The hardware and software consist of digitally-encoded signs widely distributed throughout a building, a handheld sign-reader based on an infrared camera, image-processing software, and a talking digital map running on a mobile device. Four groups of subjects-blind, low vision, blindfolded sighted, and normally sighted controls-were evaluated on three navigation tasks. The results demonstrate that the technology can be used reliably in retrieving information from the signs during active mobility, in finding nearby points of interest, and following routes in a building from a starting location to a destination. The visually impaired subjects accurately and independently completed the navigation tasks, but took substantially longer than normally sighted controls. This fully functional prototype system demonstrates the feasibility of technology enabling independent indoor navigation by people with visual impairment. © 2013 Legge et al.","","Adult; Aged; Blindness; Electronics, Medical; Equipment Design; Female; Humans; Male; Middle Aged; Movement; Questionnaires; Reproducibility of Results; Self-Help Devices; Sensory Aids; Software; User-Computer Interface; Visually Impaired Persons; Young Adult; adult; aged; article; assistive technology device; camera; computer interface; computer program; controlled study; female; geographic mapping; human; human experiment; male; mobile phone; questionnaire; reliability; visual impairment","G. E. Legge; Minnesota laboratory for Low-Vision Research, University of Minnesota, Twin Cities, MN, United States; email: legge@umn.edu","","","19326203","","POLNC","24116156","English","PLoS ONE","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-84885155056"
"Landa-Hernández A.; Casarubias-Vargas H.; Bayro-Corrochano E.","Landa-Hernández, Adán (25647747300); Casarubias-Vargas, Heriberto (36622225400); Bayro-Corrochano, Eduardo (55995861000)","25647747300; 36622225400; 55995861000","Geometric fuzzy techniques for guidance of visually impaired people","2013","Applied Bionics and Biomechanics","10","4","","139","157","18","5","10.3233/ABB-140081","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84901839542&doi=10.3233%2fABB-140081&partnerID=40&md5=36824ed6b4375b6bbd90718f077a1967","Department of Electrical Engineering and Computer Science, CINVESTAV Campus Guadalajara Jalisco, Mexico","Landa-Hernández A., Department of Electrical Engineering and Computer Science, CINVESTAV Campus Guadalajara Jalisco, Mexico; Casarubias-Vargas H., Department of Electrical Engineering and Computer Science, CINVESTAV Campus Guadalajara Jalisco, Mexico; Bayro-Corrochano E., Department of Electrical Engineering and Computer Science, CINVESTAV Campus Guadalajara Jalisco, Mexico","In this paper we present the design of a device to guide the visually impaired person who normally uses a cane. We propose a non-invasive device that will help blind and visually impaired people to navigate. The system uses stereoscopic vision, a RGB-D sensor and an IMU to process images and to compute the distances from obstacles relative to cameras and to search for free walking paths in the scene. This computing is done using stereo vision, vanishing points, and fuzzy rules. Vanishing points are used to obtain a main orientation in structured spaces. Since the guidance system is related to a spatial reference system, the vanishing point is used like a virtual compass that helps the blind to orient him-or herself towards a goal. Reinforced with fuzzy decision rules, the system supports the blind in avoiding obstacles, thus the blind person is able to cross structured spaces and avoid obstacles without the need for a cane. © 2013-IOS Press and the authors. All rights reserved.","","Stereo image processing; Stereo vision; Vision aids; Avoiding obstacle; Blind and visually impaired; Fuzzy decision rules; Fuzzy techniques; Spatial reference systems; Stereoscopic vision; Visually impaired people; Visually impaired persons; article; blindness; cane; fuzzy logic; geometry; human; mathematical computing; medical device; priority journal; sensor; stereoscopic vision; visual impairment; walking; Search engines","E. Bayro-Corrochano; Department of Electrical Engineering and Computer Science, CINVESTAV Campus Guadalajara Jalisco, Mexico; email: edb@gdl.cinvestav.mx","","Hindawi Limited","11762322","","","","English","Appl. Bionics Biomech.","Article","Final","","Scopus","2-s2.0-84901839542"
"Chaccour K.; Badr G.","Chaccour, Kabalan (56979407700); Badr, Georges (35101305600)","56979407700; 35101305600","Novel indoor navigation system for visually impaired and blind people","2015","2015 1st International Conference on Applied Research in Computer Science and Engineering, ICAR 2015","","","7338143","","","","11","10.1109/ARCSE.2015.7338143","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84961773777&doi=10.1109%2fARCSE.2015.7338143&partnerID=40&md5=1aabb1bf41bcfada8dc67db6353c82cf","TICKET Lab, Antonine University, P.O. Box 40016, Hadat-Baabda, Lebanon","Chaccour K., TICKET Lab, Antonine University, P.O. Box 40016, Hadat-Baabda, Lebanon; Badr G., TICKET Lab, Antonine University, P.O. Box 40016, Hadat-Baabda, Lebanon","Visual impairment or vision loss is the main reason for reduced mobility in humans. Visually Impaired (VI) people require continuous assistance during their movement. Researchers and scientists have put many solutions to assist those people in the activities of their daily living (ADL). The current research delivers a novel indoor navigation system for visually impaired people. The proposed solution is designed for indoor use only (house, office, companies, etc.). It provides the visually impaired person the ability to navigate without any other hardware assistance. The proposed system architecture uses a network of IP cameras installed at the ceiling of each room. A remote processing system analyzes-by computer vision algorithms-photos taken from the environment in order to inform the subject about his location and reacts accordingly to deliver the adequate assistance. A guidance algorithm helps him reach his destination using a simple interactive mobile application installed on his smart phone. The proof of concept prototype was designed with one camera on top of a wooden floor model to simulate the system. Results showed good reliability in indoor navigation and obstacles avoidance. © 2015 IEEE.","assistance; blind; camera; computer vision; mobile application; navigation; obstacle avoidance; Visually impaired","Cameras; Collision avoidance; Computer vision; Mobile computing; Mobile telecommunication systems; Navigation; Navigation systems; Smartphones; assistance; blind; Computer vision algorithms; Interactive mobile applications; Mobile applications; Visually impaired; Visually impaired people; Visually impaired persons; Indoor positioning systems","","","Institute of Electrical and Electronics Engineers Inc.","","978-146738542-8","","","English","Int. Conf. Appl. Res. Comput. Sci. Eng., ICAR","Conference paper","Final","","Scopus","2-s2.0-84961773777"
"Bourbakis N.; Makrogiannis S.K.; Dakopoulos D.","Bourbakis, Nikolaos (7006247012); Makrogiannis, Sokratis K. (6602210816); Dakopoulos, Dimitrios (24472639800)","7006247012; 6602210816; 24472639800","A system-prototype representing 3D space via alternative-sensing for visually impaired navigation","2013","IEEE Sensors Journal","13","7","6482166","2535","2547","12","32","10.1109/JSEN.2013.2253092","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84878774538&doi=10.1109%2fJSEN.2013.2253092&partnerID=40&md5=9fc3c22882f78b15a55f159ba68786a4","Assistive Technologies Research Center, Wright State University, Dayton, OH 45435, United States; National Institute on Aging (NIH), Baltimore, MD 21225, United States","Bourbakis N., Assistive Technologies Research Center, Wright State University, Dayton, OH 45435, United States; Makrogiannis S.K., National Institute on Aging (NIH), Baltimore, MD 21225, United States; Dakopoulos D., Assistive Technologies Research Center, Wright State University, Dayton, OH 45435, United States","Offering an alternative mode of interaction with the surrounding 3-D space to the visually impaired for collision free navigation is a goal of great significance that includes several key challenges. In this paper, we study the alternative 3-D space sensation that is interpreted by our computer vision prototype system and transferred to the user via a vibration array. There are two main tasks for conducting such a study. The first task is to detect obstacles in close proximity, and motion patterns in image sequences, both important issues for a safe navigation in a 3-D dynamic space. To achieve this task, the images from the left and right cameras are acquired to produce new stereo images, followed by video stabilization as a preprocessing stage, a nonlinear spatio-temporal diffusion and kernel based density estimation method to assess the motion activity, and finally watershed-based detection of moving regions (or obstacles) of interest. The second task is to efficiently represent the information of the captured static and dynamic visual scenes as 3-D detectable patterns of vibrations applied on the human body to create a 3-D sensation of the space during navigation. To accomplish this task, considering the current limitations imposed by the technology, we create a high-to-low (H-L) image resolution representation scheme to facilitate the mapping onto a low-resolution 2-D array of vibrators. The H-L scheme uses pyramidal modeling to obtain low-resolution images of interest-preserving motion and obstacles-that are mapped onto a vibration array. These patterns are utilized to train and test the performance of the users in free space navigation. Thus, in this paper we study the synergy of these two important schemes to offer an alternative sensation of the 3-D space to the visually impaired via an array of vibrators. Particularly, the motion component is employed as an element for the identification of visual information of interest to be retained during the H-L transformation. The role of the array vibrators is to create a small-scale front representation of the space via various levels of vibrations. Thus, 3-D vibrations applied on the user's body (chest, abdomen) offer a 3-D sensation of the surrounding space and the motion in it. In addition, we present experimental results that indicate the efficiency of this navigation scheme in creating low-resolution 3-D views of the free navigation space and detecting obstacles and moving areas. © 2001-2012 IEEE.","2-D vibration arrays; alternative sensing; assistive technologies; blind's navigation; high-to-low image representation; human computer interface; motion detection","Human computer interaction; Image resolution; Vibrators; 2-D vibration arrays; alternative sensing; Assistive technology; Human computer interfaces; Image representations; Motion detection; Navigation","","","","1530437X","","","","English","IEEE Sensors J.","Article","Final","","Scopus","2-s2.0-84878774538"
"Salido J.; Deniz O.; Bueno G.","Salido, Jesus (6602573624); Deniz, Oscar (8562422200); Bueno, Gloria (7003988757)","6602573624; 8562422200; 7003988757","Sainet: An image processing app for assistance of visually impaired people in social interaction scenarios","2016","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9656","","","467","477","10","1","10.1007/978-3-319-31744-1_42","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84973902460&doi=10.1007%2f978-3-319-31744-1_42&partnerID=40&md5=af4298100ca54b628989f92dfd0bf8bb","Department of IEEAC, Castilla-La Mancha University, Ciudad Real, Spain","Salido J., Department of IEEAC, Castilla-La Mancha University, Ciudad Real, Spain; Deniz O., Department of IEEAC, Castilla-La Mancha University, Ciudad Real, Spain; Bueno G., Department of IEEAC, Castilla-La Mancha University, Ciudad Real, Spain","This work describes a mobile application (Sainet) for image processing as an assistive technology devoted to visually impaired users. The app is targeted to the Android platform and usually executed in a mobile device equipped with a back camera for image acquisition. Moreover, a wireless bluetooth headphone provides the audio feedback to the user. Sainet has been conceived as an assistance tool to the user in a social interaction scenario. It is capable of providing audible information about the number and position (distance and orientation) of the interlocutors in the user frontal scenario. For validation purposes the app has been tested by a blind user who has provided valuable insights about its strengths and weaknesses. © Springer International Publishing Switzerland 2016.","Image processing; Mobile computing; Visually impaired assistance","Bioinformatics; Biomedical engineering; Human rehabilitation equipment; Image acquisition; Mobile computing; Mobile devices; Social sciences; Android platforms; Assistive technology; Audio feedbacks; Mobile applications; Social interaction scenarios; Visually impaired; Visually impaired people; Visually-impaired users; Image processing","G. Bueno; Department of IEEAC, Castilla-La Mancha University, Ciudad Real, Spain; email: gloria.bueno@uclm.es","Ortuno F.; Rojas I.","Springer Verlag","03029743","978-331931743-4","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","","Scopus","2-s2.0-84973902460"
"Herghelegiu P.; Burlacu A.; Caraiman S.","Herghelegiu, Paul (37161364500); Burlacu, Adrian (23666339700); Caraiman, Simona (36093924000)","37161364500; 23666339700; 36093924000","Robust ground plane detection and tracking in stereo sequences using camera orientation","2016","2016 20th International Conference on System Theory, Control and Computing, ICSTCC 2016 - Joint Conference of SINTES 20, SACCS 16, SIMSIS 20 - Proceedings","","","7790717","514","519","5","16","10.1109/ICSTCC.2016.7790717","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85010433546&doi=10.1109%2fICSTCC.2016.7790717&partnerID=40&md5=b25c6e60a903aed57592bf40b1bd6588","Faculty of Automatic Control and Computer Engineering, Technical University Gheorghe Asachi, Iasi, Romania","Herghelegiu P., Faculty of Automatic Control and Computer Engineering, Technical University Gheorghe Asachi, Iasi, Romania; Burlacu A., Faculty of Automatic Control and Computer Engineering, Technical University Gheorghe Asachi, Iasi, Romania; Caraiman S., Faculty of Automatic Control and Computer Engineering, Technical University Gheorghe Asachi, Iasi, Romania","The research on assisting visually impaired people to navigate in unknown environments commonly employs image processing or computer vision techniques. One key element that needs to be identified in the surrounding environment of a blind user is the ground plane. Its accurate detection is highly important because the user is able to move freely in that area. An assistive device should primarily assist the user in avoiding the obstacles that lie on the ground surface. The images that will be further processed are usually acquired using depth acquisition devices worn by the user. Therefore, in contrast with automotive or robotics applications, which also heavily rely on ground plane extraction for obstacle detection, the camera orientation has more degrees of freedom. This leads to the requirement of designing more complex solutions for the ground plane detection in the case of assistive systems for the visually impaired. In this paper we introduce an algorithm to detect the ground plane taking into consideration the orientation of the camera, namely its pitch and roll. The proposed algorithm is based on an efficient processing of the v-disparity map associated to each frame. A two-step decision making approach to determine the most suited area that corresponds to the ground plane is described. Experimental results with synthetic datasets are provided to prove the efficiency and robustness of the proposed approach. © 2016 IEEE.","assistive technologies; ground plane detection; visually impaired people","Behavioral research; Cameras; Computation theory; Computer vision; Decision making; Degrees of freedom (mechanics); Human rehabilitation equipment; Image processing; Obstacle detectors; System theory; Assistive technology; Computer vision techniques; Ground planes; Obstacle detection; Robotics applications; Surrounding environment; Synthetic datasets; Visually impaired people; Stereo image processing","","Petre E.; Brezovan M.","Institute of Electrical and Electronics Engineers Inc.","","978-150902720-0","","","English","Int. Conf. Syst. Theory, Control Comput., ICSTCC - Jt. Conf. SINTES, SACCS, SIMSIS - Proc.","Conference paper","Final","","Scopus","2-s2.0-85010433546"
"Min B.-C.; Saxena S.; Steinfeld A.; Dias M.B.","Min, Byung-Cheol (39161762500); Saxena, Suryansh (56742696900); Steinfeld, Aaron (7103257672); Dias, M. Bernardine (55664937300)","39161762500; 56742696900; 7103257672; 55664937300","Incorporating information from trusted sources to enhance urban navigation for blind travelers","2015","Proceedings - IEEE International Conference on Robotics and Automation","2015-June","June","7139824","4511","4518","7","9","10.1109/ICRA.2015.7139824","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84938282743&doi=10.1109%2fICRA.2015.7139824&partnerID=40&md5=e6b6ade0566c9bde423f030d282dcd39","Robotics Institute, Carnegie Mellon University, Pittsburgh, 15213, PA, United States; Delhi Technological University, New Delhi, 110042, India","Min B.-C., Robotics Institute, Carnegie Mellon University, Pittsburgh, 15213, PA, United States; Saxena S., Delhi Technological University, New Delhi, 110042, India; Steinfeld A., Robotics Institute, Carnegie Mellon University, Pittsburgh, 15213, PA, United States; Dias M.B., Robotics Institute, Carnegie Mellon University, Pittsburgh, 15213, PA, United States","Dynamic changes can present significant challenges for visually impaired travelers to safely and independently navigate urban environments. To address these challenges, we are developing the NavPal suite of technology tools [1]. NavPal includes a dynamic guidance tool [2] in the form of a smartphone app that can provide real-time instructions based on available map information to guide navigation in indoor environments. In this paper we enhance our past work by introducing a framework for blind travelers to add map/navigation information to the tool, and to invite trusted sources to do the same. The user input is realized through audio breadcrumb annotations that could be useful for future trips. The trusted sources mechanism provides invited trusted individuals or organizations an interface to contribute real-time information about the surrounding environment. We demonstrate the feasibility of our solution through a prototype Android smartphone-based outdoor navigation aid for blind travelers. An initial usability study with visually impaired adults informed the design and implementation of this prototype. © 2015 IEEE.","","Agricultural robots; Blind source separation; Indoor positioning systems; Navigation; Robotics; Smartphones; Android smartphone; Design and implementations; Indoor environment; Outdoor navigation; Real-time information; Surrounding environment; Urban environments; Usability studies; Robots","","","Institute of Electrical and Electronics Engineers Inc.","10504729","978-147996923-4","PIIAE","","English","Proc IEEE Int Conf Rob Autom","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-84938282743"
"Lee Y.H.; Medioni G.","Lee, Young Hoon (58839377100); Medioni, Gérard (7006497957)","58839377100; 7006497957","Wearable RGBD indoor navigation system for the blind","2015","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8927","","","493","508","15","56","10.1007/978-3-319-16199-0_35","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84928787123&doi=10.1007%2f978-3-319-16199-0_35&partnerID=40&md5=c74fe9c9a3f789ab0bc4564f5fa3135e","Institute for Robotics and Intelligent Systems, University of Southern California, Los Angeles, CA, United States","Lee Y.H., Institute for Robotics and Intelligent Systems, University of Southern California, Los Angeles, CA, United States; Medioni G., Institute for Robotics and Intelligent Systems, University of Southern California, Los Angeles, CA, United States","In this paper, we present a novel wearable RGBD camera based navigation system for the visually impaired. The system is composed of a smartphone user interface, a glass-mounted RGBD camera device, a real-time navigation algorithm, and haptic feedback system. A smartphone interface provides an effective way to communicate to the system using audio and haptic feedback. In order to extract orientational information of the blind users, the navigation algorithm performs real-time 6-DOF feature based visual odometry using a glass-mounted RGBD camera as an input device. The navigation algorithm also builds a 3D voxel map of the environment and analyzes 3D traversability. A path planner of the navigation algorithm integrates information from the egomotion estimation and mapping and generates a safe and an efficient path to a waypoint delivered to the haptic feedback system. The haptic feedback system consisting of four micro-vibration motors is designed to guide the visually impaired user along the computed path and to minimize cognitive loads. The proposed system achieves real-time performance at 28.4Hz in average on a laptop, and helps the visually impaired extends the range of their activities and improve the mobility performance in a cluttered environment. The experiment results show that navigation in indoor environments with the proposed system avoids collisions successfully and improves mobility performance of the user compared to conventional and state-of-the-art mobility aid devices. © Springer International Publishing Switzerland 2015.","","Algorithms; Cameras; Computer vision; Glass; Haptic interfaces; Human rehabilitation equipment; Navigation systems; Signal encoding; Smartphones; User interfaces; Vision aids; Wearable technology; Cluttered environments; Ego-motion estimation; Haptic feedback systems; Indoor navigation system; Mobility performance; Navigation algorithms; Real time performance; Visually-impaired users; Indoor positioning systems","","Rother C.; Agapito L.; Bronstein M.M.","Springer Verlag","03029743","978-331916198-3","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-84928787123"
"Nagarajan R.; Sainarayanan G.; Yaacob S.; Porle R.R.","Nagarajan, R. (7202927600); Sainarayanan, G. (23986115600); Yaacob, Sazali (6602262501); Porle, Rosalyn R. (10439854300)","7202927600; 23986115600; 6602262501; 10439854300","Fuzzy-rule-based object identification methodology for NAVI system","2005","Eurasip Journal on Applied Signal Processing","2005","14","","2260","2267","7","4","10.1155/ASP.2005.2260","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-29544439323&doi=10.1155%2fASP.2005.2260&partnerID=40&md5=8a9703d93f17fdcc72601034c0e789f2","Artificial Intelligence Research Group, School of Engineering and Information Technology, Universiti Malaysia Sabah, Kota Kinabalu 88999, Sabah, Locked Bag 2073, Malaysia; School of Mechatronic Engineering, Northern Malaysia University, College of Engineering, Kubang Gajah, Arau 02600, Perils, Malaysia; School of Engineering and Information Technology, Universiti Malaysia Sabah, Malaysia; Institution Engineers, India; IEEE, United States; UICEE, Australia; Universiti Malaysia Sabah, Malaysia; School of Mechatronic Engineering, Northern Malaysia University, College of Engineering, Malaysia","Nagarajan R., Artificial Intelligence Research Group, School of Engineering and Information Technology, Universiti Malaysia Sabah, Kota Kinabalu 88999, Sabah, Locked Bag 2073, Malaysia, School of Engineering and Information Technology, Universiti Malaysia Sabah, Malaysia, Institution Engineers, India, IEEE, United States, UICEE, Australia; Sainarayanan G., Artificial Intelligence Research Group, School of Engineering and Information Technology, Universiti Malaysia Sabah, Kota Kinabalu 88999, Sabah, Locked Bag 2073, Malaysia, Universiti Malaysia Sabah, Malaysia; Yaacob S., School of Mechatronic Engineering, Northern Malaysia University, College of Engineering, Kubang Gajah, Arau 02600, Perils, Malaysia, School of Mechatronic Engineering, Northern Malaysia University, College of Engineering, Malaysia; Porle R.R., Artificial Intelligence Research Group, School of Engineering and Information Technology, Universiti Malaysia Sabah, Kota Kinabalu 88999, Sabah, Locked Bag 2073, Malaysia","We present an object identification methodology applied in a navigation assistance for visually impaired (NAVI) system. The NAVI has a single board processing system (SBPS), a digital video camera mounted headgear, and a pair of stereo earphones. The captured image from the camera is processed by the SBPS to generate a specially structured stereo sound suitable for vision impaired people in understanding the presence of objects/obstacles in front of them. The image processing stage is designed to identify the objects in the captured image. Edge detection and edge-linking procedures are applied in the processing of image. A concept of object preference is included in the image processing scheme and this concept is realized using a fuzzy-rule base. The blind users are trained with the stereo sound produced by NAVI for achieving a collision-free autonomous navigation. © 2005 Hindawi Publishing Corporation.","Blind navigation; Edge detection; Fuzzy-rule-based; Object identification; Stereo sound","Digital devices; Earphones; Fuzzy sets; Image processing; Video cameras; Blind navigation; Fuzzy-rule-based; Object identification; Stereo sound; Navigation","R. Nagarajan; Artificial Intelligence Research Group, School of Engineering and Information Technology, Universiti Malaysia Sabah, Kota Kinabalu 88999, Sabah, Locked Bag 2073, Malaysia; email: nagaraja@ums.edu.my","","","11108657","","EJASC","","English","Eurasip J. Appl. Sign. Process.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-29544439323"
"Rahhal Jamal; Wang Yu-Lin; Atkin Guillermo E.","Rahhal, Jamal (6506488632); Wang, Yu-Lin (10839992000); Atkin, Guillermo E. (7004402979)","6506488632; 10839992000; 7004402979","Template matching for a local guidance system","1996","Midwest Symposium on Circuits and Systems","3","","","1268","1271","3","1","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-0030354232&partnerID=40&md5=aaf75b0e991ea787dc93d75cf7abbada","Illinois Inst of Technology, Chicago, United States","Rahhal Jamal, Illinois Inst of Technology, Chicago, United States; Wang Yu-Lin, Illinois Inst of Technology, Chicago, United States; Atkin Guillermo E., Illinois Inst of Technology, Chicago, United States","In this paper we describe an important component of an Interactive Assistance System designed to help the blind and visually impaired to become more independent, increasing their mobility and ability to work. A major part of the system is a Local Guidance System (LGS) that uses image processing techniques to extract the most important features in the path of the blind. The LGS provides information to the user about obstacles and general characteristics of the walking path. It uses two miniature cameras to capture the image from two different locations. A processor uses a template matching algorithm to detect the main features in the path. Then the images are processed and delivered in an (text to speech) audio format. The objects are chosen and classified by a statistical study of actual street images, then the templates are formed for each main feature in the image by averaging all the images obtained for that feature.","","Algorithms; Cameras; Data acquisition; Feature extraction; Image processing; Local guidance system (LGS); Template matching algorithms; Electronic guidance systems","","","IEEE","","","MSCSD","","English","Midwest Symp Circuits Syst","Conference paper","Final","","Scopus","2-s2.0-0030354232"
"Nagarajan R.; Sainarayanan G.; Yacoob S.; Porle R.R.","Nagarajan, R. (7202927600); Sainarayanan, G. (23986115600); Yacoob, Sazali (36482179800); Porle, Rosalyn R. (10439854300)","7202927600; 23986115600; 36482179800; 10439854300","An improved object identification for NAVI","2004","IEEE Region 10 Annual International Conference, Proceedings/TENCON","A","","","A455","A458","3","7","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-27944500287&partnerID=40&md5=8b58fdf53b25ebc857f721c4c7edce43","Artificial Intelligence Research Group, School of Engineering and Information Technology, Universiti Malaysia Sabah, 88999 Kota Kinabalu, Sabah, Malaysia","Nagarajan R., Artificial Intelligence Research Group, School of Engineering and Information Technology, Universiti Malaysia Sabah, 88999 Kota Kinabalu, Sabah, Malaysia; Sainarayanan G., Artificial Intelligence Research Group, School of Engineering and Information Technology, Universiti Malaysia Sabah, 88999 Kota Kinabalu, Sabah, Malaysia; Yacoob S., Artificial Intelligence Research Group, School of Engineering and Information Technology, Universiti Malaysia Sabah, 88999 Kota Kinabalu, Sabah, Malaysia; Porle R.R., Artificial Intelligence Research Group, School of Engineering and Information Technology, Universiti Malaysia Sabah, 88999 Kota Kinabalu, Sabah, Malaysia","Navigation Assistance for Visually Impaired (NAVI) is a vision substitute system designed to assist blind people for autonomous navigation. NAVI working concept is based on 'image to sound' conversion whereby input image captured from the video sensor is processed and transformed into stereo sound pattern. The sound is conveyed to blind's auditory system via stereo headphones. Processing in NAVI can be divided into two sub modules, which are the vision module and sonification module. In the vision module, image processing is employed to identify objects from the image. This paper presents an improved object identification methodology for NAVI. Objects are identified by its closed boundary. Properties of each object are evaluated using AI technique as to assign vision preference, so that blind user can determine the location as well as other properties of that object. Experimentation and results are presented to evaluate the proposed methodology for blind navigation purposes. © 2004 IEEE.","","Audition; Computer vision; Headphones; Image sensors; Navigation systems; Video signal processing; Auditory systems; Blind navigation; Navigation assistance for visually impaired (NAVI); Object identification; Object recognition","","","","","","85QXA","","English","IEEE Reg 10 Annu Int Conf Proc TENCON","Conference paper","Final","","Scopus","2-s2.0-27944500287"
"Uddin M.S.; Shioyama T.","Uddin, Mohammad Shorif (55257116400); Shioyama, Tadayoshi (35612237700)","55257116400; 35612237700","Detection of pedestrian crossing and measurement of crossing length an image-based navigational aid for blind people","2005","IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC","2005","","1520112","331","336","5","9","10.1109/ITSC.2005.1520112","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-33747392913&doi=10.1109%2fITSC.2005.1520112&partnerID=40&md5=d4eca7d3df2632f3bdeb649fdc489401","Department of Electronics and Computer Science, Jahangirnagar University, Savar, Dhaka 1342, Bangladesh; Department of Mechanical and System Engineering, Kyoto Institute of Technology, Sakyo-ku, Kyoto 606-8585, Japan","Uddin M.S., Department of Electronics and Computer Science, Jahangirnagar University, Savar, Dhaka 1342, Bangladesh, Department of Mechanical and System Engineering, Kyoto Institute of Technology, Sakyo-ku, Kyoto 606-8585, Japan; Shioyama T., Department of Mechanical and System Engineering, Kyoto Institute of Technology, Sakyo-ku, Kyoto 606-8585, Japan","Pedestrian crossings are oangerous places for a visually impaired person to cross safely. This paper describes a simple method for the detection of the location of a pedestrian crossing as well as measurement of its length to enhance the safety and mobility of blind people while crossing a road. A crossing is characterized by zebra pattern i.e. by evenly spaced white stripes on a usual black road surface. Detection of the crossing location as well as measurement of its length is done using this criterion from the image captured by a single camera. The method at first checks for the existence of a crossing; if there exists a crossing then it goes to measure the crossing length. Experimental results on a large number of street scenes with and without crossings demonstrate the effectiveness of the proposed technique. © 2005 IEEE.","","Cameras; Image processing; Measurement theory; Object recognition; Pedestrian safety; Signal processing; Blind people; Crossing length; Image capture; Zebra patterns; Railroad crossings","M.S. Uddin; Department of Electronics and Computer Science, Jahangirnagar University, Savar, Dhaka 1342, Bangladesh; email: shorif@yahoo.com","","Institute of Electrical and Electronics Engineers Inc.","","0780392159; 978-078039215-1","","","English","IEEE Conf Intell Transport Syst Proc ITSC","Conference paper","Final","","Scopus","2-s2.0-33747392913"
"Kaneko Y.; Harada T.; Hirahara Y.; Yanashima K.; Magatani K.","Kaneko, Yuki (55112172400); Harada, Tetsuya (7403056404); Hirahara, Yoshiaki (7004191005); Yanashima, Kenji (7004401605); Magatani, Kazushige (6602092722)","55112172400; 7403056404; 7004191005; 7004401605; 6602092722","Development of the navigation system for the visually impaired","2003","Annual International Conference of the IEEE Engineering in Medicine and Biology - Proceedings","2","","","1640","1643","3","2","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-1542271544&partnerID=40&md5=92c69d04f3e2ccbbcfb737551d9955d3","Department of Electrical Engineering, Tokai University, Japan; Natl. Rehab. Center for the Disabled, Japan","Kaneko Y., Department of Electrical Engineering, Tokai University, Japan; Harada T., Department of Electrical Engineering, Tokai University, Japan; Hirahara Y., Department of Electrical Engineering, Tokai University, Japan; Yanashima K., Natl. Rehab. Center for the Disabled, Japan; Magatani K., Department of Electrical Engineering, Tokai University, Japan","There are about 300,000 visually impaired persons in Japan. Most of them are old persons and, cannot become skillful in using a white cane, even if they make effort to learn how to use a white cane. Therefore, some instruments which support the independent activities of the visually impaired are required. In this paper, we will talk about our developed instrument which support the independent walking of the visually impaired in the indoor space. This instrument is composed of a map information system and a navigation system. In map information system, optical beacons and a receiver of them are used. Optical beacons are set on the ceiling and emit the position code as infrared signal. A receiver receives signal from beacons and informs a map information by artificial voices. The navigation system can follow the colored guide line which is set on the floor. This system is composed of a white cane and a color sensor which is set on the white cane. This color sensor senses colored guide line on the floor. If the cane is on the line, the cane informs a visually impaired user that he or she is on the guide line by vibration. Five normal subjects who were blindfolded were tested with our developed system. All of them could walk along guide line. And map information system worked well. Therefore, we have concluded that our system will be a very valuable one to support activities of the visually impaired.","Navigation; Optical beacon; The visually impaired; White cane","Condition monitoring; Global positioning system; Infrared radiation; Light emitting diodes; Microprocessor chips; Optical systems; Sensors; Signal encoding; Vibration measurement; Walking aids; Optical beacon; Visually impaired; White cane; Mobility aids for blind persons","","","","05891019","","CEMBA","","English","Annu Int Conf IEEE Eng Med Biol Proc","Conference paper","Final","","Scopus","2-s2.0-1542271544"
"Fernandes H.; Costa P.; Filipe V.; Hadjileontiadis L.; Barroso J.","Fernandes, H. (35172835800); Costa, P. (7201895673); Filipe, V. (6507061487); Hadjileontiadis, L. (7004037926); Barroso, J. (20435746800)","35172835800; 7201895673; 6507061487; 7004037926; 20435746800","Stereo vision in blind navigation assistance","2010","2010 World Automation Congress, WAC 2010","","","5665579","","","6","37","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-78651445357&partnerID=40&md5=c93e8ceeb0c39c819a511ddd1f24a339","University of Trás-os-Montes e Alto Douro, 5001-801 Vila Real, Portugal; Knowledge Engineering and Decision Support Research Group, ISEP, 4200-072 Porto, Portugal; Polytechnic Institute of Leiria, Department of Lriformatics Engineering, Leiria, Portugal; Department of Electrical and Computer Engineering, Aristotle University of Thessaloniki, Greece","Fernandes H., University of Trás-os-Montes e Alto Douro, 5001-801 Vila Real, Portugal; Costa P., Polytechnic Institute of Leiria, Department of Lriformatics Engineering, Leiria, Portugal; Filipe V., University of Trás-os-Montes e Alto Douro, 5001-801 Vila Real, Portugal; Hadjileontiadis L., Department of Electrical and Computer Engineering, Aristotle University of Thessaloniki, Greece; Barroso J., University of Trás-os-Montes e Alto Douro, 5001-801 Vila Real, Portugal, Knowledge Engineering and Decision Support Research Group, ISEP, 4200-072 Porto, Portugal","Visual impairment and blindness caused by infectious diseases has been greatly reduced, but increasing numbers of people are at risk of age-related visual impairment. Visual information is the basis for most navigational tasks, so visually impaired individuals are at disadvantage because appropriate information about the surrounding environment is not available. With the recent advances in inclusive technology it is possible to extend the support given to people with visual impairment during their mobility. In this context we propose a system, named SmartVision, whose global objective is to give blind users the ability to move around in unfamiliar environments, whether indoor or outdoor, through a user friendly interface. This paper is focused mainly in the development of the computer vision module of the SmartVision system.","Accessibility; Blind navigation; Computer vision; GIS; Hough transform","Computer vision; Hough transforms; Mathematical transformations; Navigation; Ophthalmology; Accessibility; Age-related; Blind navigation; Blind users; GIS; Global objective; Infectious disease; Surrounding environment; User friendly interface; Visual impairment; Visual information; Visually impaired; Stereo vision","","","","","978-142449673-0","","","English","World Autom. Congr., WAC","Conference paper","Final","","Scopus","2-s2.0-78651445357"
"Nagarajan R.; Yaacob S.; Sainarayanan G.","Nagarajan, R. (7202927600); Yaacob, Sazali (6602262501); Sainarayanan, G. (23986115600)","7202927600; 6602262501; 23986115600","Role of object identification in sonification system for visually impaired","2003","IEEE Region 10 Annual International Conference, Proceedings/TENCON","3","","","735","739","4","19","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-2342625200&partnerID=40&md5=91ae77a3ce44a307b07bb6f8c1566496","Artif. Intell. Applic. Res. Group, Sch. of Eng./Information Technology, Universiti Malaysia Sabah, 88999 Kota Kinabalu, Malaysia","Nagarajan R., Artif. Intell. Applic. Res. Group, Sch. of Eng./Information Technology, Universiti Malaysia Sabah, 88999 Kota Kinabalu, Malaysia; Yaacob S., Artif. Intell. Applic. Res. Group, Sch. of Eng./Information Technology, Universiti Malaysia Sabah, 88999 Kota Kinabalu, Malaysia; Sainarayanan G., Artif. Intell. Applic. Res. Group, Sch. of Eng./Information Technology, Universiti Malaysia Sabah, 88999 Kota Kinabalu, Malaysia","In this paper the role of object identification in sonification system for Navigation Assistance Visually Impaired (NAVI) is discussed. The developed system includes Single Board Processing System (SBPS), vision sensor mounted headgear and stereo earphones. The vision sensor captures the vision information in front of blind user. The captured image is processed to identify the object in the image. Object identification is achieved by a real time image processing methodology using fuzzy algorithms. The processed image is mapped onto stereo acoustic patterns and transferred to the stereo earphones in the system. Blind individuals were trained with NAVI system and tested for obstacle identification. Suggestions from the blind volunteers regarding pleasantness and discrimination of sound pattern were also incorporated in the prototype. With the object identification, the discrimination of object and background with the sound is found to be easier compared to sound produced from the unprocessed image.","","Algorithms; Cameras; Computer hardware; Fuzzy sets; Hearing aids; Mathematical models; Sensors; Vision; Electronic travel aids (ETA); Object identification; Single board processing systems (SBPS); Sound conversions; Pattern recognition","","","","","","85QXA","","English","IEEE Reg 10 Annu Int Conf Proc TENCON","Conference paper","Final","","Scopus","2-s2.0-2342625200"
"Yang X.; Tian Y.","Yang, Xiaodong (59108799200); Tian, Yingli (16556710700)","59108799200; 16556710700","Robust door detection in unfamiliar environments by combining edge and corner features","2010","2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops, CVPRW 2010","","","5543830","57","64","7","49","10.1109/CVPRW.2010.5543830","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-77956536755&doi=10.1109%2fCVPRW.2010.5543830&partnerID=40&md5=efc38dbfd5782b8f63ac522da8a90521","Department of Electrical Engineering, City College, City University of New York, New York, NY 10031, 160 Convent Avenue, United States","Yang X., Department of Electrical Engineering, City College, City University of New York, New York, NY 10031, 160 Convent Avenue, United States; Tian Y., Department of Electrical Engineering, City College, City University of New York, New York, NY 10031, 160 Convent Avenue, United States","Camera-based indoor navigation and wayfinding can assist blind people to independently access unfamiliar buildings. In indoor environments, doors are significant landmarks and door detection plays important roles for navigation and wayfinding. Most existing algorithms of door detection are limited to work for familiar environments with restricted features without taking account of the diversity and variance of doors in different environments. In this paper, we present an image-based door detection algorithm that utilizes the general and stable features of doors - edges and corners. Furthermore, we develop a general geometric model to characterize the door shape by combining edge and corner features without a training process. To validate the robustness and generalizability of our method, we collected a large dataset of door images from a variety of environments. The proposed algorithm achieves 91.7% true positive rate with a low false positive rate of 2.9%. The results demonstrate that our door detection method is generic and robust to different environments with variations of color, texture, occlusions, illumination, scales, and viewpoints. © 2010 IEEE.","","Algorithms; Computer vision; Edge detection; Navigation; Technical presentations; Blind people; Corner feature; Data sets; Door detection; False positive rates; Geometric models; Image-based; Indoor environment; Indoor navigation and wayfinding; Training process; True positive rates; Way-finding; Doors","X. Yang; Department of Electrical Engineering, City College, City University of New York, New York, NY 10031, 160 Convent Avenue, United States; email: xyang02@ccny.cuny.edu","","","","978-142447029-7","","","English","IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recogn. - Workshops, CVPRW","Conference paper","Final","","Scopus","2-s2.0-77956536755"
"Gaudissart V.; Ferreira S.; Thillou C.; Gosselin B.","Gaudissart, Vincent (55308057600); Ferreira, Silvio (9941620500); Thillou, Céline (16053275700); Gosselin, Bernard (57162610800)","55308057600; 9941620500; 16053275700; 57162610800","Sypole: A mobile assistant for the blind","2005","13th European Signal Processing Conference, EUSIPCO 2005","","","","2497","2500","3","7","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84863639476&partnerID=40&md5=c877d211c201d5db1b8e30bde7c9c742","Laboratoire de Théorie des Circuits et de Traitement du Signal, Faculté Polytechnique de Mons, Bâtiment Multitel, 7000, Mons, 1, Avenue Copernic, Belgium","Gaudissart V., Laboratoire de Théorie des Circuits et de Traitement du Signal, Faculté Polytechnique de Mons, Bâtiment Multitel, 7000, Mons, 1, Avenue Copernic, Belgium; Ferreira S., Laboratoire de Théorie des Circuits et de Traitement du Signal, Faculté Polytechnique de Mons, Bâtiment Multitel, 7000, Mons, 1, Avenue Copernic, Belgium; Thillou C., Laboratoire de Théorie des Circuits et de Traitement du Signal, Faculté Polytechnique de Mons, Bâtiment Multitel, 7000, Mons, 1, Avenue Copernic, Belgium; Gosselin B., Laboratoire de Théorie des Circuits et de Traitement du Signal, Faculté Polytechnique de Mons, Bâtiment Multitel, 7000, Mons, 1, Avenue Copernic, Belgium","This paper describes an embedded device designed to provide textual information to the blind or visually impaired. For them, all that information, which exists in the daily life as banknotes, schedule of train, books, postal letters, is not easily accessible. The main aim of this system is to build an automatic text reading assistant which combines small-size, mobility and low cost price. Moreover, to be as efficient as possible, a specific human-machine interface has been created and a lot of additional modules have been developedto answer blind people requests. The goal of this system is to improve their autonomy by the use of a mobile device anywhere and anytime.","","Handicapped persons; Mobile devices; Blind people; Daily lives; Embedded device; Human Machine Interface; Low cost prices; Schedule of train; Textual information; Visually impaired; Signal processing","V. Gaudissart; Laboratoire de Théorie des Circuits et de Traitement du Signal, Faculté Polytechnique de Mons, Bâtiment Multitel, 7000, Mons, 1, Avenue Copernic, Belgium; email: gaudissart@tcts.fpms.ac.be","","","","1604238216; 978-160423821-1","","","English","Eur. Signal Process. Conf., EUSIPCO","Conference paper","Final","","Scopus","2-s2.0-84863639476"
"Pradeep V.; Medioni G.; Weiland J.","Pradeep, Vivek (57217189069); Medioni, Gerard (7006497957); Weiland, James (7006322846)","57217189069; 7006497957; 7006322846","Robot vision for the visually impaired","2010","2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops, CVPRW 2010","","","5543579","15","22","7","118","10.1109/CVPRW.2010.5543579","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-77956541429&doi=10.1109%2fCVPRW.2010.5543579&partnerID=40&md5=c794bbe9d67e6e33809e8a8d73c8fd7f","University of Southern California, Los Angeles, CA, United States","Pradeep V., University of Southern California, Los Angeles, CA, United States; Medioni G., University of Southern California, Los Angeles, CA, United States; Weiland J., University of Southern California, Los Angeles, CA, United States","We present a head-mounted, stereo-vision based navigational assistance device for the visually impaired. The head mounted design enables our subjects to stand and scan the scene for integrating wide-field information, compared to shoulder or waist-mounted designs in literature which require body rotations. In order to extract and maintain orientation information for creating a sense of egocentricity in blind users, we incorporate visual odometry and feature based metric-topological SLAM into our system. Using camera pose estimates with dense 3D data obtained from stereo triangulation, we build a vicinity map of the user's environment. On this map, we perform 3D traversability analysis to steer subjects away from obstacles in the path. A tactile interface consisting of micro vibration motors provides cues for taking evasive action, as determined by our vision processing algorithms. We report experimental results of our system (running at 10 Hz) and conduct mobility tests with blindfolded subjects to demonstrate the usefulness of our approach over conventional navigational aids like the white cane. Copy; 2010 IEEE.","","Computer vision; Feature extraction; Navigation; Technical presentations; Three dimensional; Topology; 3D data; Blind users; Feature-based; Microvibrations; Navigational aids; Orientation information; Robot vision; Stereo triangulation; Tactile Interface; Traversability; Vision processing; Visual odometry; Visually impaired; White cane; Wide-field; Stereo vision","V. Pradeep; University of Southern California, Los Angeles, CA, United States; email: vivekpra@usc.edu","","","","978-142447029-7","","","English","IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recogn. - Workshops, CVPRW","Conference paper","Final","","Scopus","2-s2.0-77956541429"
"Jihong L.; Xiaoye S.","Jihong, Liu (36027239700); Xiaoye, Sun (16178509700)","36027239700; 16178509700","A survey of vision aids for the blind","2006","Proceedings of the World Congress on Intelligent Control and Automation (WCICA)","1","","1713189","4312","4316","4","13","10.1109/WCICA.2006.1713189","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-34047216829&doi=10.1109%2fWCICA.2006.1713189&partnerID=40&md5=746e2cd8d9e3428e91fdd65d27dde6ba","College of Information Science and Engineering, Northeastern University, Shenyang Liaoning Province 110004, China","Jihong L., College of Information Science and Engineering, Northeastern University, Shenyang Liaoning Province 110004, China; Xiaoye S., College of Information Science and Engineering, Northeastern University, Shenyang Liaoning Province 110004, China","The development of effective user interfaces, appropriate sensors, and information processing techniques make it is possible to enable the blind to achieve additional perception of the environment. Since the beginning of the 1970's, the research of vision aids for the visually impaired people has been broadly extended. After introducing the traditional methods for guiding blind, two typical modes of mobility aid are presented in this paper. One called as ETA (short for Electronic Travel Aids), bases on the other natural senses of the blind, such as hearing, touch, smell, feeling and etc, focusing on Meijer's vOICe system which is based on the blind's sensitive auditory senses and ENVS which is by means of haptic feedback. The other technique is artificial vision, using surgical methods of implanting visual prosthesis in the blind's healthy retina, cortex, or optic nerve. The prosthesis generates electrical impulse and evokes the perception of points of light in the patients' visual cortex. The first type is non-wounded for the blind while the second type is wounded for the blind. Besides these, comparisons between the above two techniques and the related researches have also been indicated in this paper. © 2006 IEEE.","Artificial vision; Auditory compensation; Haptic compensation; Phosphene","Computer vision; Handicapped persons; Haptic interfaces; Human rehabilitation equipment; Prosthetics; Surveys; Auditory compensation; Haptic compensation; Phosphene; Vision aids","L. Jihong; College of Information Science and Engineering, Northeastern University, Shenyang Liaoning Province 110004, China; email: liujihong@ise.neu.edu.cn","","","","1424403324; 978-142440332-5","","","English","Proc. World Congr. Intelligent Control Autom. WCICA","Conference paper","Final","","Scopus","2-s2.0-34047216829"
"Shoval S.; Borenstein J.; Koren Y.","Shoval, Shraga (7004180117); Borenstein, Johann (7005627914); Koren, Yoram (7004934603)","7004180117; 7005627914; 7004934603","Auditory guidance with the navbelt-a computerized travel aid for the blind","1998","IEEE Transactions on Systems, Man and Cybernetics Part C: Applications and Reviews","28","3","","459","467","8","127","10.1109/5326.704589","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-0032141288&doi=10.1109%2f5326.704589&partnerID=40&md5=36c5652fc2852a97ac8358e6d5a6e146","Faculty of Industrial Engineering and Management, Technion-Israel Institute of Technology, Haifa 32000, Israel; Department of Mechanical Engineering and Applied Mechanics, University of Michigan, Ann Arbor, MI 48109, United States","Shoval S., Faculty of Industrial Engineering and Management, Technion-Israel Institute of Technology, Haifa 32000, Israel; Borenstein J., Department of Mechanical Engineering and Applied Mechanics, University of Michigan, Ann Arbor, MI 48109, United States; Koren Y., Faculty of Industrial Engineering and Management, Technion-Israel Institute of Technology, Haifa 32000, Israel","A blind traveler walking through an unfamiliar environment and a mobile robot navigating through a cluttered environment have an important feature in common: both have the kinematic ability to perform the motion, but they are dependent on a sensory system to detect and avoid obstacles. This paper describes the use of a mobile robot obstacle avoidance system as a guidance device for blind and visually impaired people. Just as electronic signals are sent to a mobile robot's motion controllers, auditory signals can guide the blind traveler around obstacles, or alternatively, they can provide an ""acoustic image"" of the surroundings. The concept has been implemented and tested in a new travel aid for the blind, called the Navbell. The Navbell introduces two new concepts to electronic travel aids (ETA's) for the blind: it provides information not only about obstacles along the traveled path, but also assists the user in selecting the preferred travel path. In addition, the level of assistance can be automatically adjusted according to changes in the environment and the user's needs and capabilities Experimental results conducted with the Navbell simulator and a portable experimental prototype are presented. © 1998 IEEE.","Auditory system; Computer-aided instruction; Handicapped aids; Headphones; Sonar navigation","Collision avoidance; Computer aided instruction; Handicapped persons; Headphones; Human rehabilitation engineering; Mobile robots; Motion control; Sonar; Electronic travel aids (ETA); Mobility aids for blind persons","S. Shoval; Faculty of Industrial Engineering and Management, Technion-Israel Institute of Technology, Haifa 32000, Israel; email: shraga@hitech.technion.ac.il","","","10946977","","ITCRF","","English","IEEE Trans Syst Man Cybern Pt C Appl Rev","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-0032141288"
"Gharpure C.P.; Kulyukin V.A.","Gharpure, Chaitanya P. (7801362187); Kulyukin, Vladimir A. (6603006662)","7801362187; 6603006662","Robot-assisted shopping for the blind: Issues in spatial cognition and product selection","2008","Intelligent Service Robotics","1","3","","237","251","14","60","10.1007/s11370-008-0020-9","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-46449132329&doi=10.1007%2fs11370-008-0020-9&partnerID=40&md5=3214fca1027b35a8a3aeefefe29abf2e","Computer Science Assistive Technology Laboratory, Department of Computer Science, Utah State University, Logan, UT, United States","Gharpure C.P., Computer Science Assistive Technology Laboratory, Department of Computer Science, Utah State University, Logan, UT, United States; Kulyukin V.A., Computer Science Assistive Technology Laboratory, Department of Computer Science, Utah State University, Logan, UT, United States","Research on spatial cognition and blind navigation suggests that a device aimed at helping blind people to shop independently should provide the shopper with effective interfaces to the locomotor and haptic spaces of the supermarket. In this article, we argue that robots can act as effective interfaces to haptic and locomotor spaces in modern supermarkets. We also present the design and evaluation of three product selection modalities-browsing, typing and speech, which allow the blind shopper to select the desired product from a repository of thousands of products. © 2008 Springer-Verlag.","Assistive robotics; Blind navigation; Haptic and locomotor interfaces; Human-robot interaction; Independent shopping for the visually impaired; Service robotics; Spatial cognition","","C. P. Gharpure; Computer Science Assistive Technology Laboratory, Department of Computer Science, Utah State University, Logan, UT, United States; email: cpg@cc.usu.edu","","","18612784","","","","English","Intelligent Serv. Rob.","Article","Final","","Scopus","2-s2.0-46449132329"
"Hersh M.A.; Johnson M.A.","Hersh, Marion A. (7005937731); Johnson, Michael A. (36071986800)","7005937731; 36071986800","A robotic guide for blind people. Part 1. A multi-national survey of the attitudes, requirements and preferences of potential end-users","2010","Applied Bionics and Biomechanics","7","4","","277","288","11","22","10.1080/11762322.2010.523626","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-78650265067&doi=10.1080%2f11762322.2010.523626&partnerID=40&md5=f545ebd2ae5a6beeada42afeda8fc122","Department of Electronics and Electrical Engineering, University of Glasgow, Glasgow, United Kingdom; Department of Electronic and Electrical Engineering, University of Strathclyde, Glasgow, United Kingdom","Hersh M.A., Department of Electronics and Electrical Engineering, University of Glasgow, Glasgow, United Kingdom; Johnson M.A., Department of Electronic and Electrical Engineering, University of Strathclyde, Glasgow, United Kingdom","This paper reports the results of a multi-national survey in several different countries on the attitudes, requirements and preferences of blind and visually impaired people for a robotic guide. The survey is introduced by a brief overview of existing work on robotic travel aids and other mobile robotic devices. The questionnaire comprises three sections on personal information about respondents, existing use of mobility and navigation devices and the functions and other features of a robotic guide. The survey found that respondents were very interested in the robotic guide having a number of different functions and being useful in a wide range of circumstances. They considered the robot's appearance to be very important but did not like any of the proposed designs. From their comments, respondentswanted the robot to be discreet and inconspicuous, small, light weight and portable, easy to use, robust to damage, require minimal maintenance, have a long life and a long battery life. Copyright © 2010 Taylor & Francis.","Blind people; End-user; Requirements analysis; Robotic guide; Survey","Handicapped persons; Machine design; Requirements engineering; Robotics; Robots; Battery life; Blind people; End users; Light weight; Long life; Mobile robotic; Navigation devices; Personal information; Requirements analysis; Travel aid; Visually impaired people; Surveys","M. A. Hersh; Department of Electronics and Electrical Engineering, University of Glasgow, Glasgow, United Kingdom; email: m.hersh@elec.gla.ac.uk","","Hindawi Limited","11762322","","","","English","Appl. Bionics Biomech.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-78650265067"
"Ivanchenko V.; Coughlan J.; Gerrey W.; Shen H.","Ivanchenko, Volodymyr (7005576389); Coughlan, James (7006567201); Gerrey, William (6508179945); Shen, Huiying (14523706200)","7005576389; 7006567201; 6508179945; 14523706200","Computer vision-based clear path guidance for blind wheelchair users","2008","ASSETS'08: The 10th International ACM SIGACCESS Conference on Computers and Accessibility","","","","291","292","1","21","10.1145/1414471.1414543","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-62949189585&doi=10.1145%2f1414471.1414543&partnerID=40&md5=b58ab59610ea538b3cf630e230111de6","Smith-Kettlewell Eye Research Institute, San Francisco, CA 94115, 2318 Fillmore St., United States","Ivanchenko V., Smith-Kettlewell Eye Research Institute, San Francisco, CA 94115, 2318 Fillmore St., United States; Coughlan J., Smith-Kettlewell Eye Research Institute, San Francisco, CA 94115, 2318 Fillmore St., United States; Gerrey W., Smith-Kettlewell Eye Research Institute, San Francisco, CA 94115, 2318 Fillmore St., United States; Shen H., Smith-Kettlewell Eye Research Institute, San Francisco, CA 94115, 2318 Fillmore St., United States","We describe a system for guiding blind and visually impaired wheelchair users along a clear path that uses computer vision to sense the presence of obstacles or other terrain features and warn the user accordingly. Since multiple terrain features can be distributed anywhere on the ground, and their locations relative to a moving wheelchair are continually changing, it is challenging to communicate this wealth of spatial information in a way that is rapidly comprehensible to the user. The main contribution of our system is the development of a novel user interface that allows the user to interrogate the environment by sweeping a standard (unmodified) white cane back and forth: the system continuously tracks the cane location and sounds an alert if a terrain feature is detected in the direction the cane is pointing. Experiments are described demonstrating the feasibility of the approach.","Blindness; Mobility; Orientation; Wheelchair","Computer vision; Eye protection; Location; Standardization; User interfaces; Blindness; Mobility; Orientation; Path guidances; Spatial informations; Terrain features; Visually impaired; White canes; Wheelchairs","V. Ivanchenko; Smith-Kettlewell Eye Research Institute, San Francisco, CA 94115, 2318 Fillmore St., United States; email: vivanchenko@ski.org","","","","978-159593976-0","","","English","ASSETS: Int. ACM SIGACCESS Conf. Comput. Accessibility","Conference paper","Final","","Scopus","2-s2.0-62949189585"
"Lacey G.; MacNamara S.","Lacey, Gerard (7004524333); MacNamara, Shane (6603425242)","7004524333; 6603425242","User involvement in the design and evaluation of a smart mobility aid","2000","Journal of Rehabilitation Research and Development","37","6","","709","723","14","52","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-0034352189&partnerID=40&md5=94b9c3bb0425a1a46ff3520495d9ffe1","Trinity College, Department of Computer Science, Dublin 2, Ireland; Vartry Research Ltd., Wicklow Town, Wicklow Enterprise Park, Ireland","Lacey G., Trinity College, Department of Computer Science, Dublin 2, Ireland, Vartry Research Ltd., Wicklow Town, Wicklow Enterprise Park, Ireland; MacNamara S., Trinity College, Department of Computer Science, Dublin 2, Ireland, Vartry Research Ltd., Wicklow Town, Wicklow Enterprise Park, Ireland","This paper describes the design and evaluation of an innovative smart mobility aid for the frail visually impaired. The Personal Adaptive Mobility AID (PAM-AID) was developed to address the difficulties in personal mobility of the frail and elderly visually impaired. The paper provides an overview of the PAM-AID research at Trinity College and describes the evolutionary nature of the design process. Because there were no existing systems to guide its development, a series of prototypes was constructed and they were regularly evaluated in the field. This approach views potential users as vital contributing members of the design team and led to rapid and hopefully useful improvements in the design.","Elderly; Mobility aid; Robot mobility aids; Visual impairment","Aged; Equipment Design; Frail Elderly; Humans; Locomotion; Questionnaires; Robotics; Visually Impaired Persons; Planning; Product design; Vision; Handicapped persons; Mobility aids for blind persons; Robot applications; article; biomedical engineering; device; human; locomotion; man machine interaction; priority journal; technical aid; visual impairment; Visually impaired; Visual impairment; Mobility aids for blind persons; Walking aids","G. Lacey; Vartry Research Ltd., Wicklow Enterprise Centre, Wicklow Town, Ireland; email: gerard.lacey@vartry.com","","","07487711","","JRRDD","11321007","English","J. Rehabil. Res. Dev.","Article","Final","","Scopus","2-s2.0-0034352189"
"Zelek J.; Bullock D.; Bromley S.; Wu H.","Zelek, John (6603746225); Bullock, Dave (8705261600); Bromley, Sam (7006401356); Wu, Haisheng (56130024800)","6603746225; 8705261600; 7006401356; 56130024800","What the Robot Sees & Understands Facilitates Dialog","2002","AAAI Fall Symposium - Technical Report","FS-02-03","","","120","129","9","3","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-34047163048&partnerID=40&md5=baaef694f008e45c4125214bf17a549a","Intelligent Systems Lab, School of Engineering, University of Guelph, Guelph, N1G 2W1, ON, Canada","Zelek J., Intelligent Systems Lab, School of Engineering, University of Guelph, Guelph, N1G 2W1, ON, Canada; Bullock D., Intelligent Systems Lab, School of Engineering, University of Guelph, Guelph, N1G 2W1, ON, Canada; Bromley S., Intelligent Systems Lab, School of Engineering, University of Guelph, Guelph, N1G 2W1, ON, Canada; Wu H., Intelligent Systems Lab, School of Engineering, University of Guelph, Guelph, N1G 2W1, ON, Canada","The particular applications that interest us include search and rescue robotics, security, elder/disabled care or assistance, or service robotics. In all of these application areas, visual perception and internal representations play a pivotal role in how humans and robots communicate. In general, visual perception plays an important role in communicating information amongst humans. It is typically imprecise and humans usually use their cognitive abilities to interpret the intent. We explore real-time probabilistic visual perception in three different roles: (1) the tracking of human limbs; (2) stereo vision for robot and human navigation; and (3) optical flow for detecting salient events and structure from motion. Our visual perception efforts are expressed in probability distribution functions (i.e., Bayesian). The robot requires to have this uncertainty propagated for any subsequent decision-making task. A related application we are also exploring is using real-time stereo vision to convey depth information to a blind person via tactile feedback. We anticipate that this will provide us some glues for internal representations that can form the basis for human-robot communications. We propose that this representation be based on a minimal spanning basis set of spatial prepositions and show how it can be used as a basis for commands. We assume that uncertainty can be conveyed through the linguistic representation in a fuzzy descriptor. Copyright © 2002, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.","","Decision making; Distribution functions; Human robot interaction; Intelligent robots; Robot vision; Stereo image processing; Application area; Cognitive ability; Human limbs; Internal representation; Probabilistics; Real- time; Search and Rescue Robotics; Service robotics; Uncertainty; Visual perception; Stereo vision","","","Association for the Advancement of Artificial Intelligence","","1577351746; 978-157735174-0","","","English","AAAI Fall Symp. Tech. Rep.","Conference paper","Final","","Scopus","2-s2.0-34047163048"
"Kim D.S.; Wall Emerson R.S.; Curtis A.B.","Kim, Dae Shik (55742923000); Wall Emerson, Robert S. (15836609000); Curtis, Amy B. (7202352967)","55742923000; 15836609000; 7202352967","Ergonomic factors related to drop-off detection with the long cane: Effects of cane tips and techniques","2010","Human Factors","52","3","","456","465","9","15","10.1177/0018720810374196","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-77958506766&doi=10.1177%2f0018720810374196&partnerID=40&md5=46131f22afe98df1e1ea8224833f0a04","Western Michigan University, Department of Blindness and Low Vision Studies, Kalamazoo, MI 49002-5218, 1903 W. Michigan Avenue, United States","Kim D.S., Western Michigan University, Department of Blindness and Low Vision Studies, Kalamazoo, MI 49002-5218, 1903 W. Michigan Avenue, United States; Wall Emerson R.S., Western Michigan University, Department of Blindness and Low Vision Studies, Kalamazoo, MI 49002-5218, 1903 W. Michigan Avenue, United States; Curtis A.B., Western Michigan University, Department of Blindness and Low Vision Studies, Kalamazoo, MI 49002-5218, 1903 W. Michigan Avenue, United States","Objective: This study examined the effect of cane tips and cane techniques on drop-off detection with the long cane. Background: Blind pedestrians depend on a long cane to detect drop-offs. Missing a drop-off may result in falls or collision with moving vehicles in the street. Although cane tips appear to affect a cane user?s ability to detect drop-offs, few experimental studies have examined such effect. Method: A repeated-measures design with block randomization was used for the study. Participants were 17 adults who were legally blind and had no other disabilities. Participants attempted to detect the drop-offs of varied depths using different cane tips and cane techniques. Results: Drop-off detection rates were similar between the marshmallow tip (77.0%) and the marshmallow roller tip (79.4%) when both tips were used with the constant contact technique, p =.294. However, participants detected drop-offs at a significantly higher percentage when they used the constant contact technique with the marshmallow roller tip (79.4%) than when they used the two-point touch technique with the marshmallow tip (63.2%), p <.001. Conclusion: The constant contact technique used with a marshmallow roller tip (perceived as a less advantageous tip) was more effective than the two-point touch technique used with a marshmallow tip (perceived as a more advantageous tip) in detecting drop-offs. Application: The findings of the study may help cane users and orientation and mobility specialists select appropriate cane techniques and cane tips in accordance with the cane user?s characteristics and the nature of the travel environment. © 2010, Human Factors and Ergonomics Society.","blind mobility; constant contact; marshmallow tip; roller tip; two-point touch; visually impaired","Accidental Falls; Adult; Aged; Blindness; Canes; Equipment Design; Female; Human Engineering; Humans; Male; Middle Aged; Self-Help Devices; Touch; Handicapped persons; Rollers (machine components); Blind mobility; Constant contact; marshmallow tip; roller tip; Two-point; Visually impaired; adult; aged; article; bioengineering; blindness; clinical trial; controlled clinical trial; controlled study; equipment design; falling; female; human; male; middle aged; randomized controlled trial; self help; touch; walking aid; Drops","D. S. Kim; Western Michigan University, Department of Blindness and Low Vision Studies, Kalamazoo, MI 49002-5218, 1903 W. Michigan Avenue, United States; email: dae.kim@wmich.edu","","","15478181","","HUFAA","21077566","English","Hum. Factors","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-77958506766"
"Freitas D.; Kouroupetroglou G.","Freitas, Diamantino (35620351600); Kouroupetroglou, Georgios (8652509400)","35620351600; 8652509400","Speech technologies for blind and low vision persons","2008","Technology and Disability","20","2","","135","156","21","50","10.3233/tad-2008-20208","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-47949100131&doi=10.3233%2ftad-2008-20208&partnerID=40&md5=1dacb2d0c9a24b72087087f8da83e642","Department of Communication Engineering, University of Porto, 4200-465 Porto, Rua Dr. Roberto Frias, Portugal; Department of Informatics and Telecommunications, University of Athens, Athens, Greece","Freitas D., Department of Communication Engineering, University of Porto, 4200-465 Porto, Rua Dr. Roberto Frias, Portugal; Kouroupetroglou G., Department of Informatics and Telecommunications, University of Athens, Athens, Greece","In this work a review of speech technologies and their applications that provide or augment access to the printed or electronic information, the daily or social activities, as well as the private or public facilities for blind or low vision persons is presented. Speech technologies are currently considered to be essential for providing general purpose interfacing besides providing accessibility for the people who are visually impaired. Speech-enabled devices, reading machines, accessible computers, software applications, World Wide Web (www) content and structured environments constitute the main areas addressed throughout this paper with reference to the background technologies, architectures, formats, on-going research activities and projects. In the state-of-the art of the accessibility field, the speech communication channel is considered by the authors as one of the most important modality to benefit the blind and low vision persons. © 2008 IOS Press. All rights reserved.","Accessibility; Audio description; Automated reading devices; Digital talking books; Mobility; Screen-readers; Speech recognition; Talking devices; Text-to-speech; Voice browsers; Voice portals","access to information; architecture; article; automatic speech recognition; automation; blindness; communication aid; computer program; computer system; daily life activity; geographic information system; human computer interaction; information retrieval; Internet; linguistics; mathematical computing; reading; social interaction; technology; television viewing; visual impairment; web browser; writing","D. Freitas; Department of Electrical and Computer Engineering, University of Porto, 4200-465 Porto, Rua Dr. Roberto Frias, Portugal; email: dfreitas@fe.up.pt","","IOS Press","10554181","","TEDIF","","English","Technol. Disabil.","Article","Final","","Scopus","2-s2.0-47949100131"
"Bostelman R.; Russo P.; Albus J.; Hong T.; Madhavan R.","Bostelman, R. (6603200295); Russo, P. (57197932968); Albus, J. (7006830229); Hong, T. (7202830480); Madhavan, R. (7005652060)","6603200295; 57197932968; 7006830229; 7202830480; 7005652060","Applications of a 3D range camera towards healthcare mobility aids","2006","Proceedings of the 2006 IEEE International Conference on Networking, Sensing and Control, ICNSC'06","","","1673182","416","421","5","28","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-34250177662&partnerID=40&md5=055a6bb15180fe11ce25940887608bc6","National Institute of Standards and Technology (NIST), Gaithersburg, MD 20899, United States; University of Maryland, United States; NIST, United States","Bostelman R., National Institute of Standards and Technology (NIST), Gaithersburg, MD 20899, United States; Russo P., National Institute of Standards and Technology (NIST), Gaithersburg, MD 20899, United States, University of Maryland, United States, NIST, United States; Albus J., National Institute of Standards and Technology (NIST), Gaithersburg, MD 20899, United States; Hong T., National Institute of Standards and Technology (NIST), Gaithersburg, MD 20899, United States; Madhavan R., National Institute of Standards and Technology (NIST), Gaithersburg, MD 20899, United States","The National Institute of Standards and Technology (NIST) has recently studied a new 3D range camera for use on mobile robots. These robots have potential applications in manufacturing, healthcare and perhaps several other service related areas beyond the scope of this paper. In manufacturing, the 3D range camera shows promise for standard size obstacle detection possibly augmenting existing safety systems on automated guided vehicles. We studied the use of this new 3D range imaging camera for advancing safety standards for automated guided vehicles. In healthcare, these cameras show promise for guiding the blind and assisting the disabled who are wheelchair dependent. Further development beyond standards efforts allowed NIST to combine the 3D camera with stereo audio feedback to help the blind or visually impaired to stereophonically hear where a clear path is from room to room as objects were detected with the camera. This paper describes the 3D range camera and the control algorithm that combines the camera with stereo audio to help guide people around objects, including the detection of low hanging objects typically undetected by a white cane. © 2006 IEEE.","","Algorithms; Cameras; Feedback control; Health care; Mobile robots; Virtual reality; Automated guided vehicles; Safety standards; Stereo audio feedback; Wheelchairs","R. Bostelman; National Institute of Standards and Technology (NIST), Gaithersburg, MD 20899, United States; email: roger.bostelman@nist.gov","","","","1424400651; 978-142440065-2","","","English","Proc. IEEE Int. Conf. Netw. Sensing Contr.","Conference paper","Final","","Scopus","2-s2.0-34250177662"
"Lacey G.; MacNamara S.","Lacey, Gerard (7004524333); MacNamara, Shane (6603425242)","7004524333; 6603425242","Context-aware shared control of a robot mobility aid for the elderly blind","2000","International Journal of Robotics Research","19","11","","1054","1065","11","38","10.1177/02783640022067968","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-0034316369&doi=10.1177%2f02783640022067968&partnerID=40&md5=7c595db34c3a47b20c7ecb9a8488f840","Department of Computer Science, O'Reilly Institute, Trinity College, Dublin 2, Ireland","Lacey G., Department of Computer Science, O'Reilly Institute, Trinity College, Dublin 2, Ireland; MacNamara S., Department of Computer Science, O'Reilly Institute, Trinity College, Dublin 2, Ireland","This paper describes the use of a Bayesian network to provide context-aware shared control of a robot mobility aid for the frail blind. The robot mobility aid, PAM-AID, is a 'smart walker' that aims to assist the frail and elderly blind to walk safety indoors. The Bayesian network combines user input with high-level information derived from the sensors to provide a context-aware estimate of the user's current navigation goals. This context-aware action selection mechanism facilitates the use of a very simple, low bandwidth user interface, which is critical for the elderly user group. The PAM-AID systems have been evaluated through a series of field trails involving over 30 potential users.","","Mobility aids for blind persons; Motion control; Neural networks; Proximity sensors; User interfaces; Bayesian networks; Context aware shared control; Visually impaired; Mobile robots","","","Sage Sci Press","02783649","","IJRRE","","English","Int J Rob Res","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-0034316369"
"Shaik A.S.; Hossain G.; Yeasin M.","Shaik, Akbar S. (36722418600); Hossain, G. (24447823600); Yeasin, M. (18039042000)","36722418600; 24447823600; 18039042000","Design, development and performance evaluation of reconfigured Mobile Android Phone for people who are blind or visually impaired","2010","SIGDOC 2010 - Proceedings of the 28th ACM International Conference on Design of Communication","","","","159","166","7","28","10.1145/1878450.1878478","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-78650854337&doi=10.1145%2f1878450.1878478&partnerID=40&md5=106c7d554fd31a2495c422994945526c","Department of EECE, University of Memphis, Memphis, TN 38152, United States","Shaik A.S., Department of EECE, University of Memphis, Memphis, TN 38152, United States; Hossain G., Department of EECE, University of Memphis, Memphis, TN 38152, United States; Yeasin M., Department of EECE, University of Memphis, Memphis, TN 38152, United States","This paper presents the design, development and performance evaluation of a Reconfigured Mobile Android Phone (R-MAP) designed and implemented to facilitate day-to-day activities for people who are blind or visually impaired. Some of these activities include but are not limited to: reading envelopes, letters, medicine bottles, food containers in refrigerators; as well as, following a route plan, shopping and browsing, walking straight and avoiding collisions, crossing traffic intersections, finding references in an open space, etc. The key objectives were to develop solutions that are light weight, low cost, un-tethered and have an intuitive and easy to use interface that can be reconfigured to perform a large number of tasks. The Android architecture was used to integrate the cell phone camera, image capturing and analysis routines, on-device implementation of robust and efficient optical character recognition (OCR) engine and text to speech (TTS) engine to develop the proposed application in real-time. Empirical analysis under various environments (such as indoor, outdoor, complex background, different surfaces, and different orientations) and usability studies were performed to illustrate the efficacy of the R-MAP. Improved feedback and new functions were added based on usability study results. Copyright 2010 ACM.","Access to printed text; Blind or visually impaired; Mobile assistive technology; OCR and TTS; On-device integration of Image; Usability study","Bottles; Cellular telephone systems; Design; Optical character recognition; Robots; Speech recognition; Telecommunication equipment; Telephone sets; Usability engineering; Device integration; Mobile assistive technology; OCR and TTS; Printed texts; Usability studies; Visually impaired; Handicapped persons","A. S. Shaik; Department of EECE, University of Memphis, Memphis, TN 38152, United States; email: asshaik@memphis.edu","","","","978-145030403-0","","","English","SIGDOC - Proc. ACM Int. Conf. Des. Commun.","Conference paper","Final","","Scopus","2-s2.0-78650854337"
"Hannuna S.; Xie X.; Mirmehdi M.; Campbell N.","Hannuna, Sion (9239823000); Xie, Xianghua (7402761335); Mirmehdi, Majid (7004105162); Campbell, Neill (7201796804)","9239823000; 7402761335; 7004105162; 7201796804","Generic motion based object segmentation for assisted navigation","2009","VISAPP 2009 - Proceedings of the 4th International Conference on Computer Vision Theory and Applications","2","","","450","457","7","1","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-70349293776&partnerID=40&md5=2f3b53fc7add5d0f6471e72477ffd071","Department of Computer Science, University of Bristol, Bristol, United Kingdom; Department of Computer Science, University of Wales Swansea, Swansea, United Kingdom","Hannuna S., Department of Computer Science, University of Bristol, Bristol, United Kingdom; Xie X., Department of Computer Science, University of Wales Swansea, Swansea, United Kingdom; Mirmehdi M., Department of Computer Science, University of Bristol, Bristol, United Kingdom; Campbell N., Department of Computer Science, University of Bristol, Bristol, United Kingdom","We propose a robust approach to annotating independently moving objects captured by head mounted stereo cameras that are worn by an ambulatory (and visually impaired) user. Initially, sparse optical flow is extracted from a single image stream, in tandem with dense depth maps. Then, using the assumption that apparent movement generated by camera egomotion is dominant, flow corresponding to independently moving objects (IMOs) is robustly segmented using MLESAC. Next, the mode depth of the feature points defining this flow (the foreground) are obtained by aligning them with the depth maps. Finally, a bounding box is scaled proportionally to this mode depth and robustly fit to the foreground points such that the number of inliers is maximised.","Assisted blind navigation; Sparse optical flow; Stereo depth; Uncategorised object detection","Cameras; Computer vision; Navigation; Special effects; Apparent movement; Assisted blind navigation; Bounding box; Dense depth map; Depth Map; Ego-motion; Feature point; Moving objects; Object segmentation; Single images; Sparse optical flow; Stereo cameras; Stereo depth; Uncategorised object detection; Visually impaired; Optical flows","S. Hannuna; Department of Computer Science, University of Bristol, Bristol, United Kingdom; email: hannuna@cs.bris.ac.uk","","","","978-989811169-2","","","English","VISAPP - Proc. Int. Conf. Comput. Vis. Theory Appl.","Conference paper","Final","","Scopus","2-s2.0-70349293776"
"Sabatini A.M.; Genovese V.; Maini E.S.","Sabatini, Angelo M. (7005616188); Genovese, Vincenzo (6603415516); Maini, Eliseo S. (6507846040)","7005616188; 6603415516; 6507846040","BE-viewer: Vision-based navigation system to assist motor-impaired people in docking their mobility aids","2003","Proceedings - IEEE International Conference on Robotics and Automation","1","","","1318","1323","5","5","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-0344444638&partnerID=40&md5=16a71b6d9a03c53744f09a7e6ab4d9b3","Scuola Superiore Sant' Anna, Pisa, Italy","Sabatini A.M., Scuola Superiore Sant' Anna, Pisa, Italy; Genovese V., Scuola Superiore Sant' Anna, Pisa, Italy; Maini E.S., Scuola Superiore Sant' Anna, Pisa, Italy","In this paper we present Bird's-Eye (BE) Viewer - a sensor-based navigation system intended to assist motor-impaired people driving their mobility aids to a specified goal. The BE-Viewer is built around a vision-based 2D localization system based on a monocular image system composed of one web-camera, and endowed with the real-time capability of estimating the location of active light-emitting markers. The 2D localization system is integrated in a Human-Machine-Interface, which provides the user with a ""top-view"" sketch of the current and desired configurations. We introduce a protocol for the functional assessment of the docking procedure, here interpreted as a pursuit tracking task. The protocol allows to characterize how subjects use their resources to achieve a balance between speed, accuracy and control effort, depending on whether the perceptual enhancement by the BE-Viewer is available or not. The benefits of the BE-View approach are presented and discussed in preliminary experiments where normal subjects perform docking procedures using a motorized anterior walker.","","Computer vision; Human computer interaction; Imaging systems; Mobility aids for blind persons; Navigation; Unmanned vehicles; User interfaces; Autonomous guided vehicles; Motor impaired people; Vision based navigation system; Mobile robots","A.M. Sabatini; Scuola Superiore Sant' Anna, Pisa, Italy; email: A.Sabatini@mail-arts.sssup.it","","","10504729","","PIIAE","","English","Proc IEEE Int Conf Rob Autom","Conference paper","Final","","Scopus","2-s2.0-0344444638"
"Bourbakis N.; Keefer R.; Dakopoulos D.; Esposito A.","Bourbakis, Nikolaos (7006247012); Keefer, Robert (36977953600); Dakopoulos, Dimitrios (24472639800); Esposito, Anna (35310158400)","7006247012; 36977953600; 24472639800; 35310158400","A multimodal interaction scheme between a blind user and the tyflos assistive prototype","2008","Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI","2","","4669814","487","494","7","23","10.1109/ICTAI.2008.52","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-57649210348&doi=10.1109%2fICTAI.2008.52&partnerID=40&md5=b8822a637429ff0283b58e1fa75c0dca","Wright State University, ATRC, Dayton, OH 45435, United States; Second University of Naples, Naples, Italy; AIIS Inc., Dayton, OH 45458, United States","Bourbakis N., Wright State University, ATRC, Dayton, OH 45435, United States, AIIS Inc., Dayton, OH 45458, United States; Keefer R., Wright State University, ATRC, Dayton, OH 45435, United States; Dakopoulos D., Wright State University, ATRC, Dayton, OH 45435, United States; Esposito A., Wright State University, ATRC, Dayton, OH 45435, United States, Second University of Naples, Naples, Italy","This paper presents the multimodal interaction scheme (visual and audio) used by the Tyflos prototype. Tyflos is a wearable prototype that provides reading and navigating assistance for visually impaired users. In particular, the Tyflos prototype integrates a wireless portable computer, cameras, range and GPS sensors, microphones, natural language processor, text-to-speech device, an ear speaker, a speech synthesizer, a 2D vibration vest and a digital audio recorder. Data collected by the Tyflos sensors is processed by appropriate modules, each of which is specialized in one or more tasks. In this paper we also present a Stochastic Petri-net model of the multimodal interaction scheme for both of the Tyflos capabilities, reading and navigation. Simple illustrative examples from reading and navigation cases are also presented to demonstrate the multimodal interaction. © 2008 IEEE.","","Artificial intelligence; Flow interactions; Interactive computer systems; Microcomputers; Mobile computing; Navigation; Sensors; Stochastic models; Blind users; Digital audio; Illustrative examples; MultiModal interactions; Natural languages; Petri-net; Portable computers; Speech synthesizers; Visually impaired; Wearable prototypes; Theorem proving","N. Bourbakis; Wright State University, ATRC, Dayton, OH 45435, United States; email: nikolaos.bourbakis@wright.edu","","","10823409","978-076953440-4","PCTIF","","English","Proc. Int. Conf. Tools Artif. Intell. ICTAI","Conference paper","Final","","Scopus","2-s2.0-57649210348"
"Kayama K.; Yairj I.E.; Igi S.","Kayama, Kentaro (23008723400); Yairj, Ikuko Eguchi (6504581876); Igi, Seiji (6603691798)","23008723400; 6504581876; 6603691798","Semi-Autonomous Outdoor Mobility Support System for Elderly and Disabled People","2003","IEEE International Conference on Intelligent Robots and Systems","3","","","2606","2611","5","11","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-0346778953&partnerID=40&md5=c0668061aa78462917558a1401c9a5f7","Communications Research Laboratory, Yokosuka, Kanagawa 239-0847, Japan","Kayama K., Communications Research Laboratory, Yokosuka, Kanagawa 239-0847, Japan; Yairj I.E., Communications Research Laboratory, Yokosuka, Kanagawa 239-0847, Japan; Igi S., Communications Research Laboratory, Yokosuka, Kanagawa 239-0847, Japan","We have been developing the Robotic Communication Terminals (RCTs), which are integrated into a mobility support system to assist elderly or disabled people who suffer from impaired mobility. The RCT system consists of three types of terminals and one server: an environment-embedded terminal, a user-carried mobile terminal, a user-carrying mobile terminal, and a barrier-free map server. The RCT is an integrated system that can be used to cope with various problems of mobility, and provide suitable support to a wide variety of users. This paper provides an in-depth description of the user-carrying mobile terminal . The system itself is a kind of intelligent wheeled vehicle. It can recognize the surrounding 3D environment through infrared sensors, sonar sensors, and a stereo vision system with three cameras, and avoid hazards semi-autonomously. It also can provide adequate navigation by communicating with the geographic information system (GIS) server and detect vehicles appearing from the blind side by communicating with environment-embedded terminals in the real-world.","","Collision avoidance; Embedded systems; Geographic information systems; Intelligent control; Local area networks; Mobile robots; Sensors; Sonar; Map servers; Robotic communication terminals (RCT); Robot applications","","","","","","85RBA","","English","IEEE Int Conf Intell Rob Syst","Conference paper","Final","","Scopus","2-s2.0-0346778953"
"Mayerhofer B.; Pressl B.; Wieser M.","Mayerhofer, Bernhard (24725290500); Pressl, Bettina (14523162600); Wieser, Manfred (14523980500)","24725290500; 14523162600; 14523980500","ODILIA - A mobility concept for the visually impaired","2008","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","5105 LNCS","","","1109","1116","7","13","10.1007/978-3-540-70540-6_166","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-50249098192&doi=10.1007%2f978-3-540-70540-6_166&partnerID=40&md5=ebcab894aac61056a793e8f755a4d2f6","Institute of Navigation and Satellite Geodesy, Graz University of Technology","Mayerhofer B., Institute of Navigation and Satellite Geodesy, Graz University of Technology; Pressl B., Institute of Navigation and Satellite Geodesy, Graz University of Technology; Wieser M., Institute of Navigation and Satellite Geodesy, Graz University of Technology","A navigation system for visually impaired people has to take into account the special requirements of these users. Within this group, there is also need for a customizable man-machine interface tailored to the individual. It has to be suitable for people depending on orientation by the sense of hearing or on tactile orientation, always avoiding disturbance of the user's remaining senses. On the other side, the hardware for data input and on-trip control should not exceed a certain size and weight. To be accepted for daily use, the overall system must not be stigmatizing the user. Stigmatizing means, that visually impaired users often do not want to be apparently distinguishable from the average pedestrian by wearing noticeable equipment. Another point is reliability and accuracy of the system which are essential features, because a blind person can be reliant on the system when entering an unknown area. The navigation system developed in ODILIA should provide accuracy, reliability of routing and guidance and the possibility to give the user an impression of the surrounding area. © 2008 Springer-Verlag Berlin Heidelberg.","","Navigation systems; Weight control; Blind person; Customizable; Daily use; Data inputs; International conferences; Man-machine interfacing; Special needs; Surrounding area; Visually impaired; Visually Impaired People; Visually-impaired users; Navigation","B. Mayerhofer; Institute of Navigation and Satellite Geodesy, Graz University of Technology, Austria; email: mayerhofer@tugraz.at","","","16113349","3540705392; 978-354070539-0","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","","Scopus","2-s2.0-50249098192"
"Haraszy Z.; Cristea D.-G.; Tiponut V.; Slavici T.","Haraszy, Zoltan (24476948300); Cristea, David-George (57195451009); Tiponut, Virgil (16041074100); Slavici, Titus (23502579200)","24476948300; 57195451009; 16041074100; 23502579200","Improved head related transfer function generation and testing for acoustic virtual reality development","2010","International Conference on Systems - Proceedings","1","","","411","416","5","12","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-77957578630&partnerID=40&md5=111b2e4a9bc20023df553277a0e7d2e2","Department of Applied Electronics, POLITEHNICA University of Timisoara, 300223 Timisoara, Bvd. Vasile Parvan 2, Romania","Haraszy Z., Department of Applied Electronics, POLITEHNICA University of Timisoara, 300223 Timisoara, Bvd. Vasile Parvan 2, Romania; Cristea D.-G., Department of Applied Electronics, POLITEHNICA University of Timisoara, 300223 Timisoara, Bvd. Vasile Parvan 2, Romania; Tiponut V., Department of Applied Electronics, POLITEHNICA University of Timisoara, 300223 Timisoara, Bvd. Vasile Parvan 2, Romania; Slavici T., Department of Applied Electronics, POLITEHNICA University of Timisoara, 300223 Timisoara, Bvd. Vasile Parvan 2, Romania","The new Acoustic Virtual Reality (AVR) concept is often used as a man-machine interface in electronic travel aid (ETA), that help blind and visually impaired individuals to navigate in real outdoor environments. According to this concept, the presence of obstacles in the surrounding environment and the path to the desired target will be signalized to the blind subject by burst of sounds, whose virtual source position suggests the position of the real obstacles and the direction of movement, respectively. The practical implementation of the AVR concept requires the so-called Head Related Transfer Functions (HRTFs) to be known in every point of the 3D space and for each subject. These functions can be determined by using a quite complex procedure, which requires many measurements for each individual. In the present paper, an improved version of the previously proposed [12] artificial neural network (ANN) is presented and used, in order to obtain the HRTFs. The proposed method, valid for only one subject, speeds up the implementation of the AVR concept after the ANN training has been completed. Finally, the experimental setup for testing, some experimental results using the new ANNs, conclusions and further developments are also presented.","Acoustic virtual reality; Artificial neural networks; Head related transfer functions; Localization experiment; Man-machine interface; Visually impaired","Neural networks; Virtual reality; Artificial Neural Network; Head related transfer function; Localization experiment; Man-machine interface; Visually impaired; Transfer functions","Z. Haraszy; Department of Applied Electronics, POLITEHNICA University of Timisoara, 300223 Timisoara, Bvd. Vasile Parvan 2, Romania; email: zoltan.haraszy@etc.upt.ro","","","","978-960474199-1","","","English","Int. Conf. Syst. - Proc.","Conference paper","Final","","Scopus","2-s2.0-77957578630"
"Kulyukin V.; Gharpure C.; Coster D.","Kulyukin, Vladimir (6603006662); Gharpure, Chaitanya (7801362187); Coster, Daniel (35901469000)","6603006662; 7801362187; 35901469000","Robot-assisted shopping for the visually impaired: Proof-of-concept design and feasibility evaluation","2008","Assistive Technology","20","2","","86","98","12","12","10.1080/10400435.2008.10131935","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-46449092436&doi=10.1080%2f10400435.2008.10131935&partnerID=40&md5=0aa16eb8c1c21a6a68e590364d1cbd95","Computer Science Assistive Technology Laboratory and Department of Computer Science, Utah State University, Logan, UT, United States; Department of Mathematics and Statistics, Utah State University, Logan, UT, United States","Kulyukin V., Computer Science Assistive Technology Laboratory and Department of Computer Science, Utah State University, Logan, UT, United States; Gharpure C., Computer Science Assistive Technology Laboratory and Department of Computer Science, Utah State University, Logan, UT, United States; Coster D., Department of Mathematics and Statistics, Utah State University, Logan, UT, United States","This article presents RoboCart, a proof-of-concept prototype of a robotic shopping cart for the visually impaired in supermarkets. RoboCart autonomously leads shoppers to required locations and cues them through synthetic speech and a portable barcode reader to the salient features of the environment sufficient for product retrieval. In a longitudinal pilot feasibility study, visually impaired shoppers (n = 10) used the device to retrieve products in Lee's Marketplace, a supermarket in Logan, Utah. The main finding is that RoboCart enables visually impaired shoppers to reliably and independently navigate to and retrieve products in a real supermarket. © 2008 Taylor and Francis Group, LLC.","Assistive robotics; Blindness; Navigation; Spatial cognition","Commerce; Equipment Design; Feasibility Studies; Food Handling; Robotics; Self-Help Devices; Visually Impaired Persons; Decision making; Machine design; Planning; Resource allocation; Robotics; Shopping centers; Bar coding; Feasibility studies; Proof-of-concept (POC); Proof-of-concept design; Retrieval (MIR); Salient features; Shopping carts; synthetic speech; Visually impaired; article; association; clinical article; cognition; depth perception; equipment design; evaluation; feasibility study; human; pilot study; reliability; robotics; shopping; speech; United States; visual impairment; Retail stores","V. Kulyukin; Department of Computer Science, Utah State University, Logan, UT, 84322-4205, 4205 Old Main Hill, United States; email: vladimir.kulyukin@usu.edu","","","10400435","","","18646431","English","Assistive Technol.","Article","Final","","Scopus","2-s2.0-46449092436"
"Costa G.; Gusberti A.; Graffigna J.P.; Guzzo M.; Nasisi O.","Costa, Gustavo (23049390400); Gusberti, Adrián (23049752500); Graffigna, Juan Pablo (55992148100); Guzzo, Martín (36699724300); Nasisi, Oscar (6507702344)","23049390400; 23049752500; 55992148100; 36699724300; 6507702344","Mobility and orientation aid for blind persons using artificial vision","2007","Journal of Physics: Conference Series","90","1","012090","","","","10","10.1088/1742-6596/90/1/012090","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-36949022083&doi=10.1088%2f1742-6596%2f90%2f1%2f012090&partnerID=40&md5=515073ce6fcf03980dfeadaa4462e819","Facultad de Ingeniería, Universidad Nacional de San Juan, San Juan, Argentina","Costa G., Facultad de Ingeniería, Universidad Nacional de San Juan, San Juan, Argentina; Gusberti A., Facultad de Ingeniería, Universidad Nacional de San Juan, San Juan, Argentina; Graffigna J.P., Facultad de Ingeniería, Universidad Nacional de San Juan, San Juan, Argentina; Guzzo M., Facultad de Ingeniería, Universidad Nacional de San Juan, San Juan, Argentina; Nasisi O., Facultad de Ingeniería, Universidad Nacional de San Juan, San Juan, Argentina","Blind or vision-impaired persons are limited in their normal life activities. Mobility and orientation of blind persons is an ever-present research subject because no total solution has yet been reached for these activities that pose certain risks for the affected persons. The current work presents the design and development of a device conceived on capturing environment information through stereoscopic vision. The images captured by a couple of video cameras are transferred and processed by configurable and sequential FPGA and DSP devices that issue action signals to a tactile feedback system. Optimal processing algorithms are implemented to perform this feedback in real time. The components selected permit portability; that is, to readily get used to wearing the device. © 2007 IOP Publishing Ltd.","","","","","Institute of Physics Publishing","17426588","","","","English","J. Phys. Conf. Ser.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-36949022083"
"Sayah J.; Baudoin G.; Venard O.; El Hassan B.","Sayah, Jinane (16047902100); Baudoin, Geneviève (55939939100); Venard, Olivier (26326015900); El Hassan, Bachar (6603225120)","16047902100; 55939939100; 26326015900; 6603225120","Localization and guidance in RAMPE/INFOMOVILLE- an interactive system of assistance for blind travelers","2009","2nd International Conference on the Applications of Digital Information and Web Technologies, ICADIWT 2009","","","5273868","243","249","6","2","10.1109/ICADIWT.2009.5273868","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-71449114980&doi=10.1109%2fICADIWT.2009.5273868&partnerID=40&md5=a0e0290d153773f4b66a03821aeb412c","ESYCOM Group, Paris-Est University, France; Engineering Faculty, Lebanese University, Kobbe, Lebanon","Sayah J., ESYCOM Group, Paris-Est University, France; Baudoin G., ESYCOM Group, Paris-Est University, France; Venard O., ESYCOM Group, Paris-Est University, France; El Hassan B., Engineering Faculty, Lebanese University, Kobbe, Lebanon","We present in this paper the RAMPE/INFOMOVILLE system- an interactive Man-Machine-Interface for the assistance, information, orientation and security of persons having visual or/and auditive handicap in public transports. We desrcibe this system, its components and operation. The current version doesn't include a localization solution to guide the visually impaired person and assist him/her to reach the public transport station. A solution is proposed to address the needs of this application in terms of Guidance and Localization. We do base on the WiFi Positioning System currently very wide spread and easy to use in localizing assets and people but that presents, however, some disadvantages highlighted in the body of this paper. Some modifications were brought to remedy these drawbacks in order to have a proprietary and adequate solution for our application. This solution is tested, an experiment is done and the results shown. ©2009 IEEE.","","Interactive system; Public transport; Visually impaired persons; Wi-Fi positioning system","","","","","978-142444457-1","","","English","Int. Conf. Appl. Digit. Inf. Web Technol., ICADIWT","Conference paper","Final","","Scopus","2-s2.0-71449114980"
"Fink W.; Tarbell M.A.","Fink, Wolfgang (9841994100); Tarbell, Mark A. (9843756000)","9841994100; 9843756000","CYCLOPS: A mobile robotic platform for testing and validating image processing and autonomous navigation algorithms in support of artificial vision prostheses","2009","Computer Methods and Programs in Biomedicine","96","3","","226","233","7","13","10.1016/j.cmpb.2009.06.009","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-70349466647&doi=10.1016%2fj.cmpb.2009.06.009&partnerID=40&md5=b2f6ba21cdfe181b6426fee2a1fb90e6","Visual and Autonomous Exploration Systems Research Laboratory, Division of Physics, Mathematics, and Astronomy, California Institute of Technology, Pasadena, CA 91125, United States","Fink W., Visual and Autonomous Exploration Systems Research Laboratory, Division of Physics, Mathematics, and Astronomy, California Institute of Technology, Pasadena, CA 91125, United States; Tarbell M.A., Visual and Autonomous Exploration Systems Research Laboratory, Division of Physics, Mathematics, and Astronomy, California Institute of Technology, Pasadena, CA 91125, United States","While artificial vision prostheses are quickly becoming a reality, actual testing time with visual prosthesis carriers is at a premium. Moreover, it is helpful to have a more realistic functional approximation of a blind subject. Instead of a normal subject with a healthy retina looking at a low-resolution (pixelated) image on a computer monitor or head-mounted display, a more realistic approximation is achieved by employing a subject-independent mobile robotic platform that uses a pixelated view as its sole visual input for navigation purposes. We introduce CYCLOPS: an AWD, remote controllable, mobile robotic platform that serves as a testbed for real-time image processing and autonomous navigation systems for the purpose of enhancing the visual experience afforded by visual prosthesis carriers. Complete with wireless Internet connectivity and a fully articulated digital camera with wireless video link, CYCLOPS supports both interactive tele-commanding via joystick, and autonomous self-commanding. Due to its onboard computing capabilities and extended battery life, CYCLOPS can perform complex and numerically intensive calculations, such as image processing and autonomous navigation algorithms, in addition to interfacing to additional sensors. Its Internet connectivity renders CYCLOPS a worldwide accessible testbed for researchers in the field of artificial vision systems. CYCLOPS enables subject-independent evaluation and validation of image processing and autonomous navigation systems with respect to the utility and efficiency of supporting and enhancing visual prostheses, while potentially reducing to a necessary minimum the need for valuable testing time with actual visual prosthesis carriers. © 2009 Elsevier Ireland Ltd. All rights reserved.","Artificial vision prostheses; Autonomous navigation; Cloud computing; Image processing; Retinal implants; Robotics; Self-commanding; Tele-commanding; Worldwide accessibility","Algorithms; Humans; Image Processing, Computer-Assisted; Motion; Prostheses and Implants; Retina; Robotics; Telemedicine; Vision, Ocular; Cameras; Computer graphics; Computer monitors; Computer science; Image processing; Imaging systems; Internet; Navigation systems; Ophthalmology; Prosthetics; Real time systems; Robotics; Robots; Test facilities; Testbeds; Vision; Artificial vision prostheses; Autonomous navigation; Cloud computing; Retinal implants; Self-commanding; Tele-commanding; Worldwide accessibility; algorithm; article; equipment design; image processing; Internet; robotics; validation process; visual prosthesis; Navigation","W. Fink; Visual and Autonomous Exploration Systems Research Laboratory, Division of Physics, Mathematics, and Astronomy, California Institute of Technology, Pasadena, CA 91125, United States; email: wfink@autonomy.caltech.edu","","","01692607","","CMPBE","19651459","English","Comput. Methods Programs Biomed.","Article","Final","","Scopus","2-s2.0-70349466647"
"Nagarajan R.; Yaacob S.; Sainarayanan G.","Nagarajan, R. (7202927600); Yaacob, S. (6602262501); Sainarayanan, G. (23986115600)","7202927600; 6602262501; 23986115600","Fuzzy clustering in vision recognition applied in NAVI","2002","Annual Conference of the North American Fuzzy Information Processing Society - NAFIPS","2002-January","","1018066","261","266","5","5","10.1109/NAFIPS.2002.1018066","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84870812684&doi=10.1109%2fNAFIPS.2002.1018066&partnerID=40&md5=dd77d7f458d3717f35cd236948aa89b6","Sch. of Eng. and Inf. Technol., Universiti Malaysia Sabah, Malaysia","Nagarajan R., Sch. of Eng. and Inf. Technol., Universiti Malaysia Sabah, Malaysia; Yaacob S., Sch. of Eng. and Inf. Technol., Universiti Malaysia Sabah, Malaysia; Sainarayanan G., Sch. of Eng. and Inf. Technol., Universiti Malaysia Sabah, Malaysia","In this paper a system for navigation assistance for visually impaired (NAVI) is presented. The system includes a single board processing system (UPS), vision sensor mounted headgear and stereo earphones. The image of environment is captured by the vision sensor. The image is then processed by a novel real time image processing methodology using the fuzzy clustering algorithm. The proposed methodology incorporates certain human vision properties for clear representation of the environment. The image processed is mapped to a stereo acoustic pattern and transferred to the user's earphones. The sound produced varies with the gray value and orientation of the obstacle in front. Blind individuals are trained with NAVI and tested for obstacle identification. Suggestions from the blind volunteers regarding pleasantness and discrimination of sound pattern were also incorporated in the prototype. Certain improvements for faster convergence of clustering are also considered. © 2002 IEEE.","Acoustic sensors; Clustering algorithms; Headphones; Humans; Image processing; Image sensors; Machine vision; Navigation; Sensor systems; Uninterruptible power systems","Clustering algorithms; Computer vision; Earphones; Fuzzy clustering; Headphones; Image sensors; Information science; Navigation; Power supply circuits; Stereo image processing; Stereo vision; Uninterruptible power systems; Acoustic Sensors; Faster convergence; Humans; Navigation assistance for visually impaired; Processing systems; Real-time image processing; Sensor systems; Vision recognition; Image processing","","Nasraoui O.; Keller J.","Institute of Electrical and Electronics Engineers Inc.","","0780374614","","","English","Annu Conf North Am Fuzzy Inf Process Soc NAFIPS","Conference paper","Final","","Scopus","2-s2.0-84870812684"
"Ferreira S.B.L.; da Silveira D.S.; Chauvel M.A.; Ferreira M.G.A.L.","Ferreira, Simone Bacellar Leal (36617265100); da Silveira, Denis Silva (22935402800); Chauvel, Marie Agnes (51763080300); Ferreira, Marcos Gurgel do Amaral Leal (55508610600)","36617265100; 22935402800; 51763080300; 55508610600","E-accessibility: Making the web accessible to the visually impaired persons","2008","14th Americas Conference on Information Systems, AMCIS 2008","4","","","2302","2309","7","0","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84870369698&partnerID=40&md5=e62d6bb6c5cef44e4d2e656506ef280f","Universidade Federal do Estado do Rio de Janeiro (UNIRIO), Brazil; Faculdades IBMEC, Brazil; IAG PUC, Brazil; Holden Comunicação Ltda, Brazil","Ferreira S.B.L., Universidade Federal do Estado do Rio de Janeiro (UNIRIO), Brazil; da Silveira D.S., Faculdades IBMEC, Brazil; Chauvel M.A., IAG PUC, Brazil; Ferreira M.G.A.L., Holden Comunicação Ltda, Brazil","Accessibility is the possibility of any person to make use of all the benefits of society, including the use of the Internet. Graphical are an obstacle for visually impaired persons to access the Internet, so they need a support technology capable of capturing interfaces and making them accessible. Interfaces should be designed so that when accessed by support technologies they continue to be friendly. For a site to be accessible to blind persons it is necessary that the information be reproduced by means of an ""equivalent"" textual description, capable of transmitting the same information as the visual resources. The present study is aimed at identifying and defining usability guidance compliant with accessibility W3C directives that can facilitate the interaction between visually impaired and the Internet and still guarantee sites with understandable navigation content. Towards this end an exploratory study was conducted, comprised of a field study and interviews with various visually disabled people from the Instituto Benjamin Constant, reference center in Brazil for the education and re-education of visually impaired persons, in order to get to know visually disabled users better. Through the understanding acquired, different types of impositions and limits that these users are subject to have been identified, enabling a better perception of their needs and special abilities. The impaired user-machine interaction were observed and analyzed, which enabled the identification of aspects that could contribute to the accessibility of sites, with emphasis on facilitating the access of those visually impaired to the Web.","Accessibility; Usability; Visually impaired","Human rehabilitation equipment; Information systems; Internet; Accessibility; Blind person; Disabled people; Exploratory studies; Field studies; Support technology; Textual description; Usability; Visually impaired; Visually impaired persons; Handicapped persons","S. B. L. Ferreira; Universidade Federal do Estado do Rio de Janeiro (UNIRIO), Brazil; email: simone@uniriotec.br","","","","978-160560953-9","","","English","Amer. Conf. Inf. Sys., AMCIS","Conference paper","Final","","Scopus","2-s2.0-84870369698"
"Hoffman D.; Grivel E.; Battle L.","Hoffman, David (57197178542); Grivel, Eric (56462533400); Battle, Lisa (8705960500)","57197178542; 56462533400; 8705960500","Designing software architectures to facilitate accessible Web applications","2005","IBM Systems Journal","44","3","","467","483","16","20","10.1147/sj.443.0467","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-23744470357&doi=10.1147%2fsj.443.0467&partnerID=40&md5=c9b3899fafce4f90d0e9562682ae1c70","MILVETS Systems Technology, Baltimore, MD 21235, 6401 Security Boulevard, United States; Lockheed Martin Information Technology, Baltimore, MD 21244, 3300 Lord Baltimore Drive, United States","Hoffman D., MILVETS Systems Technology, Baltimore, MD 21235, 6401 Security Boulevard, United States; Grivel E., Lockheed Martin Information Technology, Baltimore, MD 21244, 3300 Lord Baltimore Drive, United States; Battle L., Lockheed Martin Information Technology, Baltimore, MD 21244, 3300 Lord Baltimore Drive, United States","The Web application is increasingly a platform of choice for complex business software and online services. However, it remains a challenge to ensure that the Web application is easy, efficient, and effective for people with disabilities. Accessibility requires that users with disabilities, including those who are blind, have low vision, or have mobility impairments, are able to use the applications effectively and with a reasonable amount of effort. Although there has been important progress in recent years in describing the relationship between architecture and usability, the topic of architectural support for accessibility has not been adequately addressed. Based on our experience in designing Web applications for the United States Social Security Administration, we have begun to identify guidelines for architectures that support accessibility. This paper describes common accessibility problems encountered in Web applications and explains how architecture can help address these problems through reusable accessible objects, supplementing information in links, buttons, and labels, assisting in access to Web page visual information, handling errors, and providing time-out notification and recovery. It also discusses the critical role of architecture in supporting the best way of meeting the needs of diverse user groups: multiple dynamic views of the user interface. © Copyright 2005 by International Business Machines Corporation.","","Handicapped persons; Human computer interaction; Requirements engineering; Usability engineering; User interfaces; World Wide Web; Accessibility requirements; Architectural support; Web applications; Software engineering","D. Hoffman; MILVETS Systems Technology, Baltimore, MD 21235, 6401 Security Boulevard, United States; email: dyhoffman@yahoo.com","","IBM Corporation","00188670","","IBMSA","","English","IBM Syst J","Article","Final","","Scopus","2-s2.0-23744470357"
"Treuillet S.; Royer E.","Treuillet, Sylvie (8707307800); Royer, Eric (8665882700)","8707307800; 8665882700","OUTDOOR/INDOOR VISION-BASED LOCALIZATION for BLIND PEDESTRIAN NAVIGATION ASSISTANCE","2010","International Journal of Image and Graphics","10","4","","481","496","15","31","10.1142/S0219467810003937","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84863186294&doi=10.1142%2fS0219467810003937&partnerID=40&md5=2d9ab9f6b20d18419cb2b72a06515d2d","Institut PRISME, Université d'Orléans, Ecole Polytechnique, 12 rue de Blois, Orléans, 47067, France; LASMEA, Université Blaise Pascal, Campus des Cézeaux, Aubiére, 63177, France","Treuillet S., Institut PRISME, Université d'Orléans, Ecole Polytechnique, 12 rue de Blois, Orléans, 47067, France; Royer E., LASMEA, Université Blaise Pascal, Campus des Cézeaux, Aubiére, 63177, France","The most challenging issue facing the navigation assistive systems for the visually impaired is the instantaneous and accurate spatial localization of the user. Most of the previously proposed systems are based on global positioning system (GPS) sensors. However, the accuracy of low-cost versions is insufficient for pedestrian use. Furthermore, GPS-based systems are confined to outdoor navigation and experience severe signal losts in urban areas. This paper presents a new approach for localizing a person by using a single-body-mounted camera and computer vision techniques. Instantaneous accurate localization and heading estimates of the person are computed from images as the user progresses along a memorized path. A portable prototype has been tested for outdoor as well as indoor pedestrian use. Experimental results demonstrate the effectiveness of the vision-based localization: the accuracy is sufficient for making it possible to guide and maintain the blind person within a navigation corridor less than 1 m wide along the intended path. In combination with a suitable guiding interface, such a localization system will be convenient to assist the visually impaired in their everyday movements outdoors as well as indoors. © 2010 World Scientific Publishing Company.","Computer vision; indoor/outdoor localization; navigation assistance; visually impaired","Computer vision; Navigation; Computer vision techniques; Indoor/outdoor; Localization system; Outdoor navigation; Pedestrian navigation; Spatial localization; Vision based localization; Visually impaired; Global positioning system","","","World Scientific","02194678","","","","English","Intl. J. Image Graphics","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-84863186294"
"Scherlen A.-C.; Dumas J.C.; Guedj B.; Vignot A.","Scherlen, Anne-Catherine (8565443400); Dumas, Jean Claude (57549749200); Guedj, Benjamin (23388770500); Vignot, Alexandre (49965091800)","8565443400; 57549749200; 23388770500; 49965091800","""ReeognizeCane"": The new concept of a cane which recognizes the most common objects and safety clues","2007","Annual International Conference of the IEEE Engineering in Medicine and Biology - Proceedings","","","4353809","6356","6359","3","11","10.1109/IEMBS.2007.4353809","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84903854323&doi=10.1109%2fIEMBS.2007.4353809&partnerID=40&md5=9bbabf442892c341914d87254e10e798","National School of Engineering of St Etienne, 42023 St Etienne, 58 rue Parot, France","Scherlen A.-C., National School of Engineering of St Etienne, 42023 St Etienne, 58 rue Parot, France; Dumas J.C., National School of Engineering of St Etienne, 42023 St Etienne, 58 rue Parot, France; Guedj B., National School of Engineering of St Etienne, 42023 St Etienne, 58 rue Parot, France; Vignot A., National School of Engineering of St Etienne, 42023 St Etienne, 58 rue Parot, France","This paper introduces the new concept of an electronic cane for blind people. While some systems inform the subject only of the presence of the object and its relative distance, RecognizeCane is also able to recognize most common objects and environment clues to increase the safety and confidence of the navigation process. The originality of RecognizeCane is the use of simple sensors, such as infrared, brilliance or water sensors to inform the subject of the presence, for example, of a stairway, a water puddle, a zebra crossing or a trash can. This cane does not use an embedded vision system. RecognizeCane is equipped with several sensors and microprocessors to collect sensor data and extract the desired information about the close environment by means of a dynamic analysis of output signals. © 2007 IEEE.","","Blindness; Canes; Environment; Humans; Recognition (Psychology); Safety; Sensory Aids; Visually Impaired Persons; Computer vision; Electronic equipment; Embedded systems; Safety factor; Sensors; article; blindness; cane; environment; human; patient; recognition; safety; sensory aid; Electronic canes; Navigation processes; Human rehabilitation equipment","A.-C. Scherlen; National School of Engineering of St Etienne, 42023 St Etienne, 58 rue Parot, France; email: scherlen@enise.fr","","","05891019","1424407885; 978-142440788-0","CEMBA","18003475","English","Annu Int Conf IEEE Eng Med Biol Proc","Conference paper","Final","","Scopus","2-s2.0-84903854323"
"Ivanchenko V.; Coughlan J.; Shen H.","Ivanchenko, V. (7005576389); Coughlan, J. (7006567201); Shen, H. (14523706200)","7005576389; 7006567201; 14523706200","Staying in the crosswalk: A system for guiding visually impaired pedestrians at traffic intersections","2009","Assistive Technology Research Series","25","","","69","73","4","18","10.3233/978-1-60750-042-1-69","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84856301375&doi=10.3233%2f978-1-60750-042-1-69&partnerID=40&md5=c5d5b73f3b57dcba537400734b2a4188","Smith-Kettlewell Eye Research Institute, San Francisco, CA 94115, United States","Ivanchenko V., Smith-Kettlewell Eye Research Institute, San Francisco, CA 94115, United States; Coughlan J., Smith-Kettlewell Eye Research Institute, San Francisco, CA 94115, United States; Shen H., Smith-Kettlewell Eye Research Institute, San Francisco, CA 94115, United States","Traffic intersections are among the most dangerous parts of a blind or visually impaired person's travel. Our ""Crosswatch"" device [4] is a handheld (mobile phone) computer vision system for orienting visually impaired pedestrians to crosswalks, to help users avoid entering the crosswalk in the wrong direction and straying outside of it. This paper describes two new developments in the Crosswatch project: (a) a new computer vision algorithm to locate the more common - but less highly visible - standard ""two- stripe"" crosswalk pattern marked by two narrow stripes along the borders of the crosswalk; and (b) 3D analysis to estimate crosswalk location relative to the user, to help him/her stay inside the crosswalk (not merely pointing in the correct direction). Experiments with blind subjects using the system demonstrate the feasibility of the approach. © 2009 The authors and IOS Press. All rights reserved.","assistive technology; Blindness; computer vision; orientation and mobility; traffic intersections; visual impairment","","","Emiliani P.L; Burzagli L.; Como A.; Gabbanini F.; Inst. Applied Physics Nello Carrara, Italian National Research Council, Florence; Salminen A.-L.; Kela Research Department, Kela Research Department","","18798071","978-160750042-1","","","English","Assistive Technol. Res. Ser.","Conference paper","Final","","Scopus","2-s2.0-84856301375"
"Haraszy Z.; Ianchis D.; Tiponut V.","Haraszy, Zoltan (24476948300); Ianchis, Daniel (24477105000); Tiponut, Virgil (16041074100)","24476948300; 24477105000; 16041074100","Generation of the head related transfer functions using artificial neural networks","2009","Proceedings of the 13th WSEAS International Conference on Circuits - Held as part of the 13th WSEAS CSCC Multiconference","","","","114","118","4","13","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-73849095795&partnerID=40&md5=aa7dbc3bb0997ee2298a3072fece666b","Department of Applied Electronics, POLITEHNICA University of Timisoara, 300223 Timisoara, Bvd. Vasile Parvan 2, Romania","Haraszy Z., Department of Applied Electronics, POLITEHNICA University of Timisoara, 300223 Timisoara, Bvd. Vasile Parvan 2, Romania; Ianchis D., Department of Applied Electronics, POLITEHNICA University of Timisoara, 300223 Timisoara, Bvd. Vasile Parvan 2, Romania; Tiponut V., Department of Applied Electronics, POLITEHNICA University of Timisoara, 300223 Timisoara, Bvd. Vasile Parvan 2, Romania","The new Acoustic Virtual Reality (AVR) concept is often used as a man-machine interface in electronic travel aid (ETA), that help blind and visually impaired individuals to navigate in real outdoor environments. According to this concept, the presence of obstacles in the surrounding environment and the path to the desired target will be signalized to the blind subject by burst of sounds, whose virtual source position suggests the position of the real obstacles and the direction of movement, respectively. The practical implementation of the AVR concept requires the so-called Head Related Transfer Functions (HRTFs) values to be known in every point of the 3D space and for each subject. These values can be determined by using a quite complex procedure, which requires many measurements for each individual. In the present paper, an artificial neural network (ANN) is proposed in order to generate the values of the HRTFs. The proposed method, valid for only on subject, speeds up the implementation of the AVR concept after the ANN training has been completed. Finally, some experimental results, conclusions and further developments are also presented.","Acoustic virtual reality; Artificial neural networks; Electronic travel aid; Head related transfer functions; Man-machine interface; Visually impaired","Acoustics; Navigation systems; Transfer functions; Virtual reality; Artificial Neural Network; Electronic travel aidss; Head related transfer function; Man-machine interface; Visually impaired; Neural networks","Z. Haraszy; Department of Applied Electronics, POLITEHNICA University of Timisoara, 300223 Timisoara, Bvd. Vasile Parvan 2, Romania; email: zoltan.haraszy@etc.upt.ro","","","","978-960474096-3","","","English","Proc. WSEAS Int. Conf. Circuits - Held part WSEAS CSCC Multiconference","Conference paper","Final","","Scopus","2-s2.0-73849095795"
"Treuillet S.; Royer E.; Chateau T.; Dhome M.; Lavest J.-M.","Treuillet, Sylvie (8707307800); Royer, Eric (8665882700); Chateau, Thierry (6602307093); Dhome, Michel (6701545982); Lavest, Jean-Marc (6603569942)","8707307800; 8665882700; 6602307093; 6701545982; 6603569942","Body mounted vision system for visually impaired outdoor and indoor wayfinding assistance","2008","CEUR Workshop Proceedings","415","","","","","6","1","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84876254683&partnerID=40&md5=6c3718757a5e8040ceee761856e6d40a","LASMEA, UMR 6602 Campus des Cézeaux, 63177 Aubiére, France","Treuillet S., LASMEA, UMR 6602 Campus des Cézeaux, 63177 Aubiére, France; Royer E., LASMEA, UMR 6602 Campus des Cézeaux, 63177 Aubiére, France; Chateau T., LASMEA, UMR 6602 Campus des Cézeaux, 63177 Aubiére, France; Dhome M., LASMEA, UMR 6602 Campus des Cézeaux, 63177 Aubiére, France; Lavest J.-M., LASMEA, UMR 6602 Campus des Cézeaux, 63177 Aubiére, France","The most challenging issue of the navigation assistive systems for the visually impaired is the instantaneous and accurate spatial localization of the user. Most of the previous proposed systems based on GPS sensors have clearly insufficient accuracy for pedestrian use and are confined to outdoor with severe failing in urban area. This paper presents an interesting alternative localization algorithm using a body mounted single camera. Instantaneous accurate localization and heading estimates of the person are computed from images as the trip progresses along a memorised path. A first portable prototype has been tested for outdoor as well as indoor pedestrian trips. Experimental results demonstrate the effectiveness of the vision based localization to keep the walker in a navigation corridor less than one meter width along the intended path. Future works will investigate multimodal adaptative interface taking account the psychological and ergonomic factors for the blind end-user to design a suitable guiding solution for the blind and visually impaired.","Blind people; Navigation assistance; Personal localisation and heading system","Computer vision; Handicapped persons; Navigation; Vision aids; Blind and visually impaired; Blind people; Localisation; Localization algorithm; Spatial localization; Vision based localization; Visually impaired; Wayfinding assistance; Technology","S. Treuillet; LASMEA, UMR 6602 Campus des Cézeaux, 63177 Aubiére, France; email: sylvie.treuillet@univ-bpclermont.fr","","","16130073","","","","English","CEUR Workshop Proc.","Conference paper","Final","","Scopus","2-s2.0-84876254683"
"Pissaloux E.E.","Pissaloux, Edwige E. (7003850978)","7003850978","A vision system design for blinds mobility assistance","2002","Annual International Conference of the IEEE Engineering in Medicine and Biology - Proceedings","3","","","2349","2350","1","11","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-0036911265&partnerID=40&md5=8e81c023487eb59ca396dc9a5a373a5a","Laboratoire de Robotique de Paris, 78 140 Vélizy, France","Pissaloux E.E., Laboratoire de Robotique de Paris, 78 140 Vélizy, France","This paper describes ongoing research project, ""Intelligent glasses"", on design and realization of technological non-invasive assistance for autonomous and secure displacements of blinds, visually impaired and elderly people. Inclusion of cognitive elements in vision system functionality design and some biologically inspired cues accelerate the system efficiency of realization.","Blinds mobility; Cognition; Vision; Vision ETA design","Algorithms; Collision avoidance; Design; Intelligent robots; Mobile robots; Mobility aids for blind persons; Robotics; Three dimensional; Blinds mobility assistance; Electronic travel assistance; Non invasive mobility assistance; Computer vision","E.E. Pissaloux; Laboratoire de Robotique de Paris, 78 140 Vélizy, France; email: Edwige.Pissaloux@robot.uvsq.fr","","","05891019","","CEMBA","","English","Annu Int Conf IEEE Eng Med Biol Proc","Conference paper","Final","","Scopus","2-s2.0-0036911265"
"Kao G.; Probert P.; Lee D.","Kao, Gordon (7007083733); Probert, Penny (56233155400); Lee, David (58560018500)","7007083733; 56233155400; 58560018500","Object Recognition with FM Sonar; An Assistive Device for Blind and Visually-Impaired People","1996","AAAI Fall Symposium - Technical Report","FS-96-05","","","38","45","7","4","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84855993761&partnerID=40&md5=c523256fdf8f5015ac45f591191fc562","Robotics Research Group, Engineering Science Department, 19 Parks Road, Oxford, OX1 3PJ, United Kingdom","Kao G., Robotics Research Group, Engineering Science Department, 19 Parks Road, Oxford, OX1 3PJ, United Kingdom; Probert P., Robotics Research Group, Engineering Science Department, 19 Parks Road, Oxford, OX1 3PJ, United Kingdom; Lee D., Robotics Research Group, Engineering Science Department, 19 Parks Road, Oxford, OX1 3PJ, United Kingdom","FM sonar sensors have been used in mobility aids for tile visually-impaired. Howeverp, revious FM sonar systems have generated continuous audio signals and rely on the user interpreting them. Our research work is carried out to solve the problem of overloading users of FM sonar system with excessive information by machine interpreting the audio signal. The signal is sampled and Fourier transformed to generate an FM sonar image. Automatic computer analysis of the FM sonar image is carried out to compress and extract information for the purpose of object recognition. A method is developed to classify an object into one of the three groups: smooth surfaces, repetitive objects and textured surfaces. This method is based on the evaluation of the autocorrelation function of a single raw FM sonar image. A second method is also developed to reliably distinguish surfaces with varying degrees of roughness. An FM sonar model is constructed to predict FM sonar images of a rough surface at different sensor orientations. Templates are generated from the model and matched against the real images. Surfaces with varying degrees of roughness can therefore be identified. © 1996 AI Access Foundation. All rights reserved.","","Audio systems; Frequency modulation; Sonar; Surface roughness; Textures; Assistive devices; Audio signal; Blind and visually impaired; Mobility aids; Objects recognition; Sonar image; Sonar sensor; Sonar system; Visually impaired; Visually impaired people; Object recognition","","","Association for the Advancement of Artificial Intelligence","","","","","English","AAAI Fall Symp. Tech. Rep.","Conference paper","Final","","Scopus","2-s2.0-84855993761"
"Balakrishnan G.; Sainarayanan G.; Nagarajan R.; Yaccob S.","Balakrishnan, G. (59157782500); Sainarayanan, G. (23986115600); Nagarajan, R. (7202927600); Yaccob, Sazali (6505641077)","59157782500; 23986115600; 7202927600; 6505641077","On stereo processing procedure applied towards blind navigation aid - SVETA","2005","Proceedings - 8th International Symposium on Signal Processing and its Applications, ISSPA 2005","2","","1581001","567","570","3","5","10.1109/ISSPA.2005.1581001","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-33847100037&doi=10.1109%2fISSPA.2005.1581001&partnerID=40&md5=e238a7799e121c27b78be63d1c444ef5","AI Research Group, School of Engineering and Information Technology, Universiti Malaysia Sabah, 88999, Locked Bag No. 2073, Malaysia; Northern Malaysia University College of Engineering, Perlis, Malaysia","Balakrishnan G., AI Research Group, School of Engineering and Information Technology, Universiti Malaysia Sabah, 88999, Locked Bag No. 2073, Malaysia; Sainarayanan G., AI Research Group, School of Engineering and Information Technology, Universiti Malaysia Sabah, 88999, Locked Bag No. 2073, Malaysia; Nagarajan R., AI Research Group, School of Engineering and Information Technology, Universiti Malaysia Sabah, 88999, Locked Bag No. 2073, Malaysia; Yaccob S., Northern Malaysia University College of Engineering, Perlis, Malaysia","This paper presents a portable traveling support system using stereo image processing for the visually impaired people's navigation. This system named as SVETA consists of a helmet molded with stereo cameras in the front, wearable computer over the top and stereo earphones. The two cameras capture the visual information infront of the blind user. The captured images are then processed using the proposed methodology in wearable computer. The methodology includes stereo image processing module to calculate distance through disparity. The information is conveyed to the blind through the set of earphones in terms of musical tones. Voices are also used to inform blind user when the obstacle is very close to him. Experimentations were conducted in both indoor and outdoor environment, and the results of this experiment verified the practicability of the newly developed system. © 2005 IEEE.","","Cameras; Earphones; Helmet mounted displays; Image processing; Stereo vision; Wearable computers; Blind navigation aid; Stereo cameras; Stereo image processing module; SVETA; Navigation systems","G. Balakrishnan; AI Research Group, School of Engineering and Information Technology, Universiti Malaysia Sabah, 88999, Locked Bag No. 2073, Malaysia; email: g_krishbalu@yahoo.com","","","","0780392434; 978-078039243-4","","","English","Proc. 8th Int. Symp. Signal Process. Applic. ISSPA 2005","Conference paper","Final","","Scopus","2-s2.0-33847100037"
"Sainarayanan G.; Nagarajan R.; Yaacob S.","Sainarayanan, G. (23986115600); Nagarajan, R. (7202927600); Yaacob, Sazali (6602262501)","23986115600; 7202927600; 6602262501","Fuzzy image processing scheme for autonomous navigation of human blind","2007","Applied Soft Computing Journal","7","1","","257","264","7","95","10.1016/j.asoc.2005.06.005","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-33750944370&doi=10.1016%2fj.asoc.2005.06.005&partnerID=40&md5=ac947da72165a3dfbf01284f580b61a5","School of Engineering and Information Technology, Universiti Malaysia Sabah, Kota Kinabalu, 88999, Malaysia","Sainarayanan G., School of Engineering and Information Technology, Universiti Malaysia Sabah, Kota Kinabalu, 88999, Malaysia; Nagarajan R., School of Engineering and Information Technology, Universiti Malaysia Sabah, Kota Kinabalu, 88999, Malaysia; Yaacob S., School of Engineering and Information Technology, Universiti Malaysia Sabah, Kota Kinabalu, 88999, Malaysia","The main objective of this work is to develop an electronic travel aid to assist the blinds for obstacle identification in their navigation. This navigation assistance for visually impaired (NAVI) system presented in this paper consists of a single board processing system (SBPS), a vision sensor mounted headgear and a pair of stereo earphones. The image environment in front of the blind is captured by the vision sensor. The image is processed by a new real time image processing scheme using fuzzy clustering algorithms. The processed image is mapped onto a specially structured stereo acoustic patterns and transferred to the stereo earphones in the system. Blind individuals were trained with NAVI system and tested for obstacle identification. Suggestions from the blind volunteers regarding pleasantness and discrimination of sound pattern were also incorporated in the prototype. The proposed processing methodology is found to be effective for object identification and for producing stereo sound patterns in the NAVI system. © 2005 Elsevier B.V. All rights reserved.","Blind navigation; Electronic travel aid; Fuzzy clustering; Image processing; Pattern recognition; Sonification; Vision substitution","Autonomous agents; Earphones; Fuzzy control; Handicapped persons; Navigation systems; Blind navigation; Electronic travel aid; Fuzzy clustering; Navigation assistance for visually impaired (NAVI) system; Obstacle identification; Stereo sound patterns; Vision substitution; Image processing","G. Sainarayanan; School of Engineering and Information Technology, Universiti Malaysia Sabah, Kota Kinabalu, 88999, Malaysia; email: jgksai@ums.edu.my","","","15684946","","","","English","Appl. Soft Comput. J.","Article","Final","","Scopus","2-s2.0-33750944370"
"Castells D.; Rodrigues J.M.F.; Du Buf J.M.H.","Castells, D. (36459731100); Rodrigues, J.M.F. (34973296100); Du Buf, J.M.H. (6604075916)","36459731100; 34973296100; 6604075916","Obstacle detection and avoidance on sidewalks","2010","VISAPP 2010 - Proceedings of the International Conference on Computer Vision Theory and Applications","2","","","235","240","5","7","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-77956313641&partnerID=40&md5=9fcd063a9f4c6ca1c666e55facf7feec","Universidad Politécnica de Madrid, Spain; Vision Laboratory, Institute for Systems and Robotics (ISR), University of the Algarve (ISE and FCT), 8005-139 Faro, Portugal","Castells D., Universidad Politécnica de Madrid, Spain; Rodrigues J.M.F., Vision Laboratory, Institute for Systems and Robotics (ISR), University of the Algarve (ISE and FCT), 8005-139 Faro, Portugal; Du Buf J.M.H., Vision Laboratory, Institute for Systems and Robotics (ISR), University of the Algarve (ISE and FCT), 8005-139 Faro, Portugal","We present part of a vision system for blind and visually impaired people. It detects obstacles on sidewalks and provides guidance to avoid them. Obstacles are trees, light poles, trash cans, holes, branches, stones and other objects at a distance of 3 to 5 meters from the camera position. The system first detects the sidewalk borders, using edge information in combination with a tracking mask, to obtain straight lines with their slopes and the vanishing point. Once the borders are found, a rectangular window is defined within which two obstacle detection methods are applied. The first determines the variation of the maxima and minima of the gray levels of the pixels. The second uses the binary edge image and searches in the vertical and horizontal histograms for discrepancies of the number of edge points. Together, these methods allow to detect possible obstacles with their position and size, such that the user can be alerted and informed about the best way to avoid them. The system works in realtime and complements normal navigation with the cane.","Obstacle avoidance; Path tracking; Sidewalk border detection; Visually impaired","Computer vision; Obstacle detectors; Binary edge images; Camera positions; Edge information; Edge point; Gray levels; Obstacle avoidance; Obstacle detection; Path tracking; Real time; Sidewalk border detection; Straight lines; Vanishing point; Vision systems; Visually impaired; Visually impaired people; Pavements","D. Castells; Universidad Politécnica de Madrid, Spain; email: delicast@gmail.com","","","","978-989674028-3","","","English","VISAPP - Proc. Int. Conf. Comput. Vis. Theory Appl.","Conference paper","Final","","Scopus","2-s2.0-77956313641"
"Shoval S.; Ulrich I.; Borenstein J.","Shoval, Shraga (7004180117); Ulrich, Iwan (6603814310); Borenstein, Johann (7005627914)","7004180117; 6603814310; 7005627914","NavBelt and the GuideCane","2003","IEEE Robotics and Automation Magazine","10","1","","9","20","11","145","10.1109/MRA.2003.1191706","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-0038370231&doi=10.1109%2fMRA.2003.1191706&partnerID=40&md5=8f6c02fde3b966832689b249dd61c411","Dept. of Industrial Eng. and Mgmt., Academic College of J and S, Ariel, Israel; Dept. of Mechanical Engineering, The University of Michigan, Ann Arbor, MI 48109, United States","Shoval S., Dept. of Industrial Eng. and Mgmt., Academic College of J and S, Ariel, Israel; Ulrich I.; Borenstein J., Dept. of Mechanical Engineering, The University of Michigan, Ann Arbor, MI 48109, United States","The NavBelt and the GuideCane are computerized devices based on advanced mobile robotics obstacle-avoidance technologies. NavBelt is worn by the user like a belt and is equipped with an array of ultrasonic sensors. It provides acoustic signals via a set of stereo earphones that guides the user around obstacles or ""displays"" a virtual acoustic panoramic image of the traveler's surroundings. However, for NavBelt it is exceedingly difficult for the user to comprehend the guidance signals in time to allow fast walking. A newer device, GuideCane, effectively overcomes this problem. The GuideCane uses the same mobile robotics technology as the NavBelt but is a wheeled device pushed ahead of the user via an attached cane. When it detects an obstacle, it steers around it. The user immediately feels this steering action and can follow the GuideCane's new path easily without any conscious effort.","Blind; Mobile robots; Rehabilitation robotics; Travel aid; Visually impaired","Collision avoidance; Handicapped persons; Mobile robots; Mobility aids for blind persons; User interfaces; Electronic travel aids; Obstacle avoidance systems; Visually impaired persons; Human engineering","J. Borenstein; Dept. of Mechanical Engineering, The University of Michigan, Ann Arbor, MI 48109, United States; email: johannb@umich.edu","","","10709932","","IRAME","","English","IEEE Rob Autom Mag","Article","Final","","Scopus","2-s2.0-0038370231"
"Ulrich I.; Borenstein J.","Ulrich, Iwan (6603814310); Borenstein, Johann (7005627914)","6603814310; 7005627914","The GuideCane-applying mobile robot technologies to assist the visually impaired","2001","IEEE Transactions on Systems, Man, and Cybernetics Part A:Systems and Humans.","31","2","","131","136","5","330","10.1109/3468.911370","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-0035277346&doi=10.1109%2f3468.911370&partnerID=40&md5=ca7400b7b3cc8682e1bf98a263eff751","Department of Mechanical Engineering, University of Michigan, Ann Arbor, MI 48109, United States; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States","Ulrich I., Department of Mechanical Engineering, University of Michigan, Ann Arbor, MI 48109, United States, Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States; Borenstein J., Department of Mechanical Engineering, University of Michigan, Ann Arbor, MI 48109, United States","The GuideCane is a novel device designed to help blind or visually impaired users navigate safely and quickly among obstacles and other hazards. During operation, the user pushes the lightweight GuideCane forward. When the GuideCane's ultrasonic sensors detect an obstacle, the embedded computer determines a suitable direction of motion that steers the GuideCane and the user around it. The steering action results in a very noticeable force felt in the handle, which easily guides the user without any conscious effort on his/her part.","Blind; Guide dog; GuideCane; Mobile robot; Obstacle avoidance; Ultrasonic sensors","Actuators; Algorithms; Collision avoidance; Computer control; Handicapped persons; Human rehabilitation equipment; Mobile robots; Motion control; Potentiometers (electric measuring instruments); Sensors; Servomotors; Sonar; GuideCane; Ultrasonic sensors; Visually impaired; Ultrasonic devices","J. Borenstein; Department of Mechanical Engineering, University of Michigan, Ann Arbor, MI 48109, United States; email: johannb@umich.edu","","","10834427","","ITSHF","","English","IEEE Trans Syst Man Cybern Pt A Syst Humans","Conference paper","Final","","Scopus","2-s2.0-0035277346"
"Morris T.; Blenkhorn P.; Crossey L.; Ngo Q.; Ross M.; Werner D.; Wong C.","Morris, Tim (7403947486); Blenkhorn, Paul (6603707614); Crossey, Luke (15769041000); Ngo, Quang (36880213200); Ross, Martin (15770206500); Werner, David (15770180900); Wong, Christina (36862872300)","7403947486; 6603707614; 15769041000; 36880213200; 15770206500; 15770180900; 36862872300","Clearspeech: A display reader for the visually handicapped","2006","IEEE Transactions on Neural Systems and Rehabilitation Engineering","14","4","11","492","500","8","27","10.1109/TNSRE.2006.881538","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-33846312874&doi=10.1109%2fTNSRE.2006.881538&partnerID=40&md5=4396334ec226f9c27577177771ad85cf","School of Informatics, University of Manchester, Manchester, M60 1QD, United Kingdom; Department of Computation, UMIST, Manchester, M60 1QD, United Kingdom","Morris T., School of Informatics, University of Manchester, Manchester, M60 1QD, United Kingdom; Blenkhorn P., School of Informatics, University of Manchester, Manchester, M60 1QD, United Kingdom; Crossey L., Department of Computation, UMIST, Manchester, M60 1QD, United Kingdom; Ngo Q., Department of Computation, UMIST, Manchester, M60 1QD, United Kingdom; Ross M., Department of Computation, UMIST, Manchester, M60 1QD, United Kingdom; Werner D., Department of Computation, UMIST, Manchester, M60 1QD, United Kingdom; Wong C., Department of Computation, UMIST, Manchester, M60 1QD, United Kingdom","Many domestic appliances and much office equipment is controlled using a keypad and a small digital display. Programming such devices is problematical for the blind and visually handicapped. In this paper, we describe a device that may be used to read the displays on these devices. The device is designed to accept a description of the display being read, which specifies the types and locations of elements of the display. Images are captured using a handheld webcam. Images are processed to remove the distortions due to the camera orientation. The elements of the screen are interpreted and a suitable audio output is generated. In suitably illuminated scenes, the display data is interpreted correctly in approximately 90% of the cases investigated. © 2006 IEEE.","Assistive devices; Audio interpretation; Display reader","Automatic Data Processing; Communication Aids for Disabled; Data Display; Humans; Image Interpretation, Computer-Assisted; Pilot Projects; Signal Processing, Computer-Assisted; User-Computer Interface; Vision Disorders; Display devices; Imaging systems; Speech communication; Vision; article; blindness; camera; computer program; device; display system; illumination; image processing; priority journal; visual impairment; Assistive devices; Audio interpretation; Camera orientation; Display readers; Human rehabilitation equipment","T. Morris; School of Informatics, University of Manchester, Manchester, M60 1QD, United Kingdom; email: tim.morris@manchester.ac.uk","","","15344320","","ITNSB","17190040","English","IEEE Trans. Neural Syst. Rehabil. Eng.","Article","Final","","Scopus","2-s2.0-33846312874"
"McMorrow G.; Wang X.; Whelan P.F.","McMorrow, Gabriel (25925138600); Wang, Xiaojun (55736795300); Whelan, Paul F. (7006278343)","25925138600; 55736795300; 7006278343","A color-to-speech sensory substitution device for the visually impaired.","1997","Proceedings of SPIE - The International Society for Optical Engineering","3205","","","272","281","9","3","10.1117/12.285572","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-57649196203&doi=10.1117%2f12.285572&partnerID=40&md5=586c92b128b31628e0b438709612b3fe","Zandar Technologies, Deans Grange, Dublin, Ireland; Microelectronics and Materials Group, Dublin City University, Ireland; Vision Systems Laboratory, Dublin City University, Ireland","McMorrow G., Zandar Technologies, Deans Grange, Dublin, Ireland; Wang X., Microelectronics and Materials Group, Dublin City University, Ireland; Whelan P.F., Vision Systems Laboratory, Dublin City University, Ireland","A hardware device is presented that converts color to speech for use by the blind and visually impaired. The use of audio tones for transferring knowledge of colors identified to individuals was investigated but was discarded in favor of the use of direct speech. A unique color-clustering algorithm was implemented using a hardware description language (VHDL). which in-turn was used to program an Altera Corporation's programmable logic device (PLD). The PLD maps all possible incoming colors into one of 24 color names, and outputs an address to a speech device, which in-turn plays back one of 24 voice recorded color names. To the author's knowledge, there are only two such color to speech systems available on the market. However, both are designed to operate at a distance of less than an inch from the surface whose color is to be checked. The device presented here uses original front-end optics to increase the range of operation from less than an inch to sixteen feet and greater. Because of the increased range of operation, the device can not only be used for color identification, but also as a navigation aid.","Clustering; Color; Sensory substitution; Speech; VHDL","Clustering algorithms; Computer hardware description languages; Computer vision; Logic devices; Optical properties; Programmable logic controllers; Vegetation; Vision aids; Clustering; Color identifications; Color names; Hardware description languages; Hardware devices; Navigation aids; Programmable Logic Devices; Sensory substitution; Speech systems; VHDL; Visually impaired; Color","","","","0277786X","","PSISD","","English","Proc SPIE Int Soc Opt Eng","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-57649196203"
"Hanif S.M.; Prevost L.","Hanif, Shehzad Muhammad (57210377659); Prevost, Lionel (6602187119)","57210377659; 6602187119","Texture based text detection in natural scene images: A help to blind and visually impaired persons","2008","CEUR Workshop Proceedings","415","","","","","6","1","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84876208637&partnerID=40&md5=7409d902486bf75c64eb4f03072200fe","Institut des Systèmes Intelligents et Robotique CNRS - FRE 2507, Université Pierre et Marie Curie, 75252 Paris Cedex 05, 4 Place Jussieu, France","Hanif S.M., Institut des Systèmes Intelligents et Robotique CNRS - FRE 2507, Université Pierre et Marie Curie, 75252 Paris Cedex 05, 4 Place Jussieu, France; Prevost L., Institut des Systèmes Intelligents et Robotique CNRS - FRE 2507, Université Pierre et Marie Curie, 75252 Paris Cedex 05, 4 Place Jussieu, France","In this paper, we propose a texture based technique to detect text in grey level natural scene images. This work is a part of the project called Intelligent Glasses. It is a wearable system to facilitate navigation and to assist the blind and visually impaired persons in real world. It has three parts, a bank of stereovision, a processing unit for visual perception and a handheld tactile surface. In its original form, it will be able to provide information about different types of obstacles and their position with respect to user. Our textual/symbolic information interpretation module to the vision system of the Intelligent Glasses will recognize the text from the captured scene and textual and/or symbolic information will be displayed on the handheld tactile. Initial results are encouraging with a text detection rate of 64%.","Handheld tactile; Image analysis; Intelligent glasses; Text detection","Computer vision; Glass; Handicapped persons; Image analysis; Image texture; Technology; Textures; Wearable computers; Blind and visually impaired; Handhelds; Information interpretation; Intelligent glass; Natural scene images; Text detection; Text detection in natural scenes; Visual perception; Character recognition","S.M. Hanif; Institut des Systèmes Intelligents et Robotique CNRS - FRE 2507, Université Pierre et Marie Curie, 75252 Paris Cedex 05, 4 Place Jussieu, France; email: shehzad.muhammad@lisif.jussieu.fr","","","16130073","","","","English","CEUR Workshop Proc.","Conference paper","Final","","Scopus","2-s2.0-84876208637"
"Maingreaud F.; Pissaloux E.; Leroux C.; Micaelli A.","Maingreaud, Flavien (15843380100); Pissaloux, Edwig (7003850978); Leroux, Christophe (57213300040); Micaelli, Alain (6507561076)","15843380100; 7003850978; 57213300040; 6507561076","Biologically inspired 3D scene depth recovery from stereo images","2004","IEEE International Symposium on Industrial Electronics","1","","1571895","721","725","4","4","10.1109/ISIE.2004.1571895","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-33846897955&doi=10.1109%2fISIE.2004.1571895&partnerID=40&md5=c60f96a905c53f7d26adb9ae3a91f110","National Nutclear Testing Commnision, SRSI/LCI, Fon-tenav-aux-Roses, rolite dii Pan-orama, France; Laboratoire de Robotiquie de Parils (LRP), Université Paris 6, Fontenav-aix-Roses, BP 61, France","Maingreaud F., National Nutclear Testing Commnision, SRSI/LCI, Fon-tenav-aux-Roses, rolite dii Pan-orama, France, Laboratoire de Robotiquie de Parils (LRP), Université Paris 6, Fontenav-aix-Roses, BP 61, France; Pissaloux E., Laboratoire de Robotiquie de Parils (LRP), Université Paris 6, Fontenav-aix-Roses, BP 61, France; Leroux C., National Nutclear Testing Commnision, SRSI/LCI, Fon-tenav-aux-Roses, rolite dii Pan-orama, France; Micaelli A., National Nutclear Testing Commnision, SRSI/LCI, Fon-tenav-aux-Roses, rolite dii Pan-orama, France","This paper proposes a biologically inspired method for depth recovery from stereo rectified images. Two principles are used in order to speed-up the imnage matching phase: the photoreceptor information transniission (spike generation) principle and convenlient coding of neuron (pixel')neighbourhood data. This latter provides the robustness to match and reduces its calculation time. The proposed method has been validated via Julesz's random dot stere-ograms. Any autonomous vision-guided system such as autonomous robots, walkers, electronic travel aids, ETA (for elderly visually impaired, blind, . can take benefit from the proposed depth recovery method a new ETA, namied Intelligent Glasses (IG), for visually impaired assistance in their secure and inidependent displacements in non-cooperating environment, which will integrate the proposed method, is presented as well. The IG system is under designi at the Robotics Laboratory of Paris (LRP), University Paris 6, and National Nuclear Testing Commission (CEA.©2004 IEEE.","Biologically inspired vision; Depth; ETA; Random Dot Stereogram; Stereo","Industrial electronics; Robots; Three dimensional; Vision aids; Biologically inspired vision; Depth; ETA; Random Dot Stereogram; Stereo; Stereo vision","F. Maingreaud; National Nutclear Testing Commnision, SRSI/LCI, Fon-tenav-aux-Roses, rolite dii Pan-orama, France; email: MaingreaudF@zoe.cea.fr","","","","","85PTA","","English","IEEE Int Symp Ind Electron","Conference paper","Final","","Scopus","2-s2.0-33846897955"
"Kammoun S.; Dramas F.; Oriola B.; Jouffrais C.","Kammoun, Slim (36809583900); Dramas, Florian (26430954400); Oriola, Bernard (26422444900); Jouffrais, Christophe (7801364165)","36809583900; 26430954400; 26422444900; 7801364165","Route selection algorithm for blind pedestrian","2010","ICCAS 2010 - International Conference on Control, Automation and Systems","","","5669846","2223","2228","5","34","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-78751492740&partnerID=40&md5=78e0af34140e56bff77def7912b1c78d","IRIT, CNRS, University of Toulouse, Toulouse, France","Kammoun S., IRIT, CNRS, University of Toulouse, Toulouse, France; Dramas F., IRIT, CNRS, University of Toulouse, Toulouse, France; Oriola B., IRIT, CNRS, University of Toulouse, Toulouse, France; Jouffrais C., IRIT, CNRS, University of Toulouse, Toulouse, France","The vast majority of existing route selection processes is designed for vehicle navigation. In this paper we describe an adapted routing algorithm for visually impaired pedestrians based on users needs. Our aim was to find the most adapted route that connects origin and destination points, and which can provide the Blind with a sparse but helpful mental representation of the itinerary and surroundings. Based on multiple brainstorming sessions and interviews with blind people and an orientation and mobility (O&M) instructor, different classes of objects were defined and tagged in the Geographical Information System. The optimal route was then selected using the Dijkstra algorithm. This method will be used in NAVIG (Navigation Assisted by Artificial VIsion and GNSS), an assistive device for the Blind, whose aim is to improve orientation, mobility and objects localization. ©ICROS.","Assistive technology; Blind; Geographical information system; Route selection; User needs","Algorithms; Geographic information systems; Handicapped persons; Information systems; Navigation; Assistive technology; Blind; Geographical Information System; Route Selection; User need; Transportation routes","S. Kammoun; IRIT, CNRS, University of Toulouse, Toulouse, France; email: fkammoun@irit.fr","","","","978-142447453-0","","","English","ICCAS - Int. Conf. Control, Autom. Syst.","Conference paper","Final","","Scopus","2-s2.0-78751492740"
"Marston J.R.; Loomis J.M.; Golledge R.G.; Klatzky R.L.; Smith E.L.","Marston, James R. (7005186067); Loomis, Jack M. (7102535032); Golledge, Reginald G. (57208763854); Klatzky, Roberta L. (7005851396); Smith, Ethan L. (55485056600)","7005186067; 7102535032; 57208763854; 7005851396; 55485056600","Evaluation of Spatial Displays for Navigation without Sight","2006","ACM Transactions on Applied Perception","3","2","","110","124","14","61","10.1145/1141897.1141900","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-85016006527&doi=10.1145%2f1141897.1141900&partnerID=40&md5=50a03d5e86b1bf3101b9a6100e410413","Department of Geography, University of California, Santa Barbara, CA 93106, United States; Department of Psychology, Carnegie Mellon University, Pittsburgh, PA 15213, United States; University College, London, United Kingdom","Marston J.R., Department of Geography, University of California, Santa Barbara, CA 93106, United States; Loomis J.M., Department of Geography, University of California, Santa Barbara, CA 93106, United States; Golledge R.G., Department of Geography, University of California, Santa Barbara, CA 93106, United States; Klatzky R.L., Department of Psychology, Carnegie Mellon University, Pittsburgh, PA 15213, United States; Smith E.L., University College, London, United Kingdom","We report on two route guidance tasks using a highly accurate GPS receiver. Eight participants who were visually impaired or blind traveled two routes, one on a city sidewalk, and one in a city park.We tested and compared two types of spatial output devices that give route guidance information. One output display used a hand-held pointer, using a standard Talking Signs receiver that integrated the GPS signal information with the Talking Signs® signal information. This device gave travel instructions and oncourse confirmation when pointed in the proper direction. The other spatial display used auditory virtual reality that presented the audible spatial information (waypoint direction and distance) through small air-tubes inserted into the ear. Travel times, distance, and errors were recorded. In addition, we tested users ‘ability to find precise locations, such as the intersections of small paths and a bus stop pole. Various subjective ratings were collected about blind participants’ needs and perception of the various display and output options that they used. All subjects completed the tasks with both output displays, found all the waypoints and locations, and rated the two displays highly. The virtual sound display produced superior times overall and received slightly higher favorable ratings. © 2006, ACM. All rights reserved.","Artificial; Auditory I/O; Augmented; Blind Navigation; Design; Ergonomics; Evaluation/Methodology; Gps; Haptic I/O; Personal Guidance System; User-Centered; Virtual Realities; Voice I/O","","","","","15443558","","","","English","ACM Trans. Appl. Percept.","Article","Final","","Scopus","2-s2.0-85016006527"
"Goldberg Lee","Goldberg, Lee (35793430700)","35793430700","Vision-based vehicle navigation system uses artificial vision to plug GPS's 'blind spots'","1996","Electronic Design","44","13","","35","36","1","0","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-0030173433&partnerID=40&md5=c8cef803918f412343457e07a088f14a","","","Recent developments in vision-based navigation systems may result in a low-cost, highly accurate electronic co-pilot that can guide one safely through a maze of city streets, a large office complex, or other places where global-positioning-satellite (GPS) signals cannot reach. Developed by scientists at the Siemens Corporate Research Inc. facility, Princeton, N.J., the automatic-vehicle-location (AVL) system can function as a standalone unit, or work with GPS technology for added accuracy and versatility. The system consists of a camera, an odometer, and a computer, plus a CD-ROM and map-display screen.","","Accelerometers; Algorithms; Artificial intelligence; Cameras; CD-ROM; Computer vision; Database systems; Global positioning system; Image compression; Image processing; Microcomputers; Compression algorithm; Image matching system; Odometer; Navigation systems","","","Penton Publ Inc","00134872","","ELODA","","English","Electron Des","Article","Final","","Scopus","2-s2.0-0030173433"
"Maingreaud F.; Pissaloux E.E.; Velázquez R.; Gaunet F.; Hafez M.; Alexandre J.-M.","Maingreaud, F. (15843380100); Pissaloux, E.E. (7003850978); Velázquez, R. (7003505296); Gaunet, F. (24922692200); Hafez, M. (7102503929); Alexandre, J.-M. (56186581600)","15843380100; 7003850978; 7003505296; 24922692200; 7102503929; 56186581600","A dynamic tactile map as a tool for space organization perception: Application to the design of an electronic travel aid for visually impaired and blind people","2005","Annual International Conference of the IEEE Engineering in Medicine and Biology - Proceedings","7 VOLS","","1616095","6912","6915","3","10","10.1109/iembs.2005.1616095","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-33846897478&doi=10.1109%2fiembs.2005.1616095&partnerID=40&md5=40bde42f4046c3081448aa69dbe36803","Laboratoire de Robotique de Paris, CNRS FRE 2507, Université Paris 6, 92265 Fontenay aux Roses, 18 Route du Panorama, France; CEA/LIST, 92265 Fontenay aux Roses, 18 Route du Panorama, France; MNHN, CNRS UMR, 75005 Paris, France","Maingreaud F., Laboratoire de Robotique de Paris, CNRS FRE 2507, Université Paris 6, 92265 Fontenay aux Roses, 18 Route du Panorama, France, CEA/LIST, 92265 Fontenay aux Roses, 18 Route du Panorama, France; Pissaloux E.E., Laboratoire de Robotique de Paris, CNRS FRE 2507, Université Paris 6, 92265 Fontenay aux Roses, 18 Route du Panorama, France; Velázquez R., Laboratoire de Robotique de Paris, CNRS FRE 2507, Université Paris 6, 92265 Fontenay aux Roses, 18 Route du Panorama, France; Gaunet F., MNHN, CNRS UMR, 75005 Paris, France; Hafez M., CEA/LIST, 92265 Fontenay aux Roses, 18 Route du Panorama, France; Alexandre J.-M., CEA/LIST, 92265 Fontenay aux Roses, 18 Route du Panorama, France","This paper addresses a new concept of visuotactile human-machine interface for a certain representation of the peripersonnal space, especially useful for visually impaired and blind people navigation. The proposed space representation have been successfully implemented on a tactile device and validated via series of experiments involving naive blindfolded sighted people. Results show that is possible to interact with the space via the proposed tactile representation. © 2005 IEEE.","Concept design; Dynamic tactile map; Electronic travel assistance; Human-centered design; User interface design","Haptic interfaces; Human computer interaction; Medical education; Patient treatment; Dynamic tactile maps; Electronic travel assistance; Human centered designs; User interface designs; Medical imaging","","","Institute of Electrical and Electronics Engineers Inc.","05891019","0780387406; 978-078038740-9","CEMBA","","English","Annu Int Conf IEEE Eng Med Biol Proc","Conference paper","Final","","Scopus","2-s2.0-33846897478"
"Freiberger H.","Freiberger, H. (6601994311)","6601994311","Mobility and reading aids for the blind: recent developments in rehabilitation devices","1974","Bulletin of the New York Academy of Medicine: Journal of Urban Health","50","6","","667","671","4","0","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-0016154791&partnerID=40&md5=f1b97e8da0252895d6728ed799cd09de","United States","Freiberger H., United States","Many mobility and reading systems for blind people exist or are in development. Only two of the mobility devices and two of the reading machines are explained. These have been selected from the most recent and widely known technological systems available. No one device or small group of devices will provide all things for all people. A broad armamentarium is needed so that all blind persons who want to use these innovations will have devices suited to their own capabilities, limitations, and needs.","","Biomedical Engineering; Blindness; Human; Lasers; Movement; Reading; Sensory Aids; blindness; low vision; reading; reading aid; rehabilitation; visual aid; walking aid","","","","00287091","","BNYMA","4524349","English","BULL. NEW YORK ACAD. MED. J. URBAN HEALTH","Article","Final","","Scopus","2-s2.0-0016154791"
"Clark-Carter D.D.; Heyes D.G.; Howarth C.I.","Clark-Carter, D.D. (6602482286); Heyes, D.G. (56944488900); Howarth, C.I. (57193691900)","6602482286; 56944488900; 57193691900","The effect of non-visual preview upon the walking speed of visually impaired people","1986","Ergonomics","29","12","","1575","1581","6","39","10.1080/00140138608967270","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-0022899891&doi=10.1080%2f00140138608967270&partnerID=40&md5=42eb18c4e1982d632a97bf8a61bedcb2","Blind Mobility Research Unit, Department of Psychology, University of Nottingham, United Kingdom","Clark-Carter D.D., Blind Mobility Research Unit, Department of Psychology, University of Nottingham, United Kingdom; Heyes D.G., Blind Mobility Research Unit, Department of Psychology, University of Nottingham, United Kingdom; Howarth C.I., Blind Mobility Research Unit, Department of Psychology, University of Nottingham, United Kingdom","Blind pedestrians lack preview of their environment and as a consequence they frequently adopt a slow walking speed. Electronic mobility aids have been designed to increase preview, and thus thinking time, over and above that provided by a long cane, but their ranges have been chosen on somewhat arbitrary grounds. Previous work on the effects of varying preview upon mobility have used artificial indoor environments. In addition, they have used either sight or an aid with a complicated display, the Swedish Laser Cane, as the medium through which to present the preview. The present experiment provided subjects with varying amounts of preview via an electronic mobility aid, the Sonic Pathfinder, as they walked a simple outdoor route. Their walking speed was recorded and found to increase significantly with increased preview. The range of the Sonic Pathfinder has now been increased on the strength of this experiment. © 1986 Taylor & Francis Group, LLC.","Blind mobility; Evaluation; Locomotion; Mobility aids; Preview","Adult; Blindness; Computers; Human; Locomotion; Microcomputers; Middle Age; Sensory Aids; Support, Non-U.S. Gov't; BIOMECHANICS - Biped Locomotion; HUMAN REHABILITATION ENGINEERING - Mobility Aids for Blind; blindness; human; mobility; pedestrian; prevention; visual system; walking speed; BLIND MOBILITY; VISUAL IMPAIRMENT; WALKING SPEED; VISION","","","","00140139","","","3816749","English","Ergonomics","Article","Final","","Scopus","2-s2.0-0022899891"
"Bessa M.; Coelho A.; Chalmers A.","Bessa, Maximino (14031038800); Coelho, Antonio (23089899600); Chalmers, Alan (7102938771)","14031038800; 23089899600; 7102938771","Selective rendering quality for an efficient navigational aid in virtual urban environments on mobile platforms","2005","Proceedings of the 4th International Conference on Mobile and Ubiquitous Multimedia, MUM '05","154","","","109","113","4","1","10.1145/1149488.1149506","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-77952389560&doi=10.1145%2f1149488.1149506&partnerID=40&md5=79fda8b7b07b8a4952948fb01a1192c1","University of Tras-os-Montes e Alto Douro, Portugal; University of Bristol, United Kingdom","Bessa M., University of Tras-os-Montes e Alto Douro, Portugal; Coelho A., University of Tras-os-Montes e Alto Douro, Portugal; Chalmers A., University of Bristol, United Kingdom","The perception that we have of our world depends on the task we are currently performing in the environment, so if we are driving a car we will pay attention to the objects that are visually important to the task we are performing such as, the road, road signs, other vehicles, etc. The same is true when we explore virtual environments. The creation of high-fidelity 3D maps on mobile devices to aid navigation in urban environments is computationally very expensive, precluding achieving this quality at interactive rates. In this paper we present a case study to show how the human visual system may be exploited, when viewers are undertaking a task, to reduce the overall quality of the displayed image, without the users being aware of this reduction in quality. The displayed images are selectively rendered with the key features used to identify location and orientation in a 3D urban environment produced in high quality and the remainder of the image in low quality.","3D maps; Inattentional blindness; Mobile devices; Visual perception","Eye protection; Image processing; Mobile devices; Navigation; Portable equipment; Roads and streets; Urban planning; Virtual reality; Vision; High fidelity; High quality; Human Visual System; Inattentional blindness; Interactive rates; Key feature; Low qualities; Mobile platform; Overall quality; Road signs; Selective rendering; Urban environments; Virtual environments; Visual perception; Three dimensional","M. Bessa; University of Tras-os-Montes e Alto Douro, Portugal; email: maxbessa@utad.pt","","","","0473106582; 978-047310658-4","","","English","Proc. Int. Conf. Mob. Ubiquitous Multimedia, MUM","Conference paper","Final","","Scopus","2-s2.0-77952389560"
"Nagarajan R.; Yaccob S.; Sainarayanan G.","Nagarajan, Ramachandran (7202927600); Yaccob, Sazali (6505641077); Sainarayanan, G. (23986115600)","7202927600; 6505641077; 23986115600","Fuzzy based human vision properties in stereo sonification system for visually impaired","2001","Proceedings of SPIE - The International Society for Optical Engineering","4572","","","525","534","9","3","10.1117/12.444223","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-0035764109&doi=10.1117%2f12.444223&partnerID=40&md5=6d96598792c538ad43da4938d7220b6e","School of Engineering and Information Technology, Universiti Malaysia Sabah, Kota Kinabalu 88999, Sabah, Locked Bag No. 2073, Malaysia; Kota Kinabalu 88999, Locked Bag No. 2073, Malaysia","Nagarajan R., School of Engineering and Information Technology, Universiti Malaysia Sabah, Kota Kinabalu 88999, Sabah, Locked Bag No. 2073, Malaysia, Kota Kinabalu 88999, Locked Bag No. 2073, Malaysia; Yaccob S., School of Engineering and Information Technology, Universiti Malaysia Sabah, Kota Kinabalu 88999, Sabah, Locked Bag No. 2073, Malaysia; Sainarayanan G., School of Engineering and Information Technology, Universiti Malaysia Sabah, Kota Kinabalu 88999, Sabah, Locked Bag No. 2073, Malaysia","This paper presents incorporation of certain human vision properties in the image processing methodologies, applied in the vision substitutive system for human blind. The prototype of the system has digital video camera fixed in a headgear, stereo earphone and a laptop computer, interconnected. The processing of the captured image is designed as human vision. It involves lateral inhibition, which is developed using Feed Forward Neural Network (FFNN) and domination of the object properties with suppression of background by means of Fuzzy based Image Processing System (FLIPS). The processed image is mapped to stereo acoustic signals to the earphone. The sound is generated using nonlinear frequency incremental sine wave. The sequence of the scanning to construct the acoustic signal is designed to produce stereo signals, which aids to locate the object in horizontal axis. Frequency variation implies the location of object in the vertical axis. The system is tested with blind volunteer and his suggestion in formatting, pleasantness and discrimination of sound pattern were also considered.","Background suppression; Edge detection; Electronic Travel Aid; Iris; Sonification; Vision aid","Acoustic signal processing; Edge detection; Feedforward neural networks; Fuzzy sets; Scanning; Video cameras; Stereo sonification systems; Image processing","","","","0277786X","","","","English","Proc SPIE Int Soc Opt Eng","Article","Final","","Scopus","2-s2.0-0035764109"
"Ching L.W.; Leung M.K.H.","Ching, Lee Wee (8593770500); Leung, Maylor K. H. (55419459400)","8593770500; 55419459400","SINVI: Smart indoor navigation for the visually impaired","2004","2004 8th International Conference on Control, Automation, Robotics and Vision (ICARCV)","2","","TuA7.03 - P0876","1072","1077","5","11","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-21244496840&partnerID=40&md5=ac42d745785bd3773027dabb784e362d","School of Computer Engineering, Nanyang Technological University, Singapore 639798, N4, Nanyang Avenue, Singapore","Ching L.W., School of Computer Engineering, Nanyang Technological University, Singapore 639798, N4, Nanyang Avenue, Singapore; Leung M.K.H., School of Computer Engineering, Nanyang Technological University, Singapore 639798, N4, Nanyang Avenue, Singapore","Vision is an important sensory functionality of humans. Without which, navigation will be difficult and hazardous. In this paper, we outline the use of a novel approach to developing an automated guiding system for the blind. The approach is to use visual clues interpreted from images captured from a lightweight camera, mounted on a user. Visual clues are deciphered from the surrounding environment of the user. With just this information, the system will be able to guide the user through an indoor environment via audio cues. Majority of the visual clues will be gathered from the ceiling region. This is due to its low probability of being obscured, thereby increasing the chances of successful navigation © 2004 IEEE.","","Cameras; Decision making; Image processing; Probability; Radio waves; Robotics; Sensory perception; Wireless telecommunication systems; Electronic navigation systems (ENS); Electronic travel aids (ETA); Indoor navigation tools; Vision based systems (VBS); Vision aids","L.W. Ching; School of Computer Engineering, Nanyang Technological University, Singapore 639798, N4, Nanyang Avenue, Singapore; email: s7918339c@ntu.edu.sg","","","","0780386531; 978-078038653-2","","","English","Int. Conf. Control. Autom. Rob. Vis. ICARCV","Conference paper","Final","","Scopus","2-s2.0-21244496840"
"Kanno T.; Yanashima K.; Magatani K.","Kanno, Tomoyuki (55994610800); Yanashima, Kenji (7004401605); Magatani, Kazushige (6602092722)","55994610800; 7004401605; 6602092722","The Navigation System for the Visually Impaired Using GPS","2009","IFMBE Proceedings","23","","","938","941","3","1","10.1007/978-3-540-92841-6_232","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84861418760&doi=10.1007%2f978-3-540-92841-6_232&partnerID=40&md5=c24c7c1c5a5459153547e36ccd65e784","Department of Electrical Engineering, Tokai University, Hiratsuka, 1117 Kitakaname, Japan; National Rehabilitation Center for the Disabled, Japan","Kanno T., Department of Electrical Engineering, Tokai University, Hiratsuka, 1117 Kitakaname, Japan; Yanashima K., National Rehabilitation Center for the Disabled, Japan; Magatani K., Department of Electrical Engineering, Tokai University, Hiratsuka, 1117 Kitakaname, Japan","It is possible for the visually impaired to walk their known area independently by using a white cane. However, in their unknown area, they cannot walk without help of others even if they can use a white cane very well. In such cases, not only a visually impaired person but he/she who assists them receives many stress in dairy activity. Therefore, many types of independent walking assist system for the visually impaired are developing. Our objective of this study is the development of an auto navigation system for the visually impaired which assists independent walking of them. This system measures the position of a visually impaired person (a user) by using GPS (Global Positioning System) and navigates him/her to the destination like as a car navigation system. In our system, a location data is stored in the map database and a position of user are analyzed and optimal route for a user to the destination is calculated. Our system guides a user to the destination along this route, and notifies the user of this route and some information for the safety walking by artificial voice. Three normal subjects were tested with our navigation system. All subjects were blind folded by an eye mask and equipped a navigation system. As a result, our system worked well and all subjects were able to walk to the destination following guidance voice. So, we are developing a new navigation system which is installed on a cellular phone. In Japan, there are some cellular phones that include GPS. And most of application programs for cellular phone are written by JAVA. Therefore, GPS included in a cellular phone will be used and every functions of the navigation system will be written by JAVA. In this paper, we also reports about this new navigation system.","GPS; Navigation; Visually impaired","Biomedical engineering; Cellular telephones; Handicapped persons; Maps; Mobile phones; Navigation; Navigation systems; Patient rehabilitation; Walking aids; Artificial voice; Car navigation systems; Cellular Phone; Gps (global positioning system); Optimal routes; Visually impaired; Visually impaired persons; Walking assists; Global positioning system","","","","16800737","978-354092840-9","","","English","IFMBE Proc.","Conference paper","Final","","Scopus","2-s2.0-84861418760"
"Kumar A.; Patra R.; Manjunatha M.; Mukhopadhyay J.; Majumdar A.K.","Kumar, Amit (57206266783); Patra, Rusha (36994018100); Manjunatha, M. (6604071152); Mukhopadhyay, J. (58722066100); Majumdar, A.K. (36765250600)","57206266783; 36994018100; 6604071152; 58722066100; 36765250600","An electronic travel aid for navigation of visually impaired persons","2011","2011 3rd International Conference on Communication Systems and Networks, COMSNETS 2011","","","5716517","","","","53","10.1109/COMSNETS.2011.5716517","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-79952540936&doi=10.1109%2fCOMSNETS.2011.5716517&partnerID=40&md5=f9dbf72dd62c85cbac650f224f4a70df","Checktronix India Pvt. Ltd., Chennai, India; Indian Institute of Technology, Kharagpur, Kharagpur, India","Kumar A., Checktronix India Pvt. Ltd., Chennai, India; Patra R., Indian Institute of Technology, Kharagpur, Kharagpur, India; Manjunatha M., Indian Institute of Technology, Kharagpur, Kharagpur, India; Mukhopadhyay J., Indian Institute of Technology, Kharagpur, Kharagpur, India; Majumdar A.K., Indian Institute of Technology, Kharagpur, Kharagpur, India","This paper presents an electronic travel aid for blind people to navigate safely and quickly, an obstacle detection system using ultrasonic sensors and USB camera based visual navigation has been considered. The proposed system detects the obstacles up to 300 cm via sonar and sends feedback (beep sound) to inform the person about its location. In addition to this, an USB webcam is connected with eBox 2300™ Embedded System for capturing the field of view of the user, which is used for finding the properties of the obstacle in particular, in the context of this work, locating a human being. Identification of human presence is based on face detection and cloth texture analysis. The major constraints for these algorithms to run on Embedded System are small image frame (160×120) having reduced faces, limited memory and very less processing time available to achieve real time image processing requirements. The algorithms are implemented in C++ using Visual Studio 5.0 IDE, which runs on Windows CE™ environment. © 2011 IEEE.","Electronic travel aid; Embedded system; Face detection; Mobility; Navigation aid; Ultrasonic range sensor; Visual impairment","Algorithms; Communication systems; Embedded systems; Image processing; Navigation; Navigation systems; Obstacle detectors; Ophthalmology; Sensors; Ultrasonic applications; Ultrasonics; Underwater acoustics; Electronic travel aidss; Face detection; Mobility; Navigation aid; Ultrasonic range sensor; Visual impairment; Handicapped persons","A. Kumar; Checktronix India Pvt. Ltd., Chennai, India; email: amitkumar@burning-glass.com","","","","978-142448953-4","","","English","Int. Conf. Commun. Syst. Networks, COMSNETS","Conference paper","Final","","Scopus","2-s2.0-79952540936"
"Coughlan J.; Shen H.","Coughlan, James (7006567201); Shen, Huiying (14523706200)","7006567201; 14523706200","Terrain analysis for blind wheelchair users: Computer vision algorithms for finding curbs and other negative obstacles","2008","CEUR Workshop Proceedings","415","","","","","6","0","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84876264908&partnerID=40&md5=1824b4acdd5f1db554c1460815d920c2","Smith-Kettlewell Eye Research Institute, San Francisco CA 94115, 2318 Fillmore St., United States","Coughlan J., Smith-Kettlewell Eye Research Institute, San Francisco CA 94115, 2318 Fillmore St., United States; Shen H., Smith-Kettlewell Eye Research Institute, San Francisco CA 94115, 2318 Fillmore St., United States","We are developing computer vision algorithms for sensing important terrain features as an aid to wheelchair navigation, which interpret visual information obtained from images collected by video cameras mounted to the wheelchair. This paper focuses specifically on a novel computer vision algorithm for detecting curbs and other negative obstacles (i.e. anything below the level of the ground, such as holes and drop-offs), which are important and ubiquitous features on and near sidewalks and other walkways. The algorithm we develop extracts as much information as possible from depth information obtained from stereo video cameras (i.e. pairs of cameras mounted close together); other information (e.g. monocular cues such as intensity edges) will be incorporated in the future. We demonstrate experimental results on typical sidewalk scenes.","Assistive technology; Blind; Computer vision; Curbs; Obstacle detection; Visually impaired; Wheelchair","Algorithms; Curbs; Obstacle detectors; Pavements; Technology; Video cameras; Wheelchairs; Assistive technology; Blind; Computer vision algorithms; Depth information; Obstacle detection; Visual information; Visually impaired; Wheelchair navigation; Computer vision","J. Coughlan; Smith-Kettlewell Eye Research Institute, San Francisco CA 94115, 2318 Fillmore St., United States; email: coughlan@ski.org","","","16130073","","","","English","CEUR Workshop Proc.","Conference paper","Final","","Scopus","2-s2.0-84876264908"
"Ito K.; Okamoto M.; Akita J.; Ono T.; Gyobu I.; Takagi T.; Hoshi T.; Mishima Y.","Ito, Kiyohide (11940286900); Okamoto, Makoto (36018819100); Akita, Junichi (35566358500); Ono, Tetsuo (37056047400); Gyobu, Ikuko (23396469100); Takagi, Tomohito (22434193500); Hoshi, Takahiro (55485238000); Mishima, Yu (55484881500)","11940286900; 36018819100; 35566358500; 37056047400; 23396469100; 22434193500; 55485238000; 55484881500","CyARM: An alternative aid device for blind persons","2005","Conference on Human Factors in Computing Systems - Proceedings","","","","1483","1486","3","75","10.1145/1056808.1056947","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84869113494&doi=10.1145%2f1056808.1056947&partnerID=40&md5=3eb82b1a117319af15e1fc60c0c4e8b7","Department of Media Architecture, Future University-Hakodate, Hakodate 041-8655, 116-2, Kameda-nakano, Japan; Department of Information and Systems Engineering, Kanazawa University, Kanazawa 920-8667, 2-40-20 Kodatsuno, Japan; Faculty of Human Life and Environmental Sciences, Ochanomizu University, Bunkyo-ku, Tokyo 112-8610, 2-1-1 Otsuka, Japan; Human Interface Design Development, Fuji Xerox Co. Ltd, Yokohama, Kanagawa 240-0005, Japan","Ito K., Department of Media Architecture, Future University-Hakodate, Hakodate 041-8655, 116-2, Kameda-nakano, Japan; Okamoto M., Department of Media Architecture, Future University-Hakodate, Hakodate 041-8655, 116-2, Kameda-nakano, Japan; Akita J., Department of Information and Systems Engineering, Kanazawa University, Kanazawa 920-8667, 2-40-20 Kodatsuno, Japan; Ono T., Department of Media Architecture, Future University-Hakodate, Hakodate 041-8655, 116-2, Kameda-nakano, Japan; Gyobu I., Faculty of Human Life and Environmental Sciences, Ochanomizu University, Bunkyo-ku, Tokyo 112-8610, 2-1-1 Otsuka, Japan; Takagi T., Human Interface Design Development, Fuji Xerox Co. Ltd, Yokohama, Kanagawa 240-0005, Japan; Hoshi T., Department of Media Architecture, Future University-Hakodate, Hakodate 041-8655, 116-2, Kameda-nakano, Japan; Mishima Y., Department of Media Architecture, Future University-Hakodate, Hakodate 041-8655, 116-2, Kameda-nakano, Japan","With the concept of 'human-machine interface', designed especially for visually impaired persons, we have developed an electric aid device for use in guiding orientation and locomotion. The device, which we call CyARM, measures the distance between a person and an object with an ultrasonic sensor and transmits the distance information to the user's haptic sense. In this report, we will: (1) outline the concept of CyARM, (2) describe its mechanism, and (3) demonstrate three preliminary experiments that verify the usability of CyARM. We conducted the experiments in terms of detection of objects, detection of space, and tracking object movement. As a result of these experiments, we have concluded that CyARM is potentially effective for visually impaired persons. Our study will encourage therelated studies of user interfaces, particularly focusing on electric aid devices that guide visually impaired persons in detecting their environment.","Electric aid device; Haptic sense; User Interfaces; Visual impairment","Experiments; Human computer interaction; User interfaces; Walking aids; Blind person; Distance information; Electric aid device; Haptic sense; Human Machine Interface; Tracking objects; Visual impairment; Visually impaired persons; Handicapped persons","K. Ito; Department of Media Architecture, Future University-Hakodate, Hakodate 041-8655, 116-2, Kameda-nakano, Japan; email: Itokiyo@fun.ac.jp","","","","1595930027; 978-159593002-6","","","English","Conf Hum Fact Comput Syst Proc","Conference paper","Final","","Scopus","2-s2.0-84869113494"
"Tjan B.S.; Beckmann P.J.; Roy R.; Giudice N.; Legge G.E.","Tjan, B.S. (6602818317); Beckmann, P.J. (7004915668); Roy, R. (35766978000); Giudice, N. (15724751900); Legge, G.E. (7005064208)","6602818317; 7004915668; 35766978000; 15724751900; 7005064208","Digital sign system for indoor wayfinding for the visually impaired","2005","IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","2005-September","","","","","","34","10.1109/CVPR.2005.442","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-84856040523&doi=10.1109%2fCVPR.2005.442&partnerID=40&md5=e2415ef3b5020bee4d8aa6158abcf2f9","University of Southern California, 3620 South McClintock, SGM 1016, Los Angeles, 90089, CA, United States; Beckmann Research, LLC, 1882 South Ln, Mendota Hgts, 55118, MN, United States; University of Minnesota, 75 East River Road, Minneapolis, 55455, MN, United States; University of California, Santa Barbara, 93106, CA, United States","Tjan B.S., University of Southern California, 3620 South McClintock, SGM 1016, Los Angeles, 90089, CA, United States; Beckmann P.J., Beckmann Research, LLC, 1882 South Ln, Mendota Hgts, 55118, MN, United States; Roy R., University of Minnesota, 75 East River Road, Minneapolis, 55455, MN, United States; Giudice N., University of California, Santa Barbara, 93106, CA, United States; Legge G.E., University of Minnesota, 75 East River Road, Minneapolis, 55455, MN, United States","Mobility challenges and independent travel are major concerns for blind and visually impaired pedestrians [1][2]. Navigation and wayfinding in unfamiliar indoor environments are particularly challenging because blind pedestrians do not have ready access to building maps, signs and other orienting devices. The development of assistive technologies to aid wayfinding is hampered by the lack of a reliable and cost-efficient method for providing location information in an indoor environment. Here we describe the design and implementation of a digital sign system based on low-cost passive retro-reflective tags printed with specially designed patterns that can be readily detected and identified by a hand-held camera and machine-vision system. Performance of the prototype showed the tag detection/recognition system could cope with the real-world environment of a typical building. © 2005 IEEE Computer Society. All rights reserved.","","Computer science; Computers; Pattern recognition; Software engineering; Assistive technology; Blind and visually impaired; Design and implementations; Indoor environment; Location information; Machine vision systems; Real world environments; Visually impaired; Computer vision","","","IEEE Computer Society","21607508","0769526608","","","English","IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recogn. Workshops","Conference paper","Final","","Scopus","2-s2.0-84856040523"
"Tian Y.; Yi C.; Arditi A.","Tian, Yingli (16556710700); Yi, Chucai (36192382300); Arditi, Aries (57207554001)","16556710700; 36192382300; 57207554001","Improving computer vision-based indoor wayfinding for blind persons with context information","2010","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","6180 LNCS","PART 2","","255","262","7","13","10.1007/978-3-642-14100-3_38","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-77955066198&doi=10.1007%2f978-3-642-14100-3_38&partnerID=40&md5=4fdb30fe2f1538526fb41bae0c22d656","Electrical Engineering Department, City College, City University of New York, New York, NY 10031, United States; Arlene R Gordon Research Institute, Lighthouse International, New York, NY 10022, 111 East 59th Street, United States","Tian Y., Electrical Engineering Department, City College, City University of New York, New York, NY 10031, United States; Yi C., Electrical Engineering Department, City College, City University of New York, New York, NY 10031, United States; Arditi A., Arlene R Gordon Research Institute, Lighthouse International, New York, NY 10022, 111 East 59th Street, United States","There are more than 161 million visually impaired people in the world today, of which 37 million are blind. Camera-based computer vision systems have the potential to assist blind persons to independently access unfamiliar buildings. Signs with text play a very important role in identification of bathrooms, exits, office doors, and elevators. In this paper, we present an effective and robust method of text extraction and recognition to improve computer vision-based indoor wayfinding. First, we extract regions containing text information from indoor signage with multiple colors and complex background and then identify text characters in the extracted regions by using the features of size, aspect ratio and nested edge boundaries. Based on the consistence of distances between two neighboring characters in a text string, the identified text characters have been normalized before they are recognized by using off-the-shelf optical character recognition (OCR) software products and output as speech for blind users. © 2010 Springer-Verlag Berlin Heidelberg.","computer vision; indoor; Indoor navigation and wayfinding; optical character recognition (OCR); text extraction","Aspect ratio; Computer vision; Navigation; Optical character recognition; Speech recognition; Blind person; Blind users; Complex background; Computer vision system; Context information; Indoor navigation; Indoor navigation and wayfinding; Robust methods; Software products; Text extraction; Text information; Text string; Visually impaired people; Way-finding; Feature extraction","Y. Tian; Electrical Engineering Department, City College, City University of New York, New York, NY 10031, United States; email: ytian@ccny.cuny.edu","","","16113349","3642140998; 978-364214099-0","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","","Scopus","2-s2.0-77955066198"
"Bourbakis N.G.; Kavraki Despina","Bourbakis, N.G. (7006247012); Kavraki, Despina (15069197800)","7006247012; 15069197800","Intelligent assistants for handicapped people's independence: Case study","1996","IEEE International Joint Symposia on Intelligence and Systems","","","","337","344","7","19","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-0030382655&partnerID=40&md5=1e8c74499d1fc6539f812a753af544d2","Binghamton Univ, Binghamton, United States","Bourbakis N.G., Binghamton Univ, Binghamton, United States; Kavraki Despina, Binghamton Univ, Binghamton, United States","This paper presents the first stage of the development of two intelligent assistants for handicapped people's independence. The first intelligent assistant, called Tyflos, will help a blind user to be independent and able to walk and work alone in a 3-D dynamic environment. The system captures images from the surrounding 3-D environment, either by the user's command or in a continued mode, and converts the visual description of each image into a verbal (low or high level) descriptions. With other words the system plays the role of human assistant, who describes to the user the 3-D visual environment. The second assistant, called Koufos, will help a deaf person to be more independent in the living environment by making him/her to visually hear sounds and spoken natural language and assisting him/her to visually learn to speak as well.","","Artificial intelligence; Image understanding; Independent living systems; Information retrieval systems; Knowledge representation; Natural language processing systems; Speech recognition; Knowledge conversion; Sound understanding; Three dimensional visual navigation; Learning aids for handicapped persons","","","IEEE","","","00249","","English","IEEE Int Jt Symp Intell Syst","Conference paper","Final","","Scopus","2-s2.0-0030382655"
"Yang X.; Tian Y.; Yi C.; Arditi A.","Yang, Xiaodong (59108799200); Tian, Yingli (16556710700); Yi, Chucai (36192382300); Arditi, Aries (57207554001)","59108799200; 16556710700; 36192382300; 57207554001","Context-based indoor object detection as an aid to blind persons accessing unfamiliar environments","2010","MM'10 - Proceedings of the ACM Multimedia 2010 International Conference","","","","1087","1090","3","23","10.1145/1873951.1874156","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-78650985002&doi=10.1145%2f1873951.1874156&partnerID=40&md5=95d939c147db6c1c350d2a86e38facb1","Dept. of Electrical Engineering, City College of New York, NY 10031, United States; Graduate Center, City University of New York, NY 10016, United States; Arlene R Gordon Research Institute, Lighthouse International, New York, NY 10222, United States","Yang X., Dept. of Electrical Engineering, City College of New York, NY 10031, United States; Tian Y., Dept. of Electrical Engineering, City College of New York, NY 10031, United States; Yi C., Graduate Center, City University of New York, NY 10016, United States; Arditi A., Arlene R Gordon Research Institute, Lighthouse International, New York, NY 10222, United States","Independent travel is a well known challenge for blind or visually impaired persons. In this paper, we propose a computer vision-based indoor wayfinding system for assisting blind people to independently access unfamiliar buildings. In order to find different rooms (i.e. an office, a lab, or a bathroom) and other building amenities (i.e. an exit or an elevator), we incorporate door detection with text recognition. First we develop a robust and efficient algorithm to detect doors and elevators based on general geometric shape, by combining edges and corners. The algorithm is generic enough to handle large intra-class variations of the object model among different indoor environments, as well as small inter-class differences between different objects such as doors and elevators. Next, to distinguish an office door from a bathroom door, we extract and recognize the text information associated with the detected objects. We first extract text regions from indoor signs with multiple colors. Then text character localization and layout analysis of text strings are applied to filter out background interference. The extracted text is recognized by using off-the-shelf optical character recognition (OCR) software products. The object type, orientation, and location can be displayed as speech for blind travelers. © 2010 ACM.","blind/visually impaired persons; computer vision; indoor wayfinding; object detection; text extraction","Algorithms; Computer vision; Doors; Elevators; Ferry boats; Object recognition; Optical character recognition; Blind people; Blind person; blind/visually impaired persons; Context-based; Door detection; Efficient algorithm; Geometric shape; Indoor environment; Intra-class variation; Layout analysis; Object Detection; Object model; Software products; Text extraction; Text information; Text recognition; Text region; Text string; Visually impaired persons; Way-finding; Way-finding systems; Handicapped persons","X. Yang; Dept. of Electrical Engineering, City College of New York, NY 10031, United States; email: xyang02@ccny.cuny.edu","","","","978-160558933-6","","","English","MM - Proc. ACM Multimedia Int. Conf.","Conference paper","Final","","Scopus","2-s2.0-78650985002"
"Deville B.; Bologna G.; Vinckenbosch M.; Pun T.","Deville, Benoît (23396648700); Bologna, Guido (7005522450); Vinckenbosch, Michel (56624817200); Pun, Thierry (7005509099)","23396648700; 7005522450; 56624817200; 7005509099","Depth-based detection of salient moving objects in sonified videos for blind users","2008","VISAPP 2008 - 3rd International Conference on Computer Vision Theory and Applications, Proceedings","1","","","434","439","5","2","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-57349133756&partnerID=40&md5=f04a54494bbbc3186fbde6eaf3aa3e98","Computer Vision and Multimedia Lab., University of Geneva, Carouge, Route de Drize 7, Switzerland; Laboratoire d'Informatique Industrielle, University of Applied Science, Geneva, Switzerland","Deville B., Computer Vision and Multimedia Lab., University of Geneva, Carouge, Route de Drize 7, Switzerland; Bologna G., Laboratoire d'Informatique Industrielle, University of Applied Science, Geneva, Switzerland; Vinckenbosch M., Laboratoire d'Informatique Industrielle, University of Applied Science, Geneva, Switzerland; Pun T., Computer Vision and Multimedia Lab., University of Geneva, Carouge, Route de Drize 7, Switzerland","The context of this work is the development of a mobility aid for visually impaired persons. We present here an original approach for a real time alerting system, based on the use of feature maps for detecting visual salient parts in images. In order to improve the quality of this method, we propose here to benefit from a new feature map constructed from the depth gradient. A specific distance function is described, which takes into account both stereoscopic camera limitations and users choices. We demonstrate here that this additional depth-based feature map allows the system to detect the salient regions with good accuracy in most situations, even with noisy disparity maps.","Depth map; Focus of attention (FOA); Mobility aid; Stereo image processing; Visual handicap; Visual saliency","Cameras; Computer vision; Digital image storage; Image processing; Imaging systems; Optical projectors; Special effects; Depth map; Focus of attention (FOA); Mobility aid; Stereo image processing; Visual handicap; Visual saliency; Stereo vision","B. Deville; Computer Vision and Multimedia Lab., University of Geneva, Carouge, Route de Drize 7, Switzerland; email: Benoit.Deville@cui.unige.ch","","","","978-989811121-0","","","English","VISAPP - Int. Conf. Comput. Vis. Theory Appl., Proc.","Conference paper","Final","","Scopus","2-s2.0-57349133756"
"Balakrishnan G.; Sainarayanan G.; Nagarajan R.; Yaacob S.","Balakrishnan, G. (59157782500); Sainarayanan, G. (23986115600); Nagarajan, R. (7202927600); Yaacob, Sazali (6602262501)","59157782500; 23986115600; 7202927600; 6602262501","Stereo image sonification for blind navigation","2007","Tamkang Journal of Science and Engineering","10","1","","67","76","9","0","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-34147198885&partnerID=40&md5=294df98e67735b74b0941c6b18f16dac","AI Research Group, School of Engineering and Information Technology, Universiti Malaysia Sabah, 88999, Kota Kinabalu, Locked Bag No. 2073, Malaysia; School of Mechatronics, Northern Malaysia University, College of Engineering, 01000 Kangar, Perils, Malaysia","Balakrishnan G., AI Research Group, School of Engineering and Information Technology, Universiti Malaysia Sabah, 88999, Kota Kinabalu, Locked Bag No. 2073, Malaysia; Sainarayanan G., AI Research Group, School of Engineering and Information Technology, Universiti Malaysia Sabah, 88999, Kota Kinabalu, Locked Bag No. 2073, Malaysia; Nagarajan R., School of Mechatronics, Northern Malaysia University, College of Engineering, 01000 Kangar, Perils, Malaysia; Yaacob S., School of Mechatronics, Northern Malaysia University, College of Engineering, 01000 Kangar, Perils, Malaysia","Visually impaired find their navigation difficult as they often lack the needed information for bypassing obstacles and hazards. Electronic Travel Aids (ETAs) are devices that use sensor technology to assist and improve the blind user's mobility in terms of safety and speed. Modern ETAs does not provide distance information directly and clearly. This paper proposes a method for determining distance using a stereo matching method to help blind individuals for their navigation. The system developed in this work, named Stereo Vision based Electronic Travel Aid (SVETA), consists of a computing device, stereo cameras and stereo earphones, all molded in a helmet. An improved area based stereo matching is performed over the transformed images to calculate dense disparity image. Low texture filter and left/right consistency check are carried out to remove the noises and to highlight the obstacles. A sonification procedure is proposed to map the disparity image to stereo musical sound, which has information about the features of the scene infront of the user. The sound is conveyed to the blind user through stereo headphones. Experimentations have been conducted and preliminary investigations have proven the viability of this method for applying in real time environment.","Disparity; Electronic travel aid; Sonification; Stereo matching; Stereo vision","Cameras; Earphones; Sensors; Stereo vision; Blind navigation; Disparity; Electronic travel aid; Sonification; Stereo matching; Image processing","G. Balakrishnan; AI Research Group, School of Engineering and Information Technology, Universiti Malaysia Sabah, 88999, Kota Kinabalu, Locked Bag No. 2073, Malaysia; email: g_krishbalu@yahoo.com","","","15606686","","","","English","Tamkang J. Sci. Eng.","Article","Final","","Scopus","2-s2.0-34147198885"
"Wong F.; Nagarajan R.; Yaacob S.","Wong, Farrah (8407774300); Nagarajan, R. (7202927600); Yaacob, Sazali (6602262501)","8407774300; 7202927600; 6602262501","Application of stereovision in a navigation aid for blind people","2003","ICICS-PCM 2003 - Proceedings of the 2003 Joint Conference of the 4th International Conference on Information, Communications and Signal Processing and 4th Pacific-Rim Conference on Multimedia","2","","1292553","734","737","3","25","10.1109/ICICS.2003.1292553","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-36348967916&doi=10.1109%2fICICS.2003.1292553&partnerID=40&md5=6d69a0948617a993adb12726cdefe9b4","Artificial Intelligence Research Group, School of Engineering and Information Technology, Universiti Malaysia Sabah, Kota Kinabalu, Sabah, 88999, Malaysia","Wong F., Artificial Intelligence Research Group, School of Engineering and Information Technology, Universiti Malaysia Sabah, Kota Kinabalu, Sabah, 88999, Malaysia; Nagarajan R., Artificial Intelligence Research Group, School of Engineering and Information Technology, Universiti Malaysia Sabah, Kota Kinabalu, Sabah, 88999, Malaysia; Yaacob S., Artificial Intelligence Research Group, School of Engineering and Information Technology, Universiti Malaysia Sabah, Kota Kinabalu, Sabah, 88999, Malaysia","A ""Navigation Aid for Visually Impaired"" (NAVI) system has been developed in UMS in 2001 and improved in 2002. The NAVI system comprises of 3-part functioning units namely a digital video camera, a single board computer and a headphone. The single camera limitation in providing the depth information, which is critical for navigation purposes, has prompted the extension into a stereovision system by using two cameras. This stereovision system also uses the fuzzy-based segmentation procedure as an image pre-processing package that was developed for the single camera NAVI but with an additional adaptive loop. The segmented images undergo a rule-based stereo matching procedure. From the matching features, the disparity is 'computed. The disparity in combination with information of focal length as well as the space between two cameras provides the information on the distance between cameras and object. This distance information is incorporated into the final processed image as four gray levels such as white, light gray, dark gray and black. The size and location of object in the visual plane is then conveyed to the blind individual by means of a structured coded sound. The distance information is represented by means of verbal sound. Preliminary experimental analysis reveals a promising new approach for developing a navigational aid for blinds through the transformation of stereo image to stereo sound. © 2003 IEEE.","","Air navigation; Cameras; Computer graphics; Image processing; Image segmentation; Multimedia signal processing; Multimedia systems; Navigation; Signal processing; Video cameras; Depth information; Digital video cameras; Distance information; Experimental analysis; Image preprocessing; Segmentation procedure; Single board computers; Stereo-vision system; Stereo image processing","","","Institute of Electrical and Electronics Engineers Inc.","","0780381858; 978-078038185-8","","","English","ICICS-PCM - Proc. Jt. Conf. Int. Conf. Inf., Commun. Signal Process. Pac.-Rim Conf. Multimed.","Conference paper","Final","","Scopus","2-s2.0-36348967916"
"Shoval Shraga; Borenstein Johann; Koren Yoram","Shoval, Shraga (7004180117); Borenstein, Johann (7005627914); Koren, Yoram (7004934603)","7004180117; 7005627914; 7004934603","Mobile robot obstacle avoidance in a computerized travel aid for the blind","1994","Proceedings - IEEE International Conference on Robotics and Automation","","pt 3","","2023","2028","5","89","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-0028554905&partnerID=40&md5=2683b01b92810fae718c4dad8e5a2d15","Univ of Michigan, Ann Arbor, United States","Shoval Shraga, Univ of Michigan, Ann Arbor, United States; Borenstein Johann, Univ of Michigan, Ann Arbor, United States; Koren Yoram, Univ of Michigan, Ann Arbor, United States","A blind traveler walking through an unfamiliar environment, and a mobile robot navigating through a cluttered environment have an important feature in common: both have the kinematic ability to perform the motion, but are depended on a sensory system to detect and avoid obstacles. This paper describes the use of a mobile robot obstacle avoidance system as a guidance device for blind and visually impaired people. Just like electronic signals are sent to a mobile robot's motor controllers, auditory signals can guide the blind traveler around obstacles, or alternatively, they can provide an 'acoustic image' of the surroundings. The concept has been implemented and tested in a new traveling aid for the blind, called the Navbelt. Experimental results of subjects traveling with the Navbelt in different surroundings are presented.","","Computer applications; Mobile robots; Navigation; Walking aids; Auditory signals; Blind people; Navbelt; Obstacle avoidance; Handicapped persons","","","Publ by IEEE","10504729","0818653329","PIIAE","","English","Proc IEEE Int Conf Rob Autom","Conference paper","Final","","Scopus","2-s2.0-0028554905"
"Ju J.S.; Ko E.; Kim E.Y.","Ju, Jin Sun (24476974600); Ko, Eunjeong (35078934400); Kim, Eun Yi (56296322200)","24476974600; 35078934400; 56296322200","EYECane: Navigating with camera embedded white cane for visually impaired person","2009","ASSETS'09 - Proceedings of the 11th International ACM SIGACCESS Conference on Computers and Accessibility","","","","237","238","1","24","10.1145/1639642.1639693","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-72249091265&doi=10.1145%2f1639642.1639693&partnerID=40&md5=7d7519dffb450e889e3fdfad3ef4e1a3","Dept. of Advanced Technology Fusion Engineering, Konkuk University, Seoul, South Korea","Ju J.S., Dept. of Advanced Technology Fusion Engineering, Konkuk University, Seoul, South Korea; Ko E., Dept. of Advanced Technology Fusion Engineering, Konkuk University, Seoul, South Korea; Kim E.Y., Dept. of Advanced Technology Fusion Engineering, Konkuk University, Seoul, South Korea","We demonstrate a novel assistive device which can help the visually impaired or blind people to gain more safe mobility, which is called as ""EYECane"". The EYECane is the white-cane with embedding a camera and a computer. It automatically detects obstacles and recommends some avoidable paths to the user through acoustic interface. For this, it is performed by three steps: Firstly, it extracts obstacles from image streaming using online background estimation, thereafter generates the occupancy grid map, which is given to neural network. Finally, the system notifies a user of an paths recommended by machine learning. To assess the effectiveness of the proposed EYECane, it was tested with 5 users and the results show that it can support more safe navigation, and diminish the practice and efforts to be adept in using the white cane.","EYECane; Navigation; Vision based navigation; Vision based white cane; Visually impaired person","Cameras; Neural networks; Patient rehabilitation; Vision based; Vision based navigation; Visually impaired person; Visually impaired persons; White cane; Navigation","J. S. Ju; Dept. of Advanced Technology Fusion Engineering, Konkuk University, Seoul, South Korea; email: vocaljs@konkuk.ac.kr","","","","978-160558819-3","","","English","ASSETS - Proc. Int. ACM SIGACCESS Conf. Comput. Accessibility","Conference paper","Final","","Scopus","2-s2.0-72249091265"
"Al Yahmadi A.S.; El-Dirdiri A.B.E.-T.; Pervez T.","Al Yahmadi, Amur S. (55404746700); El-Dirdiri, Abu Baker El-Tahir (35317355600); Pervez, Tasneem (9735826400)","55404746700; 35317355600; 9735826400","Behavior based control of a robotic based navigation aid for the blind","2009","Proceedings of the 11th IASTED International Conference on Control and Applications, CA 2009","","","","161","166","5","1","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-74549155802&partnerID=40&md5=ac889be571c8c5b2f33f1f0e6b9026ea","Department of Mechanical and Industrial Engineering, Sultan Qaboos University, Muscat, PO Box 33, Oman","Al Yahmadi A.S., Department of Mechanical and Industrial Engineering, Sultan Qaboos University, Muscat, PO Box 33, Oman; El-Dirdiri A.B.E.-T., Department of Mechanical and Industrial Engineering, Sultan Qaboos University, Muscat, PO Box 33, Oman; Pervez T., Department of Mechanical and Industrial Engineering, Sultan Qaboos University, Muscat, PO Box 33, Oman","This paper presents a navigational approach that is intended to eventually be used in a GuideCane device. This is a device to help blind or visually impaired users to navigate safely and quickly among obstacles and other hazards. During operation, the user will be pushing the lightweight GuideCane forward. When the GuideCane's sensors detect an obstacle, a suitable direction of motion is decided that steers the GuideCane and the user around the obstacle. The ability to navigate autonomously, avoiding modelled and un-modelled obstacles especially in crowded and unpredictably changing environment is a challenging issue in the design of a travel aid for the blind. A successful way of structuring the navigation task in order to deal with the problem is within behavior based navigation approaches. The approach constitutes a new method for behaviour coordination that suits the utilization of mobile robots in blind aid application. In addition to that a multi-structure controller was employed to enhance the overall navigation process. The behaviour coordination in our case is based on a reactive approach inspired by the Bug algorithm.","Behavior based navigation; Fuzzy logic; Guidecane; Mobile robots","Fuzzy logic; Fuzzy systems; Mobile robots; Behavior based navigation; Behavior-based; Behavior-based control; Changing environment; Direction of motion; Guidecane; Multi-structure; Navigation aids; Navigation tasks; Travel aid for the blind; Visually-impaired users; Navigation","A. S. Al Yahmadi; Department of Mechanical and Industrial Engineering, Sultan Qaboos University, Muscat, PO Box 33, Oman; email: amery@squ.edu.om","","","","978-088986794-9","","","English","Proc. IASTED Int. Conf. Control Appl., CA","Conference paper","Final","","Scopus","2-s2.0-74549155802"
"Cohen R.F.; Meacham A.; Skaff J.","Cohen, Robert F. (12240725700); Meacham, Arthur (12241009800); Skaff, Joelle (56518201000)","12240725700; 12241009800; 56518201000","Teaching graphs to visually impaired students using an active auditory interface","2007","Proceedings of the Thirty-Seventh SIGCSE Technical Symposium on Computer Science Education","","","","279","282","3","32","10.1145/1121341.1121428","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-37849011247&doi=10.1145%2f1121341.1121428&partnerID=40&md5=f801a42668232803e66afddbd868b621","Department of Computer Science, UMass Boston, Boston, MA 02125, United States","Cohen R.F., Department of Computer Science, UMass Boston, Boston, MA 02125, United States; Meacham A., Department of Computer Science, UMass Boston, Boston, MA 02125, United States; Skaff J., Department of Computer Science, UMass Boston, Boston, MA 02125, United States","We present our ongoing research in the communication of graphs and relational information to blind computer science students. We have developed a system called exPLoring graphs at UMB (PLUMB) that displays a drawn graph on a tablet PC and uses auditory cues to help a blind user navigate the graph. Beyond Computer Science education, this work has applications to assist blind individuals in navigation, map manipulation and other applications that require graph visualization. Copyright 2006 ACM.","Accessibility; Audio; C# programming language; Graph","C (programming language); Computer science; Computer vision; Data visualization; Engineering education; Students; Computer science students; Graph visualization; User navigation; Graphical user interfaces","R.F. Cohen; Department of Computer Science, UMass Boston, Boston, MA 02125, United States; email: rfc@cs.umb.edu","","","","978-159593259-4","","","English","Proc. Thirty-Seventh SIGCSE Tech. Symp. Comput. Sci. Educ.","Conference paper","Final","","Scopus","2-s2.0-37849011247"
"Pissaloux E.E.","Pissaloux, Edwige E. (7003850978)","7003850978","Some comments on design of electronic travel aids for visually impaired; [À propos de la conception de systèmes d'aide aux déplacements des déficients visuels]","2003","Annales des Telecommunications/Annals of Telecommunications","58","5-6","","905","917","12","0","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-0141611068&partnerID=40&md5=8a935faa678c7e2714e1ee0aeb2d7862","Laboratoire de Robotique de Paris, 92 265 Fontenay-aux-Roses, BP. 61, France","Pissaloux E.E., Laboratoire de Robotique de Paris, 92 265 Fontenay-aux-Roses, BP. 61, France","This paper aims to contribute to studies related to visual handicap in order to reduce/eliminate it with new technologies. It addresses shortly the main principle of 3D world human perception, the main rules for environmental pertinent data acquisition and their processing. The proposed state of art on blind people travel assisting aids (TAA) allow to understand the requirements for new aids design. A few examples of research project show how solution from new information technologies (such as artificial vision, virtual reality, high density VLSI, tactile systems, GPS, wireless networks, etc) can be integrated in new systems. These examples allow also to determine which concepts should be targeted when designing any new travel assistance for blind people autonomous and safe displacement, and to characterise new devices.","Handicapped assistance; Obstacle; Perception; Person blind; State of the art; Three dimensional presentation","Data acquisition; Product design; Vision; Travel assisting aids (TAA); Human rehabilitation equipment","E.E. Pissaloux; Laboratoire de Robotique de Paris, 92 265 Fontenay-aux-Roses, BP. 61, France; email: pissaloux@robot.jussieu.fr","","","19589395","","ANTEA","","French","Ann Telecommun","Article","Final","","Scopus","2-s2.0-0141611068"
"Nye P.W.; Bliss J.C.","Nye, Patrick W. (7004293284); Bliss, James C. (7102928665)","7004293284; 7102928665","Sensory Aids for the Blind: A Challenging Problem with Lessons for the Future","1970","Proceedings of the IEEE","58","12","","1878","1898","20","34","10.1109/PROC.1970.8061","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-0014897830&doi=10.1109%2fPROC.1970.8061&partnerID=40&md5=4fb1271e66c942d16e8ce4f73dad3c9b","California Institute of Technology, Pasadena, Calif. 91109, United States; Stanford Research Institute and Stanford University, Stanford, Calif. 94305, United States","Nye P.W., California Institute of Technology, Pasadena, Calif. 91109, United States; Bliss J.C., Stanford Research Institute and Stanford University, Stanford, Calif. 94305, United States","The two major objectivas of sensory aids for the blind are to permit access to printed matter and to permit safe travel through the environment. The difficulties of designing technological means to achieve these objectives are in many respects unrelated to the concerns of the engineering laboratory. Social, economic, political, and logistic considerations all play a role. The “blind population” in the United States includes both the totally blind and those with a wide range of visual impairment. This population totals about 400 000 people in which the aged, the multiply handicapped, and those with significant residual vision predominate. Singly handicapped, working-aged people are the initial targets of the current sensory aids. Expansion of their range of usefulness to larger fractions of the blind population is expected to come later. About 800 agencies serve the blind population in the United States, and in 1967 they were responsible for an annual expenditure of $500 million. In contrast the sensory-aids research and development budget was less than $1 million. Nevertheless, several potentially useful prototype devices have been developed and are about to be evaluated in this country; at least one is of foreign origin. But if these devices are ever to have the opportunity of reaching the blind public, then mechanisms for evaluation, field trials, manufacture, and deployment must be set up. The field of currently active sensory-aids research programs is reviewed. Several programs are concerned with increasing the convenience and accessibility of braille by the application of computer technology. Nevertheless, despite the unquestionable value of these developments, the usefulness of braille is limited by its bulk, its cost, and the transcription time. To provide direct access to printed documents several devices are being developed that transform optical images from a printed page into auditory or tactile displays requiring motivation and training for effective use. These machines are termed “direct-translation” units and are designed for simplicity and low cost. Other systems utilize print recognition techniques to create a reading machine providing braille or speech as an output. These machines offer potentially faster reading rates and their use promises to be easier to learn than direct-translation machines, but at the penalty of complexity and high cost. Several mobility aids designed to augment the cane or guide dog have recently been developed. These are also described. The prospects of achieving direct input to the visual cortex are discussed. It is apparent that the cost of this research is likely to be extremely high in relation to the size of the blind population which might ultimately benefit. Somewhat more easily realizable is a visual substitution system involving stimulation of an area of the skin. Several systems are being developed but all suffer from limitations in image resolution. Finally, an examination of the organization of research and funding reveals that the U.S. program is small, poorly coordinated, and contains some seemingly unnecessary duplication of effort. Several obvious lessons emerge which, if heeded, could greatly improve the effectiveness of sensory-aids research by providing development, manufacture, evaluation, and deployment services within an integrated program. Copyright © 1970 by The Institute of Electrical and Electronics Engineers, Inc.","","ACCIDENT PREVENTION; BLIND; GUIDANCE DEVICES FOR THE BLIND; IEEPA; ITU; LASERS; PATTERN RECOGNITION SYSTEMS","","","","00189219","","","","English","Proc. IEEE","Note","Final","All Open Access; Green Open Access","Scopus","2-s2.0-0014897830"
"Murphy E.; Kuber R.; McAllister G.; Strain P.; Yu W.","Murphy, Emma (55771749100); Kuber, Ravi (22834722400); McAllister, Graham (14831703600); Strain, Philip (22837117900); Yu, Wai (22837321100)","55771749100; 22834722400; 14831703600; 22837117900; 22837321100","An empirical investigation into the difficulties experienced by visually impaired internet users","2008","Universal Access in the Information Society","7","1-2","","79","91","12","73","10.1007/s10209-007-0098-4","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-41849128511&doi=10.1007%2fs10209-007-0098-4&partnerID=40&md5=32139b17264a2cbba1ac06c67b6a861d","Queen's University Belfast, Belfast BT7 1NN, University Road, United Kingdom","Murphy E., Queen's University Belfast, Belfast BT7 1NN, University Road, United Kingdom; Kuber R., Queen's University Belfast, Belfast BT7 1NN, University Road, United Kingdom; McAllister G., Queen's University Belfast, Belfast BT7 1NN, University Road, United Kingdom; Strain P., Queen's University Belfast, Belfast BT7 1NN, University Road, United Kingdom; Yu W., Queen's University Belfast, Belfast BT7 1NN, University Road, United Kingdom","In this paper, an empirical based study is described which has been conducted to gain a deeper understanding of the challenges faced by the visually impaired community when accessing the Web. The study, involving 30 blind and partially sighted computer users, has identified navigation strategies, perceptions of page layout and graphics using assistive devices such as screen readers. Analysis of the data has revealed that current assistive technologies impose navigational constraints and provide limited information on web page layout. Conveying additional spatial information could enhance the exploration process for visually impaired Internet users. It could also assist the process of collaboration between blind and sighted users when performing web-based tasks. The findings from the survey have informed the development of a non-visual interface, which uses the benefits of multimodal technologies to present spatial and navigational cues to the user. © Springer-Verlag 2007.","Accessibility barriers; Assistive technology; Internet; Multimodality; Visually impaired","Graphic methods; Human computer interaction; Internet; Man machine systems; Web services; Assistive technology; Non-visual interface; Screen reader; Handicapped persons","E. Murphy; Queen's University Belfast, Belfast BT7 1NN, University Road, United Kingdom; email: e.murphy@qub.ac.uk","","","16155297","","","","English","Univers. Access Inf. Soc.","Article","Final","","Scopus","2-s2.0-41849128511"
"Haraszy Z.; Micut S.; Tiponut V.; Slavici T.","Haraszy, Zoltan (24476948300); Micut, Sebastian (39861754900); Tiponut, Virgil (16041074100); Slavici, Titus (23502579200)","24476948300; 39861754900; 16041074100; 23502579200","Multi-subject head related transfer function generation using artificial neural networks","2010","International Conference on Systems - Proceedings","1","","","399","404","5","2","","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-77957559316&partnerID=40&md5=32806821ffe214dad06058422e5cc9fb","Department of Applied Electronics, POLITEHNICA University of Timisoara, 300223 Timisoara, Bvd. Vasile Parvan 2, Romania","Haraszy Z., Department of Applied Electronics, POLITEHNICA University of Timisoara, 300223 Timisoara, Bvd. Vasile Parvan 2, Romania; Micut S., Department of Applied Electronics, POLITEHNICA University of Timisoara, 300223 Timisoara, Bvd. Vasile Parvan 2, Romania; Tiponut V., Department of Applied Electronics, POLITEHNICA University of Timisoara, 300223 Timisoara, Bvd. Vasile Parvan 2, Romania; Slavici T., Department of Applied Electronics, POLITEHNICA University of Timisoara, 300223 Timisoara, Bvd. Vasile Parvan 2, Romania","The new Acoustic Virtual Reality (AVR) concept is often used as a man-machine interface in electronic travel aid (ETA), that help blind and visually impaired individuals to navigate in real outdoor environments. According to this concept, the presence of obstacles in the surrounding environment and the path to the desired target will be signalized to the blind subject by burst of sounds, whose virtual source position suggests the position of the real obstacles and the direction of movement, respectively. The practical implementation of the AVR concept requires the so-called Head Related Transfer Functions (HRTFs) to be known in every point of the 3D space and for each subject. These functions can be determined by using a quite complex procedure, which requires many measurements for each individual. In the present paper, an updated version of the previously proposed artificial neural network (ANN) is presented and used, in order to generate the HRTFs. The proposed method, valid for any subject, speeds up the implementation of the AVR concept after the ANN training has been completed. Finally, some experimental results, conclusions and further developments are also presented.","Acoustic virtual reality; Artificial neural networks; Head related transfer functions; Man-machine interface; Performance evaluation; Visually impaired","Transfer functions; Virtual reality; Artificial Neural Network; Head related transfer function; Man-machine interface; Performance evaluation; Visually impaired; Neural networks","Z. Haraszy; Department of Applied Electronics, POLITEHNICA University of Timisoara, 300223 Timisoara, Bvd. Vasile Parvan 2, Romania; email: zoltan.haraszy@etc.upt.ro","","","","978-960474199-1","","","English","Int. Conf. Syst. - Proc.","Conference paper","Final","","Scopus","2-s2.0-77957559316"
"Shen H.; Chan K.-Y.; Coughlan J.; Brabyn J.","Shen, Huiying (14523706200); Chan, Kee-Yip (25821662400); Coughlan, James (7006567201); Brabyn, John (7004481174)","14523706200; 25821662400; 7006567201; 7004481174","A mobile phone system to find crosswalks for visually impaired pedestrians","2008","Technology and Disability","20","3","","217","224","7","21","10.3233/tad-2008-20304","https://scopus.proxyuao.elogim.com/inward/record.uri?eid=2-s2.0-54949091148&doi=10.3233%2ftad-2008-20304&partnerID=40&md5=41c9a5235a8d0b02bf5ab1da15756da7","The Smith-Kettlewell Eye Research Institute, San Francisco, CA 94115, 2318 Fillmore St., United States; Department of Computer Engineering, University of California, Santa Cruz, CA, United States","Shen H., The Smith-Kettlewell Eye Research Institute, San Francisco, CA 94115, 2318 Fillmore St., United States; Chan K.-Y., Department of Computer Engineering, University of California, Santa Cruz, CA, United States; Coughlan J., The Smith-Kettlewell Eye Research Institute, San Francisco, CA 94115, 2318 Fillmore St., United States; Brabyn J., The Smith-Kettlewell Eye Research Institute, San Francisco, CA 94115, 2318 Fillmore St., United States","Urban intersections are the most dangerous parts of a blind or visually impaired pedestrian's travel. A prerequisite for safely crossing an intersection is entering the crosswalk in the right direction and avoiding the danger of straying outside the crosswalk. This paper presents a proof of concept system that seeks to provide such alignment information. The system consists of a standard mobile phone with built-in camera that uses computer vision algorithms to detect any crosswalk visible in the camera's field of view; audio feedback from the phone then helps the user align him/herself to it. Our prototype implementation on a Nokia mobile phone runs in about one second per image, and is intended for eventual use in a mobile phone system that will aid blind and visually impaired pedestrians in navigating traffic intersections. © 2008 IOS Press. All rights reserved.","Blindness; Camera phone; Cell phone; Computer vision; Mobile phone; Navigation; Visual impairments","algorithm; article; assistive technology device; blindness; camera; computer system; human; mobile phone; pedestrian; traffic safety; travel; visual impairment","J. Coughlan; The Smith-Kettlewell Eye Research Institute, San Francisco, CA 94115, 2318 Fillmore St., United States; email: Coughlan@ski.org","","IOS Press","10554181","","TEDIF","","English","Technol. Disabil.","Article","Final","","Scopus","2-s2.0-54949091148"
