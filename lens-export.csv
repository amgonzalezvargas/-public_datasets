Lens ID,Title,Date Published,Publication Year,Publication Type,Source Title,ISSNs,Publisher,Source Country,Author/s,Abstract,Volume,Issue Number,Start Page,End Page,Fields of Study,Keywords,MeSH Terms,Chemicals,Funding,Source URLs,External URL,PMID,DOI,Microsoft Academic ID,PMCID,Citing Patents Count,References,Citing Works Count,Is Open Access,Open Access License,Open Access Colour
000-102-656-132-186,Construction And Verification Of Distributed Medical Database Using Resource Description Framework,2015-05-19,2015,journal article,International journal of computer assisted radiology and surgery,18616429; 18616410,Springer Science and Business Media LLC,Germany,Y. Hayakawa; M. Shinoda; M. Hirose; Y. Chiba; 吉彦 早川,"Sensorineural hearing loss is becoming one the most common reasons of disability. Worldwide 278 million people (around 25% of people above 45 years) suffer from moderate to several hearing disorders. Cochlear implantation (CI) enables to convert sound to an electrical signal that directly stimulates the auditory nerves via the electrode array surgically placed. However, this technique is intrinsically patient-dependent and its range of outcomes is very broad. A major source of outcome variability resides in the electrode array insertion. It has been reported to be one of the most important steps in cochlear implant surgery. In this context, we propose a method for patient-specific virtual electrode insertion further used into a finite element electrical simulation, and consequently improving the planning of the surgical implantation. The anatomical parameters involved in the electrode insertion such as the curvature and the number of turns of the cochlea, make virtual insertion highly challenging. Moreover, the influence of the insertion parameters and the use of different manufactured electrode arrays increase the range of scenarios to be considered for the implantation of a given patient. To this end, the method we propose is fast, easily parameterizable and applicable to a wide range of anatomies and insertion configurations. Our method is novel for targeting automatic virtual electrode insertion. Also, it combines high-resolution imaging techniques and clinical data to be further used into a finite element study and predict implantation outcomes in humans.",10,S1,300,301,RDF; Computer science; Database; Radiology; Ultrasound; Prostate surgery; Medicine; Finite element method; Biomedical engineering; Electrode; Cochlear implant; Context (language use); Cochlea; Sensorineural hearing loss; Patient specific; Audiology; Electrode array; Mesh generation; Surgical planning; Implant; Population; Monopolar stimulation; Computational model; Process (computing); Anatomy; Image registration; Inner ear; Cochlear implant surgery; Materials science; Nuclear medicine; Cone beam ct; Patient dose; Anisotropy; Reduction (complexity); Artificial intelligence; Medical physics; Software; Computer vision; Radiation therapy; Hard tissue; Segmentation; PET-CT; Workflow; Biopsy; CT guided biopsy; Software modules; Open source software; Needle biopsy; Image guidance; Internal medicine; Cardiology; Artery; Anastomosis; Suite; Interventional radiology; History; Archaeology,,,,,https://kitami-it.repo.nii.ac.jp/record/7981/files/Hayakawa_etal_PO-177_S296-297_CARS2015.pdf https://kitami-it.repo.nii.ac.jp/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=7981&item_no=1&page_id=55&block_id=91,http://dx.doi.org/10.1007/s11548-015-1213-2,25985879,10.1007/s11548-015-1213-2,2594051837; 2615461126; 2121539792; 3172853177; 3192964861; 2504444933; 2603399793; 2729948766; 2174408889; 2244999250,,0,028-396-621-844-004; 074-722-268-937-945,13,true,,green
000-122-946-719-709,Recent progress in road and lane detection: a survey,2012-02-07,2012,journal article,Machine Vision and Applications,09328092; 14321769,Springer Science and Business Media LLC,Germany,Aharon Bar Hillel; Ronen Lerner; Dan Levi; Guy Raz,,25,3,727,745,Block (data storage); Artificial intelligence; Bridge (nautical); Scheme (programming language); Data science; Implementation; Modalities; Task (project management); Field (computer science); Computer science; Simulation; Advanced driver assistance systems,,,,,http://www.scienceopen.com/document/vid/45783794-26c4-4340-b42a-91ef4496dd4e https://dx.doi.org/10.1007/s00138-011-0404-2 https://dl.acm.org/doi/10.1007/s00138-011-0404-2 https://link.springer.com/article/10.1007/s00138-011-0404-2 https://www.researchgate.net/profile/Dan_Levi/publication/257334033_Recent_progress_in_road_and_lane_detection_a_survey/links/53f443d70cf256ab87b7a725.pdf https://link.springer.com/content/pdf/10.1007/s00138-011-0404-2 https://doi.org/10.1007/s00138-011-0404-2 https://dblp.uni-trier.de/db/journals/mva/mva25.html#Bar-HillelLLR14 http://dx.doi.org/10.1007/s00138-011-0404-2,http://dx.doi.org/10.1007/s00138-011-0404-2,,10.1007/s00138-011-0404-2,2159132531,,62,003-399-951-983-687; 004-706-176-014-650; 006-749-680-518-498; 007-536-388-960-534; 008-155-008-034-625; 009-908-658-304-279; 012-092-692-833-982; 020-389-752-972-51X; 022-416-187-624-857; 022-608-873-804-686; 023-219-311-017-880; 023-438-345-162-875; 023-892-606-208-318; 024-083-403-690-34X; 028-303-905-295-179; 030-365-625-357-983; 031-684-514-693-022; 038-031-146-405-248; 038-031-942-148-668; 041-468-374-827-127; 047-864-113-831-737; 048-303-380-897-139; 049-412-336-559-22X; 049-424-713-259-301; 052-721-654-905-666; 056-542-126-286-63X; 056-878-189-897-39X; 059-317-212-119-625; 060-609-776-792-583; 061-117-920-553-532; 062-299-623-881-069; 063-745-126-644-146; 064-576-402-841-536; 066-019-853-782-097; 068-194-404-753-11X; 070-615-124-659-190; 074-202-882-241-738; 078-594-002-370-330; 080-081-462-247-198; 081-093-753-869-434; 081-141-746-741-399; 085-686-689-031-272; 086-735-729-925-33X; 087-577-313-400-228; 091-323-362-463-537; 092-892-462-429-439; 103-507-952-678-319; 110-391-025-153-054; 111-066-322-041-523; 114-174-628-565-52X; 117-580-436-643-098; 117-584-635-247-408; 122-110-803-299-124; 123-042-865-565-39X; 124-516-884-600-46X; 127-838-921-512-161; 134-387-908-662-222; 135-997-527-888-118; 144-516-071-833-157; 150-444-153-276-208; 154-654-270-756-711; 154-664-177-679-925; 159-476-720-763-197; 170-263-200-028-358; 181-029-579-747-587; 188-959-785-048-061; 190-129-777-763-021,676,false,,
000-130-488-383-514,Guidance System for Visually Impaired People,2021-03-25,2021,conference proceedings article,2021 International Conference on Artificial Intelligence and Smart Systems (ICAIS),,IEEE,,Kanchan Patil; Avinash Kharat; Pratik Chaudhary; Shrikant Bidgar; Rushikesh Gavhane,"There are some visually impaired people throughout the world. Some of them may be around us. The visually impaired person finds difficulty while performing day-to-day life tasks. So this research work aims to develop a device which helps them as personal assistant. This paper represents the proposed device’s integrated modules and functionalities that can help a blind person. The proposed idea is to provide a wearable device with a Virtual assistant system for the visually impaired person, for some of the basic tasks without requiring the help of others. The system is aimed to provide voice-over assistants for blind people to do tasks like understanding surroundings, looking for an object, recognizing the face of a person with emotion, and reading etc. There is a total of five components merged into one system in this project. The navigation through these components is possible through hardware buttons and voice-over commands given by the user. There are many deep learning methodologies and core libraries of python language used for programming. The complete project is dedicated to being simple to use by visually impaired people and making day to day tasks easy for them.",,,988,993,Human–computer interaction; Deep learning; Wearable computer; Artificial intelligence; Task analysis; Guidance system; Visually impaired; Facial recognition system; Computer science; Reading (process); Object (computer science),,,,,https://ieeexplore.ieee.org/document/9395973,http://dx.doi.org/10.1109/icais50930.2021.9395973,,10.1109/icais50930.2021.9395973,3156103225,,0,003-438-603-684-41X; 004-269-574-716-057; 011-495-244-480-650; 018-759-071-730-519; 029-286-933-718-321; 033-226-351-329-326; 039-617-434-681-815; 046-976-822-348-780; 049-317-239-158-314; 052-497-442-344-365; 076-683-406-883-046; 078-075-482-445-906; 085-110-073-770-331; 085-838-847-713-173; 092-060-815-275-796; 103-418-273-855-681; 104-294-939-489-222; 122-845-792-192-360; 147-244-887-173-609; 196-023-913-683-572,12,false,,
000-198-878-209-061,An Real Time Object Detection Method for Visually Impaired Using Machine Learning,2023-01-23,2023,conference proceedings article,2023 International Conference on Computer Communication and Informatics (ICCCI),,IEEE,,Saravanan Alagarsamy; T. Dhiliphan Rajkumar; K. P. L. Syamala; Ch. Sandya Niharika; D. Usha Rani; K. Balaji,"Vision, one of the five fundamental human senses, is crucial for defining how people perceive the objects around them. Visual impairments affect more than 200 million people worldwide, severely limiting their ability to perform numerous activities of daily living. Thus, it is essential for blind people to understand their surroundings and the objects they are interacting with. In this work, we created a tool that helps blind persons recognize diverse items in their environment by utilizing the YOLO V3 algorithm combined with R-CNN. This comprises a variety of approaches to develop an app that not only instantly recognizes different objects in the visually impaired person’s environment but also guides them using audio output. A convolutional neural network (CNN) called YOLO (You Only Look at Once) recognizes objects in real time. This suggested method is more effective and accurate than other algorithms for recognizing things, according to research results, and it produces results for object detection that are extremely similar in real time. It is crucial for persons who are blind or visually impaired to be able to reliably and effectively detect and recognize objects in order to navigate both common and unfamiliar situations safely, become stronger, and become more independent.",,,,,Computer science; Convolutional neural network; Object (grammar); Artificial intelligence; Variety (cybernetics); Object detection; Limiting; Cognitive neuroscience of visual object recognition; Human–computer interaction; Computer vision; Visually impaired; Machine learning; Pattern recognition (psychology); Engineering; Mechanical engineering,,,,,,http://dx.doi.org/10.1109/iccci56745.2023.10128388,,10.1109/iccci56745.2023.10128388,,,0,002-259-580-134-449; 003-934-283-970-929; 007-316-191-175-159; 017-015-940-032-109; 019-709-337-567-917; 020-233-013-143-936; 033-276-344-273-171; 038-719-018-767-24X; 041-815-882-503-229; 049-317-239-158-314; 050-155-083-765-308; 064-747-692-971-18X; 066-133-135-091-645; 071-284-842-172-320; 080-827-163-858-579; 081-729-617-745-550; 095-295-018-241-847; 098-926-109-358-108; 098-956-213-255-632; 101-973-438-487-531; 111-100-797-063-482; 115-040-483-924-773; 120-165-937-956-56X; 143-813-392-282-928; 180-517-300-923-149; 198-316-108-089-53X,2,false,,
000-241-912-419-005,Development of indoor localization system using a mobile data acquisition platform and BoW image matching,2016-03-28,2016,journal article,KSCE Journal of Civil Engineering,12267988; 19763808,Springer Science and Business Media LLC,Germany,Nuri Lee; Changjae Kim; Won Seok Choi; Mu-Wook Pyeon; Yongil Kim,,21,1,418,430,Codebook; Bag-of-words model; Laser scanning; Artificial intelligence; Data acquisition; Image resolution; Systems design; Computer vision; Computer science; Histogram; Coordinate system,,,,,https://www.infona.pl/resource/bwmeta1.element.springer-doi-10_1007-S12205-016-1057-5 https://link.springer.com/article/10.1007/s12205-016-1057-5 https://link.springer.com/content/pdf/10.1007%2Fs12205-016-1057-5.pdf,http://dx.doi.org/10.1007/s12205-016-1057-5,,10.1007/s12205-016-1057-5,2323891005,,0,001-281-965-177-444; 002-345-873-500-677; 007-932-530-313-421; 008-875-693-823-045; 017-374-599-277-886; 026-726-113-728-919; 029-687-449-269-736; 029-825-397-097-284; 031-624-156-392-291; 032-699-171-109-40X; 033-759-346-540-161; 039-073-115-205-018; 040-760-388-110-874; 044-751-220-350-672; 047-647-472-033-525; 049-033-489-257-054; 055-805-113-680-238; 061-464-701-910-137; 075-831-396-135-399; 079-409-455-501-38X; 081-646-650-299-077; 085-242-767-142-895; 100-379-403-442-837; 102-786-306-974-437; 103-091-397-984-221; 104-694-342-259-021; 116-368-700-319-568; 122-855-605-453-430; 126-915-683-383-48X; 159-289-575-823-523; 174-260-789-699-77X; 189-165-673-838-837; 192-623-080-180-332,9,false,,
000-361-225-089-231,Intraoperative Ultrasonography-based Augmented Reality For Application In Image Guided Robotic Surgery,2018-05-15,2018,journal article,International journal of computer assisted radiology and surgery,18616429; 18616410,Springer Science and Business Media LLC,Germany,Jun Shen; Nabil Zemiti; Agnès Viquesnel; Oscar Caravaca Mora; Auguste Courtin; Renaud Garrel; Jean-Louis Dillenseger; Philippe Poignet,"Purpose Accurate Tumor delineation during the surgery is a big challenge for surgeons. For instance, in transoral robotic surgery (TORS) for tongue base tumor resection, the preoperative images cannot accurately reﬂect the tumor area in the tongue, because of the soft tissue deformation during the surgery. Furthermore, due to the camera’s small ﬁeld of view and the loss of cross-modality landmarks in the tongue base, it is difﬁcult to register the preoperative image to the intraoperative stereo camera with deformable registration. We propose an intraoperative ultrasonography (IOUS)-based augmented reality (AR) framework which is able to accurately delimit the tumor boundaries and provide them to the surgeon’s view. Instead of some works requiring manual registration [1], additional ﬁducial markers [2], or intraoperative imaging modalities using ionizing radiation [2, 3], our solution uses safe and cheap US imaging and does not need additional ﬁducial markers disturbing the TORS workﬂow.",13,Suppl 1,1,273,Robotic surgery; Radiology; Augmented reality; Stereo camera; Transoral robotic surgery; Tongue Base; Soft tissue deformation; Tumor resection; Intraoperative ultrasonography; Computer science; Transfer of learning; Artificial intelligence; Pattern recognition; Artery; Coronary arteries; Optical coherence tomography; Context (language use); Convolutional neural network; Context (archaeology); Medicine; Feature (linguistics); Medical physics; Paleontology; Linguistics; Philosophy; Biology,,"Congresses as Topic; Germany; Humans; Radiography/methods; Radiology/methods; Societies, Medical; Surgery, Computer-Assisted/methods",,,https://hal-lirmm.ccsd.cnrs.fr/lirmm-01828409/document https://hal-lirmm.ccsd.cnrs.fr/lirmm-01828409 https://hal.archives-ouvertes.fr/lirmm-01828409,http://dx.doi.org/10.1007/s11548-018-1766-y,29766372,10.1007/s11548-018-1766-y,2796926701; 2913568676,,0,001-549-038-461-246; 020-155-961-716-457; 028-696-653-010-949,16,true,,green
000-426-623-688-517,Fuzzy-Based Approach Using IoT Devices for Smart Home to Assist Blind People for Navigation.,2020-06-30,2020,journal article,"Sensors (Basel, Switzerland)",14248220; 14243210,Multidisciplinary Digital Publishing Institute (MDPI),Switzerland,Shahzadi Tayyaba; Muhammad Waseem Ashraf; Thamer Alquthami; Zubair Ahmad; Saher Manzoor,The demand of devices for safe mobility of blind people is increasing with advancement in wireless communication. Artificial intelligent devices with multiple input and output methods are used for reliable data estimation based on maximum probability. A model of a smart home for safe and robust mobility of blind people has been proposed. Fuzzy logic has been used for simulation. Outputs from the internet of things (IoT) devices comprising sensors and bluetooth are taken as input of the fuzzy controller. Rules have been developed based on the conditions and requirements of the blind person to generate decisions as output. These outputs are communicated through IoT devices to assist the blind person or user for safe movement. The proposed system provides the user with easy navigation and obstacle avoidance.,20,13,3674,,Controller (computing); Computer science; Home automation; Bluetooth; Wireless; Fuzzy logic; Obstacle avoidance; Real-time computing; Control theory; Internet of Things,IoT; bluetooth protocol; fuzzy approach; navigation; smart home; ultrasonic sensors,Artificial Intelligence; Computer Simulation; Environment Design; Fuzzy Logic; Housing; Humans; Internet of Things; Visually Impaired Persons,,,https://dblp.uni-trier.de/db/journals/sensors/sensors20.html#TayyabaAAAM20 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7374361 https://www.mdpi.com/1424-8220/20/13/3674 https://www.mdpi.com/1424-8220/20/13/3674/pdf,http://dx.doi.org/10.3390/s20133674,32630055,10.3390/s20133674,3040400337,PMC7374361,0,012-708-812-164-051; 015-218-998-128-071; 018-399-013-530-512; 038-803-377-768-502; 038-896-052-918-520; 040-350-896-436-334; 048-239-259-879-103; 051-364-525-033-110; 058-240-677-612-752; 058-743-550-973-731; 061-034-459-511-970; 065-240-986-885-782; 067-852-848-172-530; 068-526-647-480-345; 085-102-500-348-494; 090-531-437-646-769; 091-594-871-475-717; 131-958-550-811-015; 137-349-480-768-756; 146-098-863-580-729; 151-336-535-437-546; 169-043-458-750-663; 178-847-367-499-362; 199-107-465-311-130,26,true,cc-by,gold
001-028-934-996-982,Multiobjective Navigation of a Guide Mobile Robot for the Visually Impaired Based on Intention Inference of Obstacles,,2001,journal article,Autonomous Robots,09295593; 15737527,Springer Science and Business Media LLC,Netherlands,Dong-Oh Kang; Sung-Hun Kim; Heyoung Lee; Zeungnam Bien,,10,2,213,230,Motion control; Artificial intelligence; Mobile robot; Mobile robot navigation; Obstacle; Computer vision; Computer science; Fuzzy logic; Workspace; Obstacle avoidance; Robot,,,,,https://koasas.kaist.ac.kr/handle/10203/80861 https://link.springer.com/article/10.1023/A:1008990105090 https://link.springer.com/content/pdf/10.1023/A:1008990105090.pdf https://dialnet.unirioja.es/servlet/articulo?codigo=727280 https://doi.org/10.1023/A:1008990105090 https://dl.acm.org/doi/10.1023/A%3A1008990105090 https://dblp.uni-trier.de/db/journals/arobots/arobots10.html#KangKLB01,http://dx.doi.org/10.1023/a:1008990105090,,10.1023/a:1008990105090,1565286397,,0,008-900-169-370-213; 020-814-991-009-717; 024-369-065-483-244; 025-278-629-527-832; 031-644-240-584-067; 031-768-944-008-214; 036-439-762-038-337; 037-763-341-142-604; 041-004-643-077-614; 045-321-254-881-478; 062-964-875-502-093; 064-309-126-323-022; 065-992-777-809-97X; 067-765-095-815-312; 086-869-440-167-789; 089-375-525-289-979; 102-496-748-953-099; 109-521-309-138-814; 114-019-820-503-695; 117-088-374-924-454; 121-685-044-550-680; 126-418-616-873-369; 127-129-010-898-639; 137-681-539-641-19X; 144-551-134-931-031,14,false,,
001-082-598-947-731,Assistive Navigation System for Visually Impaired and Blind People: A Review,2021-09-24,2021,conference proceedings article,2021 International Conference on Artificial Intelligence and Machine Vision (AIMV),,IEEE,,Noopur Tyagi; Deepika Sharma; Jaiteg Singh; Bhisham Sharma; Sushil Narang,"The emergence of modern technologies in healthcare systems like the Internet of Things, Wireless Sensor Network, Machine Learning, etc. has ameliorated the cognitive abilities of humans. The increased accessibility of healthcare data and the exponential growth of advanced analytics can be attributed to the innovative amalgamation of these technologies. These technologies have adaptive and self-correcting capabilities to enhance accuracy depending on the information. Assistive technology enables independence and attainment of quality of life for blind and visually impaired people. With the support of guided navigation tools, assistive technologies aid the people with the facility to move across inside as well as the outside environment. The major concern of a visually challenged and blind person is to live a life with quality and safety. This study contributes information about distinctive wearable and portable assistive tools and devices which are designed to provide support to visually impaired people. Also, it was revealed that traditional navigation devices lacked a few features that are crucial for independent navigation. To overcome those navigation deficiencies, IoT technology is exploited to provide better solutions. Global Positioning System (GPS) tracker can assist to discover several opportunities in numerous areas such as location detection, mapping, healthcare, security, etc. Navigation gadgets embedded with sensors have a huge variety of programs and benefits. The major objective of this comprehensive study is to showcase a clearer perspective about the wearable or embedded devices used by visually impaired or blind persons.",,,,,Wearable computer; Computer science; Human–computer interaction; Global Positioning System; Wearable technology; Independent living; Variety (cybernetics); Multimedia; Artificial intelligence; Embedded system; Telecommunications; Gerontology; Medicine,,,,,,http://dx.doi.org/10.1109/aimv53313.2021.9670951,,10.1109/aimv53313.2021.9670951,,,0,009-829-120-033-790; 013-056-435-015-274; 014-764-979-190-592; 023-703-322-837-643; 025-764-110-104-300; 028-094-088-190-140; 037-376-150-587-539; 045-205-438-482-211; 049-592-217-207-783; 053-230-236-592-142; 056-745-340-837-164; 065-078-747-021-140; 066-479-281-229-604; 066-526-177-885-125; 068-526-647-480-345; 080-304-413-097-782; 085-102-500-348-494; 103-920-056-075-762; 115-168-206-177-741; 162-646-338-130-900,12,false,,
001-112-325-321-05X,Consideration of uncertainty in computer vision: Necessity and chance,2008-06-11,2008,journal article,Pattern Recognition and Image Analysis,10546618; 15556212,Pleiades Publishing Ltd,United States,J. Meidow,,18,2,216,221,Machine learning; Pattern recognition (psychology); Artificial intelligence; Context (language use); Cover (telecommunications); Computer vision; Optimal estimation; Computer science; Feature extraction; Interpretation (logic); Object (computer science),,,,,https://link.springer.com/article/10.1134%2FS1054661808020053,http://dx.doi.org/10.1134/s1054661808020053,,10.1134/s1054661808020053,2032319864,,0,002-239-769-795-12X; 007-515-516-555-398; 008-372-947-045-737; 008-509-173-206-668; 009-612-733-037-965; 018-055-777-278-389; 024-425-004-562-321; 025-218-336-182-063; 028-303-905-295-179; 033-379-698-785-061; 035-261-192-517-042; 040-218-035-484-010; 042-153-325-385-799; 055-676-274-953-454; 059-167-755-749-649; 062-177-723-012-068; 063-205-388-595-56X; 065-191-672-581-163; 074-006-536-890-200; 081-656-606-217-280; 084-679-921-430-947; 086-981-442-999-431; 096-130-399-446-15X; 099-612-858-681-003; 119-071-812-538-054; 121-565-008-467-672; 121-960-198-427-865; 132-871-498-857-108; 186-730-951-823-02X; 194-109-019-383-49X,0,false,,
001-118-222-924-277,Template matching for a local guidance system,,,conference proceedings article,Proceedings of the 39th Midwest Symposium on Circuits and Systems,,IEEE,,Jamal S. Rahhal; Yu-Lin Wang; G.E. Atkin,"We describe an important component of an Interactive Assistance System designed to help the blind and visually impaired to become more independent, increasing their mobility and ability to work. A major part of the system is a Local Guidance System (LGS) that uses image processing techniques to extract the most important features in the path of the blind. The LGS provides information to the user about obstacles and general characteristics of the walking path. It uses two miniature cameras to capture the image from two different locations. A processor uses a template matching algorithm to detect the main features in the path. Then the images are processed and delivered in (text to speech) audio format. The objects are chosen and classified by a statistical study of actual street images, then the templates are formed for each main feature in the image by averaging all the images obtained for that feature.",3,,1268,1271,Speech synthesis; Artificial intelligence; Template matching; Pixel; Guidance system; Computer vision; Computer science; Feature extraction; Feature (computer vision); Path (graph theory); Image processing,,,,,http://ieeexplore.ieee.org/document/593151/,http://dx.doi.org/10.1109/mwscas.1996.593151,,10.1109/mwscas.1996.593151,2102267422,,0,092-029-784-601-900,3,false,,
001-154-350-168-291,A touring machine: Prototyping 3D mobile augmented reality systems for exploring the urban environment,,1997,journal article,Personal Technologies,09492054; 16174917,Springer Science and Business Media LLC,,Steven Feiner; Blair MacIntyre; Tobias Höllerer; Anthony Webster,"We describe a prototype system that combines the overlaid 3D graphics of augmented reality with the untethered freedom of mobile computing. The goal is to explore how these two technologies might together make possible wearable computer systems that can support users in their everyday interactions with the world. We introduce an application that presents information about our university's campus, using a head-tracked, see-through, head-worn, 3D display, and an untracked, opaque, hand-held, 2D display with stylus and trackpad. We provide an illustrated explanation of how our prototype is used, and describe our rationale behind designing its software infrastructure and selecting the hardware on which it runs.",1,4,208,217,,,,,,,http://dx.doi.org/10.1007/bf01682023,,10.1007/bf01682023,,,114,002-553-865-119-83X; 011-021-179-522-812; 013-901-391-839-508; 015-020-732-040-554; 015-693-656-426-469; 022-649-239-219-843; 026-709-227-369-841; 028-402-447-635-28X; 033-988-731-651-510; 040-111-163-897-75X; 058-468-795-176-403; 059-975-163-043-752; 062-995-863-811-173; 075-438-078-934-741; 079-885-218-191-565; 088-470-436-866-921; 094-280-406-839-64X; 112-068-204-603-148; 115-966-881-261-791; 123-611-415-442-437; 129-046-878-286-169; 147-723-514-498-230; 157-073-743-888-639,277,true,,green
001-211-139-420-373,A realtime portable and accessible aiding system for the blind – a cloud based approach,2023-01-27,2023,journal article,Multimedia Tools and Applications,13807501; 15737721,Springer Science and Business Media LLC,Netherlands,S. Venkat Ragavan; A. H. Tarun; S. Yogeeshwar; B. S. Vishwath Kumar; S. Sofana Reka,,82,13,20641,20654,Computer science; Cloud computing; Human–computer interaction; Real-time computing; Artificial intelligence; Operating system,,,,,,http://dx.doi.org/10.1007/s11042-023-14419-9,,10.1007/s11042-023-14419-9,,,0,004-019-652-142-123; 004-306-253-497-690; 007-024-005-789-989; 007-961-487-496-498; 008-254-023-648-418; 016-083-199-514-532; 019-082-228-431-934; 022-490-123-344-35X; 023-309-209-821-636; 027-038-067-308-602; 027-652-211-846-887; 028-744-192-669-778; 033-409-303-885-100; 033-715-892-488-001; 036-407-649-013-756; 037-833-381-346-602; 038-738-991-699-334; 038-858-720-835-740; 042-689-946-148-898; 043-281-975-871-986; 045-233-211-477-950; 056-838-853-715-333; 067-894-235-988-508; 069-480-623-909-181; 069-726-386-089-18X; 072-710-510-756-106; 079-102-141-538-60X; 082-007-089-941-639; 086-309-210-823-335; 088-822-320-203-25X; 091-405-827-080-958; 094-206-454-118-462; 112-913-173-544-602; 113-668-950-819-544; 115-425-559-919-864; 118-819-410-192-91X; 119-816-295-280-254; 120-914-907-494-056; 135-114-124-987-400; 151-139-899-653-714; 190-898-853-315-543,0,false,,
001-267-464-226-809,Superpixels for the Visible Human Project and human anatomical voxel model construction,2016-05-20,2016,journal article,International journal of computer assisted radiology and surgery,18616429; 18616410,Springer Science and Business Media LLC,Germany,Gobert N. Lee; Mariusz Bajger; Martin Caon; Giovanni Bibbo; Michael Ng,"et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.",11,S1,194,195,Artificial intelligence; Visible human project; Computer vision; Computer science; Voxel; Geology; Biomedical engineering; Texture (geology); Achilles tendon; Mri image; Motion (physics); Surgical procedures; Table (database); General surgery; Workflow; Medical physics; Guardian; Patient specific; Medicine; Pattern recognition; Hessian matrix; Narrow band; Segmentation; Pathology; Nuclear medicine; Morphometric analysis; Automatic segmentation; Condyle; Ontology (information science); Context (language use); Surgical assistance; Knowledge management; Exhibition; Library science; Art history; Art,,,,,https://researchnow.flinders.edu.au/en/publications/superpixels-for-the-visible-human-project-and-human-anatomical-vo,http://dx.doi.org/10.1007/s11548-016-1412-5,27206418,10.1007/s11548-016-1412-5,3103115189; 2594708524; 2899556992; 2967156615; 2512148952; 2594865033; 2792117424,,0,028-303-905-295-179; 072-016-906-192-79X; 125-183-998-630-935; 142-892-306-861-627,8,true,,green
001-374-925-660-223,Plenary lecture 4: work directions and new results in electronic travel aids for blind and visually impaired people,2010-07-22,2010,conference proceedings,,,,,Virgil Tiponut,"There are approximately 45 million blind & visually impaired people world-wide according to the World Health Report. Vision loss limits the access of these individuals to the educational opportunities, social events, public transportation and leads to a higher rate of unemployment.; ; Many efforts have been invested in the last years, based on sensor technology and signal processing, to develop electronic travel aids (ETA) capable to improve the mobility of blind users in unknown or dynamically changing environment. In spite of these efforts, the already proposed ETAs do not meet the requirements of the blind community and the traditional tools (white cane and guiding dogs) are still the only used by visually impaired to navigate in their working and living environment.; ; In this paper, research efforts to improve the main two components of an ETA tool: the Obstacles Detection System (ODS) and the Man-machine Interface (MMI) are presented. Now, for the first time, the ODS under development is bioinspired from the visual system of insects, particularly from the Lobula Giant Motion Detector (LGMD) found in locusts. LGMD is a large neuron found in optical lobule of the locust, which mainly responds at the approaching objects. Starting from the mathematical model of the LGMD, known in the literature, it has been developed an ODS that can be used by visually impaired to navigate autonomously with obstacles avoidance. The already obtained results are very promising, but some improvements are also possible. We are developing now preprocessing algorithms for the visual information applied to the input of the LGMD neuron, in order to improve the response of the ODS. In the proposed solution, the position of the detected obstacles is correlated with the attitude parameters of the subject's head. In this way, the visually impaired person detects obstacles in a similar way as a subject with normal sight is looking for obstacles in front of him.; ; The man-machine interface developed in the present research exploits the remarkable abilities of the human hearing system in identifying sound source positions in 3D space. The proposed solution relies on the Acoustic Virtual Reality (AVR) concept, which can be considered as a substitute for the lost sight of blind and visually impaired individuals. According to the AVR concept, the presence of obstacles in the surrounding environment and the path to the target will be signalized to the subject by burst of sounds, whose virtual source position suggests the position of the real obstacles and the direction of movement, respectively. The practical implementation of this method encounters some difficulties due to the Head Related Transfer Functions (HRTF) which should be known for each individual and for a limited number of points in the 3D space. These functions can be determined using a quite complex procedure, which requires many experimental measurements. The proposed solution in our research avoids these difficulties by generating the HRTF's coefficients using an Artificial Neural Network (ANN). The ANN has been trained using a public data base, available for the whole scientific community and which contains HRTF's coefficients for a limited number of individuals and a limited number of points in 3D space for each individual.; ; The ODS and the MMI presented in the above have been implemented on a specific hardware build around an ARM-based microcontroller system. The obtained results and some conclusions are also presented.",,,22,23,Interface (computing); Sight; Artificial intelligence; Virtual reality; Computer vision; Computer science; Artificial neural network,,,,,http://dl.acm.org/citation.cfm?id=1984140.1984145,http://dl.acm.org/citation.cfm?id=1984140.1984145,,,167817854,,0,,0,false,,
001-665-143-609-511,The Navigation System for the Visually Impaired Using GPS,,2009,book chapter,IFMBE Proceedings,16800737; 14339277,Springer Berlin Heidelberg,,Tomoyuki Kanno; Kenji Yanashima; Kazushige Magatani,"It is possible for the visually impaired to walk their known area independently by using a white cane. However, in their unknown area, they cannot walk without help of others even if they can use a white cane very well. In such cases, not only a visually impaired person but he/she who assists them receives many stress in dairy activity. Therefore, many types of independent walking assist system for the visually impaired are developing. Our objective of this study is the development of an auto navigation system for the visually impaired which assists independent walking of them. This system measures the position of a visually impaired person (a user) by using GPS (Global Positioning System) and navigates him/her to the destination like as a car navigation system. In our system, a location data is stored in the map database and a position of user are analyzed and optimal route for a user to the destination is calculated. Our system guides a user to the destination along this route, and notifies the user of this route and some information for the safety walking by artificial voice. Three normal subjects were tested with our navigation system. All subjects were blind folded by an eye mask and equipped a navigation system. As a result, our system worked well and all subjects were able to walk to the destination following guidance voice. So, we are developing a new navigation system which is installed on a cellular phone. In Japan, there are some cellular phones that include GPS. And most of application programs for cellular phone are written by JAVA. Therefore, GPS included in a cellular phone will be used and every functions of the navigation system will be written by JAVA. In this paper, we also reports about this new navigation system.",,,938,941,Human–computer interaction; Navigation system; Phone; Independent walking; Location data; White cane; Visually impaired; Computer science; Java; Global Positioning System,,,,,https://link.springer.com/chapter/10.1007%2F978-3-540-92841-6_232 https://rd.springer.com/chapter/10.1007/978-3-540-92841-6_232,http://dx.doi.org/10.1007/978-3-540-92841-6_232,,10.1007/978-3-540-92841-6_232,205471834,,0,051-165-693-014-38X,1,false,,
002-120-131-025-320,Wearable Guide System for Obstacle Avoidance Based on Stereo Computer Vision,,2015,conference proceedings article,Proceedings of the 2015 6th International Conference on Manufacturing Science and Engineering,,Atlantis Press,,Jiexuan Feng; Tong Wei,"wearable Abstract: ETA (Electronic Travel Aids) is an important method to guide the visual impaired to navigate, in which avoiding the obstacle effectively is the most essential content. A novel wearable ETA system and corresponding algorithms for obstacle avoidance based on binocular stereo computer vision were proposed in this paper. By utilizing RANSAC(Random Sample Consensus) algorithm, the equation of pavement plane is extracted from the images collected by the binocular cameras, and the obstacles are distinguished according to their height relative to the pavement plane, and obstacles coordinates relative to the blind in the world coordinate system are calculated and determined. This presented appliance can provide the information of walkable pavement and safe direction where there is no obstacle. The algorithms were verified by using the experimental prototype. The results show that it can detect obstacles effectively and stably with strong robustness, which provides good foundation for developing blind guide equipment further.",,,1811,1817,RANSAC; Wearable computer; Artificial intelligence; Geography; Obstacle; Binocular stereo; Computer vision; Obstacle avoidance; Coordinate system; Image processing; Robustness (computer science),,,,,https://www.atlantis-press.com/proceedings/icmse-15/25845856,http://dx.doi.org/10.2991/icmse-15.2015.329,,10.2991/icmse-15.2015.329,2316810960,,0,024-131-470-445-452; 027-278-877-933-090; 050-417-654-825-843; 059-407-478-677-340; 099-061-925-709-208; 170-387-846-090-147; 192-638-195-032-497,0,true,cc-by-nc,gold
002-123-448-882-287,Development of a wearable guide device based on convolutional neural network for blind or visually impaired persons,2020-08-11,2020,journal article,Multimedia Tools and Applications,13807501; 15737721,Springer Science and Business Media LLC,Netherlands,Yi-Zeng Hsieh; Shih-Syun Lin; Fu-Xiong Xu,,79,39,29473,29491,Deep learning; Wearable computer; Artificial intelligence; Visually Impaired Persons; Computer vision; Computer science; Artificial neural network; Feature (computer vision); Convolutional neural network; RGB color model,,,,"Ministry of Science and Technology, Taiwan; Ministry of Science and Technology, Taiwan; Ministry of Science and Technology, Taiwan; Ministry of Science and Technology, Taiwan",https://link.springer.com/article/10.1007/s11042-020-09464-7 https://doi.org/10.1007/s11042-020-09464-7 http://dblp.uni-trier.de/db/journals/mta/mta79.html#HsiehLX20 https://dblp.uni-trier.de/db/journals/mta/mta79.html#HsiehLX20,http://dx.doi.org/10.1007/s11042-020-09464-7,,10.1007/s11042-020-09464-7,3048053741,,0,001-751-550-748-114; 004-362-007-540-346; 008-783-981-093-093; 012-170-976-975-801; 013-037-085-272-233; 013-998-581-285-650; 017-255-815-189-927; 018-521-217-611-054; 018-843-508-574-144; 019-678-159-217-797; 020-233-013-143-936; 020-932-767-809-91X; 022-766-933-331-387; 027-904-757-841-234; 028-019-684-441-516; 029-195-931-387-115; 030-540-246-658-722; 035-152-016-615-193; 037-425-985-423-004; 041-314-165-476-904; 041-565-305-258-017; 047-147-456-191-311; 047-391-381-180-646; 048-149-094-961-264; 050-838-499-112-880; 053-567-501-093-090; 056-905-403-377-274; 061-294-318-497-272; 063-753-562-544-942; 065-383-697-266-916; 066-586-656-822-732; 066-785-118-231-304; 067-860-693-051-223; 068-605-974-602-536; 072-015-966-174-329; 072-546-768-857-084; 083-857-356-361-530; 084-358-547-990-751; 087-159-059-782-512; 088-570-511-100-511; 089-879-850-830-924; 094-812-084-023-094; 094-871-870-293-980; 099-633-821-597-96X; 125-968-647-741-078; 126-823-184-394-653; 151-551-319-524-065; 167-790-127-535-746,24,false,,
002-195-025-800-326,Real-Time Mobile Application for Assisting Blind People Using YoloV2 and Google cloud vision API,2020-08-01,2020,journal article,Journal of emerging technologies and innovative research,23495162,,,Chitra Nambirajan Thevar,"Visually impaired people can’t move safely outdoors because they cannot perceive the outside obstacles as normal people. The prototype application in this study aims to make the visually impaired people's lives easier with the mobile devices. The mobile application with the designed to see object nearby and read any text documents.The application is developed for the Android platform. Image processing and machine learning technologies are used.; ; Blind assistance is promoting a widely challenge in computer vision such as navigation and path finding. In this paper, two modules are designed text recognition and object detection are employed to provide the necessary information about the surrounding environment. Objects detection is used to find objects in the real world from an image of the world such as faces, bicycles, chairs, doors, or tables that are common in the scenes of a blind. Object detection is used to detect any obstacle at a medium to long distance. YOLOV2 machine learning algorithm is used to perform the object recognition and Google Vision Cloud API is used to perform Text Recognition. The proposed method for the blind aims at expanding possibilities to people with vision loss to achieve their full potential. The experimental results reveal the performance of the proposed work in about real time system.",7,8,2139-2144,2139-2144,Human–computer interaction; Mobile device; Object detection; Obstacle; Computer science; Android (operating system); Object (computer science); Cognitive neuroscience of visual object recognition; Cloud computing; Image processing,,,,,https://www.jetir.org/papers/JETIR2008284.pdf https://www.jetir.org/view?paper=JETIR2008284,https://www.jetir.org/view?paper=JETIR2008284,,,3080158302,,0,,0,false,,
002-230-222-181-051,Look AI – An Intelligent System for Socialization of Visually Impaired,2023-05-26,2023,conference proceedings article,2023 6th International Conference on Artificial Intelligence and Big Data (ICAIBD),,IEEE,,G. L. M. M. Mendis; W. M. Y Deshan; H. M. G. M. Bandara; K. C. Gunethilake; Dinuka Wijendra; Jenny Krishara,"It has found that 30% of the people among the visually impaired people are having a significant number of depressive symptoms whereas, the prevalence of depression in blind people was reported to be 33%, or nearly double the rate of the general population. Therefore, social activities among visually impaired people should increase. This research paper provides a unique system for assisting visually impaired individuals in navigating indoor environments heavily relies on the use of artificial intelligence, particularly deep learning and computer vision techniques. By leveraging these advanced technologies, the system can analyze and process large amounts of visual data in real-time, accurately identifying and locating various objects in the environment, including faces, doors, stairs, cross walks, traffic lights, potholes and other obstacles. The system then provides real-time feedback to the user via an audio interface, enabling them to navigate indoor and outdoor environments safely and independently. The use of artificial intelligence in this system is particularly significant, as it enables the system to adapt and improve over time based on user feedback and additional data. That means, the system can continuously improve its performance and accuracy, ultimately resulting in a more effective and reliable tool for visually impaired individuals. The authors have also evaluated the system in a real-world setting, demonstrating its effectiveness in assisting visually impaired individuals with indoor navigation. Overall, the proposed system has the potential to significantly enhance the quality of life for visually impaired individuals, showcasing the transformative power of artificial intelligence in addressing real-world challenges.",,,,,Computer science; Human–computer interaction; Visually impaired; Population; Deep learning; Artificial intelligence; Process (computing); Multimedia; Demography; Sociology; Operating system,,,,,,http://dx.doi.org/10.1109/icaibd57115.2023.10206191,,10.1109/icaibd57115.2023.10206191,,,0,020-505-230-186-621; 021-214-599-337-684; 037-199-326-802-024; 047-749-445-596-16X; 067-852-718-385-888; 072-862-266-440-195; 112-901-991-045-76X; 114-872-024-036-852; 132-106-346-661-546; 149-088-018-987-783; 151-039-447-100-109,1,false,,
002-341-994-164-78X,IROS - Semi-autonomous outdoor mobility support system for elderly and disabled people,,,conference proceedings article,Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453),,IEEE,,K. Kayama; I.E. Yairi; S. Igi,"We have been developing the Robotic Communication Terminals (RCTs), which are integrated into a mobility support system to assist elderly or disabled people who suffer from impaired mobility. The RCT system consists of three types of terminals and one server: an environment-embedded terminal, a user-carried mobile terminal, a user-carrying mobile terminal, and a barrier-free map server. The RCT is an integrated system that can be used to cope with various problems of mobility, and provide suitable support to a wide variety of users. This paper provides an in-depth description of the user-carrying mobile terminal. The system itself is a kind of intelligent wheeled vehicle. It can recognize the surrounding 3D environment through infrared sensors, sonar sensors, and a stereo vision system with three cameras, and avoid hazards semi-autonomously. It also can provide adequate navigation by communicating with the geographic information system (GIS) server and detect vehicles appearing from the blind side by communicating with environment-embedded terminals in the real-world.",3,,2606,2611,Human–computer interaction; Knowledge-based systems; Engineering; Variety (cybernetics); Embedded system; Mobile robot; Terminal (electronics); Geographic information system; Sonar; Stereopsis; User interface,,,,,https://dblp.uni-trier.de/db/conf/iros/iros2003.html#KayamaYI03 https://ieeexplore.ieee.org/document/1249263/,http://dx.doi.org/10.1109/iros.2003.1249263,,10.1109/iros.2003.1249263,2155744739,,1,037-361-153-445-93X; 039-820-940-384-083; 049-887-358-717-806; 055-224-290-217-847; 057-532-297-824-591; 090-770-657-164-911; 117-512-698-497-567; 147-623-138-454-147; 150-881-028-273-601,11,false,,
002-373-202-845-779,Design of a Multi-Functional Module for Visually Impaired Persons,2018-11-13,2018,journal article,International Journal of Precision Engineering and Manufacturing,22347593; 20054602,Springer Science and Business Media LLC,,Dong-Soo Choi; Tae-Heon Yang; Won-Chul Bang; Sang-Youn Kim,,19,11,1745,1751,Haptic technology; Artificial intelligence; Actuator; Visually Impaired Persons; Functional module; Visually impaired; Computer vision; Focus (computing); Computer science; Object (computer science); Ultrasonic sensor,,,,,https://link.springer.com/article/10.1007/s12541-018-0202-0,http://dx.doi.org/10.1007/s12541-018-0202-0,,10.1007/s12541-018-0202-0,2810394173,,0,001-076-828-128-846; 003-039-180-452-511; 011-429-831-000-820; 013-056-435-015-274; 016-281-236-532-869; 017-369-467-982-683; 018-170-211-594-394; 030-258-385-598-396; 030-651-513-452-166; 031-223-487-502-873; 032-338-804-930-895; 034-936-259-439-98X; 042-170-086-843-08X; 046-098-711-060-761; 052-698-035-156-042; 063-138-395-684-264; 066-526-177-885-125; 080-282-808-165-175; 080-798-205-793-401; 085-539-061-673-050; 096-999-411-622-779; 098-418-447-204-485; 103-787-976-293-420; 122-806-643-658-436; 176-619-480-167-936,2,false,,
002-445-086-865-974,An Intelligent Auto-Organizing Aerial Robotic Sensor Network System for Urban Surveillance,2021-05-17,2021,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Wai Lun Leong; Niki Martinel; Sunan Huang; Christian Micheloni; Gian Luca Foresti; Rodney Teo,,102,2,1,22,User requirements document; Debugging; Wireless sensor network; Control (management); Cover (telecommunications); Software architecture; Computer science; Analytics; Real-time computing; Incremental build model,,,,,https://link.springer.com/article/10.1007/s10846-021-01398-y https://dblp.uni-trier.de/db/journals/jirs/jirs102.html#LeongMHMFT21,http://dx.doi.org/10.1007/s10846-021-01398-y,,10.1007/s10846-021-01398-y,3160495714,,0,002-454-037-173-378; 003-769-498-954-68X; 004-285-030-392-242; 005-156-087-929-456; 006-769-921-122-418; 008-404-759-914-732; 009-836-279-581-778; 010-434-190-926-761; 012-231-098-593-063; 020-447-369-101-239; 028-829-459-366-630; 031-218-334-653-826; 032-207-933-062-457; 035-095-998-272-338; 035-162-063-903-226; 035-277-967-833-996; 038-544-089-669-143; 039-102-688-139-097; 039-226-848-674-369; 039-475-701-149-049; 040-190-471-309-428; 040-629-090-704-721; 042-178-862-663-783; 042-251-157-585-319; 045-396-638-423-835; 049-317-239-158-314; 056-456-489-206-482; 058-500-245-692-304; 058-597-140-062-887; 064-128-897-801-413; 064-205-547-606-964; 064-281-098-500-949; 065-016-983-183-164; 066-133-135-091-645; 066-579-919-900-056; 073-376-574-462-030; 074-012-524-755-387; 078-402-050-201-217; 085-623-785-655-079; 087-560-666-151-369; 089-613-551-988-23X; 090-476-748-500-610; 091-186-625-928-853; 092-638-398-386-433; 092-639-739-308-316; 097-619-038-637-612; 097-848-954-817-685; 098-926-109-358-108; 104-567-156-366-024; 107-008-776-706-462; 109-806-014-448-128; 109-870-248-168-360; 115-127-296-491-628; 120-147-196-684-982; 120-475-199-385-278; 124-194-834-553-352; 127-359-705-890-709; 135-100-216-646-331; 138-330-647-078-455; 139-848-107-992-804; 140-921-274-168-71X; 156-720-291-599-562; 165-174-670-633-143; 178-316-428-977-768; 197-261-077-665-511; 197-773-523-217-938,11,false,,
002-697-240-386-463,Artificial Intelligence on Visually Impaired People: A Comprehensive Review,2024-03-11,2024,conference proceedings article,2024 5th International Conference on Intelligent Communication Technologies and Virtual Mobile Networks (ICICV),,IEEE,,Shreya Chaple; Vedanti Raut; Jagdish Chandra Patni; Ayush Banode; Samiksha Ninawe; Nilesh Shelke,"Artificial Intelligence has become a significant tool in modern technology, enabling people to interact with machines through various methods. Individuals with visual impairments have trouble doing tasks because they are either blind or have poor vision. BVI stands for Blind and Visually Impaired. Solutions must also advance with technology to ensure that individuals can effectively navigate their surroundings and assist them in real-time navigation. The study conducts surveys on visually challenged individuals in the community and aims to assist them by providing smart gadgets to identify faces, colours, and objects. Moreover, this study emphasizes more on different technologies and methods that are used earlier to help visually impaired people in their day-to-day life.",,,,,Visually impaired; Computer science; Assistive technology; Impaired Vision; Human–computer interaction; Low vision; Visual impairment; Artificial intelligence; Multimedia; Psychology; Optometry; Medicine; Psychiatry,,,,,,http://dx.doi.org/10.1109/icicv62344.2024.00052,,10.1109/icicv62344.2024.00052,,,0,002-123-448-882-287; 008-003-844-059-876; 008-361-673-204-978; 009-298-692-762-606; 023-309-209-821-636; 029-098-339-143-539; 031-500-753-826-436; 055-858-990-745-91X; 064-140-744-806-266; 071-586-026-267-763; 084-536-270-844-012; 119-553-924-738-845,0,false,,
002-758-512-171-658,A Survey on Load Transportation Using Multirotor UAVs,2019-10-14,2019,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Daniel K. D. Villa; Alexandre Santos Brandao; Mario Sarcinelli-Filho,,98,2,267,296,Engineering management; Artificial intelligence; Work (electrical); Key (cryptography); Control (management); Multirotor; Experimental validation; Graduate students; Robotics; Computer science; Reading (process),,,,Conselho Nacional de Desenvolvimento Científico e Tecnológico; Fundação de Amparo à Pesquisa do Estado de Minas Gerais,https://link.springer.com/article/10.1007/s10846-019-01088-w https://dblp.uni-trier.de/db/journals/jirs/jirs98.html#VillaBF20 https://dialnet.unirioja.es/servlet/articulo?codigo=7570065 https://doi.org/10.1007/s10846-019-01088-w,http://dx.doi.org/10.1007/s10846-019-01088-w,,10.1007/s10846-019-01088-w,2979356185,,1,000-005-372-098-778; 002-335-684-004-042; 003-471-816-359-690; 005-346-307-151-402; 006-772-010-760-803; 007-292-377-312-787; 008-002-018-324-936; 008-164-881-241-965; 009-703-991-840-746; 009-826-883-697-057; 009-881-173-768-14X; 011-971-046-223-025; 012-161-428-981-456; 016-651-846-023-979; 017-818-648-456-181; 019-594-650-520-499; 020-142-740-005-982; 023-487-300-424-342; 024-155-579-071-18X; 024-803-646-186-685; 026-684-536-513-319; 028-378-468-993-564; 028-903-939-353-719; 031-215-073-605-725; 032-176-365-406-370; 034-094-117-598-207; 035-184-594-120-746; 035-652-850-405-607; 036-447-327-297-609; 036-797-067-855-781; 037-047-206-493-156; 039-023-545-115-172; 040-745-021-774-812; 041-495-333-043-979; 042-557-199-943-577; 043-942-451-242-287; 044-888-916-154-298; 045-430-353-344-398; 046-251-738-659-633; 052-235-122-216-508; 053-509-445-526-30X; 057-776-766-700-809; 057-817-374-375-849; 058-324-507-095-738; 061-595-357-966-966; 062-336-182-826-362; 062-535-355-548-623; 063-357-946-177-710; 064-813-495-182-491; 066-684-779-158-48X; 066-761-431-562-218; 071-537-862-454-387; 081-435-464-306-683; 081-510-263-955-940; 082-592-434-578-237; 082-816-146-405-905; 082-918-129-490-121; 084-485-571-712-403; 084-654-058-781-955; 085-444-802-361-619; 085-691-590-473-640; 085-983-835-009-489; 086-541-669-783-302; 089-702-446-947-916; 089-771-456-571-739; 090-492-660-847-134; 091-757-120-151-328; 091-997-206-401-909; 094-002-954-136-590; 095-121-188-667-920; 095-231-711-346-919; 095-975-427-577-084; 097-918-264-546-532; 098-652-654-835-875; 100-774-048-664-493; 104-744-328-757-967; 107-166-729-909-551; 107-637-676-228-83X; 107-937-223-349-975; 108-144-198-279-704; 109-512-542-347-589; 109-559-560-462-546; 111-841-765-121-711; 112-605-667-435-778; 115-516-682-047-284; 117-296-656-090-319; 120-051-345-889-679; 121-513-656-662-770; 125-297-842-802-783; 127-395-779-488-614; 129-663-991-049-329; 130-042-395-815-06X; 135-983-490-375-003; 138-617-654-735-351; 143-604-360-712-70X; 154-168-611-881-227; 155-627-821-238-981; 156-267-028-120-937; 156-661-976-744-85X; 173-440-682-299-049; 178-443-988-565-704; 179-421-454-526-11X; 180-161-005-827-758; 196-128-312-167-864,156,false,,
002-796-730-932-475,An Algorithm Based on Neuro-Fuzzy Controller Implemented in A Smart Clothing System For Obstacle Avoidance,,2013,journal article,International Journal of Computational Intelligence Systems,18756883; 18756891,Springer Science and Business Media LLC,United Kingdom,Senem Kursun Bahadir; Sébastien Thomassey; Vladan Koncar; Fatma Kalaoglu,"Abstract In this study, to overcome navigation concerns of visually impaired people, an algorithm based on neuro-fuzzy controller composed of multi-layer fuzzy inference systems (FIS) for obstacle avoidance was developed and it was implemented in a smart clothing system. The success of the proposed algorithm was tested in real environment and it was compared with one layer FIS. Results showed that the proposed algorithm is capable of guiding user to a right orientation and it presented better results than the one layer FIS.",6,3,503,517,Algorithm; Layer (object-oriented design); Fuzzy inference; Neuro fuzzy controller; Visually impaired; Computer science; Clothing; Obstacle avoidance; Control theory; Orientation (computer vision),,,,,https://www.atlantis-press.com/journals/ijcis/25868400 https://dblp.uni-trier.de/db/journals/ijcisys/ijcisys6.html#BahadirTKK13 https://core.ac.uk/display/153652981,http://dx.doi.org/10.1080/18756891.2013.781336,,10.1080/18756891.2013.781336,2020850715,,0,001-795-015-376-210; 003-133-164-079-658; 004-423-967-957-962; 006-564-678-737-164; 012-276-220-240-800; 013-630-164-616-687; 014-454-377-327-797; 016-134-303-747-465; 017-294-932-617-662; 025-261-165-786-877; 027-219-069-437-582; 029-083-861-464-210; 031-490-832-460-229; 034-483-947-937-086; 036-754-539-313-535; 039-509-844-234-796; 041-011-585-047-964; 044-611-975-624-921; 052-711-698-723-275; 058-430-751-202-194; 061-074-947-491-186; 061-341-893-440-60X; 061-418-771-118-367; 063-283-272-890-149; 065-530-055-223-145; 072-464-841-933-030; 073-722-974-726-137; 084-295-211-596-034; 093-437-769-726-181; 093-501-382-278-680; 101-112-986-722-85X; 101-979-351-618-823; 108-238-633-305-258; 110-484-992-565-284; 123-717-916-373-682; 129-410-703-731-118; 163-058-400-694-471,3,true,cc-by,gold
002-869-217-250-38X,Blind Navigation Support System using Raspberry Pi & YOLO,2023-05-04,2023,conference proceedings article,2023 2nd International Conference on Applied Artificial Intelligence and Computing (ICAAIC),,IEEE,,R Parvadhavardhni.; Pankhuri Santoshi; A. Mary Posonia,"Visually impaired encounter several challenges in their daily lives to impact their independence, safety, and overall quality of life. Visual impairment can be caused by a range of conditions, such as age-related macular degeneration, glaucoma, cataracts, or genetic disorders. A blind navigation system with object detection is designed to assist visually impaired individuals in navigating their environment safely and independently. This system uses a combination of TensorFlow (YOLO), OpenCV, Noir camera, ultrasonic sensor, and Raspberry Pi to achieve real-time object detection and provide audio feedback to the user about the type of detected objects. The use of TensorFlow (YOLO), OpenCV, Noir Camera, Ultrasonic sensors, and Raspberry Pi, in particular, has made it possible to develop a highly effective and accurate system for visually impaired individuals by providing real-time feedback about the user's environment, this system can help improve the user's confidence and independence while navigating through their environment, and can greatly improve their quality of life.",,,,,Computer science; Computer vision; Raspberry pi; Artificial intelligence; Macular degeneration; Object detection; Visually impaired; Object (grammar); Independence (probability theory); Human–computer interaction; Embedded system; Medicine; Pattern recognition (psychology); Statistics; Mathematics; Ophthalmology; Internet of Things,,,,,,http://dx.doi.org/10.1109/icaaic56838.2023.10140484,,10.1109/icaaic56838.2023.10140484,,,0,008-361-673-204-978; 026-345-407-862-479; 033-994-078-542-995; 040-220-230-984-080; 041-902-102-240-495; 045-875-857-925-198; 049-317-239-158-314; 081-908-529-651-291; 117-341-795-638-719,3,false,,
002-929-683-447-78X,Work directions and new results in electronic travel aids for blind and visually impaired people,2010-10-01,2010,journal article,WSEAS TRANSACTIONS on SYSTEMS archive,,,,Virgil Tiponut; Daniel Ianchis; Zoltan Haraszy; Mihai-Emanuel Basch; Ivan Bogdanov,"Many efforts have been invested in the last years, based on sensor technology and signal processing, to develop electronic travel aids (ETA) capable to improve the mobility of blind users in unknown or dynamically changing environment. In spite of these efforts, the already proposed ETAs do not meet the requirements of the blind community and the traditional tools (white cane and guiding dogs) are still the only used by visually impaired. In this paper, research efforts to improve the main two components of an ETA tool: the Obstacles Detection System (ODS) and the Man-machine Interface (MMI) are presented. Now, for the first time, the ODS under development is bioinspired from the visual system of insects, particularly from the locust and from the fly. Some original results of the author's team, related to the new concept of Acoustical Virtual Reality (AVR) used as a MMI is then discussed in more detail. Some conclusions and the further developments in this area are also presented.",9,10,1086,1097,Human–computer interaction; Interface (computing); Engineering; Artificial intelligence; Human–machine interface; Work (electrical); Virtual reality; Visually Impaired Persons; White cane; Visually impaired,,,,,http://www.wseas.us/e-library/transactions/systems/2010/88-416.pdf,http://www.wseas.us/e-library/transactions/systems/2010/88-416.pdf,,,2220455911,,0,011-458-682-081-403; 019-306-790-422-912; 019-431-359-682-592; 031-523-666-942-844; 032-342-378-386-840; 041-484-446-914-300; 042-186-837-332-894; 045-531-424-234-149; 048-770-790-068-483; 063-957-925-086-43X; 066-114-366-503-885; 066-469-450-548-325; 071-405-244-808-059; 073-495-407-090-665; 074-671-802-599-660; 077-113-362-690-85X; 084-533-882-547-840; 094-108-145-447-415; 099-646-885-015-028; 111-536-450-707-100,5,false,,
003-095-665-980-116,ViT Cane: Visual Assistant for the Visually Impaired.,2021-09-26,2021,preprint,arXiv: Computer Vision and Pattern Recognition,,,,Bhavesh Kumar,"Blind and visually challenged face multiple issues with navigating the world independently. Some of these challenges include finding the shortest path to a destination and detecting obstacles from a distance. To tackle this issue, this paper proposes ViT Cane, which leverages a vision transformer model in order to detect obstacles in real-time. Our entire system consists of a Pi Camera Module v2, Raspberry Pi 4B with 8GB Ram and 4 motors. Based on tactile input using the 4 motors, the obstacle detection model is highly efficient in helping visually impaired navigate unknown terrain and is designed to be easily reproduced. The paper discusses the utility of a Visual Transformer model in comparison to other CNN based models for this specific application. Through rigorous testing, the proposed obstacle detection model has achieved higher performance on the Common Object in Context (COCO) data set than its CNN counterpart. Comprehensive field tests were conducted to verify the effectiveness of our system for holistic indoor understanding and obstacle avoidance.",,,,,Artificial intelligence; Terrain; Face (geometry); Shortest path problem; Obstacle; Context (language use); Transformer (machine learning model); Computer vision; Computer science; Obstacle avoidance; Camera module,,,,,http://arxiv.org/pdf/2109.13857.pdf https://arxiv.org/abs/2109.13857 https://dblp.uni-trier.de/db/journals/corr/corr2109.html#abs-2109-13857,https://arxiv.org/abs/2109.13857,,,3203047301,,0,043-196-700-803-427; 051-263-461-368-647; 052-118-817-856-285; 103-518-013-304-638; 142-968-657-337-219,0,true,,unknown
003-342-986-507-202,User-centered design of a depth data based obstacle detection and avoidance system for the visually impaired,2018-05-04,2018,journal article,Human-centric Computing and Information Sciences,21921962,Springer Science and Business Media LLC,United States,Rabia Jafri; Marwa Mahmoud Khan,"The development of a novel depth-data based real-time obstacle detection and avoidance application for visually impaired (VI) individuals to assist them in navigating independently in indoors environments is presented in this paper. The application utilizes a mainstream, computationally efficient mobile device as the development platform in order to create a solution which not only is aesthetically appealing, cost-effective, lightweight and portable but also provides real-time performance and freedom from network connectivity constraints. To alleviate usability problems, a user-centered design approach has been adopted wherein semi-structured interviews with VI individuals in the local context were conducted to understand their micro-navigation practices, challenges and needs. The invaluable insights gained from these interviews have not only informed the design of our system but would also benefit other researchers developing similar applications. The resulting system design along with a detailed description of its obstacle detection and unique multimodal feedback generation modules has been provided. We plan to iteratively develop and test the initial prototype of the system with the end users to resolve any usability issues and better adapt it to their needs.",8,1,1,30,Human–computer interaction; User-centered design; Usability; Mobile device; Obstacle; Context (language use); Systems design; Computer science; Obstacle avoidance; End user,,,,,https://dblp.uni-trier.de/db/journals/hcis/hcis8.html#JafriK18 https://link.springer.com/article/10.1186/s13673-018-0134-9 https://dl.acm.org/doi/10.1186/s13673-018-0134-9 https://hcis-journal.springeropen.com/articles/10.1186/s13673-018-0134-9 https://link.springer.com/content/pdf/10.1186/s13673-018-0134-9.pdf https://rd.springer.com/article/10.1186/s13673-018-0134-9 https://doi.org/10.1186/s13673-018-0134-9,http://dx.doi.org/10.1186/s13673-018-0134-9,,10.1186/s13673-018-0134-9,2805626681,,1,000-501-534-285-334; 001-461-750-787-745; 010-719-927-108-990; 012-081-521-655-479; 013-678-295-106-102; 017-571-875-650-919; 019-375-434-287-899; 019-925-883-027-48X; 020-511-701-478-016; 022-489-327-390-755; 024-624-229-589-340; 028-628-140-541-774; 028-683-545-086-724; 030-440-500-148-114; 031-175-485-676-104; 031-551-875-561-966; 036-552-223-342-289; 043-536-853-133-602; 044-134-998-307-372; 044-775-532-488-650; 045-205-438-482-211; 045-599-200-378-144; 052-554-976-425-07X; 057-492-693-315-218; 063-367-028-428-831; 063-372-733-367-946; 063-849-833-130-044; 063-898-780-585-045; 069-640-368-596-762; 071-670-768-414-672; 073-829-360-441-444; 073-923-489-011-948; 073-928-284-197-323; 074-025-785-307-622; 075-903-419-661-27X; 077-507-367-411-516; 083-870-431-489-961; 085-196-579-365-317; 087-408-536-412-018; 092-762-008-214-491; 096-733-124-330-978; 103-830-248-892-565; 105-278-566-106-933; 105-790-942-783-392; 106-769-662-475-969; 110-650-958-929-874; 118-719-054-138-205; 120-499-391-106-298; 121-439-134-242-08X; 129-798-203-326-961; 133-300-802-059-627; 134-486-618-445-40X; 136-589-096-644-720; 139-233-304-780-009; 171-819-741-311-815; 198-875-521-506-588,22,true,cc-by,hybrid
003-447-160-542-977,Deep Learning and Particle Swarm Optimisation-Based Techniques for Visually Impaired Humans' Text Recognition and Identification,2021-10-29,2021,journal article,Augmented Human Research,23654317; 23654325,Springer Science and Business Media LLC,,Binay Kumar Pandey; Digvijay Pandey; Subodh Wariya; Gaurav Aggarwal; Rahul Rastogi,"Blind people can benefit greatly from a system capable of localising and reading comprehension text embedded in natural scenes and providing useful information that boosts their self-esteem and autonomy in everyday situations. Regardless of the fact that existing optical character recognition programmes seem to be quick and effective, the majority of them are not able to correctly recognise text embedded in usual panorama images. The methodology described in this paper is to localise textual image regions and pre-process them using the naive Bayesian algorithm. A weighted reading technique is used to generate the correct text data from the complicated image regions. Usually, images hold some disturbance as a result of the fact that filtration is proposed during the early pre-processing step. To restore the image's quality, the input image is processed employing gradient and contrast image methods. Following that, the contrast of the source images would be enhanced using an adaptive image map. The stroke width transform, Gabor’s transform, and weighted naive Bayesian classifier methodologies have been used in complicated degraded images to segment, feature extraction, and detect textual and non-textual elements. Finally, to identify categorised textual data, the confluence of deep neural networks and particle swarm optimisation is being used. The text in the image is transformed into an acoustic output after identification. The dataset IIIT5K is used for the development portion, and the performance of the suggested come up is evaluated using parameters such as accuracy, recall, precision, and F1-score.",6,1,1,14,Deep learning; Image map; Computational intelligence; Artificial intelligence; Pattern recognition; Computer science; Naive Bayes classifier; Feature extraction; Optical character recognition; Identification (information); Particle swarm optimization,,,,,https://europepmc.org/article/PMC/PMC8553597 https://link.springer.com/article/10.1007%2Fs41133-021-00051-5,http://dx.doi.org/10.1007/s41133-021-00051-5,,10.1007/s41133-021-00051-5,3209185090,,0,000-021-089-118-497; 001-150-448-049-305; 002-765-683-071-024; 003-272-559-387-161; 004-797-524-101-176; 006-934-807-940-405; 013-107-381-709-939; 016-334-111-178-33X; 016-686-614-790-829; 017-314-090-466-057; 019-143-403-851-343; 020-629-382-232-53X; 021-174-228-967-560; 021-693-474-257-323; 022-873-261-023-691; 023-060-159-179-700; 023-151-712-486-110; 023-784-570-329-813; 025-539-484-164-26X; 026-270-111-115-884; 026-287-947-734-205; 028-573-523-748-566; 029-805-447-153-187; 034-339-950-267-451; 035-703-798-718-127; 036-555-363-525-873; 040-425-904-139-30X; 041-438-314-834-395; 041-827-747-682-429; 043-959-935-844-437; 044-039-973-602-417; 049-892-685-028-772; 050-898-852-794-398; 056-921-916-495-524; 059-560-432-280-98X; 059-994-491-562-58X; 063-849-669-955-81X; 064-657-401-679-973; 069-196-784-599-665; 072-438-632-945-917; 073-489-566-423-374; 073-951-247-604-642; 074-229-273-706-27X; 074-383-378-886-153; 076-786-873-576-426; 077-744-609-077-055; 083-268-168-773-380; 084-386-802-204-315; 086-539-734-790-11X; 088-771-804-086-769; 104-571-048-811-284; 109-911-921-483-167; 112-651-838-599-249; 135-470-811-312-609; 138-858-787-247-607; 147-150-128-112-541; 157-778-773-552-781; 176-274-394-625-59X; 193-622-749-903-427,29,true,implied-oa,bronze
003-554-777-067-727,Towards a neurally-inspired computer architecture,,2003,journal article,Natural Computing,15677818,Springer Science and Business Media LLC,Netherlands,Michael A. Arbib,,2,1,1,46,Computational neuroscience; Artificial intelligence; Theory of computation; Perceptual robotics; Computer technology; Models of neural computation; Computer science; Artificial neural network; Function (engineering); Adaptation (computer science),,,,,https://link.springer.com/content/pdf/10.1023/A:1023390900317.pdf https://link.springer.com/article/10.1023/A:1023390900317 https://dblp.uni-trier.de/db/journals/nc/nc2.html#Arbib03,http://dx.doi.org/10.1023/a:1023390900317,,10.1023/a:1023390900317,83322102,,0,000-043-857-305-541; 000-364-492-255-39X; 000-693-757-615-176; 002-389-673-919-996; 002-623-587-486-883; 005-037-831-719-288; 005-461-097-545-123; 007-690-359-819-836; 009-636-205-008-235; 011-275-964-057-215; 014-576-160-079-513; 016-458-227-218-684; 020-258-505-706-687; 022-648-915-777-29X; 023-138-527-276-799; 024-751-654-697-790; 031-674-287-545-53X; 031-909-553-637-650; 033-820-137-055-688; 037-445-452-111-532; 038-703-666-621-197; 038-994-796-845-853; 039-294-768-456-518; 039-986-093-583-427; 042-550-659-218-973; 044-502-457-821-447; 045-210-201-864-98X; 047-266-807-527-561; 049-776-796-208-858; 050-148-203-023-46X; 050-460-819-286-424; 050-470-781-720-174; 057-332-889-690-703; 058-892-886-431-927; 060-418-349-323-553; 060-761-794-856-305; 064-082-933-282-50X; 064-500-009-321-226; 065-331-728-142-488; 066-658-629-951-547; 066-668-838-978-972; 066-713-972-700-71X; 070-248-172-490-885; 074-867-171-092-971; 076-311-837-847-496; 082-744-294-072-647; 084-085-469-206-202; 087-117-503-259-088; 090-692-404-942-438; 093-632-035-865-602; 099-847-574-650-607; 108-901-707-963-842; 108-916-843-470-446; 113-505-255-600-800; 117-285-800-338-566; 119-321-490-917-375; 120-939-919-583-081; 122-103-385-197-372; 125-854-415-830-236; 126-718-646-740-06X; 128-734-000-285-523; 128-880-185-006-775; 139-205-316-229-021; 139-676-115-178-37X; 142-924-883-554-540; 163-734-341-132-583; 165-481-111-558-538; 166-736-465-884-523; 189-964-429-526-197; 193-318-744-762-614,15,false,,
003-561-233-382-310,Trans4Trans: Efficient Transformer for Transparent Object Segmentation to Help Visually Impaired People Navigate in the Real World,,2021,conference proceedings article,2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW),,IEEE,,Jiaming Zhang; Kailun Yang; Angela Constantinescu; Kunyu Peng; Karin Müller; Rainer Stiefelhagen,"Common fully glazed facades and transparent objects present architectural barriers and impede the mobility of people with low vision or blindness, for instance, a path detected behind a glass door is inaccessible unless it is correctly perceived and reacted. However, segmenting these safety-critical objects is rarely covered by conventional assistive technologies. To tackle this issue, we construct a wearable system with a novel dual-head Transformer for Transparency (Trans4Trans) model, which is capable of segmenting general and transparent objects and performing real-time wayfinding to assist people walking alone more safely. Especially, both decoders created by our proposed Transformer Parsing Module (TPM) enable effective joint learning from different datasets. Besides, the efficient Trans4Trans model composed of symmetric transformer-based encoder and decoder, requires little computational expenses and is readily deployed on portable GPUs. Our Trans4Trans model outperforms state-of-the-art methods on the test sets of Stanford2D3D and Trans10K-v2 datasets and obtains mIoU of 45.13% and 75.14%, respectively. Through various pre-tests and a user study conducted in indoor and outdoor scenarios, the usability and reliability of our assistive system have been extensively verified.",,,1760,1770,Human–computer interaction; Encoder; Wearable computer; Artificial intelligence; Reliability (computer networking); Usability; Transformer (machine learning model); Computer science; Object (computer science); Segmentation; Transparency (human–computer interaction),,,,,https://arxiv.org/abs/2107.03172 https://openaccess.thecvf.com/content/ICCV2021W/ACVR/html/Zhang_Trans4Trans_Efficient_Transformer_for_Transparent_Object_Segmentation_To_Help_Visually_ICCVW_2021_paper.html https://arxiv.org/pdf/2107.03172 https://dblp.uni-trier.de/db/journals/corr/corr2107.html#abs-2107-03172 https://openaccess.thecvf.com/content/ICCV2021W/ACVR/papers/Zhang_Trans4Trans_Efficient_Transformer_for_Transparent_Object_Segmentation_To_Help_Visually_ICCVW_2021_paper.pdf,http://dx.doi.org/10.1109/iccvw54120.2021.00202,,10.1109/iccvw54120.2021.00202,3217258974,,1,002-123-448-882-287; 008-361-673-204-978; 008-972-948-454-945; 009-604-493-900-415; 012-100-716-148-822; 013-662-330-010-928; 015-773-733-169-647; 016-628-086-107-33X; 016-769-111-907-953; 020-233-013-143-936; 021-176-270-241-156; 021-360-232-245-594; 026-209-853-248-181; 026-292-486-698-315; 028-671-432-434-245; 030-540-246-658-722; 031-294-750-698-550; 034-893-292-526-13X; 036-842-985-307-311; 037-550-015-414-716; 039-933-184-572-951; 041-314-165-476-904; 043-814-030-738-536; 045-599-200-378-144; 046-309-239-761-288; 050-261-233-769-785; 051-263-461-368-647; 051-674-567-787-301; 052-046-438-827-353; 052-118-817-856-285; 055-495-282-581-758; 062-960-343-963-966; 065-374-293-809-860; 068-246-254-541-665; 068-657-742-648-601; 070-204-788-245-541; 071-391-947-989-576; 078-370-310-843-504; 084-065-630-749-740; 090-373-070-341-406; 091-317-478-968-667; 095-403-198-199-994; 104-208-613-793-423; 114-655-190-795-896; 129-674-023-352-404; 149-488-483-135-976; 152-145-055-495-637; 168-302-024-970-818; 170-680-544-359-887,60,true,,green
003-828-556-159-797,ICCE-TW - Smart guiding glasses with descriptive video service and spoken dialogue system for visually impaired,2020-09-28,2020,conference proceedings article,2020 IEEE International Conference on Consumer Electronics - Taiwan (ICCE-Taiwan),,IEEE,,Ching-Han Chen; Ming-Fang Shiu,"This paper proposed a wearable equipment for walking navigation to help the visually impaired people. It is a pair of smart glasses combining 3D vision and microphone/speaker, which can help the visually impaired to be convenient in outdoor. We used 3D object detection and recognition to handle visual understanding. A dialogue system is developed for human/machine communication. Finally, we completed the SW/HW integration and verification. This result invited blind people to test in a real environment and continue to improve its practicality and convenience.",,,9258014,,Human–computer interaction; Image segmentation; Wearable computer; Object detection; Microphone; Test (assessment); Visually impaired; Service (systems architecture); Computer science,,,,,https://scholars.ncu.edu.tw/en/publications/smart-guiding-glasses-with-descriptive-video-service-and-spoken-d https://doi.org/10.1109/ICCE-Taiwan49838.2020.9258014,http://dx.doi.org/10.1109/icce-taiwan49838.2020.9258014,,10.1109/icce-taiwan49838.2020.9258014,3110090241,,0,002-649-617-859-886; 006-757-908-435-833; 007-243-786-853-452; 008-022-315-050-765; 008-501-008-079-339; 016-162-261-436-025; 020-162-374-925-540; 026-333-865-330-463; 041-314-165-476-904; 046-413-262-477-736; 103-756-813-883-66X; 112-154-051-246-370; 127-936-963-662-204; 165-770-407-696-973; 181-005-281-103-23X; 192-697-687-929-866,5,false,,
004-019-652-142-123,Real Time Multi Object Detection for Blind Using Single Shot Multibox Detector,2019-03-30,2019,journal article,Wireless Personal Communications,09296212; 1572834x,Springer Science and Business Media LLC,Netherlands,Adwitiya Arora; Atul Grover; Raksha Chugh; S. Sofana Reka,,107,1,651,661,Image segmentation; Speech synthesis; Artificial intelligence; Object detection; Visual impairment; Population; Computer vision; Computer science; Artificial neural network; Detector,,,,,https://dblp.uni-trier.de/db/journals/wpc/wpc107.html#AroraGCR19 https://dl.acm.org/doi/abs/10.1007/s11277-019-06294-1 https://doi.org/10.1007/s11277-019-06294-1 https://link.springer.com/article/10.1007/s11277-019-06294-1,http://dx.doi.org/10.1007/s11277-019-06294-1,,10.1007/s11277-019-06294-1,2927635115,,0,001-761-218-710-594; 004-865-492-170-047; 006-500-640-442-683; 008-558-848-807-295; 010-864-608-924-297; 013-056-435-015-274; 016-078-981-428-347; 024-564-544-309-001; 027-691-714-369-535; 031-218-334-653-826; 031-223-487-502-873; 033-657-380-175-877; 037-814-010-140-919; 039-858-859-289-592; 041-181-257-812-593; 044-425-314-936-558; 049-317-239-158-314; 063-367-485-286-808; 073-928-284-197-323; 085-902-615-371-307; 086-097-067-425-016; 090-884-510-770-832; 098-926-109-358-108; 100-171-062-123-631; 129-689-411-638-908; 130-552-169-471-000; 131-503-960-053-375; 143-715-638-632-694; 150-303-609-521-445; 162-837-825-948-39X,48,false,,
004-071-983-923-934,Smart Glass System Using Deep Learning for the Blind and Visually Impaired,2021-11-11,2021,journal article,Electronics,20799292,MDPI AG,,Mukhriddin Mukhiddinov; Jinsoo Cho,"Individuals suffering from visual impairments and blindness encounter difficulties in moving independently and overcoming various problems in their routine lives. As a solution, artificial intelligence and computer vision approaches facilitate blind and visually impaired (BVI) people in fulfilling their primary activities without much dependency on other people. Smart glasses are a potential assistive technology for BVI people to aid in individual travel and provide social comfort and safety. However, practically, the BVI are unable move alone, particularly in dark scenes and at night. In this study we propose a smart glass system for BVI people, employing computer vision techniques and deep learning models, audio feedback, and tactile graphics to facilitate independent movement in a night-time environment. The system is divided into four models: a low-light image enhancement model, an object recognition and audio feedback model, a salient object detection model, and a text-to-speech and tactile graphics generation model. Thus, this system was developed to assist in the following manner: (1) enhancing the contrast of images under low-light conditions employing a two-branch exposure-fusion network; (2) guiding users with audio feedback using a transformer encoder–decoder object detection model that can recognize 133 categories of sound, such as people, animals, cars, etc., and (3) accessing visual information using salient object extraction, text recognition, and refreshable tactile display. We evaluated the performance of the system and achieved competitive performance on the challenging Low-Light and ExDark datasets.",10,22,2756,,Human–computer interaction; Deep learning; Artificial intelligence; Graphics; Object detection; Contrast (vision); Smart glass; Transformer (machine learning model); Audio feedback; Computer science; Cognitive neuroscience of visual object recognition,,,,National Research Foundation of Korea,https://www.mdpi.com/2079-9292/10/22/2756/html https://www.mdpi.com/2079-9292/10/22/2756/pdf,http://dx.doi.org/10.3390/electronics10222756,,10.3390/electronics10222756,3214498996,,0,000-065-347-587-89X; 000-394-661-764-185; 000-714-992-864-471; 002-991-693-624-222; 005-682-125-066-509; 006-583-465-461-540; 007-573-197-769-65X; 007-777-266-701-195; 008-558-848-807-295; 009-803-986-992-487; 009-864-381-071-748; 011-624-374-578-064; 011-870-106-178-455; 012-370-328-340-986; 014-078-268-465-000; 015-040-823-132-436; 015-058-340-255-429; 015-691-248-937-375; 017-499-516-520-553; 018-775-168-927-916; 020-260-934-897-437; 020-302-288-518-801; 021-312-325-457-67X; 024-295-930-710-511; 025-442-974-267-13X; 026-005-792-695-329; 030-451-478-449-511; 032-661-597-680-28X; 033-117-407-459-080; 033-444-431-573-172; 035-047-062-450-233; 035-277-967-833-996; 035-901-214-148-704; 036-105-726-377-235; 039-269-931-001-501; 041-058-139-703-974; 044-927-941-960-397; 045-608-632-117-863; 047-186-279-206-675; 048-509-571-989-646; 048-581-143-834-273; 049-516-070-604-623; 049-519-975-039-482; 052-764-834-376-124; 053-219-839-728-849; 054-246-406-994-711; 055-495-282-581-758; 057-027-544-245-355; 057-952-634-830-177; 060-903-554-310-080; 069-480-623-909-181; 072-710-510-756-106; 076-823-621-907-858; 079-476-601-954-783; 081-849-165-179-450; 082-353-064-620-654; 083-671-887-012-721; 085-560-687-499-839; 085-564-340-397-815; 086-841-963-712-198; 088-680-189-402-753; 088-986-265-620-149; 093-361-210-208-887; 094-206-454-118-462; 094-364-626-703-646; 095-523-740-665-876; 098-926-109-358-108; 099-334-901-486-872; 104-208-613-793-423; 107-476-876-480-20X; 115-427-285-754-757; 120-321-123-351-715; 121-277-606-552-440; 124-796-642-799-946; 129-047-277-550-176; 129-620-371-177-493; 135-583-248-281-524; 142-454-747-085-715; 147-516-729-009-367; 158-651-279-838-117; 171-129-200-331-129,49,true,cc-by,gold
004-124-001-976-903,Evaluating the efficacy of UNav: A computer vision-based navigation aid for persons with blindness or low vision.,2024-08-13,2024,journal article,Assistive technology : the official journal of RESNA,19493614; 10400435,Taylor and Francis Ltd.,United Kingdom,Anbang Yang; Nattachart Tamkittikhun; Giles Hamilton-Fletcher; Vinay Ramdhanie; Thu Vu; Mahya Beheshti; Todd Hudson; Wachara Riewpaiboon; Pattanasak Mongkolwat; Chen Feng; John-Ross Rizzo,"UNav is a computer-vision-based localization and navigation aid that provides step-by-step route instructions to reach selected destinations without any infrastructure in both indoor and outdoor environments. Despite the initial literature highlighting UNav's potential, clinical efficacy has not yet been rigorously evaluated. Herein, we assess UNav against standard in-person travel directions (SIPTD) for persons with blindness or low vision (PBLV) in an ecologically valid environment using a non-inferiority design. Twenty BLV subjects (age = 38 ± 8.4; nine females) were recruited and asked to navigate to a variety of destinations, over short-range distances (<200 m), in unfamiliar spaces, using either UNav or SIPTD. Navigation performance was assessed with nine dependent variables to assess travel confidence, as well as spatial and temporal performances, including path efficiency, total time, and wrong turns. The results suggest that UNav is not only non-inferior to the standard-of-care in wayfinding (SIPTD) but also superior on 8 out of 9 metrics, as compared to SIPTD. This study highlights the range of benefits computer vision-based aids provide to PBLV in short-range navigation and provides key insights into how users benefit from this systematic form of computer-aided guidance, demonstrating transformative promise for educational attainment, gainful employment, and recreational participation.",,,1,15,,UNav; assistive technology; indoor navigation; outdoor navigation; visually impaired,,,NEI NIH HHS (R33 EY033689) United States,,http://dx.doi.org/10.1080/10400435.2024.2382113,39137956,10.1080/10400435.2024.2382113,,,0,003-204-682-225-868; 003-245-050-569-137; 004-895-779-508-444; 007-183-320-134-024; 007-652-375-981-035; 008-847-763-917-947; 008-882-474-638-988; 011-414-630-627-246; 012-219-678-476-099; 012-260-191-822-476; 013-589-881-594-538; 014-503-420-035-326; 016-156-913-844-198; 016-415-106-921-058; 019-385-187-941-855; 024-335-765-549-544; 026-244-307-943-578; 026-391-982-060-912; 029-959-075-285-198; 030-130-800-632-377; 030-440-500-148-114; 032-815-287-990-724; 034-426-109-112-770; 036-879-014-788-692; 038-351-334-715-107; 038-529-429-962-974; 045-505-911-987-050; 045-669-898-745-284; 046-672-610-405-71X; 049-592-217-207-783; 051-943-091-696-561; 056-609-203-368-078; 057-847-555-860-541; 061-658-210-595-895; 061-866-460-804-908; 062-264-089-396-88X; 063-691-951-015-349; 064-090-537-619-350; 065-989-043-835-282; 067-781-477-125-907; 073-582-515-181-308; 073-736-156-085-427; 075-984-142-197-167; 079-433-787-480-087; 080-349-668-563-369; 087-017-559-994-34X; 087-290-999-125-583; 100-441-217-539-453; 102-117-007-401-147; 110-146-529-835-535; 118-432-281-415-657; 125-143-915-401-031; 141-417-261-005-384; 143-858-441-690-063; 160-624-112-591-798; 167-867-829-659-13X; 175-293-960-991-250; 183-947-709-586-992; 198-397-830-588-085,0,false,,
004-160-881-991-72X,SMART MACHINE LEARNING SYSTEM FOR BLIND ASSISTANCE,2023-04-02,2023,journal article,International Research Journal of Modernization in Engineering Technology and Science,25825208,International Research Journal of Modernization in Engineering Technology and Science,,B Gnana Chand; Naimur Rahman; Gaurav Rajak; Suman Debnath; Azad Ali; Navneeth Kaur,"A blind assistance system is a piece of technology that helps the visually impaired navigate their environment.The system uses a variety of sensors and algorithms to recognize and understand the surroundings, and it then conveys this knowledge to the user via tactile or aural input.Features like obstacle detection, interior navigation, and face recognition might be part of the system.Blind assistance systems are designed to increase the freedom and security of visually impaired people in their everyday lives, allowing them to move around their surroundings more confidently and easily.This paper surveys the state of art techniques for blind assistance systems.",,,,,Computer science; Artificial intelligence; Machine learning; Human–computer interaction,,,,,,http://dx.doi.org/10.56726/irjmets35265,,10.56726/irjmets35265,,,0,,0,true,,bronze
004-162-991-909-925,ICC - A vision-based system to detect potholes and uneven surfaces for assisting blind people,,2016,conference proceedings article,2016 IEEE International Conference on Communications (ICC),,IEEE,,Aravinda S. Rao; Jayavardhana Gubbi; Marimuthu Palaniswami; Elaine Wong,"Vision is one of the most advanced and important sensory input in humans. However, many people have vision problems due to birth defects, uncorrected errors, work nature, accidents, and aging. The white cane and guide dog are the most widely used means of navigation for the vision-impaired. With advancements in technology, electronic devices have been created using different sensors and technologies to help navigate the blind. Electronic Travel Aids (ETAs) assist in navigating a person by collecting information about the environment and relaying this information in a form that allows a blind or vision-impaired person to understand the nature of the environment. However, there is still a lack of devices to detect potholes and uneven pavements, which inhibits mobility after dark. This pilot study proposes a computer vision based pothole and uneven surface detection approach to assist blind people in meeting their mobility needs. The system includes projecting laser patterns, recording the patterns through a monocular video, analyzing the patterns to extract features and then providing path cues for the blind user. With over 90% accuracy in detecting potholes, the proposed system aims to assist blind people in real-time navigation.",,,1,6,Artificial intelligence; Vision based; White cane; Computer vision; Computer science; Sonar,,,,,https://doi.org/10.1109/ICC.2016.7510832 http://dx.doi.org/10.1109/ICC.2016.7510832 https://ieeexplore.ieee.org/document/7510832/ https://dblp.uni-trier.de/db/conf/icc/icc2016.html#RaoGPW16 https://dx.doi.org/10.1109/ICC.2016.7510832 https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7510832,http://dx.doi.org/10.1109/icc.2016.7510832,,10.1109/icc.2016.7510832,2505733433,,1,000-424-243-390-605; 001-076-828-128-846; 009-142-401-900-87X; 013-056-435-015-274; 013-551-333-215-556; 013-847-555-426-159; 030-127-369-356-741; 034-350-308-512-672; 036-280-949-293-766; 036-742-969-097-379; 054-041-970-960-409; 057-879-237-577-755; 058-903-603-934-059; 060-257-784-475-928; 064-309-126-323-022; 066-526-177-885-125; 094-671-071-786-280; 098-811-589-762-176; 100-150-172-276-556; 112-121-560-424-373; 127-483-973-270-088; 189-029-312-730-111,22,false,,
004-330-362-156-385,EMBC - Audible vision for the blind and visually impaired in indoor open spaces,,2012,journal article,Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference,26940604; 23757477,,United States,Xunyi Yu; Aura Ganz,"In this paper we introduce Audible Vision, a system that can help blind and visually impaired users navigate in large indoor open spaces. The system uses computer vision to estimate the location and orientation of the user, and enables the user to perceive his/her relative position to a landmark through 3D audio. Testing shows that Audible Vision can work reliably in real-life ever-changing environment crowded with people.",2012,,5110,5113,Engineering; Artificial intelligence; Landmark; Blindness; Visually Impaired Persons; Sensory Aid; Visually impaired; Computer vision; User interface; Orientation (computer vision),,"Acoustic Stimulation/instrumentation; Artificial Intelligence; Biofeedback, Psychology/instrumentation; Blindness/rehabilitation; Equipment Design; Equipment Failure Analysis; Humans; Imaging, Three-Dimensional/instrumentation; Sensory Aids; Therapy, Computer-Assisted/instrumentation; Visually Impaired Persons/rehabilitation",,NEI NIH HHS (1 R21EY018231-01A1) United States,http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.ieee-000006347143 http://europepmc.org/abstract/MED/23367078 https://www.ncbi.nlm.nih.gov/pubmed/23367078 https://dblp.uni-trier.de/db/conf/embc/embc2012.html#YuG12,http://dx.doi.org/10.1109/embc.2012.6347143,23367078,10.1109/embc.2012.6347143,2089786298,,0,024-254-051-411-020; 028-303-905-295-179; 029-687-449-269-736; 029-850-495-079-869; 030-440-500-148-114; 056-294-604-111-077; 065-819-195-728-230; 068-446-370-600-681; 082-559-093-627-752; 089-738-656-495-377; 090-884-510-770-832; 121-197-907-645-173; 156-096-618-588-00X,5,false,,
004-398-909-518-572,Interactive mixed reality white cane simulation for the training of the blind and the visually impaired,2007-04-25,2007,journal article,Personal and Ubiquitous Computing,16174909; 16174917,Springer Science and Business Media LLC,Germany,Dimitrios Tzovaras; Konstantinos Moustakas; Georgios Nikolakis; Michael G. Strintzis,,13,1,51,58,Computer-mediated reality; Haptic technology; Augmented reality; Artificial intelligence; Virtual reality; Collision detection; Mixed reality; Training system; Auditory feedback; Computer vision; Computer science,,,,,https://dl.acm.org/doi/10.1007/s00779-007-0171-2 https://doi.org/10.1007/s00779-007-0171-2 https://dblp.uni-trier.de/db/journals/puc/puc13.html#TzovarasMNS09 https://link.springer.com/article/10.1007/s00779-007-0171-2/fulltext.html https://rd.springer.com/article/10.1007/s00779-007-0171-2 https://link.springer.com/content/pdf/10.1007%2Fs00779-007-0171-2.pdf https://link.springer.com/article/10.1007%2Fs00779-007-0171-2,http://dx.doi.org/10.1007/s00779-007-0171-2,,10.1007/s00779-007-0171-2,2138303294,,2,014-142-725-849-119; 016-144-009-446-951; 021-101-109-439-397; 024-855-215-002-295; 049-502-642-789-225; 051-777-507-359-745; 080-091-760-037-884; 083-863-500-179-698; 097-322-289-851-026; 101-247-388-233-659; 115-940-056-507-071; 120-171-959-125-135; 143-073-654-482-540; 188-939-797-162-541,28,false,,
004-415-926-098-612,Fast Component-Based QR Code Detection in Arbitrarily Acquired Images,2012-06-28,2012,journal article,Journal of Mathematical Imaging and Vision,09249907; 15737683,Springer Science and Business Media LLC,Netherlands,Luiz Belussi; Nina S. T. Hirata,,45,3,277,292,Haar-like features; Frame (networking); Artificial intelligence; Code (cryptography); Set (abstract data type); Barcode; Alphanumeric; Speech recognition; Computer vision; Cascading classifiers; Computer science; Robot,,,,,http://dx.doi.org/10.1007/s10851-012-0355-x https://dl.acm.org/citation.cfm?id=2436177 https://www.microsoft.com/en-us/research/publication/fast-component-based-qr-code-detection-in-arbitrarily-acquired-images/ https://dblp.uni-trier.de/db/journals/jmiv/jmiv45.html#BelussiH13 https://rd.springer.com/article/10.1007/s10851-012-0355-x https://dl.acm.org/doi/10.1007/s10851-012-0355-x https://doi.org/10.1007/s10851-012-0355-x https://link.springer.com/article/10.1007/s10851-012-0355-x/fulltext.html https://link.springer.com/article/10.1007/s10851-012-0355-x,http://dx.doi.org/10.1007/s10851-012-0355-x,,10.1007/s10851-012-0355-x,2096074125,,0,006-152-646-401-219; 007-316-191-175-159; 011-303-316-169-492; 012-016-836-079-639; 013-096-867-472-301; 013-165-817-928-725; 015-425-422-664-770; 016-834-308-590-249; 019-274-086-790-355; 034-377-320-403-84X; 037-232-759-751-412; 037-283-001-813-35X; 037-535-562-418-577; 040-927-395-073-380; 044-957-086-143-78X; 054-449-609-851-745; 058-041-313-038-33X; 064-336-342-050-771; 068-858-525-190-391; 071-340-632-029-028; 077-590-099-677-464; 083-830-628-647-402; 089-098-407-824-325; 125-215-400-077-068; 151-489-947-969-554; 152-529-213-821-18X; 155-378-919-619-972; 156-872-843-255-497; 159-903-694-495-777; 174-617-446-947-179,31,false,,
004-559-796-546-992,CRV - Self-Localization at Street Intersections,,2014,journal article,Proceeding of the ... Conference on Computer and Robot Vision. Conference on Computer and Robot Vision,,,United States,Giovanni Fusco; Huiying Shen; James M. Coughlan,"There is growing interest among smartphone users in the ability to determine their precise location in their environment for a variety of applications related to way finding, travel and shopping. While GPS provides valuable self-localization estimates, its accuracy is limited to approximately 10 meters in most urban locations. This paper focuses on the self-localization needs of blind or visually impaired travelers, who are faced with the challenge of negotiating street intersections. These travelers need more precise self-localization to help them align themselves properly to crosswalks, signal lights and other features such as walk light pushbuttons. We demonstrate a novel computer vision-based localization approach that is tailored to the street intersection domain. Unlike most work on computer vision-based localization techniques, which typically assume the presence of detailed, high-quality 3D models of urban environments, our technique harnesses the availability of simple, ubiquitous satellite imagery (e.g., Google Maps) to create simple maps of each intersection. Not only does this technique scale naturally to the great majority of street intersections in urban areas, but it has the added advantage of incorporating the specific metric information that blind or visually impaired travelers need, namely, the locations of intersection features such as crosswalks. Key to our approach is the integration of IMU (inertial measurement unit) information with geometric information obtained from image panorama stitchings. Finally, we evaluate the localization performance of our algorithm on a dataset of intersection panoramas, demonstrating the feasibility of our approach.",,,40,47,Artificial intelligence; Intersection; Key (cryptography); Metric (mathematics); Panorama; Image stitching; Computer vision; Computer science; Scale (map); Global Positioning System; Inertial measurement unit,IMU (inertial measurement unit); assistive technology; blindness and low vision; image stitching; mobile vision; self-localization,,,NEI NIH HHS (R01 EY018345) United States,https://dblp.uni-trier.de/db/conf/crv/crv2014.html#FuscoSC14 https://europepmc.org/articles/PMC4157747 http://dx.doi.org/10.1109/CRV.2014.14 http://ieeexplore.ieee.org/document/6816822/ https://ieeexplore.ieee.org/document/6816822/ http://doi.ieeecomputersociety.org/10.1109/CRV.2014.14 https://dx.doi.org/10.1109/CRV.2014.14 https://www.ncbi.nlm.nih.gov/pubmed/25210926,http://dx.doi.org/10.1109/crv.2014.14,25210926,10.1109/crv.2014.14,2129233453,PMC4157747,2,024-236-322-414-073; 027-780-924-643-322; 033-161-824-018-647; 040-445-706-184-474; 051-425-281-185-199; 052-300-735-572-263; 067-210-074-115-887; 068-430-792-728-072; 075-402-037-032-739; 123-863-288-894-65X; 125-892-214-262-380,6,true,,green
004-754-524-837-052,Designing Motion Gesture Interfaces in Mobile Phones for Blind People,2014-09-12,2014,journal article,Journal of Computer Science and Technology,10009000; 18604749,Springer Science and Business Media LLC,China,Nem Khan Dim; Xiangshi Ren,,29,5,812,824,Motion (physics); Interface (computing); Gesture recognition; Set (psychology); Usability; Voice command device; Gesture; Phone; Computer science; Multimedia,,,,,https://dblp.uni-trier.de/db/journals/jcst/jcst29.html#DimR14 https://link.springer.com/article/10.1007%2Fs11390-014-1470-5,http://dx.doi.org/10.1007/s11390-014-1470-5,,10.1007/s11390-014-1470-5,2017628824,,1,000-082-081-025-981; 004-811-275-324-317; 018-520-871-669-328; 022-220-313-695-548; 024-307-953-321-143; 029-262-364-136-164; 029-464-443-770-396; 032-517-859-134-478; 039-157-409-797-926; 044-470-686-989-071; 045-973-141-514-739; 046-371-721-786-878; 047-794-617-772-549; 053-708-391-472-829; 062-187-480-453-633; 067-988-723-618-546; 073-645-364-913-027; 076-455-738-292-176; 088-983-693-754-422; 091-850-163-565-025; 093-925-865-515-918; 094-766-879-707-073; 101-801-662-674-904; 123-372-868-843-086; 127-131-032-743-85X; 137-693-305-428-642; 142-575-622-508-48X; 145-328-406-474-247; 164-053-589-667-509; 172-962-395-556-024; 182-592-972-628-225; 189-841-524-584-846,30,false,,
004-796-220-360-369,OUTDOOR/INDOOR VISION-BASED LOCALIZATION FOR BLIND PEDESTRIAN NAVIGATION ASSISTANCE,,2010,journal article,International Journal of Image and Graphics,02194678; 17936756,World Scientific Pub Co Pte Lt,,Sylvie Treuillet; Eric Royer,"The most challenging issue facing the navigation assistive systems for the visually impaired is the instantaneous and accurate spatial localization of the user. Most of the previously proposed systems are based on global positioning system (GPS) sensors. However, the accuracy of low-cost versions is insufficient for pedestrian use. Furthermore, GPS-based systems are confined to outdoor navigation and experience severe signal losts in urban areas. This paper presents a new approach for localizing a person by using a single-body-mounted camera and computer vision techniques. Instantaneous accurate localization and heading estimates of the person are computed from images as the user progresses along a memorized path. A portable prototype has been tested for outdoor as well as indoor pedestrian use. Experimental results demonstrate the effectiveness of the vision-based localization: the accuracy is sufficient for making it possible to guide and maintain the blind person within a navigation corridor less than 1 m wide along the intended path. In combination with a suitable guiding interface, such a localization system will be convenient to assist the visually impaired in their everyday movements outdoors as well as indoors.",10,04,481,496,Interface (computing); Artificial intelligence; Mobile robot navigation; Heading (navigation); Pedestrian; PATH (variable); Vision based; Visually impaired; Computer vision; Computer science; Global Positioning System,,,,,https://www.worldscientific.com/doi/abs/10.1142/S0219467810003937 https://dblp.uni-trier.de/db/journals/ijig/ijig10.html#TreuilletR10 https://hal.archives-ouvertes.fr/hal-00648507 https://hal.archives-ouvertes.fr/hal-00648507/document,http://dx.doi.org/10.1142/s0219467810003937,,10.1142/s0219467810003937,2061509101,,23,007-391-459-034-77X; 011-922-536-348-671; 021-311-500-600-230; 028-303-905-295-179; 028-873-953-403-845; 034-013-696-326-654; 034-443-036-134-243; 035-291-375-623-97X; 047-752-801-880-421; 051-766-223-654-722; 056-785-601-003-691; 061-190-872-413-287; 072-321-573-561-059; 074-969-968-020-614; 084-679-921-430-947; 089-738-656-495-377; 092-207-666-926-877; 102-551-479-494-136; 102-633-298-647-083; 121-153-008-338-562; 127-006-931-445-95X; 130-124-223-427-094; 141-489-442-115-69X; 151-015-279-636-660; 159-050-405-963-799; 162-266-085-847-377; 169-421-672-075-385; 171-967-210-532-103; 193-123-800-849-774,30,true,,green
004-921-512-974-709,Artificial intelligence applied to breast pathology.,2021-11-18,2021,journal article,Virchows Archiv : an international journal of pathology,14322307; 09456317,Springer Science and Business Media LLC,Germany,Mustafa Yousif; Paul J. van Diest; Arvydas Laurinavicius; David L. Rimm; Jeroen van der Laak; Anant Madabhushi; Stuart J. Schnitt; Liron Pantanowitz,,480,1,1,19,Deep learning; Artificial intelligence; Digital pathology; Breast pathology; Breast carcinoma; Field (computer science); Computer science; Artificial neural network; Breast cancer; Convolutional neural network,Artificial intelligence; Breast; Breast cancer; Computational pathology; Convolutional neural network; Deep learning; Handcrafted features; Machine learning; Quantitative image analysis,"Algorithms; Artificial Intelligence; Breast; Breast Neoplasms/diagnosis; Female; Humans; Machine Learning; Neural Networks, Computer",,,https://pubmed.ncbi.nlm.nih.gov/34791536/ https://link.springer.com/article/10.1007/s00428-021-03213-3,http://dx.doi.org/10.1007/s00428-021-03213-3,34791536,10.1007/s00428-021-03213-3,3213866750,,2,000-135-144-716-285; 000-292-891-633-669; 001-326-241-749-415; 002-079-297-146-278; 002-788-198-131-62X; 003-284-351-410-893; 003-673-864-720-191; 003-713-515-328-136; 003-917-698-861-252; 005-581-516-764-58X; 005-674-665-208-280; 006-266-566-444-126; 009-007-517-010-471; 009-622-074-427-837; 009-672-728-202-545; 009-743-932-420-203; 009-994-579-428-197; 014-299-776-857-392; 015-132-651-292-983; 016-325-303-690-114; 016-462-131-207-245; 016-804-899-171-232; 018-772-664-665-085; 019-022-333-722-958; 020-150-590-237-517; 024-078-323-042-75X; 024-331-798-181-530; 025-286-484-398-223; 026-147-234-909-330; 026-680-777-351-797; 026-829-499-704-793; 028-526-033-511-951; 028-695-248-528-336; 031-125-382-869-856; 031-228-894-236-659; 031-661-372-138-207; 032-042-693-996-45X; 033-000-426-874-723; 035-079-726-186-188; 035-488-724-832-833; 036-855-810-401-181; 036-919-318-013-638; 037-502-025-122-307; 037-619-770-645-73X; 037-684-061-069-62X; 039-631-239-102-655; 043-213-034-345-306; 044-501-887-945-269; 045-448-673-090-564; 046-815-015-563-495; 048-890-246-138-405; 051-150-469-882-21X; 054-111-247-780-311; 054-159-918-455-599; 057-199-851-775-680; 058-899-088-589-262; 059-149-073-001-124; 062-359-435-963-747; 063-930-761-946-041; 067-344-326-620-34X; 067-426-474-297-213; 070-675-849-603-893; 073-015-524-987-492; 073-339-985-481-751; 073-745-215-667-174; 074-420-235-499-164; 075-248-794-599-561; 076-399-045-801-288; 076-540-149-034-845; 077-024-289-166-802; 081-116-884-209-283; 082-011-942-887-185; 082-182-221-794-815; 082-819-804-422-04X; 083-190-968-569-006; 086-004-753-136-673; 087-653-549-086-476; 088-109-073-036-365; 088-899-468-533-832; 095-689-656-085-393; 098-015-221-895-66X; 104-970-451-818-783; 107-030-597-734-557; 107-491-486-094-691; 107-834-984-588-290; 120-949-818-757-934; 121-661-550-270-618; 121-707-574-802-675; 123-327-094-675-134; 124-637-522-963-803; 125-246-374-034-671; 139-522-517-152-197; 144-328-791-024-122; 147-443-043-208-914,38,false,,
005-135-800-876-298,A Transfer Learning Approach for Indoor Object Identification,2021-08-23,2021,journal article,SN Computer Science,2662995x; 26618907,Springer Science and Business Media LLC,,Mouna Afif; Riadh Ayachi; Yahia Said; Mohamed Atri,,2,6,1,9,Machine learning; Transfer of learning; Artificial intelligence; Landmark; Variation (game tree); Identification system; Computer science; Object (computer science); Cognitive neuroscience of visual object recognition; Identification (information); Convolutional neural network,,,,,https://link.springer.com/article/10.1007/s42979-021-00790-7 https://paperity.org/p/268993660/a-transfer-learning-approach-for-indoor-object-identification https://doi.org/10.1007/s42979-021-00790-7 https://dblp.uni-trier.de/db/journals/sncs/sncs2.html#AfifASA21 https://link.springer.com/content/pdf/10.1007/s42979-021-00790-7.pdf,http://dx.doi.org/10.1007/s42979-021-00790-7,,10.1007/s42979-021-00790-7,3194205152,,0,001-811-998-049-508; 004-344-893-999-071; 006-637-087-577-95X; 007-369-939-356-099; 017-176-978-939-440; 025-522-385-395-133; 030-273-939-499-337; 030-376-794-797-30X; 032-918-245-581-393; 036-062-021-424-014; 036-064-591-892-943; 037-182-418-585-695; 041-092-364-826-312; 044-396-615-675-996; 045-309-399-228-849; 052-682-735-634-643; 057-567-246-168-045; 061-579-383-511-500; 072-267-449-400-798; 075-533-194-094-730; 076-111-507-166-077; 078-625-982-931-330; 079-048-025-371-422; 079-310-940-223-758; 083-319-656-823-911; 083-786-445-197-335; 085-608-142-942-300; 085-882-717-593-677; 104-208-613-793-423; 133-822-744-570-176; 146-093-154-688-224; 149-375-705-640-142; 150-232-030-254-67X; 152-892-499-470-659; 166-225-748-020-544; 180-559-374-049-445,6,false,,
005-138-757-108-413,Low-cost sensor to detect overtaking based on optical flow,2011-12-10,2011,journal article,Machine Vision and Applications,09328092; 14321769,Springer Science and Business Media LLC,Germany,Pablo Guzman; Javier Diaz; Jarno Ralli; Rodrigo Agís; Eduardo Ros,,25,3,699,711,Artificial intelligence; Optical flow; Microcontroller; Intelligent sensor; Overtaking; Computer vision; Analog signal processing; Computer science; Feature extraction; Machine vision; Real-time computing; Vehicle tracking system,,,,,https://rd.springer.com/article/10.1007/s00138-011-0392-2 https://link.springer.com/content/pdf/10.1007%2Fs00138-011-0392-2.pdf https://link.springer.com/article/10.1007/s00138-011-0392-2 https://dblp.uni-trier.de/db/journals/mva/mva25.html#GuzmanDRAR14,http://dx.doi.org/10.1007/s00138-011-0392-2,,10.1007/s00138-011-0392-2,2037072622,,0,002-478-696-370-777; 003-247-664-270-585; 009-612-733-037-965; 012-195-769-710-289; 013-287-605-098-577; 018-614-701-595-685; 020-696-263-383-132; 024-321-909-925-792; 024-428-596-062-841; 026-492-475-056-495; 032-089-842-057-412; 035-235-837-628-366; 035-349-054-771-153; 046-314-805-764-384; 046-638-536-430-140; 054-370-198-649-750; 057-037-636-095-146; 061-156-711-220-637; 072-038-738-880-039; 074-006-536-890-200; 075-600-489-997-248; 077-882-518-530-163; 080-022-185-865-923; 084-679-921-430-947; 102-491-476-913-401; 107-831-956-253-860; 115-717-125-270-054; 115-874-459-042-00X; 118-178-543-573-823; 128-467-122-887-465; 143-201-521-304-20X; 162-289-063-216-285; 177-065-215-617-152,5,false,,
005-191-781-331-343,User-centered system design for assisted navigation of visually impaired individuals in outdoor cultural environments,2020-10-20,2020,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Charis Ntakolia; George Dimas; Dimitris K. Iakovidis,,21,1,249,274,,,,,"Ministry of Education and Religious Affairs, Sport and Culture",,http://dx.doi.org/10.1007/s10209-020-00764-1,,10.1007/s10209-020-00764-1,,,0,000-082-081-025-981; 000-501-534-285-334; 001-076-828-128-846; 002-109-727-479-297; 002-391-643-288-41X; 002-682-563-503-44X; 003-342-986-507-202; 005-390-653-936-315; 006-529-989-340-419; 007-327-111-370-733; 011-356-906-186-611; 016-473-557-742-214; 017-571-875-650-919; 017-717-029-766-92X; 018-439-029-616-453; 019-925-883-027-48X; 019-995-754-511-586; 021-617-113-257-673; 022-489-327-390-755; 026-062-137-058-607; 026-244-307-943-578; 028-628-140-541-774; 030-127-369-356-741; 030-846-671-006-321; 032-821-668-886-438; 033-687-327-893-105; 033-906-806-767-146; 034-002-517-188-484; 034-350-308-512-672; 034-936-259-439-98X; 036-318-437-523-020; 038-048-782-562-704; 038-364-111-957-515; 040-660-173-610-646; 041-380-323-842-523; 045-028-479-512-531; 045-599-200-378-144; 048-524-984-854-593; 048-603-106-043-151; 049-252-395-723-210; 049-592-217-207-783; 052-554-976-425-07X; 055-495-282-581-758; 071-806-151-845-245; 072-096-421-217-306; 075-633-076-909-925; 083-095-642-169-058; 087-321-099-310-942; 088-206-525-984-388; 088-470-436-866-921; 091-899-892-919-468; 097-856-436-101-193; 098-640-382-982-581; 099-081-268-683-922; 100-850-495-456-719; 105-790-942-783-392; 114-478-622-485-855; 115-439-483-580-16X; 119-696-240-312-542; 121-439-134-242-08X; 130-996-872-331-760; 143-759-708-309-521; 145-898-655-711-084; 150-655-389-803-190; 151-608-461-049-845; 158-769-392-676-037; 163-386-731-087-791; 174-788-000-710-093; 186-810-372-950-411,18,false,,
005-398-665-989-98X,Proposal of a simultaneous ultrasound emission for efficient obstacle searching in autonomous wheelchairs,2013-04-14,2013,journal article,Biomedical Engineering Letters,20939868; 2093985x,Springer Science and Business Media LLC,Germany,Chang-Geol Kim; Byung-Seop Song,,3,1,47,50,Engineering; Position (vector); Ultrasound; Artificial intelligence; Obstacle; Wheelchair; Ultrasound wave; Accurate estimation; Computer vision; Simulation,,,,,https://link.springer.com/article/10.1007/s13534-013-0088-9,http://dx.doi.org/10.1007/s13534-013-0088-9,,10.1007/s13534-013-0088-9,2025873621,,0,011-872-517-632-48X; 023-789-116-963-884; 046-649-192-050-710; 065-992-777-809-97X; 068-138-060-660-975; 073-928-284-197-323; 086-869-440-167-789; 088-553-672-251-80X; 096-803-870-115-728; 098-867-223-476-32X; 111-671-069-947-166; 164-363-699-206-969; 168-267-400-439-166,2,false,,
005-776-917-968-487,A Visual Global Positioning System for Unmanned Aerial Vehicles Used in Photogrammetric Applications,2010-11-23,2010,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,A. Cesetti; Emanuele Frontoni; Adriano Mancini; Andrea Ascani; Primo Zingaretti; Sauro Longhi,,61,1,157,168,Beacon; Photogrammetry; GPS signals; Artificial intelligence; Aerial survey; Computer vision; Computer science; Machine vision; Visual odometry; Global Positioning System; Robustness (computer science),,,,,https://rd.springer.com/content/pdf/10.1007%2Fs10846-010-9489-5.pdf https://link.springer.com/article/10.1007%2Fs10846-010-9489-5 https://dialnet.unirioja.es/servlet/articulo?codigo=3363233 https://dl.acm.org/doi/10.1007/s10846-010-9489-5 https://www.researchgate.net/profile/Emanuele_Frontoni/publication/220062160_A_Visual_Global_Positioning_System_for_Unmanned_Aerial_Vehicles_Used_in_Photogrammetric_Applications/links/00b7d52cb432ce7840000000.pdf,http://dx.doi.org/10.1007/s10846-010-9489-5,,10.1007/s10846-010-9489-5,2091347416,,0,000-158-361-031-997; 002-182-992-577-999; 002-818-989-768-322; 008-958-935-328-586; 011-656-485-064-783; 012-510-404-417-917; 015-085-620-313-033; 025-192-618-273-833; 028-303-905-295-179; 034-996-635-135-619; 037-953-614-732-691; 043-999-955-569-475; 044-224-538-340-955; 052-368-678-154-268; 055-440-901-374-216; 063-445-119-614-243; 066-027-729-902-83X; 088-587-711-335-287; 090-356-924-590-461; 094-076-667-917-273; 097-435-583-783-387; 098-440-015-522-27X; 099-230-453-362-975; 103-439-852-628-187; 132-542-923-913-282; 138-677-497-374-859; 144-488-569-314-324,54,false,,
005-948-508-100-466,I see therefore i read: improving the reading capabilities of individuals with visual disabilities through immersive virtual reality,2021-11-25,2021,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Kurtis Weir; Fernando Loizides; Vinita Nahar; Amar Aggoun; Andrew Pollard,"We aim to help improve the quality of life of people with visual disabilities through the application of emerging technologies. Our current research investigates the viability of virtual reality (VR) as an aid for persons with visual disabilities. In this article, we explore the potential of VR-assisted reading. We investigate the reading effects of VR equipment on persons with visual disabilities by utilising variations of standardised optometry-informed reading tests conducted across 24 participants. Test results uncovered that, when comparing a worn VR head-mounted display (HMD) to physical unaided tests, results within a HMD scaled better at closer distances, while unaided tests scaled better with further distances. Using the findings collected and requirements elicited from participants, a prototype document reader was developed for reading text within a VR-immersed 3D environment, allowing low-vision users to customise and configure accessibility features for enhanced reading. This software was tested with 11 new participants alongside user evaluations, allowing us to discover how users perceived text best within our 3D virtual environments, and what features and techniques are required to evolve this accessibility tool further. The user test reported an overwhelmingly positive response to our tool as a feasible reading aid, allowing persons who could not engage (or, due to the difficulty, refusing to) in the reading of material to do so. We also register some limitations and areas for improvement, such as a need for non-functional requirements to be improved, and the aesthetics of our design to be improved going forward.",22,2,387,413,Human–computer interaction; Psychology; Reading (process),,,,University of Wolverhampton; Beacon Centre for the Blind,https://link.springer.com/content/pdf/10.1007/s10209-021-00854-8.pdf,http://dx.doi.org/10.1007/s10209-021-00854-8,,10.1007/s10209-021-00854-8,3216242835,,0,002-358-580-039-664; 005-288-001-504-209; 008-014-399-435-493; 008-356-634-157-21X; 011-269-403-292-29X; 011-654-644-357-112; 012-908-388-509-730; 018-129-426-590-134; 020-302-288-518-801; 021-279-729-672-630; 031-746-947-751-217; 032-448-073-941-968; 040-335-036-012-704; 040-419-618-479-011; 041-155-921-602-190; 043-918-057-779-30X; 046-841-766-286-109; 048-983-916-690-710; 055-704-268-515-146; 060-230-697-077-382; 062-285-911-785-808; 063-991-428-674-15X; 082-407-666-936-593; 084-067-462-286-079; 084-261-633-107-653; 085-196-579-365-317; 086-912-926-607-909; 088-650-511-631-742; 094-280-406-839-64X; 097-770-753-968-295; 100-635-332-895-23X; 112-298-421-260-018; 117-932-818-190-158; 121-538-744-414-030; 151-950-980-398-143; 164-435-774-052-50X; 168-824-267-918-342,6,true,,green
006-062-014-283-059,An approach to automatic evaluate registration performance for image guided navigation.,2013-05-16,2013,journal article,International journal of computer assisted radiology and surgery,18616429; 18616410,Springer Science and Business Media LLC,Germany,Senhu Li; N Clinthorne,,8,Suppl 1,S145,S154,Artificial intelligence; Image registration; Computer-assisted surgery; Image guided navigation; Computer vision; Computer science,Computer-assisted surgery; Image guided; Image registration; Navigation,,,NCI NIH HHS (R44 CA112966) United States,http://europepmc.org/articles/PMC4184430/ https://www.ncbi.nlm.nih.gov/pubmed/25285174,http://dx.doi.org/10.1007/s11548-013-0863-1,25285174,10.1007/s11548-013-0863-1,2418530112,PMC4184430,0,019-063-643-817-478,1,true,,green
006-224-237-251-379,A wearable guidance system with interactive user interface for persons with visual impairment,2014-11-23,2014,journal article,Multimedia Tools and Applications,13807501; 15737721,Springer Science and Business Media LLC,Netherlands,Jin-Hee Lee; Dongho Kim; Byeong-Seok Shin,,75,23,15275,15296,Wearable computer; Artificial intelligence; Natural user interface; Assisted GPS; Guidance system; PATH (variable); Computer vision; Computer science; User interface,,,,,https://dblp.uni-trier.de/db/journals/mta/mta75.html#LeeKS16a https://link.springer.com/article/10.1007/s11042-014-2385-4/fulltext.html https://link.springer.com/content/pdf/10.1007%2Fs11042-014-2385-4.pdf http://dblp.uni-trier.de/db/journals/mta/mta75.html#LeeKS16a https://rd.springer.com/article/10.1007/s11042-014-2385-4 https://link.springer.com/article/10.1007/s11042-014-2385-4,http://dx.doi.org/10.1007/s11042-014-2385-4,,10.1007/s11042-014-2385-4,2094842773,,3,002-022-322-129-035; 007-527-228-704-510; 008-663-335-419-582; 010-115-633-498-893; 013-056-435-015-274; 016-794-760-209-80X; 018-328-920-497-703; 018-911-741-393-317; 022-697-627-694-941; 024-131-470-445-452; 025-735-795-306-032; 030-146-317-204-248; 031-093-203-495-219; 036-280-949-293-766; 040-755-185-230-407; 044-862-091-455-233; 060-886-589-965-181; 061-498-478-732-010; 064-053-186-811-574; 064-846-316-680-673; 065-276-856-923-220; 065-992-777-809-97X; 069-231-188-624-028; 069-419-248-108-995; 069-575-138-312-148; 073-928-284-197-323; 074-100-513-027-130; 076-690-800-964-405; 078-708-146-352-222; 078-875-379-560-055; 080-436-925-917-62X; 082-646-219-309-246; 085-948-188-966-448; 087-221-147-428-948; 089-746-066-711-420; 104-454-311-761-171; 104-982-807-825-22X; 106-095-219-922-009; 107-887-647-672-757; 112-335-378-321-769; 115-246-676-960-424; 122-631-967-023-822; 127-483-973-270-088; 129-243-738-783-182; 135-646-014-794-306; 136-899-105-271-668; 138-551-849-904-97X; 151-790-631-478-110; 154-366-740-675-873; 157-597-772-762-418; 162-341-554-676-273; 168-530-441-278-675; 170-387-846-090-147; 183-935-933-940-273; 194-250-936-089-848,9,false,,
006-312-077-911-322,SSD - Indoor sign Detection System for Indoor Assistance Navigation,2021-03-22,2021,conference proceedings article,"2021 18th International Multi-Conference on Systems, Signals & Devices (SSD)",,IEEE,,Mouna Afif; Riadh Ayachi; Yahia Said; Mohamed Atri,"Indoor signage plays an important role in finding specific destinations and way-finding especially for blind and visually impaired people (VIP). In this paper, we developed a new indoor signage classifier using deep convolutional neural Network (DCNN). Computer vision-based systems using cameras-based present a potential intermediate to assist blind and VIP persons on accessing unfamiliar buildings. Experiments were performed on a new dataset taken in an indoor building in France. The proposed dataset present 800 natural images divided into 4 indoor signs. Results achieved show that our proposed approach presents very encouraging results coming to 99.8% as recognition precision rate.",,,1383,1387,Deep learning; Signage; Artificial intelligence; Classifier (linguistics); Sign detection; Visually impaired; Computer vision; Computer science; Convolutional neural network,,,,,https://ieeexplore.ieee.org/document/9429495 https://dblp.uni-trier.de/db/conf/IEEEssd/ssd2021.html#AfifASA21,http://dx.doi.org/10.1109/ssd52085.2021.9429495,,10.1109/ssd52085.2021.9429495,3164542136,,0,004-269-574-716-057; 008-361-673-204-978; 015-672-084-912-775; 017-159-670-613-140; 018-843-508-574-144; 020-233-013-143-936; 028-628-140-541-774; 046-262-856-533-736; 071-806-151-845-245; 076-111-507-166-077; 077-101-929-493-550; 116-715-441-276-690; 179-771-774-454-944; 186-810-372-950-411,2,false,,
006-522-549-295-691,Making touch-based kiosks accessible to blind users through simple gestures,2011-09-27,2011,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Frode Eika Sandnes; Tek Beng Tan; Anders Johansen; Edvin Sulic; Eirik Vesterhus; Eirik Rud Iversen,"Touch-based interaction is becoming increasingly popular and is commonly used as the main interaction paradigm for self-service kiosks in public spaces. Touch-based interaction is known to be visually intensive, and current non-haptic touch-display technologies are often criticized as excluding blind users. This study set out to demonstrate that touch-based kiosks can be designed to include blind users without compromising the user experience for non-blind users. Most touch-based kiosks are based on absolute positioned virtual buttons which are difficult to locate without any tactile, audible or visual cues. However, simple stroke gestures rely on relative movements and the user does not need to hit a target at a specific location on the display. In this study, a touch-based train ticket sales kiosk based on simple stroke gestures was developed and tested on a panel of blind and visually impaired users, a panel of blindfolded non-visually impaired users and a control group of non-visually impaired users. The tests demonstrate that all the participants managed to discover, learn and use the touch-based self-service terminal and complete a ticket purchasing task. The majority of the participants completed the task in less than 4 min on the first attempt.",11,4,421,431,Sensory cue; Interactive kiosk; Set (psychology); User experience design; Gesture; Ticket; Task (project management); Audio feedback; Computer science; Multimedia,,,,,https://doi.org/10.1007/s10209-011-0258-4 https://link.springer.com/article/10.1007/s10209-011-0258-4 https://link.springer.com/article/10.1007/s10209-011-0258-4/fulltext.html https://rd.springer.com/article/10.1007%2Fs10209-011-0258-4 https://core.ac.uk/download/pdf/35073936.pdf,http://dx.doi.org/10.1007/s10209-011-0258-4,,10.1007/s10209-011-0258-4,2090185530,,0,000-029-358-228-130; 004-315-451-874-148; 014-193-014-608-968; 016-680-170-637-745; 019-516-407-653-933; 019-858-836-073-646; 020-374-980-887-665; 021-950-050-933-492; 024-662-475-024-668; 026-244-307-943-578; 028-929-779-533-184; 032-517-859-134-478; 043-423-699-646-854; 044-335-439-924-260; 045-973-141-514-739; 047-794-617-772-549; 050-050-528-236-986; 052-068-223-587-898; 053-686-908-452-536; 053-796-216-611-468; 056-393-022-196-940; 060-317-562-179-543; 066-272-521-768-95X; 073-131-476-631-683; 073-901-362-164-536; 076-455-738-292-176; 080-336-593-244-132; 095-773-864-726-740; 096-579-629-460-230; 100-850-495-456-719; 106-463-479-857-836; 118-613-273-148-467; 128-905-311-775-929; 136-674-554-567-061; 136-908-502-256-738; 139-318-710-905-041; 142-575-622-508-48X; 156-066-619-903-086; 173-447-695-406-773; 174-955-746-691-453; 180-003-121-425-197; 184-120-739-510-746; 198-313-807-169-658,39,true,,green
006-847-266-985-989,A robust negative obstacle detection method using seed-growing and dynamic programming for visually-impaired/blind persons,2011-11-26,2011,journal article,Optical Review,13406000; 13499432,Springer Science and Business Media LLC,Germany,Saeid Fazli; Hajar Mohammadi Dehnavi; Payman Moallem,,18,6,415,422,Belief propagation; Artificial intelligence; Terrain; Obstacle; Stairs; Blind persons; Visually impaired; Computer vision; Dynamic programming; Computer science; Stereopsis,,,,,https://link.springer.com/article/10.1007%2Fs10043-011-0079-y http://ui.adsabs.harvard.edu/abs/2011OptRv..18..415F/abstract,http://dx.doi.org/10.1007/s10043-011-0079-y,,10.1007/s10043-011-0079-y,2072375829,,0,011-969-064-214-381; 017-421-189-510-98X; 030-167-497-654-555; 032-688-114-680-593; 053-095-745-160-488; 060-981-662-954-127; 061-808-278-411-803; 078-452-188-034-285; 099-057-909-820-879; 114-438-596-752-377; 129-319-201-847-340; 140-667-675-668-458; 158-881-326-339-93X,5,false,,
006-950-044-012-028,Exploring the Effect of Obscurants on Safe Landing Zone Identification,2009-08-28,2009,journal article,Journal of Intelligent and Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Keith W. Sevcik; Noah R. Kuntz; Paul Y. Oh,,57,1,281,295,Engineering; Terrain; Systems engineering; Range (aeronautics); Landing zone; Environment controlled; Field tests; Simulation; Verification and validation; Firefighting; Identification (information),,,,,https://link.springer.com/article/10.1007/s10846-009-9358-2 https://dialnet.unirioja.es/servlet/articulo?codigo=3115912 https://dblp.uni-trier.de/db/journals/jirs/jirs57.html#SevcikKO10 https://doi.org/10.1007/s10846-009-9358-2 https://link.springer.com/content/pdf/10.1007%2F978-90-481-8764-5_14.pdf https://rd.springer.com/chapter/10.1007/978-90-481-8764-5_14,http://dx.doi.org/10.1007/s10846-009-9358-2,,10.1007/s10846-009-9358-2,2099770645,,0,000-158-361-031-997; 011-969-064-214-381; 020-506-358-802-256; 027-687-582-276-094; 027-764-460-113-226; 033-467-284-515-707; 042-540-922-383-492; 047-136-585-894-036; 073-693-482-113-772; 077-038-147-021-306; 098-059-732-445-50X; 104-215-281-040-69X; 109-326-541-878-610; 111-261-180-102-880; 140-806-315-536-045; 197-957-273-754-680,9,false,,
007-069-437-291-059,Smart Navigation Detection using Deep-learning for Visually Impaired Person,2021-12-10,2021,conference proceedings article,2021 IEEE 2nd International Conference On Electrical Power and Energy Systems (ICEPES),,IEEE,,Nitin Kumar; Anuj Jain,"In order to minimize the problem of unknown paths movements for a VI person, we propose an assistive navigational system to help VI person to navigate in an known or unknown environment. A modified approach is developed in which a visual impaired person or blind person is assisted through a live feed. The model comprises of YOLO(You Only Look Once) based algorithm and a stick to detect objects through IOU (Intersection of Union). The collected pictures of path are annotated and then the model is developed by training of 300 images each of around 25 classes using Yolo v3 network. The path detection is executed through a Novel tracker system that utilizes an offline trained neural network. The accuracy of the proposed model with wearable mask founds to be 81% while with the stick it is found to be 96.14 %.",,,,,Intersection (aeronautics); Computer science; Artificial intelligence; Computer vision; Wearable computer; Path (computing); Deep learning; Artificial neural network; Engineering; Embedded system; Programming language; Aerospace engineering,,,,,,http://dx.doi.org/10.1109/icepes52894.2021.9699479,,10.1109/icepes52894.2021.9699479,,,0,006-565-780-206-428; 015-946-657-655-768; 023-868-294-867-153; 039-274-384-165-289; 049-166-030-940-238; 058-268-022-053-157; 068-708-501-300-740; 075-903-419-661-27X; 104-982-807-825-22X; 109-525-288-959-67X; 144-216-205-445-391,3,false,,
007-223-103-783-187,Vision-Based Mobile Indoor Assistive Navigation Aid for Blind People,2018-06-01,2018,journal article,IEEE transactions on mobile computing,15580660; 15361233; 21619875,Institute of Electrical and Electronics Engineers (IEEE),United States,Bing Li; Juan Pablo Munoz; Xuejian Rong; Qingtian Chen; Jizhong Xiao; Yingli Tian; Aries Arditi; Mohammed Yousuf,"This paper presents a new holistic vision-based mobile assistive navigation system to help blind and visually impaired people with indoor independent travel. The system detects dynamic obstacles and adjusts path planning in real-time to improve navigation safety. First, we develop an indoor map editor to parse geometric information from architectural models and generate a semantic map consisting of a global 2D traversable grid map layer and context-aware layers. By leveraging the visual positioning service (VPS) within the Google Tango device, we design a map alignment algorithm to bridge the visual area description file (ADF) and semantic map to achieve semantic localization. Using the on-board RGB-D camera, we develop an efficient obstacle detection and avoidance approach based on a time-stamped map Kalman filter (TSM-KF) algorithm. A multi-modal human-machine interface (HMI) is designed with speech-audio interaction and robust haptic interaction through an electronic SmartCane. Finally, field experiments by blindfolded and blind subjects demonstrate that the proposed system provides an effective tool to help blind individuals with indoor navigation and wayfinding.",18,3,702,714,Interface (computing); Artificial intelligence; Grid reference; Kalman filter; Bridge (nautical); Mobile robot; Obstacle; Navigation system; Computer vision; Computer science; Obstacle avoidance; Motion planning,Google Tango device; Indoor assistive navigation; blind and visually impaired people; obstacle avoidance; semantic maps,,,NIEHS NIH HHS (27398C0007) United States; NEI NIH HHS (R43 EY023483) United States,https://europepmc.org/article/MED/30774566 https://ieeexplore.ieee.org/document/8370712 https://www.computer.org/csdl/journal/tm/2019/03/08370712/17D45WXIkHr https://dl.acm.org/doi/10.1109/TMC.2018.2842751 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6371975 https://pubmed.ncbi.nlm.nih.gov/30774566/ https://dblp.uni-trier.de/db/journals/tmc/tmc18.html#LiMRCXTAY19,http://dx.doi.org/10.1109/tmc.2018.2842751,30774566,10.1109/tmc.2018.2842751,2805440904,PMC6371975,3,006-322-344-260-050; 009-256-511-526-132; 015-016-116-643-819; 029-360-136-216-567; 030-164-764-932-529; 030-877-543-890-441; 036-552-223-342-289; 040-993-123-810-610; 044-367-117-261-667; 045-205-438-482-211; 046-386-633-304-965; 046-722-488-206-778; 057-731-809-827-780; 058-038-802-882-120; 060-448-827-026-324; 073-222-472-617-845; 085-815-105-604-187; 089-574-772-970-499; 090-738-057-382-94X; 098-670-098-718-190; 102-292-510-043-963; 104-013-878-921-179; 105-278-566-106-933; 108-207-763-016-292; 110-650-958-929-874; 113-764-349-033-065; 128-975-670-807-332; 132-753-659-325-317; 134-348-975-295-31X; 137-953-980-920-452; 151-602-964-183-539; 151-608-461-049-845; 159-668-380-289-754; 166-405-745-827-631; 170-387-846-090-147; 171-180-431-893-536,148,true,"publisher-specific, author manuscript",green
007-236-577-946-489,Electronic Eye for Visually Impaired Persons,,2013,,,,,,S. Nithya; A. S. L Shravani,"This paper presents an electronic travel aid for blind people to navigate safely and quickly, an obstacle detection system using UVC camera based visual navigation has been considered. The proposed system detects the obstacles up to 300 cm via sonar and sends feedback in form of beep sound to inform the person about its location. In addition to this, an UVC webcam is connected to 32 bit ARM micro controller which supports features and algorithms for designing of blind people guidance stick. This supports image processing which are used to processes images and give voice response after detection. which is used for finding the properties of the obstacle in particular, in the context of this work, Identification of human presence is based on face detection and object detection. The algorithms are implemented in open CV ,which runs on LINUX environment.",,,,,Engineering; Artificial intelligence; Microcontroller; Object detection; Obstacle; Context (language use); Computer vision; Face detection; Sonar; Identification (information); Image processing,,,,,,,,,2183223463,,0,006-956-729-410-739; 013-847-555-426-159; 056-773-999-189-634; 063-122-584-706-147; 068-581-156-196-413; 096-803-870-115-728; 101-333-962-375-602; 130-097-955-008-505; 132-287-528-433-371; 192-600-440-415-958,0,false,,
007-409-223-391-642,Admittance Controller with Spatial Modulation for Assisted Locomotion using a Smart Walker,2018-05-25,2018,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Mario F. Jiménez; Matias Monllor; Anselmo Frizera; Teodiano Bastos; Flavio Roberti; Ricardo Carelli,,94,3,621,637,Admittance; Multimodal interaction; Position (vector); Haptic technology; Kinematics; Computer science; Tracking (particle physics); Control theory; Control theory; Path (graph theory),,,,CAPES; FAPES; CNPq,https://notablesdelaciencia.conicet.gov.ar/handle/11336/89233 https://doi.org/10.1007/s10846-018-0854-0 https://ri.conicet.gov.ar/handle/11336/89233 https://link.springer.com/article/10.1007/s10846-018-0854-0 https://pure.urosario.edu.co/es/publications/admittance-controller-with-spatial-modulation-for-assisted-locomo https://dblp.uni-trier.de/db/journals/jirs/jirs94.html#JimenezMFBRC19 https://dialnet.unirioja.es/servlet/articulo?codigo=7055987,http://dx.doi.org/10.1007/s10846-018-0854-0,,10.1007/s10846-018-0854-0,2803831479,,0,001-781-565-386-619; 002-307-327-464-798; 006-500-640-442-683; 012-034-911-983-433; 019-413-432-291-100; 020-014-515-472-537; 028-228-257-985-122; 028-355-201-727-652; 028-601-113-310-794; 032-877-710-084-301; 032-943-517-732-63X; 033-500-637-438-279; 038-189-158-765-996; 039-304-078-515-230; 047-903-575-818-413; 050-489-372-109-454; 069-523-876-223-951; 076-121-436-407-96X; 082-786-122-573-731; 112-731-483-510-777; 112-843-104-186-679; 119-937-626-294-88X; 120-093-276-975-914; 126-245-592-460-611; 127-803-035-710-075; 135-075-382-709-615; 135-212-223-281-648; 172-015-732-361-098; 189-859-933-301-910,46,false,,
007-530-851-453-706,An insight into assistive technology for the visually impaired and blind people: state-of-the-art and future trends,2017-01-07,2017,journal article,Journal on Multimodal User Interfaces,17837677; 17838738,Springer Science and Business Media LLC,Germany,Alexy Bhowmick; Shyamanta M. Hazarika,,11,2,149,172,Data science; State (polity); Pace; Field (Bourdieu); Social impact; Information analysis; Assistive technology; Visually impaired; Computer science; Multimedia; Interpretation (philosophy),,,,,https://link.springer.com/article/10.1007/s12193-016-0235-6 https://www.infona.pl/resource/bwmeta1.element.springer-doi-10_1007-S12193-016-0235-6 https://doi.org/10.1007/s12193-016-0235-6,http://dx.doi.org/10.1007/s12193-016-0235-6,,10.1007/s12193-016-0235-6,2568039188,,1,000-420-215-400-716; 000-659-254-096-600; 000-960-521-563-561; 002-367-324-166-044; 002-773-466-340-11X; 003-215-340-330-223; 003-634-369-751-95X; 004-626-240-357-077; 004-679-405-970-494; 004-931-010-653-795; 005-553-361-556-816; 006-086-935-786-917; 006-225-758-714-927; 006-487-017-505-503; 007-249-528-807-784; 008-794-020-739-258; 008-859-948-927-189; 009-063-048-778-483; 010-004-081-294-240; 012-960-499-936-91X; 013-056-435-015-274; 013-424-116-822-761; 013-732-636-601-123; 013-829-872-250-493; 013-847-555-426-159; 013-870-781-835-669; 014-355-665-073-740; 014-387-823-436-92X; 016-702-175-545-473; 018-192-197-397-892; 019-451-535-847-535; 021-909-628-200-486; 025-491-543-482-066; 026-836-294-825-036; 027-780-023-753-583; 028-628-140-541-774; 028-683-545-086-724; 028-709-784-462-857; 029-252-743-409-493; 030-238-764-985-187; 030-869-182-891-085; 033-820-477-736-56X; 034-196-646-262-503; 034-302-603-898-452; 034-412-649-320-055; 034-463-620-626-325; 036-774-982-375-077; 040-030-169-396-345; 040-660-173-610-646; 041-504-400-904-233; 042-225-738-360-151; 043-536-853-133-602; 044-395-575-720-718; 045-321-254-881-478; 046-991-476-786-467; 047-838-261-582-286; 048-446-984-045-429; 049-592-217-207-783; 050-541-704-593-816; 051-472-067-465-255; 053-598-695-896-482; 053-968-972-722-364; 058-155-799-281-81X; 058-268-022-053-157; 058-443-047-291-990; 059-808-349-359-714; 060-281-953-830-485; 062-630-991-312-529; 063-048-719-705-828; 064-069-666-882-597; 064-264-695-662-36X; 066-278-641-931-35X; 066-671-206-757-407; 067-448-429-225-410; 068-172-489-044-242; 070-770-092-316-15X; 075-402-037-032-739; 079-228-979-733-12X; 080-021-648-530-499; 085-102-500-348-494; 085-972-264-981-114; 089-629-145-629-991; 092-391-305-190-481; 093-668-999-939-359; 097-724-288-443-153; 099-086-994-780-682; 099-454-996-703-003; 100-731-218-785-790; 101-273-981-009-614; 102-952-660-914-374; 106-231-251-265-639; 108-460-964-270-915; 109-260-833-365-957; 114-777-477-981-296; 120-745-573-411-796; 125-897-318-346-362; 128-288-848-632-871; 132-028-339-875-078; 132-342-571-633-36X; 135-501-390-548-400; 135-912-309-671-935; 142-347-146-857-667; 145-523-738-709-309; 150-753-294-369-298; 162-234-848-484-409; 166-692-136-810-935; 167-876-981-583-007; 176-613-431-700-098; 178-363-336-640-342; 189-586-193-904-36X,178,false,,
007-536-388-960-534,Finding multiple lanes in urban road networks with vision and lidar,2009-03-24,2009,journal article,Autonomous Robots,09295593; 15737527,Springer Science and Business Media LLC,Netherlands,Albert S. Huang; David Moore; Matthew Antone; Edwin Olson; Seth Teller,"This paper describes a system for detecting and estimating the properties of multiple travel lanes in an urban road network from calibrated video imagery and laser range data acquired by a moving vehicle. The system operates in real-time in several stages on multiple processors, fusing detected road markings, obstacles, and curbs into a stable non-parametric estimate of nearby travel lanes. The system incorporates elements of a provided piecewise-linear road network as a weak prior.; ; Our method is notable in several respects: it detects and estimates multiple travel lanes; it fuses asynchronous, heterogeneous sensor streams; it handles high-curvature roads; and it makes no assumption about the position or orientation of the vehicle with respect to the road.; ; We analyze the system's performance in the context of the 2007 DARPA Urban Challenge. With five cameras and thirteen lidars, our method was incorporated into a closed-loop controller to successfully guide an autonomous vehicle through a 90 km urban course at speeds up to 40 km/h amidst moving traffic.",26,2,103,122,Controller (computing); Vehicle Information and Communication System; Artificial intelligence; Context (language use); Land navigation; Computer vision; Sensor fusion; Computer science; Lidar; Incident management (ITSM); Orientation (computer vision),,,,,http://core.ac.uk/display/23535114 http://april.eecs.umich.edu/pdfs/huang2009ra.pdf https://apps.dtic.mil/sti/citations/ADA517013 https://link.springer.com/article/10.1007/s10514-009-9113-3 http://dspace.mit.edu/handle/1721.1/49455 https://trid.trb.org/view/920677 https://apps.dtic.mil/sti/pdfs/ADA517013.pdf https://dblp.uni-trier.de/db/journals/arobots/arobots26.html#HuangMAOT09,http://dx.doi.org/10.1007/s10514-009-9113-3,,10.1007/s10514-009-9113-3,2157076480,,27,002-253-963-823-586; 003-622-690-298-890; 024-242-963-177-996; 028-303-905-295-179; 036-059-573-448-762; 038-221-980-942-413; 043-607-559-812-917; 043-976-846-740-331; 044-121-251-464-393; 049-069-687-352-102; 067-222-818-008-397; 069-321-191-875-521; 072-056-220-954-979; 079-570-504-854-335; 081-204-205-067-245; 085-951-962-459-515; 087-887-890-493-528; 093-050-180-655-88X; 093-184-843-363-321; 098-156-392-926-60X; 109-439-011-122-083; 112-931-442-676-910; 114-174-628-565-52X; 115-602-753-359-615; 117-580-436-643-098; 129-694-569-030-94X; 146-163-150-843-806; 171-458-835-309-416; 187-778-953-183-213; 193-123-800-849-774,120,true,cc-by-nc,green
007-545-670-481-419,Music Training Interface for Visually Impaired through a Novel Approach to Optical Music Recognition,2014-05-27,2014,journal article,GSTF Journal on Computing (JoC),20102283,Springer Science and Business Media LLC,,Dawpadee B. Kiriella; Shyama C. Kumari; Kavindu Ranasinghe; Lakshman Jayaratne,,3,4,1,14,Interface (computing); Braille; Optical music recognition; Visually impaired; Computer science; Multimedia; Scripting language; Musical notation,,,,,https://link.springer.com/article/10.7603/s40601-013-0045-6 https://link.springer.com/content/pdf/10.7603/s40601-013-0045-6.pdf,http://dx.doi.org/10.7603/s40601-013-0045-6,,10.7603/s40601-013-0045-6,2007057397,,0,000-424-243-390-605; 000-670-993-759-946; 007-568-509-275-653; 028-073-862-681-895; 034-785-056-948-412; 056-164-294-689-723; 058-145-630-585-384; 062-326-531-896-061; 062-739-232-757-331; 072-509-279-437-748; 095-029-288-005-770; 099-221-593-191-499; 102-903-998-143-916; 110-725-107-781-422; 116-219-900-812-032; 158-238-223-239-896; 165-397-289-740-001; 169-296-862-363-45X,2,false,,
007-573-197-769-65X,Interactive sonification of U-depth images in a navigation aid for the visually impaired,2018-11-08,2018,journal article,Journal on Multimodal User Interfaces,17837677; 17838738,Springer Science and Business Media LLC,Germany,Piotr Skulimowski; Mateusz Owczarek; A. Radecki; Michal Bujacz; Dariusz Rzeszotarski; Pawel Strumillo,"In this paper we propose an electronic travel aid system for the visually impaired that utilizes interactive sonification of U-depth maps of the environment. The system is comprised of a depth sensor connected to a mobile device and a dedicated application for segmenting depth images and converting them into sounds in real time. An important feature of the system is that the user can interactively select the 3D scene region for sonification by simple touch gestures on the mobile device screen. The sonification scheme is using stereo panning for azimuth angle localization of scene objects, loudness for their size and frequency for distance encoding. Such a sonic representation of 3D scenes allows the user to identify the geometric structure of the environment and determine the distances to potential obstacles. The prototype application was tested by three visually impaired users who managed to successfully perform indoor mobility tasks. The system’s usefulness was evaluated quantitatively by means of system usability and task-related questionnaires.",13,3,219,230,Panning (audio); Encoding (memory); Artificial intelligence; Azimuth; Usability; Mobile device; Gesture; Computer vision; Computer science; Feature (computer vision); Sonification,,,,National Science Centre of Poland,https://link.springer.com/article/10.1007/s12193-018-0281-3 https://dblp.uni-trier.de/db/journals/jmui/jmui13.html#SkulimowskiORBR19 https://link.springer.com/content/pdf/10.1007/s12193-018-0281-3.pdf,http://dx.doi.org/10.1007/s12193-018-0281-3,,10.1007/s12193-018-0281-3,2899563586,,0,005-252-108-406-567; 005-844-144-383-701; 008-263-052-171-190; 009-160-989-834-373; 013-056-435-015-274; 028-304-495-955-73X; 030-062-476-514-114; 033-517-762-587-514; 036-149-176-762-271; 039-720-388-801-921; 043-427-884-127-659; 046-991-476-786-467; 055-733-796-028-192; 058-350-834-986-051; 064-665-266-625-066; 083-751-589-100-555; 098-116-610-021-176; 099-542-466-724-114; 134-140-870-020-800; 158-675-713-708-734; 173-383-716-653-758,33,true,cc-by,hybrid
007-675-376-892-714,The Visual Foucauldian: Institutional Coercion and Surveillance in Frederick Wiseman's Multi-handicapped Documentary Series,,2003,journal article,Journal of Medical Humanities,10413545; 15733645; 02789523; 08826498,Springer Science and Business Media LLC,United States,Sharon L. Snyder; David T. Mitchell,,24,3,291,308,Gender studies; Sociology; Institutionalisation; Interpersonal communication; Contemplation; Law; Power (social and political); Coercion; Film director; Vocational education; Discipline,,,,,https://philpapers.org/rec/SHATVF https://link.springer.com/article/10.1023/A%3A1026066621961,http://dx.doi.org/10.1023/a:1026066621961,,10.1023/a:1026066621961,1603606871,,0,021-818-222-435-775; 033-454-422-963-047; 036-974-943-270-332; 051-203-948-384-097; 068-253-526-129-689; 109-951-973-523-534; 155-877-864-037-065; 162-475-534-894-998; 195-276-404-165-821,9,false,,
008-214-681-885-646,Stereo disparity estimation algorithm for blind assisting system,2012-10-23,2012,journal article,CSI Transactions on ICT,22779078; 22779086,Springer Science and Business Media LLC,,Vimal Mohandas; Roy Paily,"The ability to explore unknown spaces independently, safely and efficiently is one of the challenging aspects of any blind assisting system. Many of the known existing electronic travel aids do not reveal spatial information to the blind. Therefore in this paper the focus is on obtaining the spatial information with the help of image processing techniques. Different disparity estimation algorithms are studied in order to arrive at a scheme specifically suitable for the blind. Real time images are captured by a pair of image sensors and the proposed algorithm will determine the surface conditions such as curbs, stair case and other general obstacles encountered on the navigational path.",1,1,3,8,Algorithm; Artificial intelligence; Spatial analysis; Focus (optics); Scheme (programming language); Computer vision; Computer science; Depth perception; Image sensor; Path (graph theory); Image processing; Estimation,,,,,https://link.springer.com/article/10.1007/s40012-012-0004-y https://www.researchgate.net/profile/Vimal_Mohandas2/publication/257808914_Stereo_disparity_estimation_algorithm_for_blind_assisting_system/links/547711aa0cf29afed61444ed.pdf https://paperity.org/p/33201174/stereo-disparity-estimation-algorithm-for-blind-assisting-system https://link.springer.com/content/pdf/10.1007%2Fs40012-012-0004-y.pdf https://link.springer.com/article/10.1007/s40012-012-0004-y/fulltext.html,http://dx.doi.org/10.1007/s40012-012-0004-y,,10.1007/s40012-012-0004-y,1986456252,,0,001-532-995-554-144; 005-630-081-789-910; 010-115-633-498-893; 013-056-435-015-274; 026-745-493-060-466; 049-220-559-972-138; 057-890-353-263-937; 058-923-977-741-380; 059-647-236-855-012; 072-360-738-669-466; 073-928-284-197-323; 081-686-605-990-084; 093-499-007-397-979; 101-333-962-375-602; 107-675-088-325-585; 119-812-859-214-837; 128-154-672-313-987; 147-832-250-508-396,6,true,,bronze
008-317-188-862-047,MechaTag: A Mechanical Fiducial Marker and the Detection Algorithm,2021-10-25,2021,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Francesca Digiacomo; Francesco Bologna; Francesco Inglese; Cesare Stefanini; Mario Milazzo,"Fiducial markers are fundamental components of many computer vision systems that help, through their unique features (e.g., shape, color), a fast localization of spatial objects in unstructured scenarios. They find applications in many scientific and industrial fields, such as augmented reality, human-robot interaction, and robot navigation. In order to overcome the limitations of traditional paper-printed fiducial markers (i.e. deformability of the paper surface, incompatibility with industrial and harsh environments, complexity of the shape to reproduce directly on the piece), we aim at exploiting existing, or additionally fabricated, structural features on rigid bodies (e.g., holes), developing a fiducial mechanical marker system called MechaTag. Our system, endowed with a dedicated algorithm, is able to minimize recognition errors and to improve repeatability also in case of ill boundary conditions (e.g., partial illumination). We assess MechaTag in a pilot study, achieving a robustness of fiducial marker recognition above 95% in different environment conditions and position configurations. The pilot study was conducted by guiding a robotic platform in different poses in order to experiment with a wide range of working conditions. Our results make MechaTag a reliable fiducial marker system for a wide range of robotic applications in harsh industrial environments without losing accuracy of recognition due to the shape and material.",103,3,46,,Algorithm; Position (vector); Augmented reality; Fiducial marker; Range (mathematics); Computer science; Robot; Robustness (computer science),,,,Baker Hughes,https://link.springer.com/article/10.1007/s10846-021-01507-x,http://dx.doi.org/10.1007/s10846-021-01507-x,,10.1007/s10846-021-01507-x,3210142628,,0,001-855-417-711-087; 004-457-862-347-67X; 015-417-609-922-215; 015-705-464-048-251; 020-583-537-901-400; 021-778-690-223-921; 022-621-188-999-653; 023-318-513-319-565; 023-624-634-714-989; 037-781-044-962-007; 040-048-371-215-830; 046-667-829-128-853; 047-961-743-495-728; 053-272-542-481-526; 071-249-739-227-57X; 074-647-841-825-720; 083-167-673-555-293; 092-257-637-515-660; 094-075-454-496-255; 097-658-260-946-207; 107-863-125-055-122; 107-913-317-932-452; 113-042-153-849-201; 124-469-323-380-808; 132-088-759-848-118; 134-163-438-959-722; 154-474-267-970-209; 177-752-447-398-90X; 177-936-180-559-802; 178-301-784-892-961; 195-867-066-368-267,2,true,cc-by,hybrid
008-361-673-204-978,Navigation Assistance for the Visually Impaired Using RGB-D Sensor With Range Expansion,,2016,journal article,IEEE Systems Journal,19328184; 19379234; 23737816,Institute of Electrical and Electronics Engineers (IEEE),United States,A. Aladren; Gonzalo López-Nicolás; Luis Puig; J. J. Guerrero,"Navigation assistance for visually impaired (NAVI) refers to systems that are able to assist or guide people with vision loss, ranging from partially sighted to totally blind, by means of sound commands. In this paper, a new system for NAVI is presented based on visual and range information. Instead of using several sensors, we choose one device, a consumer RGB-D camera, and take advantage of both range and visual information. In particular, the main contribution is the combination of depth information with image intensities, resulting in the robust expansion of the range-based floor segmentation. On one hand, depth information, which is reliable but limited to a short range, is enhanced with the long-range visual information. On the other hand, the difficult and prone-to-error image processing is eased and improved with depth information. The proposed system detects and classifies the main structural elements of the scene providing the user with obstacle-free paths in order to navigate safely across unknown scenarios. The proposed system has been tested on a wide variety of scenarios and data sets, giving successful results and showing that the system is robust and works in challenging indoor environments.",10,3,922,932,Ranging; Image segmentation; Engineering; Artificial intelligence; Computer vision; Visualization; RGB color model; Segmentation; Image processing; Search engine; Robustness (computer science),,,,VINEA; Fondo Europeo de Desarrollo Regional,http://webdiis.unizar.es/~jguerrer/Publicaciones_archivos/2014_IeeeSystemsAladren_Navi.pdf https://doi.org/10.1109/JSYST.2014.2320639 https://jglobal.jst.go.jp/public/201702222605519450 https://dblp.uni-trier.de/db/journals/sj/sj10.html#AladrenLPG16 http://ieeexplore.ieee.org/document/6819807 https://dx.doi.org/10.1109/JSYST.2014.2320639 https://ui.adsabs.harvard.edu/abs/2016ISysJ..10..922A/abstract https://ieeexplore.ieee.org/document/6819807,http://dx.doi.org/10.1109/jsyst.2014.2320639,,10.1109/jsyst.2014.2320639,2318964089,,2,005-235-961-320-360; 013-056-435-015-274; 014-454-377-327-797; 014-724-098-723-866; 016-441-714-814-036; 021-885-923-962-770; 027-489-370-304-359; 028-303-905-295-179; 028-683-545-086-724; 029-731-103-826-078; 036-096-374-435-654; 042-829-936-625-437; 043-976-846-740-331; 046-540-172-422-610; 047-288-576-788-787; 047-803-510-999-614; 052-540-569-867-869; 058-762-231-206-897; 060-041-403-626-524; 060-504-044-262-324; 062-415-088-548-605; 074-324-981-337-749; 077-042-912-905-881; 081-433-531-051-926; 083-146-360-435-079; 092-695-710-025-685; 093-256-914-961-610; 101-181-196-868-714; 115-246-676-960-424; 128-009-352-638-941; 130-524-507-836-36X; 132-920-664-117-745; 134-658-451-919-279; 137-351-591-563-449; 153-500-827-666-679; 186-412-044-017-67X,192,true,,green
008-376-241-563-522,Underwater Image Dehazing via Unpaired Image-to-image Translation,2020-02-28,2020,journal article,"International Journal of Control, Automation and Systems",15986446; 20054092,Springer Science and Business Media LLC,South Korea,Younggun Cho; Hyesu Jang; Ramavtar Malav; Gaurav Pandey; Ayoung Kim,,18,3,605,614,Image restoration; Image (mathematics); Consistency (database systems); Artificial intelligence; Color correction; Image translation; Feature matching; Computer vision; Robotics; Computer science; Underwater,,,,,http://dspace.kci.go.kr/handle/kci/1430355 https://link.springer.com/article/10.1007/s12555-019-0689-x https://jglobal.jst.go.jp/en/detail?JGLOBAL_ID=202002221593010188,http://dx.doi.org/10.1007/s12555-019-0689-x,,10.1007/s12555-019-0689-x,3006747402,,0,001-024-805-670-427; 005-984-508-842-599; 006-509-055-201-185; 006-825-696-554-348; 007-473-742-159-032; 007-757-171-343-032; 010-450-515-477-109; 020-233-013-143-936; 020-488-748-695-701; 020-912-686-981-082; 029-224-009-885-681; 041-424-813-744-266; 042-845-284-598-479; 044-149-876-777-861; 059-038-254-421-097; 068-446-370-600-681; 068-583-155-248-313; 071-275-406-999-255; 071-521-520-071-558; 074-659-331-213-319; 080-650-787-283-784; 081-942-344-112-16X; 107-620-316-209-845; 113-230-865-482-610; 139-813-293-606-419; 142-878-073-587-039; 160-928-271-842-630; 172-737-331-805-203; 190-878-387-956-82X; 192-697-687-929-866,20,false,,
009-097-030-325-625,Optical flow based obstacle avoidance for the visually impaired,,2012,conference proceedings article,"2012 IEEE Business, Engineering & Industrial Applications Colloquium (BEIAC)",,IEEE,,D. K. Liyanage; M.U.S. Perera,"Vision is a vital cue for human navigation. Thus, visually impaired people encounter many challenges in day-today travelling. Identifying and avoiding obstacles in the environment is the most crucial among them. To empower blind navigation, numerous electronic travel aids were created in the past few decades, by using various obstacle sensing technologies such as sonar, infrared, and stereo vision. However, optical flow estimations based navigation, which is heavily used by insects and experimented in the field of robotics, has not been used in them. This project aimed to evaluate the potential optical flow estimation based techniques has in guiding a visually impaired person to avoid obstacles using auditory and tactile feedback. To demonstrate the researched core concepts, a prototype consisting of a virtual reality world was designed and developed. It also employs an existing optical flow algorithm for motion estimation, other image processing techniques, speech synthesis for auditory feedback and embedded programming for tactile feedback. The modular design of the prototype enables it to be used either in simulation mode, or as a standalone application in a real world environment. This work demonstrates the attractive possibilities of using optical flow estimations for visually impaired navigation.",,,284,289,Motion estimation; Engineering; Artificial intelligence; Optical flow; Obstacle; Auditory feedback; Computer vision; Robotics; Obstacle avoidance; Stereopsis; Image processing,,,,,http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.ieee-000006226068 https://ieeexplore.ieee.org/document/6226068/ http://dlib.iit.ac.lk/xmlui/handle/123456789/72,http://dx.doi.org/10.1109/beiac.2012.6226068,,10.1109/beiac.2012.6226068,2007885428,,3,001-076-828-128-846; 001-238-012-944-030; 008-476-566-427-877; 008-868-208-096-513; 013-056-435-015-274; 013-847-555-426-159; 020-637-948-609-616; 021-184-857-172-84X; 036-280-949-293-766; 037-322-633-453-662; 037-491-194-003-965; 037-549-560-243-360; 066-470-164-025-396; 071-473-658-566-47X; 077-882-518-530-163; 092-706-279-141-363; 096-228-788-502-069; 127-483-973-270-088; 134-495-725-289-224,19,false,,
009-378-740-021-387,Roboscan: a combined 2D and 3D vision system for improved speed and flexibility in pick-and-place operation,2013-07-11,2013,journal article,The International Journal of Advanced Manufacturing Technology,02683768; 14333015,Springer Science and Business Media LLC,Germany,Paolo Bellandi; Franco Docchio; Giovanna Sansoni,,69,5,1873,1886,Engineering; Artificial intelligence; Template matching; SMT placement equipment; Projector; Computer vision; Machine vision; Triangulation (computer vision); Pattern matching; Robot; Orientation (computer vision),,,,,https://link.springer.com/article/10.1007/s00170-013-5138-z https://core.ac.uk/display/53617635 https://dialnet.unirioja.es/servlet/articulo?codigo=5499094,http://dx.doi.org/10.1007/s00170-013-5138-z,,10.1007/s00170-013-5138-z,2076372420,,0,007-291-971-421-228; 016-450-689-491-427; 019-391-893-766-661; 020-748-036-840-358; 042-588-436-277-363; 049-523-954-678-658; 051-537-926-980-189; 059-328-254-271-872; 072-095-645-763-942; 073-617-683-463-790; 079-564-163-321-125; 090-779-277-344-794; 100-839-410-281-300; 102-849-290-060-125; 103-881-071-813-736; 103-938-330-459-474; 110-759-510-999-435; 121-340-418-949-802; 124-025-845-598-030; 141-486-565-886-079; 150-934-227-763-087; 158-747-326-567-775; 182-018-490-989-080; 186-547-770-771-114,20,false,,
009-419-505-638-721,When will the blind be able to take their first steps with GDR guidance under artificial intelligence?,2024-04-01,2024,journal article,AI & SOCIETY,09515666; 14355655,Springer Science and Business Media LLC,Germany,Meimei Chen; Bin Hong,,,,,,Performing arts; Artificial intelligence; Psychology; Computer science; Cognitive science; Engineering; Visual arts; Art,,,,,https://link.springer.com/content/pdf/10.1007/s00146-024-01912-4.pdf https://doi.org/10.1007/s00146-024-01912-4,http://dx.doi.org/10.1007/s00146-024-01912-4,,10.1007/s00146-024-01912-4,,,0,,0,true,,bronze
009-425-707-683-513,Benefits of thermal and distance-filtered imaging for wayfinding with prosthetic vision.,2024-01-15,2024,journal article,Scientific reports,20452322,Springer Science and Business Media LLC,United Kingdom,Roksana Sadeghi; Arathy Kartha; Michael P Barry; Paul Gibson; Avi Caspi; Arup Roy; Duane R Geruschat; Gislin Dagnelie,"Visual prostheses such as the Argus II provide partial vision for individuals with limited or no light perception. However, their effectiveness in daily life situations is limited by scene complexity and variability. We investigated whether additional image processing techniques could improve mobility performance in everyday indoor environments. A mobile system connected to the Argus II provided thermal or distance-filtered video stimulation. Four participants used the thermal camera to locate a person and the distance filter to navigate a hallway with obstacles. The thermal camera allowed for finding a target person in 99% of trials, while unfiltered video led to confusion with other objects and a success rate of only 55% ([Formula: see text]). Similarly, the distance filter enabled participants to detect and avoid 88% of obstacles by removing background clutter, whereas unfiltered video resulted in a detection rate of only 10% ([Formula: see text]). For any given elapsed time, the success rate with filtered video was higher than with unfiltered video. After 90 s, participants' success rate reached above 50% with filtered video and 24% and 3% with normal camera in the first and second tasks, respectively. Despite individual variations, all participants showed significant improvement when using the thermal and distance filters compared to unfiltered video. Adding a thermal and distance filter to a visual prosthesis system can enhance the performance of mobility activities by removing clutter in the background, showing people and warm objects with the thermal camera, or nearby obstacles with the distance filter.",14,1,1313,,Computer science; Artificial intelligence; Computer vision; Algorithm,,"Humans; Visual Prosthesis; Prosthesis Implantation; Vision Disorders; Image Processing, Computer-Assisted; Diagnostic Imaging",,NEI NIH HHS (R44 EY024498) United States,https://www.nature.com/articles/s41598-024-51798-x.pdf https://doi.org/10.1038/s41598-024-51798-x,http://dx.doi.org/10.1038/s41598-024-51798-x,38225344,10.1038/s41598-024-51798-x,,PMC10789760,0,002-309-369-097-681; 003-696-604-838-62X; 009-986-494-239-653; 019-880-694-532-640; 020-646-167-504-39X; 024-144-206-344-655; 025-559-633-265-274; 033-224-758-180-923; 034-138-145-625-518; 040-216-420-272-699; 043-817-286-698-893; 051-454-925-419-994; 054-684-802-520-18X; 055-438-211-287-323; 055-924-784-640-60X; 059-910-708-580-797; 070-370-012-873-763; 079-574-573-024-51X; 103-865-753-211-046; 109-892-197-942-092; 127-817-632-406-052; 153-727-072-743-758,1,true,"CC BY, CC BY-NC-ND",gold
009-500-285-234-079,Smart Visual Assistance for Visually Impaired,2022-03-01,2022,journal article,ACS Journal for Science and Engineering,25829610,Shanlax International Journals,,null Neetha Das; null Prajwal V; null Pranav K Bharadwaj; null Punith Kumar J; null Rahul V,"<jats:p>The visually impaired personnel face a variety problem in their day to day life. They come under very challenging situation on a daily basis. Walking with a stick and confusion in mind due to the trial and error method making it very hard to navigate in unfamiliar locations. With the advent of technology in this data driven world, the barrier for blind society can be cut down with innovative adaptations in technologies such as machine learning, deep learning and computer vision.&#x0D;; QR codes can be used to inform some familiarized locations giving quick response in a closed environment for the blind person. This makes it is possible to help navigate the blind person in a closed environment.&#x0D;; The project attempts to help the blind people to get comfortable and be confident in unfamiliar locations through a speech assistive system. The projects consist of 3 modules namely:(i) Object Detection and positioning.(ii) Text to Speech Conversion.&#x0D;; The project helps the user to find obstacles in his path and avoid them, it also makes it possible to locates some of the locations in a closed environment, this is done with the help of CNNs and some APIs like Opencv, Yolo and Tensorflow, QRCode, pyzbar (used for generation of qrcode). The detected items are reported to the user in the form of speech with the help of some python libraries like pyttsx3.</jats:p>",2,1,41,62,Computer science; Python (programming language); Human–computer interaction; Confusion; Visually impaired; Artificial intelligence; Visual impairment; Multimedia; Computer vision; Psychology; Psychoanalysis; Operating system; Psychiatry,,,,,http://acsjse.in/index.php/acsjse/article/download/27/21 https://doi.org/10.34293/acsjse.v2i1.27,http://dx.doi.org/10.34293/acsjse.v2i1.27,,10.34293/acsjse.v2i1.27,,,0,,0,true,cc-by-sa,hybrid
009-576-310-142-055,Exploring Audio Interfaces for Vertical Guidance in Augmented Reality via Hand-Based Feedback.,2024-04-19,2024,journal article,IEEE transactions on visualization and computer graphics,19410506; 10772626; 21609306,Institute of Electrical and Electronics Engineers (IEEE),United States,Renan Guarese; Emma Pretty; Aidan Renata; Deb Polson; Fabio Zambetta,"This research proposes an evaluation of pitch-based sonification methods via user experiments in real-life scenarios, specifically vertical guidance, with the aim of standardizing the use of audio interfaces in AR in guidance tasks. Using literature on assistive technology for people who are blind or visually impaired, we aim to generalize their applicability to a broader population and for different use cases. We propose and test sonification methods for vertical guidance in a series of hand-navigation assessments with users without visual feedback. Including feedback from a visually impaired expert in digital accessibility, results (N=19) outlined that methods that do not rely on memorizing pitch had the most promising accuracy and self-reported workload performances. Ultimately, we argue for audio AR's ability to enhance user performance in different scenarios, from video games to finding objects in a pantry.",30,5,2818,2828,Sonification; Computer science; Audio feedback; Human–computer interaction; Workload; Population; Augmented reality; Multimedia; Artificial intelligence; Physics; Demography; Sociology; Acoustics; Operating system,,,,Australian Technology Network of Universities; RMIT,,http://dx.doi.org/10.1109/tvcg.2024.3372040,38437120,10.1109/tvcg.2024.3372040,,,0,000-200-715-120-003; 001-379-048-700-15X; 003-336-865-939-489; 003-590-526-843-953; 007-132-957-762-730; 010-760-250-306-833; 011-021-749-873-551; 011-797-116-120-363; 012-723-750-075-242; 015-548-194-367-749; 016-559-883-595-181; 021-381-798-427-728; 021-530-756-916-351; 023-029-400-817-751; 024-331-031-810-75X; 028-683-545-086-724; 031-820-578-198-964; 032-717-020-313-936; 037-972-364-518-409; 038-103-027-460-199; 042-413-798-916-80X; 042-743-736-746-923; 043-622-268-441-63X; 049-873-592-491-836; 052-154-104-302-344; 052-469-793-711-827; 055-761-882-196-422; 058-081-424-637-980; 058-385-494-485-698; 059-125-673-835-249; 059-644-292-671-783; 062-583-659-619-79X; 063-566-179-472-567; 064-629-046-703-060; 067-371-896-140-851; 068-163-377-582-711; 068-582-210-945-812; 069-226-894-925-753; 069-689-572-820-466; 073-390-338-582-488; 079-110-056-027-493; 079-966-361-805-173; 082-172-375-360-711; 083-658-428-460-801; 084-025-651-251-507; 085-188-114-318-57X; 086-269-577-982-531; 089-644-778-089-986; 091-391-418-720-681; 102-411-582-232-885; 103-774-457-019-603; 108-250-372-373-579; 111-587-471-971-605; 113-807-297-754-203; 115-402-897-374-404; 127-053-152-069-344; 127-971-245-003-865; 133-207-564-161-274; 134-646-379-274-753; 140-613-507-034-188; 140-940-230-860-580; 150-417-479-948-584; 154-074-069-073-221; 154-089-472-332-328; 156-214-189-834-050; 161-633-237-382-181; 163-402-189-030-96X; 172-140-288-200-10X; 176-502-346-995-125; 178-962-326-858-49X; 180-921-337-606-619; 193-123-211-751-082,1,false,,
009-613-419-125-881,Extra-Ordinary Human–Machine Interaction: What can be Learned from People with Disabilities?,1999-09-01,1999,journal article,"Cognition, Technology & Work",14355558; 14355566,Springer Science and Business Media LLC,Germany,Alan F. Newell; Peter Gregor,,1,2,78,85,Internet privacy; Engineering; Industrial and organizational psychology; Human factors and ergonomics; Legislation; Workforce; Workload; Population; Poison control; Computer security; Suicide prevention,,,,,https://www.safetylit.org/citations/index.php?fuseaction=citations.viewdetails&citationIds[]=citjournalarticle_407641_38 https://link.springer.com/article/10.1007/s101110050034 https://dblp.uni-trier.de/db/journals/ctw/ctw1.html#NewellG99,http://dx.doi.org/10.1007/s101110050034,,10.1007/s101110050034,1987689167,,0,006-383-919-912-064; 041-455-443-340-820; 062-629-668-582-838; 065-410-346-361-787; 070-875-178-006-938; 092-645-402-737-79X; 121-202-499-155-119; 127-655-973-982-972; 135-521-929-530-377; 140-491-641-326-728; 144-958-848-696-672; 161-194-730-323-845,32,false,,
009-615-489-379-050,Research Paper on Blind Assist System Using Ml And Image Processing,2024-05-17,2024,journal article,International Journal of Research Publication and Reviews,25827421,Genesis Global Publication,,Prof. Deepu G; Adarsh N M; Bhuvan R; Darshan A; Darshan R,"This paper presents a Blind Assist System (BAS) leveraging Machine Learning (ML) and Image Processing (IP) techniques to enhance the autonomy and safety of visually impaired individuals.The system utilizes a convolutional neural network (CNN) to process real-time image inputs from a wearable camera device.Through ML classification, it identifies objects, obstacles, and environmental cues.The IP module further refines data, providing depth perception and spatial awareness.Leveraging ML's adaptability, the system continuously learns and improves its recognition accuracy.Integration with auditory feedback facilitates intuitive interaction, conveying vital information to users.In evaluations, the BAS demonstrates promising results in aiding navigation and increasing users' independence.The fusion of ML and IP offers a robust solution for empowering the visually impaired in navigating complex environments.",5,5,9253,9256,Computer science; Image processing; Computer vision; Artificial intelligence; Image (mathematics); Computer graphics (images),,,,,,http://dx.doi.org/10.55248/gengpi.5.0524.1356,,10.55248/gengpi.5.0524.1356,,,0,,0,true,,bronze
009-645-272-092-857,ASSETS - EYECane: navigating with camera embedded white cane for visually impaired person,2009-10-25,2009,conference proceedings article,Proceedings of the 11th international ACM SIGACCESS conference on Computers and accessibility,,ACM,,Jin Sun Ju; Eunjeong Ko; Eun Yi Kim,"We demonstrate a novel assistive device which can help the visually impaired or blind people to gain more safe mobility, which is called as ""EYECane"". The EYECane is the white-cane with embedding a camera and a computer. It automatically detects obstacles and recommends some avoidable paths to the user through acoustic interface. For this, it is performed by three steps: Firstly, it extracts obstacles from image streaming using online background estimation, thereafter generates the occupancy grid map, which is given to neural network. Finally, the system notifies a user of an paths recommended by machine learning. To assess the effectiveness of the proposed EYECane, it was tested with 5 users and the results show that it can support more safe navigation, and diminish the practice and efforts to be adept in using the white cane.",,,237,238,Interface (computing); Artificial intelligence; White cane; Assistive device; Visually impaired; Computer vision; Computer science; Artificial neural network; Occupancy grid mapping,,,,,http://portal.acm.org/citation.cfm?doid=1639642.1639693 http://dblp.uni-trier.de/db/conf/assets/assets2009.html#JuKK09 https://doi.org/10.1145/1639642.1639693 https://dblp.uni-trier.de/db/conf/assets/assets2009.html#JuKK09 https://dl.acm.org/doi/10.1145/1639642.1639693 https://dl.acm.org/citation.cfm?doid=1639642.1639693,http://dx.doi.org/10.1145/1639642.1639693,,10.1145/1639642.1639693,2019019739,,0,048-063-380-760-239; 065-992-777-809-97X; 076-406-569-127-303; 186-412-044-017-67X,26,false,,
009-682-527-269-03X,Structural properties of spatial representations in blind people: Scanning images constructed from haptic exploration or from locomotion in a 3-D audio virtual environment.,,2010,journal article,Memory & cognition,15325946; 0090502x,Springer Science and Business Media LLC,United States,Amandine Afonso; Alan Blum; Brian F. G. Katz; Philippe Tarroux; Grégoire Borst; Michel Denis,"When people scan mental images, their response times increase linearly with increases in the distance to be scanned, which is generally taken as reflecting the fact that their internal representations incorporate the metric properties of the corresponding objects. In view of this finding, we investigated the structural properties of spatial mental images created from nonvisual sources in three groups (blindfolded sighted, late blind, and congenitally blind). In Experiment 1, blindfolded sighted and late blind participants created metrically accurate spatial representations of a small-scale spatial configuration under both verbal and haptic learning conditions. In Experiment 2, late and congenitally blind participants generated accurate spatial mental images after both verbal and locomotor learning of a full-scale navigable space (created by an immersive audio virtual reality system), whereas blindfolded sighted participants were selectively impaired in their ability to generate precise spatial representations from locomotor experience. These results attest that in the context of a permanent lack of sight, encoding spatial information on the basis of the most reliable currently functional system (the sensorimotor system) is crucial for building a metrically accurate representation of a spatial environment. The results also highlight the potential of spatialized audio-rendering technology for exploring the spatial representations of visually impaired participants.",38,5,591,604,Haptic technology; Artificial intelligence; Psychology; Spatial analysis; Virtual reality; Perception; Context (language use); Computer vision; Stereognosis; Mental image; Communication; Concept learning,,Adult; Blindness/psychology; Concept Formation; Distance Perception; Female; Humans; Imagination; Locomotion; Male; Middle Aged; Reaction Time; Sensory Deprivation; Social Environment; Space Perception; Speech Perception; Stereognosis; User-Computer Interface; Young Adult,,,https://pubmed.ncbi.nlm.nih.gov/20551339/ https://www.ncbi.nlm.nih.gov/pubmed/20551339 https://europepmc.org/article/MED/20551339 https://www.questia.com/library/journal/1P3-2089535651/structural-properties-of-spatial-representations-in https://link.springer.com/article/10.3758/MC.38.5.591 https://link.springer.com/content/pdf/10.3758/MC.38.5.591.pdf http://www.micheldenis.fr/wp-content/uploads/2011/05/2010-MC.pdf,http://dx.doi.org/10.3758/mc.38.5.591,20551339,10.3758/mc.38.5.591,2090603057,,0,000-009-769-121-794; 000-659-254-096-600; 002-748-287-818-793; 006-813-235-552-771; 009-943-096-722-260; 011-159-707-931-081; 016-795-216-845-76X; 025-109-296-128-130; 025-501-666-878-788; 026-787-153-358-181; 028-643-991-515-625; 028-962-012-557-783; 031-643-110-129-889; 036-412-383-735-458; 037-389-881-152-996; 047-276-304-008-427; 047-969-307-899-765; 050-036-996-905-385; 050-872-860-831-851; 052-042-701-176-044; 053-373-073-400-553; 056-691-132-979-140; 057-858-896-548-189; 062-735-025-685-031; 063-465-393-399-185; 064-852-119-630-133; 067-864-218-348-667; 067-892-061-809-776; 070-238-608-444-718; 070-239-186-295-808; 075-497-014-828-317; 077-219-180-539-193; 079-090-335-218-377; 080-531-636-802-67X; 080-909-262-690-790; 085-813-873-031-335; 092-790-777-755-979; 093-500-669-863-559; 104-123-334-751-180; 109-912-280-570-475; 112-672-324-422-609; 114-702-499-529-480; 123-308-755-543-996; 133-289-426-096-349; 144-389-164-478-765; 154-751-089-924-826; 156-622-609-489-971; 159-885-799-496-243; 172-196-206-416-533; 188-006-649-973-495; 190-621-332-208-185; 192-136-128-163-163; 192-409-020-335-868; 192-456-421-826-264,72,true,,bronze
009-748-285-068-532,Assistive Navigation Using Deep Reinforcement Learning Guiding Robot With UWB/Voice Beacons and Semantic Feedbacks for Blind and Visually Impaired People.,2021-06-22,2021,journal article,Frontiers in robotics and AI,22969144,Frontiers Media SA,Switzerland,Chen Lung Lu; Zi Yan Liu; Jui-Te Huang; Ching-I Huang; Bo Hui Wang; Yi Chen; Nien Hsin Wu; Hsueh-Cheng Wang; Laura Giarre; Pei-Yi Kuo,"Facilitating navigation in pedestrian environments is critical for enabling people who are blind and visually impaired (BVI) to achieve independent mobility. A deep reinforcement learning (DRL)–based assistive guiding robot with ultrawide-bandwidth (UWB) beacons that can navigate through routes with designated waypoints was designed in this study. Typically, a simultaneous localization and mapping (SLAM) framework is used to estimate the robot pose and navigational goal; however, SLAM frameworks are vulnerable in certain dynamic environments. The proposed navigation method is a learning approach based on state-of-the-art DRL and can effectively avoid obstacles. When used with UWB beacons, the proposed strategy is suitable for environments with dynamic pedestrians. We also designed a handle device with an audio interface that enables BVI users to interact with the guiding robot through intuitive feedback. The UWB beacons were installed with an audio interface to to obtain environmental information. The on-handle and on-beacon verbal feedback provides points of interests and turn-by-turn information to BVI users. BVI users were recruited in this study to conduct navigation tasks in different scenarios. A route was designed in a simulated ward to represent daily activities. In real-world situations, SLAM-based state state estimation might be affected by dynamic obstacles, and the visual-based trail may suffer from occlusions from pedestrians or other obstacles. The proposed system successfully navigated through environments with dynamic pedestrians, in which systems based on existing SLAM algorithms have failed.",8,,654132,654132,Beacon; Human–computer interaction; Interface (computing); Pedestrian; Visually impaired; Computer science; State (computer science); Simultaneous localization and mapping; Robot; Reinforcement learning,UWB beacon; blind and visually impaired; deep reinforcement learning; guiding robot; indoor navigation; navigation; verbal instruction,,,"Ministry of Science and Technology, Taiwan; Qualcomm",https://scholar.nycu.edu.tw/en/publications/assistive-navigation-using-deep-reinforcement-learning-guiding-ro https://europepmc.org/article/PMC/PMC8258111 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8258111 https://www.frontiersin.org/articles/10.3389/frobt.2021.654132/full https://dblp.uni-trier.de/db/journals/firai/firai8.html#LuLHHWCWWGK21 https://iris.unimore.it/handle/11380/1245875,http://dx.doi.org/10.3389/frobt.2021.654132,34239900,10.3389/frobt.2021.654132,3165654247,PMC8258111,0,001-996-430-142-340; 006-097-745-051-390; 006-805-229-333-310; 013-527-855-338-983; 026-672-312-718-982; 028-460-767-882-226; 028-595-872-311-095; 030-127-369-356-741; 039-108-632-536-824; 042-541-176-033-013; 048-724-584-671-516; 054-266-777-562-123; 064-053-186-811-574; 066-246-145-459-483; 066-465-928-804-01X; 069-523-876-223-951; 070-286-633-745-869; 104-694-342-259-021; 110-831-712-365-194; 125-836-353-927-657,18,true,cc-by,gold
009-829-120-033-790,A computer vision-based perception system for visually impaired,2016-05-21,2016,journal article,Multimedia Tools and Applications,13807501; 15737721,Springer Science and Business Media LLC,Netherlands,Ruxandra Tapu; Bogdan Mocanu; Titus Zaharia,,76,9,11771,11807,Artificial intelligence; Object detection; Obstacle; Landmark; Perception system; Computer vision; Computer science; Android (operating system); Global Positioning System,,,,,https://hal.archives-ouvertes.fr/hal-01451497 http://dblp.uni-trier.de/db/journals/mta/mta76.html#TapuMZ17 https://dblp.uni-trier.de/db/journals/mta/mta76.html#TapuMZ17 https://link.springer.com/article/10.1007/s11042-016-3617-6/fulltext.html https://link.springer.com/10.1007/s11042-016-3617-6 https://link.springer.com/article/10.1007/s11042-016-3617-6,http://dx.doi.org/10.1007/s11042-016-3617-6,,10.1007/s11042-016-3617-6,2401611213,,0,000-631-028-807-828; 001-076-828-128-846; 006-322-344-260-050; 006-757-908-435-833; 006-885-886-738-097; 006-900-457-124-049; 007-527-228-704-510; 009-612-733-037-965; 009-653-756-956-877; 010-719-927-108-990; 012-179-568-585-098; 012-642-196-605-159; 014-157-934-526-940; 015-998-086-943-276; 018-036-210-381-349; 021-216-022-389-446; 023-438-345-162-875; 023-868-294-867-153; 023-904-810-846-451; 025-719-251-064-370; 025-733-573-160-622; 028-037-989-331-413; 029-083-674-347-975; 029-301-905-679-081; 030-440-500-148-114; 030-767-617-573-338; 034-370-857-470-389; 034-551-334-308-190; 035-466-732-882-016; 035-727-224-551-125; 036-280-949-293-766; 038-191-359-405-869; 039-274-384-165-289; 040-805-793-656-420; 046-602-555-403-557; 047-748-096-379-970; 048-395-692-930-517; 050-397-619-218-217; 053-574-289-747-231; 056-357-720-667-801; 057-793-015-492-337; 058-856-467-394-924; 060-884-484-928-318; 065-573-549-553-726; 066-133-135-091-645; 066-526-177-885-125; 067-867-064-779-555; 068-708-501-300-740; 071-583-782-423-553; 074-006-536-890-200; 075-903-419-661-27X; 080-062-035-851-274; 080-099-992-812-436; 084-679-921-430-947; 085-507-633-373-250; 085-608-142-942-300; 087-105-444-728-025; 097-213-962-358-002; 098-440-015-522-27X; 099-643-392-623-259; 107-528-931-773-18X; 109-224-276-116-641; 116-186-710-453-990; 124-468-387-518-672; 127-772-927-997-658; 130-321-222-024-616; 138-774-270-601-871; 139-403-362-393-17X; 145-898-655-711-084; 170-387-846-090-147; 170-766-759-348-189; 181-797-068-816-79X; 183-935-933-940-273; 189-411-480-672-384; 192-623-080-180-332; 192-751-999-674-131,23,false,,
009-896-970-982-095,An ARCore Based User Centric Assistive Navigation System for Visually Impaired People,2019-03-09,2019,journal article,Applied Sciences,20763417,MDPI AG,,Xiaochen Zhang; Xiaoyu Yao; Yi Zhu; Fei Hu,"In this work, we propose an assistive navigation system for visually impaired people (ANSVIP) that takes advantage of ARCore to acquire robust computer vision-based localization. To complete the system, we propose adaptive artificial potential field (AAPF) path planning that considers both efficiency and safety. We also propose a dual-channel human–machine interaction mechanism, which delivers accurate and continuous directional micro-instruction via a haptic interface and macro-long-term planning and situational awareness via audio. Our system user-centrically incorporates haptic interfaces to provide fluent and continuous guidance superior to the conventional turn-by-turn audio-guiding method; moreover, the continuous guidance makes the path under complete control in avoiding obstacles and risky places. The system prototype is implemented with full functionality. Unit tests and simulations are conducted to evaluate the localization, path planning, and human–machine interactions, and the results show that the proposed solutions are superior to those of the present state-of-the-art solutions. Finally, integrated tests are carried out with low-vision and blind subjects to verify the proposed system.",9,5,989,,Human–computer interaction; Situation awareness; Unit testing; Haptic technology; User-centered design; Control (management); Navigation system; PATH (variable); Computer science; Motion planning,,,,Humanity and Social Science Youth foundation of the Ministry of Education of China,https://www.mdpi.com/2076-3417/9/5/989/pdf https://www.mdpi.com/2076-3417/9/5/989 https://doaj.org/article/4a492896d48642f49d0e02e92392cdaf,http://dx.doi.org/10.3390/app9050989,,10.3390/app9050989,2920946314,,0,002-279-515-155-647; 005-497-078-814-142; 006-792-447-761-719; 007-223-103-783-187; 007-305-095-446-752; 008-199-346-698-808; 012-106-175-644-619; 016-233-703-646-667; 016-427-148-905-29X; 019-828-744-033-99X; 021-120-692-570-758; 027-720-558-803-335; 033-245-290-160-646; 049-485-542-422-096; 053-313-869-081-24X; 063-204-152-739-454; 065-078-747-021-140; 067-983-077-920-296; 072-500-681-304-715; 075-827-600-908-546; 083-299-642-195-322; 092-374-505-118-21X; 092-923-598-568-103; 098-494-228-486-413; 116-764-383-274-937; 118-628-165-016-127; 152-183-517-423-628,40,true,cc-by,gold
009-907-497-152-037,Wayfinding tasks in visually impaired people: the role of tactile maps,2006-08-08,2006,journal article,Cognitive Processing,16124782; 16124790,Springer Science and Business Media LLC,Germany,Pierluigi Caddeo; Ferdinando Fornara; A. M. Nenci; Amelia Piroddi,,7,1,168,169,Encoding (memory); Psychology; Cognitive map; Cognitive complexity; Cognitive psychology; Blindness; Cognitive patterns; Visually impaired; Specific population; Behavioural sciences,,,,,https://core.ac.uk/display/54607239 https://www.infona.pl/resource/bwmeta1.element.springer-1018a440-2853-3cb3-a928-c49e13948405 https://link.springer.com/article/10.1007/s10339-006-0128-9,http://dx.doi.org/10.1007/s10339-006-0128-9,,10.1007/s10339-006-0128-9,1976070335,,0,007-408-212-113-300; 013-678-295-106-102; 027-135-179-205-530; 055-481-145-387-599; 085-207-379-891-276; 094-763-081-817-982; 097-145-406-647-709; 117-223-181-806-311; 147-734-220-180-60X; 154-633-836-958-329; 157-193-965-055-591; 170-269-561-937-933,28,false,,
010-231-330-383-19X,Image Processing and Display,2012-05-24,2012,journal article,International Journal of Computer Assisted Radiology and Surgery,18616410; 18616429,Springer Science and Business Media LLC,Germany,,,7,S1,71,88,,,,,,,http://dx.doi.org/10.1007/s11548-012-0710-9,,10.1007/s11548-012-0710-9,,,0,,0,false,,
010-237-806-158-244,Sonification and interaction design in computer games for visually impaired individuals,2022-01-28,2022,journal article,Multimedia Tools and Applications,13807501; 15737721,Springer Science and Business Media LLC,Netherlands,Yoones A. Sekhavat; Mohammad Reza Azadehfar; Hossein Zarei; Samad Roohi,,81,6,7847,7871,Sonification; Computer science; Visually impaired; Human–computer interaction; Set (abstract data type); Focus (optics); Multimedia; Game design; Game mechanics; Video game; Optics; Physics; Programming language,,,,,,http://dx.doi.org/10.1007/s11042-022-11984-3,,10.1007/s11042-022-11984-3,,,0,002-646-377-924-410; 003-485-284-705-210; 003-900-973-806-936; 004-013-457-445-703; 006-003-167-773-438; 014-984-968-440-511; 021-016-840-171-389; 023-578-851-301-713; 024-523-991-572-417; 026-426-106-254-255; 028-020-604-458-341; 028-460-807-397-699; 033-377-454-485-098; 037-238-607-476-236; 038-086-832-398-63X; 039-853-000-175-960; 039-866-062-340-880; 044-239-489-717-892; 045-175-114-910-238; 050-143-285-951-784; 051-235-822-259-73X; 056-304-327-045-900; 057-083-293-597-247; 064-655-585-902-72X; 077-703-665-866-285; 080-826-552-499-501; 087-452-847-569-31X; 096-198-966-272-805; 096-822-905-308-09X; 104-094-039-147-570; 104-377-419-657-812; 109-029-947-213-124; 109-499-688-167-173; 111-502-400-852-515; 113-894-768-157-834; 125-164-846-756-383; 138-148-139-576-473; 143-030-096-983-788; 159-499-262-728-168; 161-400-895-530-88X; 165-338-796-057-75X; 181-158-829-403-578; 192-542-767-952-699,11,false,,
010-296-337-305-570,ICVGIP - Object Discrimination Using Stereo Vision for Blind through Stereo Sonification.,,2004,conference proceedings,,,,,G. Balakrishnan; G. Sainarayanan; R. Nagarajan; Sazali Yaacob,"In this paper, a development of image processing, stereovision methodology and a sonification procedure for image sonification system for vision substitution are presented. The hardware part consists of a sunglass fitted with two mini cameras, laptop computer and stereo earphones. The image of the scene in front of blind people is captured by stereo cameras. The captured image is processed to enhance the important features in the scene in front of blind user. The image processing is designed to extract the objects from the image and the stereo vision method is applied to calculate the disparity which is required to determine the distance between the blind user and the objects. The processed image is mapped on to stereo sound for the blind’s understanding of the scene in front. Experimentations were conducted in the indoor environment and the proposed methodology is found to be effective for object identification, and thus the sound produced will assists the visually impaired for their collision free navigation.",,,234,239,Stereophonic sound; Artificial intelligence; Stereo camera; Computer stereo vision; Computer vision; Stereo cameras; Computer science; Object (computer science); Stereopsis; Sonification; Image processing,,,,,http://www.cse.iitb.ac.in/~sharat/icvgip.org/icvgip2004/proceedings/cv2.11_108.pdf https://www.cse.iitb.ac.in/~sharat/icvgip.org/icvgip2004/proceedings/cv2.11_108.pdf https://dblp.uni-trier.de/db/conf/icvgip/icvgip2004.html#BalakrishnanSNY04,https://dblp.uni-trier.de/db/conf/icvgip/icvgip2004.html#BalakrishnanSNY04,,,28329360,,0,013-847-555-426-159; 021-745-032-529-768; 032-786-641-709-377; 067-009-123-253-596; 074-324-981-337-749; 078-725-425-518-672; 136-878-651-956-733; 192-600-440-415-958,0,false,,
010-307-299-039-910,Computer Vision-Based Assistive Technology for Helping Visually Impaired and Blind People Using Deep Learning Framework,,2020,book chapter,Handbook of Research on Emerging Trends and Applications of Machine Learning,23270411; 2327042x,IGI Global,,Mohamamd Farukh Hashmi; Vishal Gupta; Dheeravath Vijay; Vinaybhai Rathwa,"<jats:p>Millions of people in this world can't understand environment because they are blind or visually impaired. They also have navigation difficulties which leads to social awkwardness. They can use some other way to deal with their life and daily routines. It is very difficult for them to find something in unknown environment. Blind and visually impaired people face many difficulties in conversation because they can't decide whether the person is talking to them or someone else. Computer vision-based technologies have increased so much in this domain. Deep convolutional neural network has developed very fast in recent years. It is very helpful to use computer vision-based techniques to help the visually impaired. In this chapter, hearing is used to understand the world. Both sight sense and hearing have the same similarity: both visual object and audio can be localized. Many people don't realise that we are capable of identifying location of the source of sound by just hearing it. </jats:p>",,,577,598,Human–computer interaction; Deep learning; Artificial intelligence; Assistive technology; Visually impaired; Computer science,,,,,https://www.igi-global.com/chapter/computer-vision-based-assistive-technology-for-helping-visually-impaired-and-blind-people-using-deep-learning-framework/247582,http://dx.doi.org/10.4018/978-1-5225-9643-1.ch027,,10.4018/978-1-5225-9643-1.ch027,3004124078,,0,004-216-069-855-831; 016-688-776-196-979; 018-031-085-269-362; 023-929-624-288-020; 031-991-132-972-341; 065-746-884-359-173; 085-507-633-373-250; 103-203-884-153-381; 112-456-136-358-543; 115-431-563-967-640; 132-602-463-268-72X; 145-108-829-550-497; 154-635-342-605-83X; 156-136-348-796-661; 193-052-734-829-39X,2,false,,
010-397-872-039-027,Distributed and adaptive location identification system for mobile devices,2018-09-29,2018,journal article,EURASIP Journal on Advances in Signal Processing,16876180; 16876172,Springer Science and Business Media LLC,Germany,Fahed Awad; Aisha Al-Sadi; Fida'a Al-Quran; Abdulsalam Alsmady,"Indoor location identification and navigation need to be as simple, seamless, and ubiquitous as its outdoor GPS-based counterpart is. It would be of great convenience to the mobile user to be able to continue navigating seamlessly as he or she moves from a GPS-clear outdoor environment into an indoor environment or a GPS-obstructed outdoor environment such as a tunnel or forest. Existing infrastructure-based indoor localization systems lack such capability, on top of potentially facing several critical technical challenges such as increased cost of installation, centralization, lack of reliability, poor localization accuracy, poor adaptation to the dynamics of the surrounding environment, latency, system-level and computational complexities, repetitive labor-intensive parameter tuning, and user privacy. To this end, this paper presents a novel mechanism with the potential to overcome most (if not all) of the abovementioned challenges. The proposed mechanism is simple, distributed, adaptive, collaborative, and cost-effective. Based on the proposed algorithm, a mobile blind device can potentially utilize, as GPS-like reference nodes, either in-range location-aware compatible mobile devices or preinstalled low-cost infrastructure-less location-aware beacon nodes. The proposed approach is model-based and calibration-free that uses the received signal strength to periodically and collaboratively measure and update the radio frequency characteristics of the operating environment to estimate the distances to the reference nodes. Trilateration is then used by the blind device to identify its own location, similar to that used in the GPS-based system. Simulation and empirical testing ascertained that the proposed approach can potentially be the core of future indoor and GPS-obstructed environments.",2018,1,1,20,Mobile device; Trilateration; Identification system; Computer science; Global Positioning System; Radio frequency; Real-time computing; Location-based service; Reliability (computer networking); Identification (information); Adaptation (computer science),,,,"Deanship of Scientific Research, Jordan University of Science and Technology",https://arxiv.org/abs/1810.07093 http://arxiv.org/abs/1810.07093,http://dx.doi.org/10.1186/s13634-018-0583-3,,10.1186/s13634-018-0583-3,3103708428; 2894118631,,0,000-881-650-594-297; 003-246-044-967-360; 003-861-794-616-214; 006-617-585-034-263; 006-913-678-628-016; 008-904-153-949-27X; 012-387-483-703-880; 015-850-021-413-767; 016-326-422-978-158; 019-297-575-277-335; 025-158-376-051-218; 028-707-243-078-925; 029-088-871-719-574; 039-550-419-384-018; 042-913-446-548-549; 045-599-200-378-144; 049-935-096-600-125; 051-444-994-733-072; 051-699-174-088-834; 054-940-192-770-818; 059-753-475-851-181; 064-335-767-440-543; 068-171-283-435-390; 068-904-963-609-169; 087-541-943-621-733; 089-396-009-071-212; 093-152-291-031-970; 096-257-269-482-513; 100-373-023-313-120; 103-919-330-733-441; 106-067-134-580-945; 116-475-192-657-774; 116-826-769-121-520; 120-974-658-987-160; 132-901-057-428-425; 148-300-738-962-057; 152-559-698-986-800; 166-253-361-552-149; 175-966-273-561-519,7,true,cc-by,gold
010-475-144-749-494,"An insight into smartphone-based assistive solutions for visually impaired and blind people: issues, challenges and opportunities",2020-07-04,2020,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Akif Khan; Shah Khusro,,20,2,265,298,Human–computer interaction; Interface (computing); Digital artifact; Usability; Quality of life (healthcare); Computer science; Reading (process); Information and Communications Technology; Identification (information); User interface,,,,,https://doi.org/10.1007/s10209-020-00733-8 https://link.springer.com/article/10.1007/s10209-020-00733-8,http://dx.doi.org/10.1007/s10209-020-00733-8,,10.1007/s10209-020-00733-8,3040344224,,1,000-082-081-025-981; 000-203-762-529-999; 000-394-661-764-185; 000-631-028-807-828; 000-661-213-569-240; 001-271-962-416-922; 002-214-697-649-842; 002-881-811-869-46X; 003-381-443-624-236; 003-851-317-359-260; 004-017-242-451-24X; 004-315-451-874-148; 004-687-536-912-016; 004-754-524-837-052; 005-868-265-379-414; 006-468-563-128-553; 007-530-851-453-706; 008-055-736-186-186; 008-529-151-900-067; 009-063-048-778-483; 010-004-081-294-240; 011-214-253-974-00X; 011-242-495-023-707; 012-012-588-838-414; 013-056-435-015-274; 013-847-555-426-159; 014-292-445-567-970; 015-672-084-912-775; 015-733-840-903-484; 015-940-288-390-612; 015-998-086-943-276; 016-229-543-751-991; 016-554-897-381-94X; 017-283-289-447-559; 018-031-824-051-966; 018-207-067-803-820; 018-932-338-794-332; 019-716-352-494-619; 020-374-980-887-665; 020-432-778-206-312; 020-595-034-996-880; 020-610-812-973-945; 021-727-126-739-396; 021-988-229-179-809; 022-068-972-084-237; 022-554-018-758-446; 023-680-889-449-204; 023-901-299-485-228; 023-929-624-288-020; 024-307-953-321-143; 024-674-099-372-674; 025-541-192-404-658; 026-048-436-398-74X; 026-201-335-374-333; 026-343-407-304-056; 027-560-729-787-051; 028-208-095-693-670; 028-628-140-541-774; 028-683-545-086-724; 031-210-875-362-363; 032-786-641-709-377; 032-919-430-760-558; 034-085-818-053-029; 034-412-649-320-055; 034-468-880-826-73X; 036-930-508-226-834; 036-990-754-479-620; 037-009-592-749-57X; 037-491-194-003-965; 037-944-441-669-318; 038-218-082-706-921; 038-634-214-371-151; 039-079-110-624-640; 039-216-012-650-106; 039-901-769-379-747; 040-477-605-889-113; 040-587-507-778-679; 040-660-173-610-646; 041-870-993-176-373; 042-510-416-329-584; 043-032-180-943-707; 043-342-420-332-512; 044-100-085-081-967; 044-202-560-313-315; 044-626-863-942-345; 045-874-715-328-967; 046-751-313-876-545; 048-860-102-902-323; 049-106-304-885-412; 049-139-218-880-325; 049-592-217-207-783; 051-065-738-489-461; 052-760-843-793-28X; 053-891-072-061-705; 054-422-119-288-064; 054-546-267-804-934; 054-605-556-759-639; 055-161-122-045-008; 056-129-148-543-44X; 057-329-027-733-178; 058-125-653-234-933; 058-268-022-053-157; 058-468-795-176-403; 059-086-041-229-241; 060-041-403-626-524; 060-585-490-761-454; 060-607-579-778-839; 060-735-096-307-06X; 060-910-198-172-157; 061-162-317-464-407; 061-517-830-110-36X; 061-773-125-131-133; 061-978-196-805-368; 062-715-835-732-081; 063-087-812-753-009; 063-372-733-367-946; 064-340-993-009-029; 064-389-656-713-653; 065-161-528-162-397; 065-379-722-350-422; 067-896-486-746-035; 068-689-734-088-575; 071-858-943-169-534; 073-785-034-209-779; 073-928-284-197-323; 074-346-457-235-698; 074-347-246-926-274; 074-521-433-394-593; 074-617-975-121-799; 075-330-543-726-901; 075-605-492-120-394; 077-352-552-120-40X; 079-159-926-330-670; 079-768-517-522-78X; 080-809-532-346-829; 081-405-957-749-776; 081-919-448-045-779; 082-276-990-325-922; 082-535-958-182-267; 083-557-246-909-564; 083-870-902-629-28X; 084-164-366-835-482; 084-623-553-306-893; 086-143-814-143-149; 086-529-566-980-221; 088-470-436-866-921; 090-069-354-521-410; 090-293-924-539-273; 090-884-510-770-832; 091-929-200-182-069; 092-391-305-190-481; 092-645-402-737-79X; 093-925-865-515-918; 094-763-081-817-982; 094-838-611-687-737; 098-844-948-463-735; 099-081-268-683-922; 099-167-286-671-498; 101-191-198-207-265; 102-263-736-254-576; 102-952-660-914-374; 103-185-330-791-684; 106-042-379-293-927; 107-526-230-054-474; 107-672-498-862-363; 108-825-903-509-346; 109-167-882-345-577; 110-409-001-674-560; 110-612-663-262-18X; 113-220-341-591-382; 115-594-732-723-118; 116-372-526-448-314; 119-526-303-815-066; 121-018-696-271-637; 122-289-852-805-364; 122-437-856-931-706; 123-501-344-440-241; 124-774-664-254-309; 125-215-400-077-068; 127-070-672-662-621; 127-583-369-096-720; 127-688-106-762-518; 127-769-262-221-224; 129-090-201-793-247; 131-016-886-187-878; 131-100-501-537-286; 132-920-664-117-745; 134-584-315-398-596; 135-786-255-909-500; 139-049-068-685-260; 141-489-442-115-69X; 142-575-622-508-48X; 147-317-681-938-156; 147-928-929-282-502; 151-015-279-636-660; 151-396-381-196-309; 158-087-806-247-948; 165-793-134-355-787; 169-284-820-727-683; 175-130-600-114-960; 178-363-336-640-342; 183-816-558-497-844; 186-675-763-128-67X; 193-134-919-907-701; 194-168-378-406-796; 197-024-748-592-925,66,false,,
010-582-518-033-890,Research on Semantics Used in GPS Based Mobile Phone Applications for Blind Pedestrian Navigation in an Outdoor Environment,,2018,conference proceedings article,2018 International Conference on Information and Communication Technology for the Muslim World (ICT4M),,IEEE,,Israh Akbar; Ahmed Fatzilah Misman,"The research is based on the study of the semantics used in GPS based mobile applications available for the blind navigation. For a blind person, the main source of information is words and how to improve the effectiveness of the explanation using machines for them is tested in this research. The long term goal of technology is to create a movable, self-contained system that allows visually impaired people to navigate through unfamiliar environments without the assistance of guides The experiment for the research took place at MFB (Malaysian Foundation for the Blind) using existing GPS based mobile applications and the results were used to prove the hypothesis that the blind are at a substantial downside because of the absence of information and providing a detailed description of the environment could be used in the GPS navigation applications for the blind pedestrians to help them navigate independently and it would enable them to take up GPS for independent navigation in unknown environment. The information derived from the research can further help in creating and improving the semantics of the GPS based navigation technology for the blind pedestrians in an unknown environment.",,,196,201,Human–computer interaction; Semantics; Mobile phone; Pedestrian navigation; Gps navigation; Visually impaired; Computer science; Radar; Global Positioning System; Term (time),,,,,http://irep.iium.edu.my/72785/ https://ieeexplore.ieee.org/document/8567120/ https://core.ac.uk/download/pdf/300474739.pdf,http://dx.doi.org/10.1109/ict4m.2018.00044,,10.1109/ict4m.2018.00044,2905095113,,0,002-391-643-288-41X; 020-601-408-107-469; 023-512-432-506-296; 029-477-876-800-602; 033-245-290-160-646; 051-845-778-402-743; 052-726-254-698-074; 075-402-037-032-739; 076-854-462-895-141; 089-247-928-551-533; 095-869-599-243-25X; 124-599-469-135-595; 125-351-519-179-754; 154-764-681-989-082; 173-171-875-940-012; 196-844-095-051-167,5,true,,green
010-862-331-275-415,Implementation of real-time positioning system using extended Kalman filter and artificial landmark on ceiling,2012-03-29,2012,journal article,Journal of Mechanical Science and Technology,1738494x; 19763824,Springer Science and Business Media LLC,South Korea,Angga Rusdinar; Jung-Min Kim; Junha Lee; Sungshin Kim,,26,3,949,958,Engineering; Artificial intelligence; Extended Kalman filter; Navigation system; Positioning system; Digital camera; Landmark; Computer vision; Odometry; Machine vision; Image processing,,,,,https://link.springer.com/article/10.1007%2Fs12206-011-1251-9 http://www.j-mst.org/On_line/admin/files/31-J2011-1367.pdf http://dspace.kci.go.kr/handle/kci/44492 https://link.springer.com/content/pdf/10.1007%2Fs12206-011-1251-9.pdf,http://dx.doi.org/10.1007/s12206-011-1251-9,,10.1007/s12206-011-1251-9,2010483416,,0,000-162-706-386-104; 001-378-971-493-620; 006-721-199-511-746; 020-047-694-485-958; 023-395-952-835-181; 027-364-421-444-804; 027-699-196-867-724; 032-313-812-641-814; 041-447-795-018-691; 053-988-462-709-053; 056-420-620-282-452; 056-589-561-320-837; 056-594-368-347-612; 060-125-651-835-016; 067-648-210-391-71X; 080-395-553-617-459; 105-851-321-962-263; 119-472-204-965-711; 120-480-525-886-120; 147-057-547-610-178; 151-744-575-459-363; 156-345-056-847-798; 157-946-366-261-215; 159-528-045-881-924; 168-447-529-399-677,33,false,,
010-887-464-918-163,Perceivers' internal state tags fixation-by-fixation visual information: An EEG-eye movement co-registration study,2015-08-27,2015,journal article,Cognitive Processing,16124782; 16124790,Springer Science and Business Media LLC,Germany,Chie Nakatani; Mojtaba Chehelcheraghi; Behnaz Jarrahi; Hironori Nakatani; Cees van Leeuwen,"Background: Previous research revealed similarities between objects’ representations stored in working memory and those formed online during concept apprehension (e.g., Moorselaar, et al., 2014) suggesting that memorial and perceptual representations are similarly grounded in sensorimotor simulation. Other studies show similar sensorimotor simulations in representations of number (van Dijck & Fias, 2011; van Dijk, et al., 2014a; Lendinez, et al., 2011), time (Bi, et al., 2014; Fischer-Baum & Benjamin, 2014), and volumetric grasp affordances (van Dijk, et al., 2014b). These reports provide support to the ATOM theory of magnitude (Walsh, 2003) suggesting an interplay between magnitude-related knowledge both in online and offline representations.; ; Aims: We investigated the interplay between magnitude-related SNARC effect and objects’ microaffordances stored in memory.; ; Method: We used the paradigm described in van Dijck et al. (2014b): Participants memorized objects with power- and precision-grip microaffordances before hearing an auditory number ([5/\5) in one of two voices. Participants were instructed to identify the voice and make a saccade: left to voice A/right to voice B in Experiment 1; up to voice A/down to voice B in experiment 2.; ; Results: Initial results confirm that memorized microaffordances interacted with the magnitude of the perceived numbers in directing visual attention: Upward and rightward saccades were initiated faster after hearing larger numbers preceded by the presentation of a powergrip object while the reverse was true for the downward and leftward saccades.",16,S1,99,99,Fixation (visual); Eye movement; Artificial intelligence; Psychology; Co registration; Computer vision; Electroencephalography; Translation (geometry); Cognitive psychology; Congenital blindness; Frame of reference; Spatial analysis; Task (project management); Communication; GRASP; Affordance; Working memory; Perception; Saccade; Apprehension; Object (philosophy); Eye tracking; Spatial cognition; Cognition; Situated cognition; Situated; Space (punctuation); Cognitive science; Computer science; Neuroscience; Operating system,,,,,https://lirias.kuleuven.be/handle/123456789/554235,http://dx.doi.org/10.1007/s10339-015-0732-7,,10.1007/s10339-015-0732-7,2561437511; 2591820600; 2340316915; 2562055048,,0,,1,true,,green
011-000-001-339-09X,14th Annual Conference of the International Society for Computer Aided Surgery,2010-06-03,2010,journal article,International Journal of Computer Assisted Radiology and Surgery,18616410; 18616429,Springer Science and Business Media LLC,Germany,,,5,S1,254,318,Computer science; General surgery; Medicine; Medical physics,,,,,,http://dx.doi.org/10.1007/s11548-010-0468-x,,10.1007/s11548-010-0468-x,,,0,,0,false,,
011-158-736-731-981,EasySVM: A visual analysis approach for open-box support vector machines,2017-03-15,2017,journal article,Computational Visual Media,20960433; 20960662,Springer Science and Business Media LLC,,Yuxin Ma; Wei Chen; Xiaohong Ma; Jiayi Xu; Xinxin Huang; Ross Maciejewski; Anthony K. H. Tung,"Support vector machines (SVMs) are supervised learning models traditionally employed for classification and regression analysis. In classification analysis, a set of training data is chosen, and each instance in the training data is assigned a categorical class. An SVM then constructs a model based on a separating plane that maximizes the margin between different classes. Despite being one of the most popular classification models because of its strong performance empirically, understanding the knowledge captured in an SVM remains difficult. SVMs are typically applied in a black-box manner where the details of parameter tuning, training, and even the final constructed model are hidden from the users. This is natural since these details are often complex and difficult to understand without proper visualization tools. However, such an approach often brings about various problems including trial-and-error tuning and suspicious users who are forced to trust these models blindly. The contribution of this paper is a visual analysis approach for building SVMs in an open-box manner. Our goal is to improve an analyst’s understanding of the SVM modeling process through a suite of visualization techniques that allow users to have full interactive visual control over the entire SVM training process. Our visual exploration tools have been developed to enable intuitive parameter tuning, training data manipulation, and rule extraction as part of the SVM training process. To demonstrate the efficacy of our approach, we conduct a case study using a real-world robot control dataset.",3,2,161,175,Machine learning; Support vector machine; Data mining; Supervised learning; Least squares support vector machine; Creative visualization; Artificial intelligence; White box; Visualization; Computer science; Categorical variable; Margin (machine learning),,,,,https://link.springer.com/article/10.1007%2Fs41095-017-0077-5 https://dc.tsinghuajournals.com/cgi/viewcontent.cgi?article=1073&context=computational-visual-media https://link.springer.com/content/pdf/10.1007%2Fs41095-017-0077-5.pdf https://www.firstacademics.com/article/10.1007/s41095-017-0077-5 https://dblp.uni-trier.de/db/journals/cvm/cvm3.html#MaCMXHMT17 https://paperity.org/p/79366841/easysvm-a-visual-analysis-approach-for-open-box-support-vector-machines https://core.ac.uk/display/81891364 https://dc.tsinghuajournals.com/computational-visual-media/vol3/iss2/6/ http://cvm.tsinghuajournals.com/EN/10.1007/s41095-017-0077-5 https://asu.pure.elsevier.com/en/publications/easysvm-a-visual-analysis-approach-for-open-box-support-vector-ma,http://dx.doi.org/10.1007/s41095-017-0077-5,,10.1007/s41095-017-0077-5,2612334796,,0,000-744-025-662-062; 004-930-618-968-061; 007-193-666-693-914; 007-663-922-833-626; 008-468-200-647-04X; 010-523-153-808-867; 014-108-579-695-086; 014-939-616-383-020; 016-953-381-721-921; 021-540-946-355-910; 024-238-656-845-390; 026-044-906-689-608; 027-042-237-527-245; 038-564-565-187-101; 038-773-271-926-280; 040-489-785-377-972; 041-003-281-444-856; 052-009-306-047-838; 052-508-659-584-344; 053-530-032-586-757; 056-110-987-235-018; 056-651-792-616-189; 063-597-014-838-796; 066-325-026-747-787; 071-503-505-635-045; 071-866-271-335-762; 072-129-325-802-394; 081-944-084-193-678; 087-863-645-601-373; 089-732-746-745-977; 094-588-939-728-564; 096-426-243-967-971; 097-485-318-728-585; 100-424-851-069-693; 101-499-067-856-506; 105-823-739-790-448; 116-140-481-491-905; 124-903-089-607-304; 139-888-759-599-078; 142-909-297-271-235; 149-575-175-018-239; 152-031-782-468-496; 153-054-229-391-604; 161-159-291-669-583; 174-179-727-108-77X; 176-356-327-060-372,36,true,cc-by,gold
011-309-426-203-927,A neural-network virtual-reality mobility aid for the severely visually impaired,1998-08-01,1998,,,,,,Mark Everingham; T Troscianko; B Thomas; N Karia; D Easty,"Many people who are registered as blind nevertheless retain some residual vision and are said to have ""low vision"". Conditions resulting in such low vision include cataracts, diabetic retinopathy, age-related maculopathy, and retinal detachment. In recent years an increasing amount of research has attempted to apply principles from computer vision to the requirements of the low vision subject. A variety of conventional image-processing techniques have been used to enhance the visual appearance of a scene, and devices from the field of virtual reality such as head-mounted displays have been investigated as an aid to low vision. However, a fundamental limitation with conventional image-processing techniques is that they are applied to an entire image with no knowledge of scene content, resulting in unwanted emphasis of noise and unimportant detail. Our aim is to produce a portable system comprising a processing unit with head-mounted camera and display which will allow a person with low vision to be self-sufficient and mobile in a typical urban environment. Our approach differs from previous research in that it uses a neural network object classifier to allow images to be enhanced in a way which considers the identity of objects in the scene. Primarily, our system transforms an original image into a classified image in which the types of objects in the scene are identified by an object outline filled with a particular high saturation colour according to the object type, chosen by the user. By classification our system allows the user to identify important objects in a scene simply by their colour, requiring no perception of shape or high spatial frequencies, and minimal contrast sensitivity. The resultant images are very simple and uncluttered and we expect that users would adapt quickly to use of the system. Results obtained to date suggest that the system is capable of providing registered-blind users with useful visual information. We are now working on improving the speed and classification accuracy of the system, and investigating the applicability of our techniques to specific conditions.",,,14,14,Spatial frequency; Engineering; Artificial intelligence; Visual appearance; Virtual reality; Perception; Mobility aid; Computer vision; Artificial neural network; Object type; Classifier (UML),,,,,http://playpen.icomtek.csir.co.za/~acdc/assistive%20devices/Artabilitation2008/archive/1998/papers/1998_23.pdf https://research-information.bris.ac.uk/en/publications/neural-network-virtual-reality-mobility-aid-for-the-severely-visu http://www.icdvrat.org/1998/papers/1998_23.pdf http://www.cs.bris.ac.uk/Publications/Papers/1000272.pdf https://www.icdvrat.org/1998/papers/1998_23.pdf https://research-information.bristol.ac.uk/en/publications/a-neuralnetwork-virtualreality-mobility-aid-for-the-severely-visually-impaired(c6285c84-9df5-48a0-8103-c1ee6051c1e2).html,https://research-information.bris.ac.uk/en/publications/neural-network-virtual-reality-mobility-aid-for-the-severely-visu,,,2129456892,,0,007-630-066-353-222; 010-025-512-242-323; 020-129-508-246-902; 024-663-955-858-041; 028-026-441-772-21X; 066-374-042-552-286; 073-439-548-015-005; 082-468-278-672-521; 098-870-862-048-054; 105-124-151-064-67X; 108-058-040-428-879; 151-590-898-915-213; 168-824-267-918-342; 174-848-941-567-235; 196-611-996-038-384,11,false,,
011-356-906-186-611,Digital Enhancement of Cultural Experience and Accessibility for the Visually Impaired,2019-07-02,2019,book chapter,Technological Trends in Improved Mobility of the Visually Impaired,25228595; 25228609,Springer International Publishing,,Dimitris K. Iakovidis; Dimitrios E. Diamantis; George Dimas; Charis Ntakolia; Evaggelos Spyrou,"Visual impairment restricts everyday mobility and limits the accessibility of places, which for the non-visually impaired is taken for granted. A short walk to a close destination, such as a market or a school becomes an everyday challenge. In this chapter, we present a novel solution to this problem that can evolve into an everyday visual aid for people with limited sight or total blindness. The proposed solution is a digital system, wearable like smart-glasses, equipped with cameras. An intelligent system module, incorporating efficient deep learning and uncertainty-aware decision-making algorithms, interprets the video scenes, translates them into speech, and describes them to the user through audio. The user can almost naturally interact with the system via a speech-based user interface, which is also capable of understanding the user’s emotions. The capabilities of this system are investigated in the context of accessibility and guidance to outdoor environments of cultural interest, such as the historic triangle of Athens. A survey of relevant state-of-the-art systems, technologies and services is performed, identifying critical system components that better adapt to the goals of the system, user needs and requirements, toward a user-centered architecture design.",,,237,271,Human–computer interaction; Deep learning; Wearable computer; Sight; Artificial intelligence; Context (language use); Visual impairment; Cultural experience; Visually impaired; Computer science; User interface,,,,,https://rd.springer.com/chapter/10.1007/978-3-030-16450-8_10 https://link.springer.com/chapter/10.1007%2F978-3-030-16450-8_10,http://dx.doi.org/10.1007/978-3-030-16450-8_10,,10.1007/978-3-030-16450-8_10,2955348319,,0,000-828-753-611-122; 002-109-727-479-297; 002-649-617-859-886; 004-216-069-855-831; 004-772-554-982-380; 005-753-721-571-304; 006-069-649-187-046; 007-680-586-073-246; 008-124-206-643-913; 008-361-673-204-978; 010-239-380-621-455; 010-759-689-155-412; 011-260-346-847-031; 012-089-923-723-370; 012-344-135-607-729; 012-515-034-217-629; 013-156-255-523-712; 015-954-008-173-996; 016-473-557-742-214; 016-645-836-018-745; 018-496-878-392-336; 018-641-674-379-971; 020-233-013-143-936; 020-885-011-437-897; 021-176-270-241-156; 021-247-965-698-975; 021-281-807-418-64X; 021-326-533-295-625; 021-360-232-245-594; 023-805-082-979-008; 024-575-750-554-124; 026-252-287-049-279; 026-410-526-630-302; 027-522-282-356-935; 027-720-558-803-335; 027-834-538-968-330; 028-739-531-420-63X; 029-367-566-864-304; 030-846-671-006-321; 031-169-956-799-784; 031-218-334-653-826; 031-731-504-000-05X; 033-860-301-495-703; 034-288-418-514-121; 035-277-967-833-996; 038-364-111-957-515; 041-058-139-703-974; 041-583-554-186-850; 042-495-614-620-409; 042-978-726-698-936; 043-685-034-593-751; 045-599-200-378-144; 048-254-076-918-785; 048-371-638-574-770; 049-317-239-158-314; 050-601-374-343-997; 052-118-817-856-285; 053-094-537-530-97X; 053-240-315-716-245; 053-346-003-124-203; 053-584-152-952-065; 063-223-027-736-995; 063-625-320-581-998; 066-133-135-091-645; 068-250-715-493-38X; 068-526-647-480-345; 073-180-689-038-913; 076-112-198-316-215; 079-310-940-223-758; 081-938-680-892-669; 082-072-860-483-821; 082-869-703-811-99X; 087-105-444-728-025; 088-206-525-984-388; 088-248-751-762-965; 093-596-358-012-81X; 095-211-223-584-688; 098-359-916-630-990; 098-778-990-820-131; 100-110-638-399-026; 101-250-365-809-122; 102-952-660-914-374; 103-580-873-199-468; 116-030-405-883-678; 125-112-083-666-544; 126-287-456-856-107; 130-917-996-139-06X; 132-602-463-268-72X; 137-064-892-215-059; 137-306-447-831-557; 139-552-118-652-140; 140-398-873-255-539; 146-093-154-688-224; 147-150-128-112-541; 148-740-585-892-543; 151-458-014-340-090; 152-145-055-495-637; 154-968-471-807-679; 155-338-218-673-080; 156-335-332-105-397; 158-691-676-803-096; 158-769-392-676-037; 160-707-206-355-327; 165-367-533-416-928; 175-661-713-504-140; 180-559-374-049-445; 184-418-590-788-505; 192-697-687-929-866; 195-091-532-336-938; 198-094-749-849-956,16,false,,
012-261-790-110-686,A Smart Device – Helps the Blind to Cross the Road Safely,2020-08-01,2020,journal article,International Journal of Engineering and Advanced Technology,22498958,Blue Eyes Intelligence Engineering and Sciences Engineering and Sciences Publication - BEIESP,,Priya Sridharan; Ritwika Ghosh,"<jats:p>Internet of Things has been a flourishing technology used in almost every aspect of modern lives to solve real-world problems. Computer vision, a state of the art technology which helps computers understand digital images, when combined with IoT can augment the existing solutions to make truly intelligent systems. Visually impaired people can use their power of hearing or guide dog to help them cross the road, but they are not much efficient and are prone to accidents as sounds subside and they cannot pace with the speed of a dog. Existing technologies which help blind people navigate do not have a viable mechanism for taking accidents and safety into account. Visually impaired people have enhanced touch abilities. Leveraging this skill, this paper aims at providing a solution that produces haptic feedback in the form of vibrations indicating the suitable time to cross the road. A camera provides live data of their surroundings, which detects the traffic signal and determines its colour with the help of computer vision. If the signal is red, an indication in the form of haptic feedback is produced using a vibrating mini disc motor, thus helping the visually impaired person to cross the road safely, thereby preventing accidents.</jats:p>",8,4s2,28,30,Pace; Haptic technology; Computer science; Human–computer interaction; Computer vision; Computer security; Simulation; Geodesy; Geography,,,,,,http://dx.doi.org/10.35940/ijeat.d1008.0484s219,,10.35940/ijeat.d1008.0484s219,,,0,,1,true,,gold
012-370-328-340-986,A Systematic Review of Urban Navigation Systems for Visually Impaired People.,2021-04-29,2021,journal article,"Sensors (Basel, Switzerland)",14248220; 14243210,Multidisciplinary Digital Publishing Institute (MDPI),Switzerland,Fatma El-zahraa El-taher; Ayman Taha; Jane Courtney; Susan McKeever,"Blind and Visually impaired people (BVIP) face a range of practical difficulties when undertaking outdoor journeys as pedestrians. Over the past decade, a variety of assistive devices have been researched and developed to help BVIP navigate more safely and independently. In addition, research in overlapping domains are addressing the problem of automatic environment interpretation using computer vision and machine learning, particularly deep learning, approaches. Our aim in this article is to present a comprehensive review of research directly in, or relevant to, assistive outdoor navigation for BVIP. We breakdown the navigation area into a series of navigation phases and tasks. We then use this structure for our systematic review of research, analysing articles, methods, datasets and current limitations by task. We also provide an overview of commercial and non-commercial navigation applications targeted at BVIP. Our review contributes to the body of knowledge by providing a comprehensive, structured analysis of work in the domain, including the state of the art, and guidance on future directions. It will support both researchers and other stakeholders in the domain to establish an informed view of research progress.",21,9,3103,,Human–computer interaction; Deep learning; Structured analysis; Variety (cybernetics); Artificial intelligence; Domain (software engineering); Body of knowledge; Structure (mathematical logic); Task (project management); Computer science; Obstacle avoidance,assistive systems; autonomous driving; independent children navigation; navigation systems; obstacle avoidance; planning journeys; robot navigation; smart cities; visually impaired people,Blindness; Humans; Machine Learning; Self-Help Devices; Sensory Aids; Visually Impaired Persons,,Science Foundation Ireland (18/CRT/6222) Ireland,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8125253 https://pubmed.ncbi.nlm.nih.gov/33946857/ https://europepmc.org/article/MED/33946857 https://doi.org/10.3390/s21093103 https://arrow.tudublin.ie/scschcomart/111/ https://dblp.uni-trier.de/db/journals/sensors/sensors21.html#El-taherTCM21 https://ui.adsabs.harvard.edu/abs/2021Senso..21.3103E/abstract https://www.mdpi.com/1424-8220/21/9/3103/pdf https://arrow.tudublin.ie/cgi/viewcontent.cgi?article=1121&context=scschcomart https://www.mdpi.com/1424-8220/21/9/3103,http://dx.doi.org/10.3390/s21093103,33946857,10.3390/s21093103,3157852503,PMC8125253,0,002-704-194-668-787; 003-787-428-155-830; 004-216-069-855-831; 004-235-937-057-332; 004-482-587-535-24X; 005-222-101-092-166; 006-143-327-194-22X; 008-361-673-204-978; 008-884-266-172-633; 008-945-464-062-140; 009-409-853-626-410; 012-344-135-607-729; 013-056-435-015-274; 015-040-823-132-436; 015-362-620-788-939; 015-672-084-912-775; 015-968-400-395-152; 016-988-681-008-984; 017-499-516-520-553; 017-756-720-130-530; 018-775-168-927-916; 018-843-508-574-144; 019-150-105-799-327; 019-592-100-498-89X; 020-581-037-678-426; 020-638-834-068-203; 021-176-270-241-156; 021-657-554-837-75X; 022-073-019-808-68X; 022-940-138-590-719; 023-596-160-630-306; 025-670-117-400-666; 028-671-432-434-245; 028-969-772-207-673; 030-122-412-155-214; 030-276-333-033-042; 030-349-308-230-559; 030-500-337-935-178; 030-884-979-386-25X; 032-661-597-680-28X; 032-930-925-006-837; 033-707-686-495-876; 034-276-887-409-510; 035-569-236-178-312; 035-718-322-228-300; 037-376-150-587-539; 037-814-581-218-142; 038-261-996-116-331; 038-326-446-864-583; 038-724-247-134-16X; 038-963-886-363-012; 039-397-253-626-970; 039-799-789-300-41X; 040-351-685-890-777; 041-762-639-825-950; 041-868-461-531-394; 042-777-177-469-047; 042-800-033-418-542; 043-196-700-803-427; 043-242-159-639-853; 043-853-364-390-746; 044-097-632-032-714; 045-115-001-126-821; 045-599-200-378-144; 046-309-239-761-288; 049-907-076-141-592; 049-921-334-736-611; 050-629-643-534-455; 051-674-567-787-301; 051-861-879-703-203; 052-046-438-827-353; 052-546-659-510-529; 055-660-300-282-556; 056-311-013-170-616; 056-404-390-495-54X; 056-418-672-464-79X; 056-456-008-278-70X; 057-343-383-968-310; 058-268-022-053-157; 059-940-817-118-734; 060-135-893-855-211; 060-371-039-424-871; 060-384-097-796-57X; 061-205-550-296-347; 064-742-750-569-146; 065-078-747-021-140; 065-561-146-681-012; 065-864-368-278-26X; 066-246-145-459-483; 067-306-741-272-680; 067-424-296-647-735; 067-894-235-988-508; 068-067-625-851-58X; 068-251-661-266-644; 069-480-623-909-181; 069-750-894-920-400; 073-898-178-314-544; 075-106-663-696-798; 077-953-340-723-43X; 078-544-250-112-589; 079-433-787-480-087; 081-825-351-668-737; 083-106-044-761-562; 083-671-887-012-721; 085-564-340-397-815; 085-794-351-629-013; 085-928-790-360-424; 086-020-884-298-699; 088-302-807-890-785; 088-470-436-866-921; 088-822-320-203-25X; 090-506-797-178-634; 090-692-932-653-189; 093-596-358-012-81X; 096-555-936-749-425; 097-606-712-041-130; 098-910-337-444-079; 099-164-837-383-131; 099-873-453-549-026; 102-263-736-254-576; 102-795-246-835-316; 104-246-965-633-543; 107-458-943-516-753; 108-107-790-423-506; 109-742-594-977-56X; 113-335-466-018-07X; 114-946-128-456-527; 116-190-225-586-417; 116-715-441-276-690; 117-368-855-666-392; 120-782-022-441-430; 121-439-923-704-187; 122-953-341-322-742; 124-552-411-131-198; 124-578-471-837-358; 124-599-469-135-595; 125-253-620-953-16X; 127-197-616-633-752; 128-896-972-345-959; 129-842-505-919-573; 131-759-217-735-571; 134-354-773-670-493; 134-827-148-857-284; 134-950-160-470-035; 136-648-400-143-451; 136-732-042-076-069; 141-490-390-908-433; 142-105-374-847-047; 144-682-868-877-599; 147-855-184-779-455; 148-404-359-976-324; 149-795-595-526-832; 154-990-932-592-836; 156-426-869-484-693; 158-728-812-316-142; 163-514-238-456-745; 164-719-327-371-917; 183-947-709-586-992; 190-968-153-214-662; 191-713-977-852-116; 192-403-569-308-659,53,true,cc-by,gold
012-456-583-984-92X,BraillePassword: accessible web authentication technique on touchscreen devices,2018-05-19,2018,journal article,Journal of Ambient Intelligence and Humanized Computing,18685137; 18685145,Springer Science and Business Media LLC,Germany,Mrim Alnfiai; Srinivas Sampalli,,10,6,2375,2391,Human–computer interaction; Password; Login; Web application; Usability; Authentication; Mobile banking; Touchscreen; Braille; Computer science; User interface,,,,Taif University,https://link.springer.com/article/10.1007/s12652-018-0860-x https://dblp.uni-trier.de/db/journals/jaihc/jaihc10.html#AlnfiaiS19,http://dx.doi.org/10.1007/s12652-018-0860-x,,10.1007/s12652-018-0860-x,2803610987,,2,000-596-396-273-494; 004-536-029-680-961; 006-898-557-790-25X; 017-102-757-920-55X; 017-688-395-818-881; 019-042-079-495-304; 020-617-174-288-91X; 020-713-614-450-416; 021-042-349-806-590; 025-687-986-032-739; 025-739-286-284-224; 027-878-758-611-928; 029-610-834-019-455; 029-731-159-124-681; 032-257-743-315-762; 033-874-287-898-983; 035-654-402-073-900; 049-330-342-536-885; 049-546-376-438-565; 062-512-355-451-344; 064-190-755-736-537; 073-945-965-595-671; 078-197-026-237-842; 084-839-870-343-834; 085-016-204-952-637; 086-678-334-535-478; 092-592-183-031-773; 093-925-865-515-918; 097-990-869-910-372; 113-596-627-153-842; 113-666-014-584-173; 115-345-987-070-692; 123-998-126-411-107; 140-249-117-766-166; 140-839-582-511-433; 154-047-472-944-994; 172-586-034-777-511; 197-596-152-618-198,14,false,,
012-476-520-800-810,Intelligent transportation systems for smart cities: a progress review,2012-11-24,2012,journal article,Science China Information Sciences,1674733x; 18691919; 18622836; 10092757,Springer Science and Business Media LLC,China,Zhang Xiong; Hao Sheng; Wenge Rong; Dave E. Cooper,,55,12,2908,2914,Systems engineering; Efficient energy use; Smart city; Computer science; Advanced Traffic Management System; Simulation; Intelligent transportation system; Internet of Things,,,,,http://dx.doi.org/10.1007/s11432-012-4725-1 https://doi.org/10.1007/s11432-012-4725-1 https://dx.doi.org/10.1007/s11432-012-4725-1 https://en.cnki.com.cn/Article_en/CJFDTOTAL-JFXG201212019.htm https://link.springer.com/article/10.1007/s11432-012-4725-1 https://dblp.uni-trier.de/db/journals/chinaf/chinaf55.html#XiongSRC12 https://rd.springer.com/article/10.1007%2Fs11432-012-4725-1,http://dx.doi.org/10.1007/s11432-012-4725-1,,10.1007/s11432-012-4725-1,2259959024,,0,000-371-951-472-939; 001-270-731-229-164; 002-580-724-068-489; 008-459-441-003-945; 014-553-535-240-747; 014-938-318-013-414; 021-189-140-179-465; 022-312-336-201-027; 022-823-033-123-695; 023-438-345-162-875; 024-267-984-711-028; 028-961-407-637-468; 029-009-842-691-754; 038-346-333-457-728; 038-923-654-596-064; 039-191-783-477-649; 041-886-153-076-405; 043-847-614-145-751; 045-305-845-516-923; 045-985-482-796-138; 054-689-665-473-235; 055-944-569-495-671; 057-399-607-501-748; 058-850-916-322-356; 060-616-841-523-87X; 061-161-162-308-009; 062-301-051-128-222; 062-668-849-588-034; 064-668-595-740-034; 068-126-625-296-35X; 069-271-699-307-370; 074-830-536-318-496; 078-775-863-539-137; 078-943-256-234-382; 084-064-737-856-47X; 086-872-174-584-762; 090-646-072-052-342; 094-483-898-807-176; 101-049-024-528-010; 102-888-687-029-89X; 112-803-691-463-434; 119-486-107-344-160; 131-360-046-695-995; 133-997-835-564-374; 135-821-664-698-83X; 142-545-040-762-018; 148-273-494-027-677; 162-885-247-729-983,156,false,,
012-572-135-446-691,Enhanced Depth Navigation Through Augmented Reality Depth Mapping in Patients with Low Vision,2019-08-02,2019,journal article,Scientific reports,20452322,Springer Science and Business Media LLC,United Kingdom,Anastasios Angelopoulos; Hossein Ameri; Debbie Mitra; Mark S. Humayun,"Patients diagnosed with Retinitis Pigmentosa (RP) show, in the advanced stage of the disease, severely restricted peripheral vision causing poor mobility and decline in quality of life. This vision loss causes difficulty identifying obstacles and their relative distances. Thus, RP patients use mobility aids such as canes to navigate, especially in dark environments. A number of high-tech visual aids using virtual reality (VR) and sensory substitution have been developed to support or supplant traditional visual aids. These have not achieved widespread use because they are difficult to use or block off residual vision. This paper presents a unique depth to high-contrast pseudocolor mapping overlay developed and tested on a Microsoft Hololens 1 as a low vision aid for RP patients. A single-masked and randomized trial of the AR pseudocolor low vision aid to evaluate real world mobility and near obstacle avoidance was conducted consisting of 10 RP subjects. An FDA-validated functional obstacle course and a custom-made grasping setup were used. The use of the AR visual aid reduced collisions by 50% in mobility testing (p = 0.02), and by 70% in grasp testing (p = 0.03). This paper introduces a new technique, the pseudocolor wireframe, and reports the first significant statistics showing improvements for the population of RP patients with mobility and grasp.",9,1,11230,11230,Augmented reality; Artificial intelligence; Peripheral vision; Virtual reality; Retinitis pigmentosa; Computer vision; Computer science; Obstacle avoidance; Sensory substitution,,"Accidental Falls/prevention & control; Adult; Audiovisual Aids; Augmented Reality; Female; Humans; Male; Quality of Life; Virtual Reality; Vision, Low/rehabilitation",,,https://pubmed.ncbi.nlm.nih.gov/31375713/ http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6677879 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6677879 http://ui.adsabs.harvard.edu/abs/2019NatSR...911230A/abstract https://www.nature.com/articles/s41598-019-47397-w.pdf https://www.nature.com/articles/s41598-019-47397-w https://europepmc.org/article/MED/31375713,http://dx.doi.org/10.1038/s41598-019-47397-w,31375713,10.1038/s41598-019-47397-w,2965752037,PMC6677879,0,001-552-736-329-522; 003-022-807-070-71X; 003-696-604-838-62X; 008-361-673-204-978; 008-422-008-067-690; 011-309-426-203-927; 013-016-358-471-463; 014-387-823-436-92X; 015-211-906-741-089; 015-627-652-468-711; 015-925-743-279-508; 016-408-213-077-520; 018-599-803-096-411; 022-925-235-552-698; 023-596-160-630-306; 027-494-067-069-84X; 029-360-136-216-567; 034-588-579-766-703; 035-753-604-715-899; 038-046-586-077-577; 038-661-539-608-384; 039-113-985-496-932; 040-016-004-608-978; 043-121-042-008-969; 045-205-438-482-211; 045-599-200-378-144; 046-399-906-994-192; 047-059-242-052-746; 049-111-190-816-764; 049-530-432-887-829; 050-071-839-171-609; 050-564-738-791-767; 052-104-972-727-451; 055-171-125-989-695; 056-992-238-016-923; 059-644-292-671-783; 060-480-982-083-628; 067-752-390-832-534; 069-730-043-740-809; 071-880-292-764-233; 076-059-869-327-354; 079-278-898-644-841; 082-597-956-517-75X; 091-144-555-750-58X; 091-560-256-438-062; 093-664-728-529-974; 096-239-462-922-956; 115-356-442-836-305; 116-336-800-669-489; 116-668-409-022-202; 120-499-391-106-298; 124-256-350-867-786; 138-632-290-347-029; 143-187-991-786-727; 147-640-024-045-067; 147-848-336-455-789; 161-740-823-890-676; 167-449-625-533-859; 185-142-390-882-654,29,true,"CC BY, CC BY-NC-ND",gold
012-733-233-438-753,Simulated visual homing in desert ant natural environments: efficiency of skyline cues,2010-03-19,2010,journal article,Biological cybernetics,14320770; 03401200,Springer Science and Business Media LLC,Germany,Kai Basten; Hanspeter A. Mallot,,102,5,413,425,Homing (biology); Gradient descent; Artificial intelligence; Pixel; Virtual machine; Landmark; Skyline; Computer vision; Computer science; Robot; Path integration,,"Algorithms; Animals; Ants; Cues; Desert Climate; Environment; Homing Behavior/physiology; Pattern Recognition, Visual/physiology",,,https://dblp.uni-trier.de/db/journals/bc/bc102.html#BastenM10 https://europepmc.org/article/MED/20300942 https://www.ncbi.nlm.nih.gov/pubmed/20300942 https://link.springer.com/article/10.1007%2Fs00422-010-0375-9,http://dx.doi.org/10.1007/s00422-010-0375-9,20300942,10.1007/s00422-010-0375-9,2024467466,,0,000-635-897-387-595; 001-984-984-166-207; 002-237-379-701-854; 003-086-124-970-834; 004-263-191-026-537; 005-634-891-641-289; 005-799-763-720-076; 011-211-654-554-205; 017-970-261-847-192; 021-370-327-657-312; 021-620-148-568-444; 023-805-573-764-124; 025-235-368-568-563; 026-795-964-013-697; 030-629-547-370-213; 034-912-935-328-315; 034-918-839-296-04X; 035-424-726-028-010; 042-274-042-841-652; 043-935-295-616-762; 044-512-896-808-18X; 046-362-475-083-346; 048-993-916-056-516; 051-825-139-515-41X; 061-927-946-062-904; 062-632-603-463-536; 064-533-587-679-976; 067-490-874-709-957; 072-642-562-642-803; 080-746-806-717-354; 083-392-110-795-948; 085-296-393-401-98X; 085-405-799-670-741; 085-580-494-394-430; 093-927-745-338-426; 093-968-778-174-625; 098-122-613-586-181; 102-897-962-893-469; 110-545-227-296-140; 114-660-379-247-638; 120-942-912-990-570; 127-298-104-994-493; 141-463-459-190-025; 167-128-535-279-773; 169-041-348-073-455; 183-107-700-653-747,42,false,,
012-755-574-956-468,A Novel Vision-Enhancing Technology for Low-Vision Impairments,2018-11-07,2018,journal article,Journal of medical systems,1573689x; 01485598,Springer Science and Business Media LLC,United States,Carmelo Lodato; Patrizia Ribino,,42,12,256,256,Optometry; Binocular vision; Peripheral vision; Perception; Visual field; Patent Cooperation Treaty; Visual Disorders; Low vision; Eye tracking; Medicine,Binocular vision; Low-vision aids; Visual field defects; Vitreoretinal impairments,"Equipment Design; Eye Movements; Humans; Image Processing, Computer-Assisted/methods; Imaging, Three-Dimensional; Quality of Life; Sensory Aids; Vision, Binocular; Vision, Low/rehabilitation; Visually Impaired Persons/rehabilitation",,,https://europepmc.org/article/MED/30406503 https://pubmed.ncbi.nlm.nih.gov/30406503/ https://link.springer.com/article/10.1007/s10916-018-1108-1 https://www.ncbi.nlm.nih.gov/pubmed/30406503 https://link.springer.com/content/pdf/10.1007%2Fs10916-018-1108-1.pdf,http://dx.doi.org/10.1007/s10916-018-1108-1,30406503,10.1007/s10916-018-1108-1,2899830387,,0,005-944-100-761-174; 007-322-947-410-27X; 015-525-553-980-122; 016-412-565-471-03X; 017-824-587-384-217; 021-317-007-724-739; 024-682-528-103-318; 027-397-838-713-900; 029-038-261-485-437; 032-448-073-941-968; 038-765-297-700-52X; 039-595-688-286-976; 040-061-989-894-149; 040-427-631-400-814; 045-686-788-260-715; 054-670-802-982-600; 065-727-635-836-115; 070-085-743-348-249; 075-039-191-762-226; 085-320-824-689-901; 088-209-578-388-33X; 092-391-305-190-481; 099-568-529-657-656; 100-401-458-006-811; 106-673-478-479-453; 114-757-171-234-281; 116-000-304-517-428; 121-819-250-387-889; 124-349-457-632-795; 127-577-129-762-13X; 189-877-512-878-825,3,false,,
012-788-164-202-525,Enhancing general spatial skills of young visually impaired people with a programmable distance discrimination training: a case control study,2019-08-28,2019,journal article,Journal of neuroengineering and rehabilitation,17430003,Springer Science and Business Media LLC,United Kingdom,Fabrizio Leo; Elisabetta Ferrari; Caterina Baccelliere; Juan Jose Zarate; Herbert Shea; Elena Cocchi; Aleksander Waszkielewicz; Luca Brayda,"The estimation of relative distance is a perceptual task used extensively in everyday life. This important skill suffers from biases that may be more pronounced when estimation is based on haptics. This is especially true for the blind and visually impaired, for which haptic estimation of distances is paramount but not systematically trained. We investigated whether a programmable tactile display, used autonomously, can improve distance discrimination ability in blind and severely visually impaired youngsters between 7 and 22 years-old. Training consisted of four weekly sessions in which participants were asked to haptically find, on the programmable tactile display, the pairs of squares which were separated by the shortest and longest distance in tactile images with multiple squares. A battery of haptic tests with raised-line drawings was administered before and after training, and scores were compared to those of a control group that did only the haptic battery, without doing the distance discrimination training on the tactile display. Both blind and severely impaired youngsters became more accurate and faster at the task during training. In haptic battery results, blind and severely impaired youngsters who used the programmable display improved in three and two tests, respectively. In contrast, in the control groups, the blind control group improved in only one test, and the severely visually impaired in no tests. Distance discrimination skills can be trained equally well in both blind and severely impaired participants. More importantly, autonomous training with the programmable tactile display had generalized effects beyond the trained task. Participants improved not only in the size discrimination test but also in memory span tests. Our study shows that tactile stimulation training that requires minimal human assistance can effectively improve generic spatial skills.",16,1,108,,Haptic technology; Memory span; Sensory stimulation therapy; Perception; Test (assessment); Task (project management); Visual impairment; Computer science; Discrimination testing; Audiology,Distance estimation; Haptics; Learning; Programmable tactile displays; Training; Visual impairment,Adolescent; Blindness/rehabilitation; Case-Control Studies; Child; Distance Perception; Female; Humans; Learning; Male; Memory; Psychomotor Performance; Reaction Time; Size Perception; Space Perception; Touch; Vision Disorders/rehabilitation; Young Adult,,EU FP7 STREP,https://infoscience.epfl.ch/record/270360 https://jneuroengrehab.biomedcentral.com/articles/10.1186/s12984-019-0580-2 https://link.springer.com/content/pdf/10.1186/s12984-019-0580-2.pdf https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6714081 https://www.ncbi.nlm.nih.gov/pubmed/31462262 https://pubmed.ncbi.nlm.nih.gov/31462262/ https://link.springer.com/article/10.1186/s12984-019-0580-2,http://dx.doi.org/10.1186/s12984-019-0580-2,31462262,10.1186/s12984-019-0580-2,2970331851,PMC6714081,0,000-185-521-073-031; 000-673-807-912-016; 000-847-470-259-601; 002-302-844-631-248; 003-841-034-797-114; 004-329-885-593-287; 004-808-757-436-795; 006-566-700-510-281; 006-635-585-195-621; 006-635-819-665-420; 007-698-585-229-208; 007-805-195-776-08X; 009-605-249-389-461; 009-647-718-775-02X; 009-961-243-759-696; 010-629-061-233-301; 010-828-572-854-662; 011-471-845-172-788; 015-251-168-245-169; 016-040-434-869-641; 016-806-739-342-070; 017-506-570-011-912; 022-235-999-585-90X; 027-886-410-913-757; 029-710-298-092-349; 029-981-112-895-94X; 030-673-874-193-208; 033-354-413-968-307; 034-443-992-617-480; 034-721-267-759-029; 038-820-890-713-312; 041-090-906-316-394; 041-893-960-738-589; 042-016-734-384-569; 042-556-239-421-614; 043-000-734-005-557; 044-209-418-938-48X; 046-430-973-921-826; 046-980-057-131-34X; 048-177-902-521-397; 052-167-195-851-498; 054-731-257-249-943; 058-171-888-408-149; 059-593-255-133-110; 061-233-905-431-623; 064-146-625-905-226; 066-709-892-111-833; 072-033-785-736-168; 075-519-293-886-898; 079-579-336-739-24X; 083-931-013-210-514; 084-934-084-521-74X; 089-138-402-043-355; 092-324-197-827-27X; 097-145-406-647-709; 101-321-503-619-956; 109-925-436-909-571; 118-176-386-497-126; 125-130-577-558-091; 132-418-470-493-669; 136-190-827-881-363; 138-426-774-385-323; 146-937-782-091-186; 154-418-006-631-173; 159-156-590-260-109; 163-719-860-638-345,5,true,"CC BY, CC0",gold
012-959-325-789-544,Structured Computational Modeling of Human Visual System for No-reference Image Quality Assessment,2021-01-04,2021,journal article,International Journal of Automation and Computing,14768186; 17518520,Springer Science and Business Media LLC,Germany,Wenhan Zhu; Wei Sun; Xiongkuo Min; Guangtao Zhai; Xiaokang Yang,"Objective image quality assessment (IQA) plays an important role in various visual communication systems, which can automatically and efficiently predict the perceived quality of images. The human eye is the ultimate evaluator for visual experience, thus the modeling of human visual system (HVS) is a core issue for objective IQA and visual experience optimization. The traditional model based on black box fitting has low interpretability and it is difficult to guide the experience optimization effectively, while the model based on physiological simulation is hard to integrate into practical visual communication services due to its high computational complexity. For bridging the gap between signal distortion and visual experience, in this paper, we propose a novel perceptual no-reference (NR) IQA algorithm based on structural computational modeling of HVS. According to the mechanism of the human brain, we divide the visual signal processing into a low-level visual layer, a middle-level visual layer and a high-level visual layer, which conduct pixel information processing, primitive information processing and global image information processing, respectively. The natural scene statistics (NSS) based features, deep features and free-energy based features are extracted from these three layers. The support vector regression (SVR) is employed to aggregate features to the final quality prediction. Extensive experimental comparisons on three widely used benchmark IQA databases (LIVE, CSIQ and TID2013) demonstrate that our proposed metric is highly competitive with or outperforms the state-of-the-art NR IQA measures.",18,2,204,218,Visual communication; Support vector machine; Artificial intelligence; Human visual system model; Pattern recognition; Scene statistics; Interpretability; Computer science; Image quality; Information processing; Black box,,,,,http://ijac.xml-journal.net/en/article/doi/10.1007/s11633-020-1270-z https://dblp.uni-trier.de/db/journals/ijautcomp/ijautcomp18.html#ZhuSMZY21 https://link.springer.com/content/pdf/10.1007/s11633-020-1270-z.pdf http://www.ijac.net/en/article/doi/10.1007/s11633-020-1270-z https://link.springer.com/article/10.1007/s11633-020-1270-z http://ir.ia.ac.cn/handle/173211/44017 https://www.scilit.net/article/354cfd3ed1892550ae4369aca63f4f77 http://159.226.21.68/handle/173211/44017?mode=full&submit_simple=Show+full+item+record,http://dx.doi.org/10.1007/s11633-020-1270-z,,10.1007/s11633-020-1270-z,3118623396,,0,001-484-504-895-566; 003-082-386-803-747; 003-809-390-403-417; 005-379-007-801-961; 006-826-831-029-488; 009-657-096-810-781; 010-037-102-410-599; 010-157-738-409-581; 010-719-710-740-151; 011-927-414-739-394; 015-571-444-508-782; 016-157-096-736-924; 016-225-890-402-990; 017-941-985-653-737; 020-174-807-646-116; 020-233-013-143-936; 023-579-839-563-336; 023-928-444-110-407; 024-742-908-621-776; 031-620-741-963-464; 033-834-329-911-749; 035-750-343-265-733; 035-805-445-049-711; 037-224-254-696-200; 039-135-942-929-563; 040-788-258-383-633; 041-058-139-703-974; 041-362-360-350-975; 042-446-148-300-553; 044-095-303-626-459; 044-567-011-351-193; 045-051-875-518-164; 045-878-790-524-130; 049-033-728-110-624; 049-880-214-943-175; 051-607-179-128-005; 054-520-461-002-550; 054-942-476-130-010; 057-950-024-937-664; 060-834-533-443-304; 064-401-628-721-647; 067-098-374-632-301; 079-247-399-160-293; 080-955-728-090-322; 094-629-970-125-559; 098-898-901-617-331; 099-150-485-567-158; 099-224-661-429-23X; 104-018-015-345-340; 116-368-791-687-007; 125-641-102-255-972; 130-129-698-795-758; 133-730-990-801-285; 146-166-619-336-013; 151-567-741-961-697; 157-034-630-272-377; 164-162-671-622-791; 169-385-037-131-231,4,true,cc-by,hybrid
012-977-902-721-152,Computer vision and GIS for the navigation of blind persons in buildings,2014-02-19,2014,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,M. Serrão; S. Shahrabadi; M. Moreno; João José; J. I. Rodrigues; J. M. Rodrigues; J. M. Buf,,14,1,67,80,Doors; Tracing; Artificial intelligence; Mobile robot navigation; Stairs; Mobile phone; Blind persons; Computer vision; Computer science; Geographic information system; Turn-by-turn navigation,,,,,https://link.springer.com/article/10.1007%2Fs10209-013-0338-8 https://dblp.uni-trier.de/db/journals/uais/uais14.html#SerraoSMJRRB15 https://link.springer.com/article/10.1007/s10209-013-0338-8/fulltext.html https://dl.acm.org/doi/10.1007/s10209-013-0338-8,http://dx.doi.org/10.1007/s10209-013-0338-8,,10.1007/s10209-013-0338-8,2048329048,,1,002-397-792-416-419; 002-456-977-338-362; 009-612-733-037-965; 016-726-236-426-061; 021-673-909-369-351; 031-057-232-736-968; 043-056-567-014-861; 045-381-213-243-298; 052-860-252-031-134; 054-374-371-905-358; 058-016-717-029-275; 074-324-981-337-749; 081-141-746-741-399; 084-679-921-430-947; 084-750-792-083-414; 086-308-757-277-927; 088-964-930-423-843; 096-846-442-673-950; 097-390-623-922-438; 100-260-517-243-811; 100-797-551-662-34X; 108-234-112-296-175; 122-729-034-369-253; 132-222-173-435-29X; 141-489-442-115-69X; 145-898-655-711-084; 150-597-344-978-541; 165-315-873-244-89X; 166-719-051-375-011; 175-405-447-678-97X,37,false,,
013-422-017-213-208,Deep learning-based application for indoor wayfinding assistance navigation,2021-05-13,2021,journal article,Multimedia Tools and Applications,13807501; 15737721,Springer Science and Business Media LLC,Netherlands,Mouna Afif; Riadh Ayachi; Yahia Said; Mohamed Atri,,80,18,27115,27130,Human–computer interaction; Deep learning; Signage; Artificial intelligence; Landmark; Visually Impaired Persons; Computer communication networks; Multimedia information systems; Computer science,,,,,https://dblp.uni-trier.de/db/journals/mta/mta80.html#AfifASA21 https://link.springer.com/article/10.1007/s11042-021-10999-6,http://dx.doi.org/10.1007/s11042-021-10999-6,,10.1007/s11042-021-10999-6,3163638933,,0,004-185-460-563-576; 004-216-069-855-831; 013-257-162-869-08X; 015-672-084-912-775; 020-233-013-143-936; 020-998-360-866-973; 022-940-138-590-719; 024-964-397-377-997; 026-400-078-913-130; 026-994-702-158-53X; 034-222-058-985-816; 035-277-967-833-996; 037-182-418-585-695; 040-032-511-261-446; 044-396-615-675-996; 045-309-399-228-849; 045-802-145-074-623; 061-579-383-511-500; 061-933-888-557-079; 063-117-355-616-790; 063-355-312-719-478; 071-338-626-777-780; 072-240-298-491-062; 077-101-929-493-550; 085-348-489-657-236; 106-598-916-263-111; 116-715-441-276-690; 149-679-295-863-636; 186-810-372-950-411,8,false,,
013-444-833-687-214,A Human Robot Interactive System “RoJi”,2004-11-01,2004,journal article,KSME International Journal,12264865,Springer Science and Business Media LLC,South Korea,Joongsun Yoon,,18,11,1900,1908,Human–computer interaction; Engineering; Human–robot interaction; Artificial intelligence; Arbitration; Interactive technology; Visually impaired; Robot control; Robot,,,,,https://link.springer.com/article/10.1007/BF02990431,http://dx.doi.org/10.1007/bf02990431,,10.1007/bf02990431,1488279419,,0,006-656-087-037-236; 010-115-633-498-893; 017-969-062-620-544; 043-111-582-343-945; 073-928-284-197-323; 076-781-057-771-298; 089-585-462-314-473; 099-283-021-363-180; 104-855-735-289-824; 107-533-479-339-622; 123-799-303-737-57X; 136-799-544-471-09X; 141-307-011-595-847; 157-103-919-829-320,0,false,,
013-501-956-117-041,Design and Development of a Wearable Assistive Device Integrating a Fuzzy Decision Support System for Blind and Visually Impaired People,2021-09-07,2021,journal article,Micromachines,2072666x,MDPI AG,Switzerland,Yassine Bouteraa,"In this article, a new design of a wearable navigation support system for blind and visually impaired people (BVIP) is proposed. The proposed navigation system relies primarily on sensors, real-time processing boards, a fuzzy logic-based decision support system, and a user interface. It uses sensor data as inputs and provides the desired safety orientation to the BVIP. The user is informed about the decision based on a mixed voice–haptic interface. The navigation aid system contains two wearable obstacle detection systems managed by an embedded controller. The control system adopts the Robot Operating System (ROS) architecture supported by the Beagle Bone Black master board that meets the real-time constraints. The data acquisition and obstacle avoidance are carried out by several nodes managed by the ROS to finally deliver a mixed haptic–voice message for guidance of the BVIP. A fuzzy logic-based decision support system was implemented to help BVIP to choose a safe direction. The system has been applied to blindfolded persons and visually impaired persons. Both types of users found the system promising and pointed out its potential to become a good navigation aid in the future.",12,9,1082,,Decision support system; Human–computer interaction; Interface (computing); Wearable computer; Navigation system; Computer science; Wearable technology; Fuzzy logic; Obstacle avoidance; User interface,assistive technology; blind and visually impaired people; fuzzy classifier; navigation aid; sensor data fusion; wearable devices,,,,https://www.mdpi.com/2072-666X/12/9/1082 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8466919 https://www.mdpi.com/2072-666X/12/9/1082/pdf,http://dx.doi.org/10.3390/mi12091082,34577725,10.3390/mi12091082,3196967874,PMC8466919,0,000-426-623-688-517; 002-123-448-882-287; 007-223-103-783-187; 008-558-848-807-295; 012-081-521-655-479; 015-016-116-643-819; 017-499-516-520-553; 018-262-121-105-137; 034-422-902-172-199; 037-376-150-587-539; 045-802-145-074-623; 048-603-106-043-151; 048-787-039-266-679; 053-826-604-836-17X; 055-171-125-989-695; 064-521-070-547-235; 066-526-177-885-125; 075-049-345-308-149; 075-519-293-886-898; 078-736-735-476-395; 079-433-787-480-087; 080-148-076-219-168; 083-671-887-012-721; 093-548-249-489-049; 095-466-674-437-104; 096-733-124-330-978; 098-910-337-444-079; 099-072-913-456-358; 101-715-245-776-087; 139-073-591-756-182; 141-649-000-612-322; 144-452-403-048-99X; 147-516-729-009-367; 185-085-881-079-518; 190-485-029-949-876,19,true,cc-by,gold
013-581-160-999-600,The development of assisted- visually impaired people robot in the indoor environment based on deep learning,2023-06-10,2023,journal article,Multimedia Tools and Applications,13807501; 15737721,Springer Science and Business Media LLC,Netherlands,Yi-Zeng Hsieh; Xiang-Long Ku; Shih-Syun Lin,,83,3,6555,6578,Computer science; Computer vision; Artificial intelligence; Convolutional neural network; Robot; Segmentation; Visually impaired; Position (finance); Human–computer interaction; Finance; Economics,,,,"Ministry of Science and Technology, Taiwan",,http://dx.doi.org/10.1007/s11042-023-15644-y,,10.1007/s11042-023-15644-y,,,0,002-123-448-882-287; 002-649-617-859-886; 003-813-258-014-256; 010-298-238-227-772; 014-157-934-526-940; 020-233-013-143-936; 030-540-246-658-722; 035-152-016-615-193; 036-842-985-307-311; 037-411-892-552-299; 041-314-165-476-904; 042-482-112-385-426; 048-254-076-918-785; 048-766-996-250-254; 049-317-239-158-314; 058-464-094-452-067; 060-559-012-308-293; 064-053-186-811-574; 064-657-343-403-492; 064-762-832-227-463; 066-133-135-091-645; 067-266-215-185-671; 082-378-852-750-231; 098-926-109-358-108; 108-012-143-484-589; 114-026-589-905-215; 126-664-490-174-266; 129-562-304-444-392,0,false,,
013-729-854-652-837,A stereo auditory display for visually impaired,,,conference proceedings article,2000 TENCON Proceedings. Intelligent Systems and Technologies for the New Millennium (Cat. No.00CH37119),,IEEE,,Farrah Wong; R. Nagarajan; S. Yaacob; Ali Chekima; N.-E. Belkhamza,"According to information from the World Health Organization (WHO) as of 1990, there are, approximately, 38 million people who are blind. This paper reports the progress of a funded research project to invent Malaysia's very own electronic travel aid (ETA). The research aims to develop an auditory display system that converts pictures to sound in order to aid the blind in navigation. The experimental system consists of a digital video camera, and a portable computer as well as earphones. The developed software functions as an extractor of the data collected by the camera, processes and transforms the information into sound patterns to indicate the presence of an obstacle while the user is navigating. Assessment for the system is currently being carried out at the laboratory level and preliminary test has shown success in interpreting the sound pattern generated.",2,,377,382,Extractor; Engineering; Experimental system; Artificial intelligence; Auditory display; Obstacle; Software; Portable computer; World health; Visually impaired; Computer vision,,,,,http://eprints.ums.edu.my/3429/,http://dx.doi.org/10.1109/tencon.2000.888766,,10.1109/tencon.2000.888766,1933085068,,0,013-847-555-426-159; 019-866-317-188-986; 045-673-862-191-277; 089-556-668-161-529; 096-803-870-115-728,7,false,,
013-844-892-323-35X,"If deep learning is the answer, what is the question?",2020-11-16,2020,journal article,Nature reviews. Neuroscience,14710048; 1471003x,Springer Science and Business Media LLC,United Kingdom,Andrew M. Saxe; Stephanie Nelli; Christopher Summerfield,"Neuroscience research is undergoing a minor revolution. Recent advances in machine learning and artificial intelligence research have opened up new ways of thinking about neural computation. Many researchers are excited by the possibility that deep neural networks may offer theories of perception, cognition and action for biological brains. This approach has the potential to radically reshape our approach to understanding neural systems, because the computations performed by deep networks are learned from experience, and not endowed by the researcher. If so, how can neuroscientists use deep networks to model and understand biological brains? What is the outlook for neuroscientists who seek to characterize computations or neural codes, or who wish to understand perception, attention, memory and executive functions? In this Perspective, our goal is to offer a road map for systems neuroscience research in the age of deep learning. We discuss the conceptual and methodological challenges of comparing behaviour, learning dynamics and neural representations in artificial and biological systems, and we highlight new research questions that have emerged for neuroscience as a direct consequence of recent advances in machine learning.",22,1,55,67,Deep learning; Artificial intelligence; Cognition; Cognitive science; Perspective (graphical); Perception; Action (philosophy); Systems neuroscience; Models of neural computation; Computer science; Artificial neural network,,"Brain; Deep Learning; Humans; Neural Networks, Computer; Neurosciences",,Wellcome Trust (216386/Z/19/Z) United Kingdom,https://ui.adsabs.harvard.edu/abs/2020arXiv200407580S/abstract https://www.nature.com/articles/s41583-020-00395-8.pdf https://www.ncbi.nlm.nih.gov/pubmed/33199854 https://www.nature.com/articles/s41583-020-00395-8 https://www.neuroscience.ox.ac.uk/publications/1146177 https://www.psy.ox.ac.uk/publications/1146177 https://pubmed.ncbi.nlm.nih.gov/33199854/ https://europepmc.org/article/MED/33199854,http://dx.doi.org/10.1038/s41583-020-00395-8,33199854,10.1038/s41583-020-00395-8,3100035092,,0,000-661-367-192-001; 000-702-548-020-549; 000-847-534-972-917; 002-361-179-653-833; 003-028-967-630-066; 003-644-529-968-074; 003-651-646-988-184; 006-555-318-429-77X; 006-996-468-421-865; 007-225-340-536-699; 008-475-135-029-389; 008-737-909-338-298; 008-958-637-407-235; 010-147-887-894-526; 010-471-849-260-674; 010-481-791-215-510; 011-033-171-401-512; 011-528-229-766-747; 011-950-520-693-882; 012-720-154-324-595; 013-928-847-300-958; 014-024-068-398-452; 014-193-402-657-200; 014-412-187-598-881; 014-925-830-767-546; 015-844-654-307-285; 015-992-321-385-693; 015-995-360-165-38X; 016-157-096-736-924; 016-267-372-390-585; 016-269-848-088-621; 016-944-456-112-14X; 017-029-255-097-091; 018-571-728-751-003; 018-614-068-987-279; 019-772-172-140-924; 020-366-781-999-736; 021-090-705-240-412; 022-323-257-688-301; 022-487-870-220-900; 022-512-090-524-219; 022-825-958-041-914; 022-826-860-789-312; 023-029-080-614-313; 023-272-958-567-763; 023-739-656-804-539; 023-766-975-956-496; 023-948-497-285-591; 024-891-740-985-435; 025-192-759-112-056; 026-982-905-606-322; 027-054-835-928-741; 028-866-298-952-155; 029-279-937-208-411; 029-979-985-449-25X; 030-168-018-403-48X; 030-686-888-173-083; 031-450-451-694-463; 034-190-283-144-470; 034-315-930-957-387; 034-788-144-462-934; 035-187-576-378-579; 036-099-809-223-751; 036-215-458-962-338; 037-120-554-968-320; 038-352-031-602-361; 038-672-345-458-473; 038-821-702-165-297; 041-093-889-515-749; 041-816-439-957-888; 042-883-989-745-851; 043-402-754-676-19X; 047-032-142-565-878; 048-142-915-950-975; 049-132-995-335-071; 050-828-246-341-264; 054-517-624-293-712; 056-150-372-039-877; 056-576-148-142-317; 057-023-391-255-035; 057-135-848-108-198; 057-346-650-035-480; 057-486-555-637-939; 058-257-524-576-245; 058-482-622-136-447; 059-149-073-001-124; 063-890-086-808-388; 064-141-092-701-120; 065-077-486-249-213; 065-863-263-255-358; 067-510-161-479-587; 067-593-942-310-641; 069-568-723-131-289; 069-654-721-324-985; 070-287-128-807-189; 070-292-537-420-07X; 070-297-699-153-640; 072-068-811-792-324; 072-088-565-382-044; 073-323-537-148-924; 073-919-115-973-710; 074-280-318-598-940; 079-984-297-397-353; 081-679-971-514-605; 082-709-912-162-591; 085-791-556-113-025; 085-901-970-539-763; 086-449-708-329-742; 088-086-242-947-84X; 089-107-938-824-246; 089-917-929-438-305; 091-536-410-796-686; 092-101-828-645-416; 093-099-664-764-406; 093-530-739-304-785; 093-914-284-026-821; 094-932-420-695-081; 096-173-629-635-037; 097-649-443-045-52X; 101-911-983-150-201; 104-862-858-615-242; 105-543-963-437-01X; 106-956-688-979-739; 107-403-876-123-555; 107-475-025-985-408; 108-356-771-644-634; 109-127-819-934-030; 110-238-487-444-437; 113-639-653-370-426; 114-385-135-756-125; 115-591-239-189-149; 116-879-809-274-900; 119-713-366-256-955; 126-688-623-005-172; 137-909-555-395-142; 139-092-221-661-198; 141-793-002-286-408; 142-404-398-869-523; 142-646-531-243-557; 145-494-565-746-848; 154-324-920-902-150; 179-116-015-392-067; 187-651-194-637-326; 190-330-258-987-933; 195-091-532-336-938; 195-852-605-262-305,234,true,,green
013-889-868-604-441,An embedded application for degraded text recognition,2005-08-15,2005,journal article,EURASIP Journal on Advances in Signal Processing,16876180; 16876172,Springer Science and Business Media LLC,Germany,Céline Thillou; Silvio Ferreira; Bernard Gosselin,"This paper describes a mobile device which tries to give the blind or visually impaired access to text information. Three key technologies are required for this system: text detection, optical character recognition, and speech synthesis. Blind users and the mobile environment imply two strong constraints. First, pictures will be taken without control on camera settings and a priori information on text (font or size) and background. The second issue is to link several techniques together with an optimal compromise between computational constraints and recognition efficiency. We will present the overall description of the system from text detection to OCR error correction.",2005,13,2127,2135,Speech synthesis; Noisy text analytics; Artificial intelligence; Thresholding; Key (cryptography); Font; Speech recognition; Computer vision; Computer science; Optical character recognition,,,,,https://dx.doi.org/10.1155/ASP.2005.2127 https://ui.adsabs.harvard.edu/abs/2005EJASP2005...55T/abstract https://asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.2127 https://paperity.org/p/75379703/an-embedded-application-for-degraded-text-recognition https://link.springer.com/content/pdf/10.1155%2FASP.2005.2127.pdf https://doi.org/10.1155/ASP.2005.2127 https://doaj.org/article/26946d5cd1f240a2a81435ae1adfe904 http://dx.doi.org/10.1155/ASP.2005.2127 https://dl.acm.org/doi/10.1155/ASP.2005.2127 https://link.springer.com/article/10.1155/ASP.2005.2127 https://dblp.uni-trier.de/db/journals/ejasp/ejasp2005.html#ThillouFG05,http://dx.doi.org/10.1155/asp.2005.2127,,10.1155/asp.2005.2127,2170727726,,3,000-400-274-350-642; 008-695-767-504-779; 010-924-044-244-65X; 014-153-976-680-876; 016-000-328-631-473; 019-117-130-044-52X; 023-838-324-213-808; 029-209-739-959-210; 048-758-506-069-138; 049-023-956-892-77X; 054-772-476-662-552; 060-302-475-860-099; 065-661-524-907-938; 076-158-577-467-746; 082-284-579-914-348; 083-528-965-691-837; 090-542-957-883-681; 111-438-359-443-90X; 129-948-090-257-939; 138-657-183-962-836; 139-436-968-424-517; 144-101-617-328-706; 145-351-302-832-877,30,true,cc-by,gold
014-090-386-279-841,ECCV Workshops (3) - 3D Glasses as Mobility Aid for Visually Impaired People,2015-03-20,2015,book chapter,Computer Vision - ECCV 2014 Workshops,03029743; 16113349,Springer International Publishing,Germany,Stefano Mattoccia; Paolo Macri,"This paper proposes an effective and wearable mobility aid aimed at improving the quality of life of people suffering for visual disabilities by enabling autonomous and safe navigation in unknown environments. Our system relies on dense and accurate depth maps, provided in real-time by a compact stereo vision system mapped into an FPGA, in order to detect obstacles in front of the user and to provide accordingly vibration feedbacks as well as audio information by means of a bone-conductive speakers. Compared to most approaches with similar purposes, even in the current prototype arrangement deployed for testing, our system is extremely compact, lightweight and energy efficient thus enabling hours of safe and autonomous navigation with standard batteries avoiding the need to carry cumbersome devices. Moreover, by conceiving the 3D sensing device as a replacement of standard glasses typically worn by visually impaired people and by using intuitive feedbacks provided by means of lightweight actuators, our system provides an ergonomic and comfortable user interface with a fast learning curve for its effective deployment. This fact has been extensively verified on the field by means of an experimental evaluation, in indoor as well as in outdoor environments, with different users simulating visual impairment including a blind person.",,,539,554,Software deployment; Human–computer interaction; Wearable computer; Efficient energy use; Mobility aid; Visual impairment; Field (computer science); Computer science; Stereopsis; User interface,,,,,https://dblp.uni-trier.de/db/conf/eccv/eccv2014w3.html#MattocciaM14 http://vigir.missouri.edu/~gdesouza/Research/Conference_CDs/ECCV_2014/workshops/w22/W22-07.pdf https://link.springer.com/chapter/10.1007/978-3-319-16199-0_38 https://rd.springer.com/chapter/10.1007/978-3-319-16199-0_38,http://dx.doi.org/10.1007/978-3-319-16199-0_38,,10.1007/978-3-319-16199-0_38,2294991447,,0,001-076-828-128-846; 001-845-909-413-538; 014-157-934-526-940; 014-724-098-723-866; 018-207-067-803-820; 021-033-546-707-589; 022-697-627-694-941; 028-303-905-295-179; 032-054-582-968-024; 033-626-844-682-323; 034-088-504-605-568; 035-550-445-634-847; 044-273-304-374-731; 051-425-281-185-199; 051-766-223-654-722; 052-687-863-770-378; 057-890-353-263-937; 061-669-253-155-324; 074-969-968-020-614; 085-102-500-348-494; 087-272-456-866-28X; 098-640-382-982-581; 101-333-962-375-602; 127-483-973-270-088; 137-707-963-300-012; 139-121-237-598-154; 141-489-442-115-69X; 151-942-228-606-223; 176-518-155-704-547; 185-783-077-691-122,21,true,,green
014-457-927-163-816,Comparing picture and video prompting in autonomous indoor wayfinding for individuals with cognitive impairments,2010-03-03,2010,journal article,Personal and Ubiquitous Computing,16174909; 16174917,Springer Science and Business Media LLC,Germany,Yao-Jen Chang; Tsen-Yung Wang,,14,8,737,747,Recall; Feeling; Mobile computing; Cognition; Cognitive psychology; Schizophrenia (object-oriented programming); Rehabilitation; Computer science; Spatial memory; Multimedia; Cognitive load; User interface,,,,,https://dblp.uni-trier.de/db/journals/puc/puc14.html#ChangW10 https://link.springer.com/article/10.1007%2Fs00779-010-0285-9 https://link.springer.com/article/10.1007/s00779-010-0285-9/fulltext.html,http://dx.doi.org/10.1007/s00779-010-0285-9,,10.1007/s00779-010-0285-9,1980861983,,1,001-001-182-139-397; 005-944-638-147-934; 006-889-624-708-70X; 007-414-293-373-809; 009-019-041-126-29X; 011-198-177-703-058; 013-952-290-699-34X; 016-432-007-366-372; 018-892-973-648-998; 025-486-712-733-692; 026-244-307-943-578; 027-185-335-277-225; 033-058-108-759-369; 034-863-781-824-456; 036-990-754-479-620; 038-391-072-216-581; 038-534-069-160-965; 039-900-383-068-730; 051-041-659-488-202; 051-996-861-121-334; 057-212-675-420-920; 061-371-039-386-664; 063-200-348-257-985; 063-698-981-210-411; 072-933-063-915-758; 079-919-054-001-210; 080-041-996-677-364; 091-258-992-552-288; 095-067-565-254-183; 100-521-182-777-24X; 126-740-335-245-687; 132-246-550-138-466; 137-869-384-652-269; 147-928-929-282-502; 150-759-601-637-350; 157-193-965-055-591; 160-992-769-794-708; 176-712-594-044-025; 181-538-496-882-244,51,false,,
014-578-508-488-836,Development of Machine Learning Model for Assistance of Visually Impaired,2023-06-16,2023,conference proceedings article,2023 International Conference on Applied Intelligence and Sustainable Computing (ICAISC),,IEEE,,Harsha Karamchandani; Sarita Kumari; Kusumika Krori Dutta; S Kehkeshan Jalall; Sahil Saurav; Subham Kumar,"People with reduced vision or total blindness frequently have trouble navigating new environments on their own. It can be difficult to travel or even just stroll down a busy street. As a result, many people with poor eyesight frequently travel with a sighted friend or relative. It is harder for them to recall where they usually are. The goal of this initiative is to completely liberate the blind individual. The project is a virtual eye that communicates with the outside environment using a camera. The system continuously gets data from the camera. The information is gathered using cameras that are meant to be in the Centre of the eyes. Input signals were processed using APIs and algorithms, followed by speech processing units are utilized to communicate with the blind individual. By employing the audio output to communicate the processed information about their environment, blind people can move and finish their task easily on their own. The accuracy of the suggested method is 98%, 95%, and 90%, respectively, for object detection, face recognition, and Optical Character Recognition (OCR) methodology. The recognized object, text and face are then processed to obtain audio instructions.",,,,,Computer science; Task (project management); Face (sociological concept); Artificial intelligence; Computer vision; Blindness; Object (grammar); Facial recognition system; Recall; Visually impaired; Optical character recognition; Object detection; Face detection; Human–computer interaction; Speech recognition; Image (mathematics); Feature extraction; Pattern recognition (psychology); Engineering; Psychology; Medicine; Social science; Systems engineering; Sociology; Optometry; Cognitive psychology,,,,,,http://dx.doi.org/10.1109/icaisc58445.2023.10199677,,10.1109/icaisc58445.2023.10199677,,,0,000-130-488-383-514; 007-223-103-783-187; 011-465-552-089-816; 014-019-198-784-152; 033-619-204-206-390; 051-942-825-317-192; 052-071-277-035-893; 055-858-990-745-91X; 076-637-838-565-044; 077-953-525-061-591; 080-589-710-669-822; 098-910-337-444-079; 117-677-849-214-196; 124-614-630-050-129; 126-998-084-890-398; 137-973-182-180-54X; 171-057-734-143-202; 185-616-751-096-539,0,false,,
014-993-068-942-65X,Image enhancement for electronic visual prostheses.,,2002,journal article,Australasian physical & engineering sciences in medicine,01589938; 18795447,Springer Science and Business Media LLC,Germany,Justin Boyle; Anthony Maeder; Wageeh Boles,,25,2,81,86,Artificial intelligence; Set (psychology); Image resolution; Quality (business); Image enhancement; Computer vision; Computer science; Image quality; Image processing,,"Algorithms; Blindness/rehabilitation; Humans; Image Enhancement/instrumentation; Pattern Recognition, Automated; Pattern Recognition, Visual/physiology; Prostheses and Implants; Prosthesis Design/instrumentation; Reproducibility of Results; Sensitivity and Specificity; Sensory Aids; Vision, Ocular/physiology",,,http://eprints.qut.edu.au/23496/ http://www.ncbi.nlm.nih.gov/pubmed/12219849 https://researchnow.flinders.edu.au/en/publications/image-enhancement-for-electronic-visual-prostheses https://link.springer.com/article/10.1007%2FBF03178470 https://link.springer.com/content/pdf/10.1007/BF03178470.pdf,http://dx.doi.org/10.1007/bf03178470,12219849,10.1007/bf03178470,2022963201,,0,003-828-471-118-54X; 010-405-790-712-609; 011-621-292-740-351; 019-179-238-633-581; 020-684-609-770-394; 021-264-347-570-75X; 042-422-564-298-433; 043-269-569-868-43X; 049-119-843-984-826; 052-691-150-775-867; 064-495-335-186-759; 118-384-547-716-200; 128-288-848-632-871; 176-188-668-905-003,8,false,,
015-008-890-486-381,Smartphone-based localization for blind navigation in building-scale indoor environments,,2019,journal article,Pervasive and Mobile Computing,15741192,Elsevier BV,Netherlands,Masayuki Murata; Dragan Ahmetovic; Daisuke Sato; Hironobu Takagi; Kris M. Kitani; Chieko Asakawa,"Abstract Continuous, accurate, and real-time smartphone-based localization is a promising technology for supporting independent mobility of people with visual impairments. However, despite extensive research on indoor localization techniques, localization technologies are still not ready for deployment in large and complex environments such as shopping malls and hospitals, where navigation assistance is needed most. We identify six key challenges for accurate smartphone localization related to the large-scale nature of the navigation environments and the user’s mobility. To address these challenges, we present a series of techniques that enhance a probabilistic localization algorithm. The algorithm utilizes mobile device inertial sensors and Received Signal Strength (RSS) from Bluetooth Low Energy (BLE) beacons. We evaluate the proposed system in a 21,000 m 2 shopping mall that includes three multi-story buildings and a large open underground passageway. Experiments conducted in this environment demonstrate the effectiveness of the proposed technologies to improve localization accuracy. Field experiments with visually impaired participants confirm the practical performance of the proposed system in realistic use cases.",57,,14,32,Beacon; Software deployment; Mobile device; Key (cryptography); RSS; Use case; Computer science; Probabilistic logic; Inertial measurement unit; Real-time computing,,,,,https://www.sciencedirect.com/science/article/pii/S1574119218303316 https://dblp.uni-trier.de/db/journals/percom/percom57.html#MurataASTKA19 https://air.unimi.it/handle/2434/697795,http://dx.doi.org/10.1016/j.pmcj.2019.04.003,,10.1016/j.pmcj.2019.04.003,2937897649,,0,001-448-544-148-575; 001-996-430-142-340; 003-171-371-661-074; 005-132-898-043-994; 005-497-078-814-142; 006-615-644-763-134; 006-792-447-761-719; 009-927-794-055-32X; 011-369-981-296-055; 014-151-331-306-036; 015-733-840-903-484; 016-732-725-225-210; 017-434-368-515-180; 018-249-047-846-554; 019-336-739-884-465; 019-811-761-679-740; 019-828-744-033-99X; 022-818-790-792-275; 024-049-874-758-655; 024-340-152-801-993; 026-684-493-399-893; 031-410-405-104-518; 037-893-410-565-455; 040-940-319-245-132; 044-901-027-282-816; 045-626-526-989-827; 049-485-542-422-096; 051-289-097-618-069; 052-058-164-372-424; 055-660-300-282-556; 056-660-722-902-221; 057-110-853-820-322; 057-847-555-860-541; 063-200-348-257-985; 064-722-056-462-519; 065-032-907-680-425; 071-473-658-566-47X; 075-827-600-908-546; 079-433-787-480-087; 081-871-676-661-821; 085-639-580-887-462; 088-305-233-913-186; 090-705-137-602-227; 095-563-603-963-814; 097-499-459-534-124; 102-976-506-139-695; 104-213-094-026-527; 105-810-803-790-416; 121-275-463-333-777; 139-114-429-196-103; 188-569-539-275-740; 189-029-312-730-111,54,true,,green
015-174-884-664-544,WACV - Beacon-Guided Structure from Motion for Smartphone-Based Navigation,,2017,conference proceedings article,2017 IEEE Winter Conference on Applications of Computer Vision (WACV),,IEEE,,Tatsuya Ishihara; Jayakorn Vongkulbhisal; Kris M. Kitani; Chieko Asakawa,"Great progress has been made in computer vision-based localization systems. However, some systems tend to work well only in certain visually feature-rich environments. It is often the case that feature-based matching techniques can have a hard time dealing with scenes with only a few features or a large number of repetitive features. In these situations, computer vision-based localization may fail to estimate camera position or may yield a large localization error. We approach this problem from a systems perspective, where we are required to obtain accurate localization of blind travellers using a smartphones app for localization. In particular, we assume that the environment is already instrumented with Bluetooth low energy (BLE) signals to provide rough proximity information, and we propose to integrate it with visual information to perform efficient structure-from-motion and camera localization. Our multi-model sensing approach can accelerate localization speed and obtain more accuracy in challenging environments when compared to traditional baseline approaches. We also show that our approach can accelerate the time for reconstructing large 3D models. Our framework is released as an open source project. It can be used by different mobile operating systems, enabling the development of navigation applications on mobile platforms.",,,769,777,Solid modeling; Perspective (graphical); Position (vector); Mobile computing; Structure from motion; Artificial intelligence; Matching (statistics); Computer vision; Visualization; Computer science; Image sensor,,,,,https://www.computer.org/csdl/proceedings-article/wacv/2017/07926674/12OmNya72no https://ieeexplore.ieee.org/document/7926674/ https://dblp.uni-trier.de/db/conf/wacv/wacv2017.html#IshiharaVKA17,http://dx.doi.org/10.1109/wacv.2017.91,,10.1109/wacv.2017.91,2613881032,,0,002-508-484-172-239; 004-511-416-923-897; 004-796-220-360-369; 005-484-178-277-214; 012-663-606-013-213; 021-678-078-234-298; 023-148-591-859-314; 023-722-691-051-623; 026-766-098-329-720; 031-340-088-964-440; 035-291-375-623-97X; 036-892-720-088-606; 038-463-742-769-346; 039-605-902-600-652; 042-056-346-349-581; 048-510-019-754-753; 049-247-368-209-869; 059-231-271-711-701; 059-732-088-154-391; 068-144-779-332-973; 074-006-536-890-200; 079-433-787-480-087; 081-871-676-661-821; 098-440-015-522-27X; 102-786-306-974-437; 113-035-668-760-809; 113-877-246-754-354; 118-896-156-747-770; 119-723-016-817-782; 125-380-792-989-988; 152-303-721-943-405; 169-183-166-487-52X,17,false,,
015-672-084-912-775,ICCCNT - Computer Vision based Assistive Technology for Blind and Visually Impaired People,2016-07-06,2016,conference proceedings article,Proceedings of the 7th International Conference on Computing Communication and Networking Technologies,,ACM,,Shankar Sivan; Gopu Darsan,"The computer vision based assistive technology for the blind and visually impaired is a developing area. The assistive technology helps the visually impaired by providing them with a greater independence. By enabling them with their day-to-day activities like indoor and outdoor navigation, obstacle detection, locating the doors and lost objects, etc. Even though different assistive technologies are available for the blind, most of them have complex designs which are developed for a specific purpose and are expensive for the commercial production. Rather than depending on a traditional white cane, the blind and visually impaired people can make use of the cheaper assistive device proposed in this paper. The proposed system incorporates several assistance features in a device which will be an asset for them according to their needs.",,,41,,Artificial intelligence; Obstacle; Wearable systems; White cane; Assistive technology; Assistive device; Visually impaired; Computer vision; Computer science; Multimedia; Asset (computer security),,,,,https://dblp.uni-trier.de/db/conf/icccnt/icccnt2016.html#SivanD16 https://dl.acm.org/doi/10.1145/2967878.2967923 https://doi.org/10.1145/2967878.2967923,http://dx.doi.org/10.1145/2967878.2967923,,10.1145/2967878.2967923,2519609847,,0,012-016-836-079-639; 012-642-196-605-159; 013-551-333-215-556; 016-834-308-590-249; 024-627-299-891-290; 040-030-169-396-345; 043-459-230-949-991; 045-673-862-191-277; 048-876-468-556-695; 050-326-534-967-202; 057-793-015-492-337; 068-446-370-600-681; 073-258-283-857-019; 074-324-981-337-749; 076-855-816-665-250; 083-294-350-082-331; 094-206-454-118-462; 097-450-938-016-597; 098-440-015-522-27X; 099-643-392-623-259; 100-731-218-785-790; 124-166-396-175-305; 128-154-672-313-987; 148-615-934-560-940; 170-179-601-701-601; 185-783-077-691-122,37,false,,
015-705-835-654-083,Pedestrian Lane Detection for Assistive Navigation of Vision-Impaired People: Survey and Experimental Evaluation,,2022,journal article,IEEE Access,21693536,Institute of Electrical and Electronics Engineers (IEEE),United States,Yunjia Lei; Son Lam Phung; Abdesselam Bouzerdoum; Hoang Thanh Le; Khoa Luu,"Pedestrian lane detection is one of the most crucial tasks in assistive navigation for vision-impaired people. It can provide information on walkable regions, help blind people stay on pedestrian lanes, and assist with obstacle detection. An accurate and real-time lane detection algorithm can improve travel safety and efficiency for the visually impaired. However, despite being an important task for assistive navigation systems, pedestrian lane detection in unstructured scenes has not attracted sufficient attention in the research community. Hence, this paper aims to provide a comprehensive review and an experimental evaluation of methods that can be applied for pedestrian lane detection, thereby laying a foundation for future researchers in this area. This study includes methods proposed for pedestrian lane detection, general road detection, and general semantic segmentation. We review these methods in two categories: traditional methods and deep learning methods. We perform an experimental evaluation of representative methods on a large benchmark dataset specifically created for pedestrian lane detection. We hope this paper can serve as an informative guide for researchers in the field of assistive navigation, and facilitate urgently-needed research for blind people.",10,,101071,101089,Pedestrian; Computer science; Pedestrian detection; Obstacle; Computer vision; Benchmark (surveying); Task (project management); Artificial intelligence; Segmentation; Field (mathematics); Visually impaired; Human–computer interaction; Transport engineering; Engineering; Cartography; Geography; Mathematics; Systems engineering; Pure mathematics; Archaeology,,,,"Discovery Project; titled “Assistive Micro-navigation for Vision Impaired People” from the Australian Research Council, and a matching Ph.D. scholarship from the University of Wollongong; Queensland Department of Transport and Main Roads; New SouthWales (NSW) Space Research Network; NSW Defence Innovation Network; NSW Government",https://ieeexplore.ieee.org/ielx7/6287639/6514899/09895423.pdf https://doi.org/10.1109/access.2022.3208128,http://dx.doi.org/10.1109/access.2022.3208128,,10.1109/access.2022.3208128,,,0,000-392-898-032-354; 002-427-035-434-913; 002-649-617-859-886; 003-813-258-014-256; 003-819-868-526-694; 004-504-602-986-917; 004-634-886-716-286; 007-619-251-889-048; 010-881-186-573-195; 012-935-988-437-797; 012-980-256-664-532; 012-993-051-360-968; 015-627-652-468-711; 017-387-981-396-440; 020-233-013-143-936; 025-670-117-400-666; 026-005-792-695-329; 030-014-279-775-853; 030-540-246-658-722; 035-159-685-124-983; 035-583-249-709-961; 035-959-212-790-631; 038-702-890-586-942; 039-231-263-817-711; 041-058-139-703-974; 041-314-165-476-904; 042-240-098-526-453; 045-574-646-622-091; 049-551-659-753-473; 064-521-070-547-235; 065-374-293-809-860; 065-924-516-942-647; 067-473-996-962-892; 073-476-694-087-771; 075-519-293-886-898; 076-425-604-565-135; 079-310-940-223-758; 081-611-157-239-10X; 081-618-070-928-596; 092-700-562-934-337; 095-569-718-308-876; 099-384-604-217-575; 099-582-031-353-605; 102-301-035-113-548; 103-728-042-717-787; 104-208-613-793-423; 105-278-566-106-933; 113-127-609-015-268; 116-030-405-883-678; 121-707-574-802-675; 124-181-359-750-070; 124-326-712-632-53X; 133-509-169-168-939; 134-950-160-470-035; 139-552-118-652-140; 144-247-345-212-401; 144-738-058-499-258; 165-770-407-696-973,5,true,"CC BY, CC BY-NC-ND",gold
015-766-589-442-454,Real-Time Robust 3D Plane Extraction for Wearable Robot Perception and Control,2018-04-09,2018,conference proceedings article,2018 Design of Medical Devices Conference,,American Society of Mechanical Engineers,,Ran Duan; Shuangyue Yu; Guang H. Yue; Richard Foulds; Chen Feng; Yingli Tian; Hao Su,"<jats:p>Wearable environment perception system has the great potential for improving the autonomous control of mobility aids [1]. A visual perception system could provide abundant information of surroundings to assist the task-oriented control such as navigation, obstacle avoidance, object detection, etc., which are essential functions for the wearers who are visually impaired or blind [2, 3, 4]. Moreover, a vision-based terrain sensing is a critical input to the decision-making for the intelligent control system. Especially for the users who find difficulties in manually achieving a seamless control model transition.</jats:p>",,,,,Artificial intelligence; Extraction (military); Plane (geometry); Perception; Control (management); Wearable robot; Intelligent control system; Computer vision; Computer science; Robot,,,,,https://asmedigitalcollection.asme.org/BIOMED/proceedings/DMD2018/40789/V001T06A003/271875 https://verification.asmedigitalcollection.asme.org/BIOMED/proceedings/DMD2018/40789/V001T06A003/271875 http://proceedings.asmedigitalcollection.asme.org/proceeding.aspx?articleid=2685377 https://turbomachinery.asmedigitalcollection.asme.org/BIOMED/proceedings/DMD2018/40789/V001T06A003/271875 https://manufacturingscience.asmedigitalcollection.asme.org/BIOMED/proceedings/DMD2018/40789/V001T06A003/271875 https://nyuscholars.nyu.edu/en/publications/real-time-robust-3d-plane-extraction-for-wearable-robot-perceptio https://proceedings.asmedigitalcollection.asme.org/proceeding.aspx?articleid=2685377,http://dx.doi.org/10.1115/dmd2018-6964,,10.1115/dmd2018-6964,2882356031,,0,022-489-327-390-755; 039-147-729-738-985; 054-071-407-783-498; 060-491-013-635-159; 063-367-028-428-831; 081-215-770-563-077; 086-255-307-364-541; 103-943-796-953-18X,1,true,,bronze
015-818-725-296-28X,"Wheelchair for Physically Disabled People with Voice, Ultrasonic and Infrared Sensor Control",,1995,journal article,Autonomous Robots,09295593; 15737527,Springer Science and Business Media LLC,Netherlands,Manuel Mazo; Francisco Rodríguez; José L. Lázaro; Jesús Ureña; Juan C. García; Enrique Santiso; P. Revenga; Juan Jesús García,"This paper describes a wheelchair for physically disabled people developed within the UMIDAM Project. A dependent-user recognition voice system and ultrasonic and infrared sensor systems has been integrated in this wheelchair. In this way we have obtained a wheelchair which can be driven with using voice commands and with the possibility of avoiding obstacles and downstairs or hole detection. The wheelchair has also been developed to allow autonomous driving (for example, following walls). The project, in which two prototypes have been produced, has been carried out totally in the Electronics Department of the University of Alcala (Spain). It has been financed by the ONCE. Electronic system configuration, a sensor system, a mechanical model, control (low level control, control by voice commands), voice recognition and autonomous control are considered. The results of the experiments carried out on the two prototypes are also given.",2,3,203,224,Electronics; Artificial intelligence; Microcontroller; Voice command device; Mobile robot; Wheelchair; Robotics; Computer science; Simulation; PID controller; Joystick,,,,,https://dblp.uni-trier.de/db/journals/arobots/arobots2.html#MazoRLUGSRG95 http://www.robesafe.com/index.php?option=com_jresearch&view=publication&task=show&id=430&Itemid=58 http://www.geintra-uah.org/system/files/private/fulltext.pdf https://dx.doi.org/10.1007/BF00710857 https://www.researchgate.net/profile/Jesus_Urena/publication/220474026_Wheelchair_for_physically_disabled_people_with_voice_ultrasonic_and_infrared_sensor_control/links/0912f505b84140922f000000.pdf http://ww.geintra-uah.org/en/system/files/private/fulltext.pdf http://dx.doi.org/10.1007/BF00710857 https://link.springer.com/article/10.1007%2FBF00710857,http://dx.doi.org/10.1007/bf00710857,,10.1007/bf00710857,2144278085,,0,002-362-556-456-937; 003-986-011-609-261; 005-502-618-808-442; 020-116-788-666-588; 028-940-299-249-122; 042-001-176-242-862; 043-220-773-195-87X; 064-568-915-366-806; 073-722-974-726-137; 082-103-997-539-215; 085-815-548-885-002; 100-042-275-658-33X; 103-171-057-840-934; 105-633-991-121-521; 141-186-469-227-888; 153-336-633-239-132; 170-726-357-414-159,84,true,,green
015-894-588-041-954,A review of robotic charging for electric vehicles,2023-12-03,2023,journal article,International Journal of Intelligent Robotics and Applications,23665971; 2366598x,Springer Science and Business Media LLC,,Hendri Maja Saputra; Nur Safwati Mohd Nor; Estiko Rijanto; Mohd Zarhamdy Md Zain; Intan Zaurah Mat Darus; Edwar Yazid,,8,1,193,229,Robustness (evolution); Robot; Computer science; Robot manipulator; Artificial intelligence; Robotics; Pose; Torque; Plug-in; Simulation; Control engineering; Engineering; Biochemistry; Chemistry; Physics; Gene; Programming language; Thermodynamics,,,,,,http://dx.doi.org/10.1007/s41315-023-00306-x,,10.1007/s41315-023-00306-x,,,0,000-982-436-172-715; 002-217-189-469-494; 002-919-206-902-488; 004-337-609-348-453; 007-038-956-562-920; 007-741-269-428-716; 008-329-903-366-05X; 008-694-847-220-798; 008-950-429-078-127; 009-021-647-220-435; 009-254-751-077-648; 009-457-756-723-32X; 010-351-072-296-11X; 011-950-316-524-192; 013-478-435-322-848; 015-184-636-068-224; 015-879-680-857-256; 016-476-283-368-694; 018-153-401-753-793; 022-703-647-160-89X; 023-303-006-050-918; 025-332-040-331-647; 026-842-603-223-161; 028-217-586-492-077; 028-595-872-311-095; 028-860-872-742-818; 030-163-467-320-046; 030-445-670-969-997; 036-237-629-965-611; 036-261-374-174-737; 036-438-276-903-95X; 037-037-734-518-725; 037-111-668-579-994; 037-516-846-251-904; 040-264-078-276-278; 040-310-999-146-034; 042-148-153-650-608; 046-805-188-360-38X; 047-419-120-059-604; 049-165-534-624-111; 053-133-930-239-096; 055-544-535-257-250; 061-273-713-788-552; 064-994-698-806-394; 066-604-651-686-619; 068-321-888-287-74X; 068-938-631-254-686; 069-177-769-534-405; 069-340-280-565-844; 071-011-104-730-18X; 072-066-449-754-202; 073-491-488-164-946; 074-256-885-609-757; 074-413-107-269-043; 074-456-796-365-929; 077-058-844-740-304; 077-806-084-983-442; 079-853-998-899-022; 079-890-693-663-37X; 080-573-726-022-744; 081-277-473-085-287; 082-409-419-333-84X; 083-222-698-568-284; 086-034-901-890-65X; 088-721-877-206-957; 088-931-158-286-714; 098-126-258-205-775; 101-750-057-960-550; 102-192-727-746-573; 105-908-515-613-410; 107-898-572-828-342; 108-530-919-101-777; 109-424-511-825-63X; 111-502-530-166-337; 113-529-596-669-634; 124-282-176-045-646; 125-504-595-842-305; 128-374-703-223-146; 130-382-686-574-092; 131-344-798-220-060; 135-554-803-334-806; 136-386-761-587-691; 139-014-823-758-84X; 139-184-218-727-537; 142-822-524-627-392; 143-306-521-427-569; 143-563-177-584-334; 146-084-519-642-137; 155-686-050-831-938; 162-795-101-971-719; 163-886-163-188-463; 166-426-746-287-895; 167-061-641-641-154; 167-893-529-118-568; 173-787-139-967-415; 184-611-542-298-32X; 195-792-952-931-612,1,false,,
016-036-208-485-154,Linespace: A sensemaking platform for the blind,2017-09-19,2017,journal article,Informatik-Spektrum,01706012; 1432122x,Springer Science and Business Media LLC,Germany,Saiganesh Swaminathan; Thijs Roumen; Robert Kovacs; David Stangl; Stefanie Mueller; Patrick Baudisch,,40,6,516,526,Human–computer interaction; Panning (audio); Zoom; Encoding (memory); Spatial analysis; Software; Space (commercial competition); Sensemaking; Memorization; Computer science,,,,,https://link.springer.com/10.1007/s00287-017-1070-1 https://dblp.uni-trier.de/db/journals/insk/insk40.html#SwaminathanRKSM17 https://doi.org/10.1007/s00287-017-1070-1,http://dx.doi.org/10.1007/s00287-017-1070-1,,10.1007/s00287-017-1070-1,2755399566,,0,000-112-427-266-614; 002-728-938-993-491; 007-807-075-975-489; 015-941-245-217-03X; 016-144-009-446-951; 022-862-302-798-933; 024-395-174-093-869; 025-923-846-626-369; 046-954-157-823-838; 048-675-644-930-981; 052-005-750-292-532; 056-202-553-078-324; 059-398-290-385-221; 071-677-146-536-538; 073-174-695-068-640; 086-913-514-510-953; 089-565-726-647-864; 096-859-999-122-913; 180-737-077-996-904,3,false,,
016-154-968-752-718,Supporting Navigation of Outdoor Shopping Complexes for Visually-impaired Users through Multi-modal Data Fusion,2017-04-05,2017,preprint,arXiv: Computer Vision and Pattern Recognition,,,,Archana Paladugu; Parag S. Chandakkar; Peng Zhang; Baoxin Li,"Outdoor shopping complexes (OSC) are extremely difficult for people with visual impairment to navigate. Existing GPS devices are mostly designed for roadside navigation and seldom transition well into an OSC-like setting. We report our study on the challenges faced by a blind person in navigating OSC through developing a new mobile application named iExplore. We first report an exploratory study aiming at deriving specific design principles for building this system by learning the unique challenges of the problem. Then we present a methodology that can be used to derive the necessary information for the development of iExplore, followed by experimental validation of the technology by a group of visually impaired users in a local outdoor shopping center. User feedback and other experiments suggest that iExplore, while at its very initial phase, has the potential of filling a practical gap in existing assistive technologies for the visually impaired.",,,,,Human–computer interaction; Visual impairment; Initial phase; Multi modal data; Visually impaired; Computer science; Global Positioning System,,,,,https://arxiv.org/abs/1704.01266 http://ui.adsabs.harvard.edu/abs/2017arXiv170401266P/abstract https://arxiv.org/pdf/1704.01266.pdf,https://arxiv.org/abs/1704.01266,,,2955704880,,0,003-204-682-225-868; 016-196-543-516-475; 061-978-196-805-368; 065-924-996-738-011; 073-618-852-661-080; 075-228-376-439-786; 083-095-642-169-058; 132-997-113-420-890; 183-935-933-940-273,0,true,,unknown
016-235-386-843-366,Real-Time View Assistance for the Blind using Image Processing,,2024,journal article,June 2024,25824252,Inventive Research Organization,,Abhiram M; null Adarsh K Rajeev; Akhil Raj V; Anirudh K; Manoj M,"<jats:p>Visually impaired people usually have difficulty in doing daily activities. Imagine a future where visually impaired people can seamlessly and independently identify objects and people in their environment. The aim of this research is to increase the independence and mobility of visually impaired people by developing a real-time object and person recognition system. This system uses the power of machine learning and uses computer vision techniques to accurately identify and classify objects and people in the user's environment. Through the integration of speakers or headphones, the system provides auditory feedback to the user and conveys important information about the detected object or person. By combining advanced image processing algorithms with audio output, this solution serves as a valuable tool for visually impaired people, allowing them to effectively perceive and understand their surroundings. This innovative approach demonstrates the potential of technology to bridge access gaps and empower people with visual impairments in their daily lives.</jats:p>",6,2,96,109,Computer science; Computer vision; Image processing; Artificial intelligence; Image (mathematics),,,,,,http://dx.doi.org/10.36548/jiip.2024.2.002,,10.36548/jiip.2024.2.002,,,0,000-783-429-447-993; 011-199-288-144-505; 023-309-209-821-636; 024-481-147-022-986; 048-224-053-996-02X; 049-729-589-103-744; 050-047-706-859-59X; 065-279-461-724-968; 077-638-890-472-479; 100-869-464-816-806; 165-422-564-417-041; 171-057-734-143-202; 186-355-273-027-320,0,false,,
016-377-100-092-469,Counselling in the blindness system: Influencing the system,,1981,journal article,International Journal for the Advancement of Counselling,01650653; 15733246,Springer Science and Business Media LLC,Netherlands,,,4,4,287,292,Industrial and organizational psychology; Blindness; Psychology; Optometry; Medicine; Social psychology,,,,,,http://dx.doi.org/10.1007/bf00118325,,10.1007/bf00118325,,,0,,0,false,,
016-575-197-492-270,Geometric model for vision-based door detection,,2014,conference proceedings article,2014 9th International Conference on Computer Engineering & Systems (ICCES),,IEEE,,Marwa M. Shalaby; Mohammed A.-M. Salem; Alaa Khamis; Farid Melgani,"Emerging assistive technologies, such as assistive domotics and socially assistive robots have considerable potential for enhancing the lives of many elderly and physically challenged people throughout the world. Blind and visually impaired people can use these technologies for many tasks recognizing objects, handling various household duties, navigation in indoor and outdoor environments. Door detection is one of the important issues in indoor navigation. This paper presents a novel vision-based door detection technique. It is based on the geometric properties of the 4-side polygon. The efficacy of the proposed method is tested using large database of images with different levels of complexity. The experimental results show the robustness of the proposed method against changes in colors, sizes, shapes, orientations, and textures of the door. Detection rate of 83% with relatively low false positive rate for simple images is achieved. This proposed algorithm is suitable for real-time, portable applications where it only requires one digital camera and low computational resources.",,,41,46,Geometric modeling; Data pre-processing; Artificial intelligence; Polygon; Digital camera; Vision based; Computer vision; Computer science; Feature extraction; False positive rate; Robustness (computer science),,,,,https://ieeexplore.ieee.org/document/7030925/ http://ieeexplore.ieee.org/document/7030925/,http://dx.doi.org/10.1109/icces.2014.7030925,,10.1109/icces.2014.7030925,2007168081,,0,002-397-792-416-419; 005-003-427-120-577; 005-298-769-265-259; 009-612-733-037-965; 012-188-527-585-176; 030-091-798-618-058; 030-200-876-346-95X; 043-459-230-949-991; 054-374-371-905-358; 054-573-724-656-644; 066-339-899-419-202; 068-985-353-383-673; 074-324-981-337-749; 077-379-629-023-529; 083-084-584-238-372; 083-294-350-082-331; 084-679-921-430-947; 084-729-807-470-497; 094-208-711-140-943; 110-687-638-630-581; 122-729-034-369-253; 123-491-827-241-873; 145-898-655-711-084; 189-577-751-973-876,11,false,,
016-589-267-926-95X,Intelligent and connected vehicles: Current status and future perspectives,2018-09-19,2018,journal article,Science China Technological Sciences,16747321; 18691900; 1862281x; 10069321,Springer Science and Business Media LLC,China,Diange Yang; Kun Jiang; Ding Zhao; ChunLei Yu; Zhong Cao; Shichao Xie; Zhongyang Xiao; Xinyu Jiao; Wang Sijia; Kai Zhang,,61,10,1446,1471,Architecture; Key (cryptography); Systems engineering; SAFER; Task (project management); Traffic efficiency; Control algorithm; Environmental perception; Computer science; Automotive industry,,,,,https://www.sciengine.com/doi/10.1007/s11431-017-9338-1?slug=fulltext https://link.springer.com/article/10.1007/s11431-017-9338-1 https://ui.adsabs.harvard.edu/abs/2018ScChE..61.1446Y/abstract https://engine.scichina.com/doi/10.1007/s11431-017-9338-1,http://dx.doi.org/10.1007/s11431-017-9338-1,,10.1007/s11431-017-9338-1,2891526102,,1,000-432-567-201-575; 000-535-726-449-052; 001-534-088-775-305; 001-678-423-293-833; 001-855-168-429-119; 002-693-535-185-361; 003-399-951-983-687; 003-622-690-298-890; 003-744-037-897-247; 004-193-609-931-550; 004-238-485-096-413; 004-269-574-716-057; 004-448-223-662-53X; 004-734-532-285-368; 005-462-356-114-382; 005-524-012-951-68X; 005-596-245-271-581; 005-660-565-993-288; 006-517-653-311-528; 006-558-619-621-283; 006-635-061-676-045; 006-749-680-518-498; 006-750-126-523-845; 006-802-571-815-047; 007-393-518-335-863; 007-442-259-952-952; 007-853-603-471-987; 008-381-353-911-081; 008-442-117-219-591; 008-633-604-987-522; 008-740-873-433-572; 008-901-429-617-476; 009-422-734-787-306; 010-094-349-362-171; 010-843-381-298-603; 011-969-064-214-381; 012-675-516-479-835; 012-887-274-543-199; 013-025-175-636-215; 013-092-195-564-722; 013-865-257-460-200; 015-636-861-312-62X; 016-477-857-167-045; 016-837-621-369-448; 016-911-474-767-966; 017-289-563-677-929; 018-276-414-854-42X; 019-028-031-962-355; 019-876-039-192-086; 020-455-568-919-017; 021-835-509-910-62X; 021-864-897-285-389; 022-431-385-409-109; 024-464-288-190-567; 024-649-549-451-253; 025-561-164-690-659; 025-604-118-174-626; 025-655-218-811-872; 025-932-575-234-770; 026-258-380-163-081; 029-579-469-848-649; 030-138-564-599-056; 030-365-625-357-983; 030-405-019-332-413; 030-450-683-806-712; 032-419-826-758-867; 033-862-165-229-804; 033-944-504-523-805; 034-547-741-278-433; 036-262-736-391-996; 038-221-980-942-413; 038-680-184-894-556; 039-259-040-583-502; 039-345-671-762-380; 039-571-020-900-722; 040-093-372-562-351; 040-098-963-432-658; 040-635-976-918-076; 041-820-330-826-333; 042-427-368-248-642; 042-539-964-589-685; 042-540-922-383-492; 042-800-033-418-542; 043-790-215-470-560; 043-853-364-390-746; 043-864-385-092-493; 044-206-365-063-398; 045-091-299-292-22X; 045-803-875-873-114; 048-281-237-637-065; 049-672-636-837-974; 049-921-334-736-611; 050-460-819-286-424; 051-027-953-886-606; 051-173-274-683-159; 051-381-938-539-229; 051-742-756-576-820; 053-293-836-429-46X; 055-560-932-438-311; 056-679-264-069-331; 057-025-425-026-986; 057-541-825-045-116; 059-149-073-001-124; 059-502-015-312-981; 060-113-870-968-814; 060-499-281-981-687; 060-501-798-817-839; 060-699-143-753-72X; 060-961-569-614-461; 061-682-400-377-434; 061-845-997-249-948; 062-833-605-043-43X; 064-003-182-773-106; 064-683-024-880-936; 064-722-056-462-519; 065-375-884-590-658; 065-782-450-069-02X; 066-609-071-663-405; 066-955-828-741-703; 067-225-649-894-415; 067-265-658-780-045; 067-521-720-959-231; 068-839-474-068-797; 069-371-104-839-924; 069-480-623-909-181; 069-582-591-946-793; 069-772-177-540-680; 070-548-293-613-751; 070-792-342-175-87X; 072-032-677-959-730; 072-174-716-098-480; 072-874-107-520-495; 075-144-242-965-66X; 075-703-478-713-600; 076-783-757-385-418; 077-699-258-890-123; 077-720-947-239-401; 078-427-890-138-338; 080-526-627-141-164; 081-413-228-965-081; 082-855-188-268-365; 082-945-313-450-254; 085-314-270-484-38X; 085-555-973-105-97X; 086-098-362-271-384; 086-636-659-315-216; 087-291-055-661-084; 087-394-536-396-164; 089-865-448-938-454; 089-914-210-748-106; 090-732-017-124-21X; 093-184-843-363-321; 093-320-551-844-274; 093-530-739-304-785; 095-293-194-454-412; 096-149-624-562-123; 096-187-074-914-928; 096-636-777-221-491; 098-059-732-445-50X; 098-300-347-491-970; 098-559-578-365-528; 099-334-507-404-613; 099-637-883-822-34X; 100-870-803-805-588; 101-230-977-136-960; 101-604-817-238-953; 102-955-101-249-316; 110-354-653-270-574; 110-474-181-089-406; 110-475-949-484-505; 111-066-322-041-523; 111-196-199-565-326; 111-252-642-263-634; 112-594-186-685-505; 112-756-289-069-456; 112-766-191-673-152; 112-788-741-419-149; 113-843-055-348-550; 114-526-970-467-323; 114-961-841-531-188; 117-105-514-004-353; 121-859-964-511-964; 124-340-831-495-09X; 124-748-362-484-556; 127-289-085-734-180; 127-838-921-512-161; 128-363-335-880-586; 130-690-226-904-964; 131-291-883-454-551; 131-572-171-679-585; 133-946-351-269-221; 134-259-148-088-080; 134-653-634-792-407; 138-113-537-090-508; 138-539-176-267-117; 139-942-757-422-038; 140-848-585-601-404; 141-263-103-989-286; 141-904-273-375-153; 144-031-320-233-705; 144-413-525-236-372; 145-340-324-909-325; 145-395-444-176-005; 146-125-348-808-11X; 150-369-549-000-614; 150-444-153-276-208; 150-987-942-254-002; 153-831-500-065-341; 155-639-794-112-686; 156-299-459-140-274; 157-292-092-851-196; 157-813-234-018-986; 162-614-700-513-423; 163-864-607-412-829; 165-481-111-558-538; 170-087-745-585-39X; 170-317-899-004-771; 172-390-234-328-951; 181-664-681-707-736; 182-550-204-811-742; 188-022-811-980-678; 189-807-791-399-03X; 189-810-007-676-177; 191-366-596-363-999; 194-149-680-363-711; 195-091-532-336-938; 196-271-838-026-89X,132,false,,
016-626-895-380-96X,Voice Assistance for Visually Impaired People,,2021,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,S. G.; R Hareesh Kumar; P. Anagha; Rajveer Arya; S. Nabitha,,,,,,Human intelligence; Human–computer interaction; Software system; Mobile device; Object detection; Footpath; Visual impairment; Computer science; Python (programming language); Convolutional neural network,,,,,https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3868269,http://dx.doi.org/10.2139/ssrn.3868269,,10.2139/ssrn.3868269,3207171514,,0,,1,false,,
016-652-727-317-020,"Integrating Exploration, Localization, Navigation and Planning with a Common Representation",,1999,journal article,Autonomous Robots,09295593; 15737527,Springer Science and Business Media LLC,Netherlands,Alan C. Schultz; William Adams; Brian Yamauchi,,6,3,293,308,Human–computer interaction; Artificial intelligence; Mobile robot; Mobile robot navigation; Representation (systemics); Basic research; Computer vision; Computer science; Natural (music),,,,,https://link.springer.com/article/10.1023%2FA%3A1008936413435 https://doi.org/10.1023/A:1008936413435 https://dblp.uni-trier.de/db/journals/arobots/arobots6.html#SchultzAY99 http://doi.org/10.1023/A:1008936413435 https://link.springer.com/content/pdf/10.1023/A:1008936413435.pdf https://dl.acm.org/doi/10.1023/A%3A1008936413435 http://dx.doi.org/10.1023/A:1008936413435,http://dx.doi.org/10.1023/a:1008936413435,,10.1023/a:1008936413435,1565308971,,1,004-550-828-567-78X; 011-242-487-490-372; 015-485-418-356-323; 024-391-264-965-335; 025-278-629-527-832; 033-240-771-804-569; 050-491-947-679-283; 051-354-872-806-094; 053-926-543-536-795; 055-224-290-217-847; 062-779-893-197-497; 068-000-178-819-599; 073-722-974-726-137; 075-029-771-945-345; 075-468-614-874-12X; 104-303-371-352-582; 117-106-708-300-294; 123-640-995-995-582; 131-830-246-564-846; 134-259-148-088-080; 145-804-086-994-546; 147-459-444-287-621; 193-495-079-980-298,60,false,,
016-691-673-114-771,Street pavement classification based on navigation through street view imagery,2022-07-08,2022,journal article,AI & SOCIETY,09515666; 14355655,Springer Science and Business Media LLC,Germany,Rafael G. de Mesquita; Tsang I. Ren; Carlos A. B. Mello; Miguel L. P. C. Silva,,39,3,1009,1025,Segmentation; Context (archaeology); Computer science; Ground truth; Artificial intelligence; Image segmentation; Computer vision; Transport engineering; Geography; Engineering; Archaeology,,,,Instituto Nacional de Engenharia de Software; CNPQ; CAPES; FACEPE; PRONEX,,http://dx.doi.org/10.1007/s00146-022-01520-0,,10.1007/s00146-022-01520-0,,,0,004-891-165-475-061; 006-204-308-090-86X; 007-911-186-434-618; 015-417-609-922-215; 016-613-957-077-518; 016-720-185-717-899; 016-733-323-201-51X; 017-419-486-144-936; 017-497-829-556-62X; 019-646-735-560-715; 022-928-493-890-706; 024-037-388-745-797; 025-670-117-400-666; 028-634-831-937-082; 029-596-977-717-535; 030-343-040-799-199; 030-727-893-828-10X; 032-326-540-657-320; 035-877-028-247-787; 036-083-643-372-24X; 040-676-133-429-179; 041-314-165-476-904; 043-853-364-390-746; 044-627-031-134-85X; 049-921-334-736-611; 052-546-659-510-529; 068-040-551-769-654; 070-720-228-821-812; 071-264-929-566-076; 075-795-844-560-722; 075-952-052-261-675; 084-763-206-875-13X; 087-930-578-098-954; 089-341-757-540-32X; 103-132-116-121-293; 109-491-491-329-398; 116-288-609-818-155; 121-640-558-245-510; 127-295-982-712-046; 137-000-353-564-599,2,false,,
017-030-903-890-177,Particle filtering strategies for data fusion dedicated to visual tracking from a mobile robot,2008-10-23,2008,journal article,Machine Vision and Applications,09328092; 14321769,Springer Science and Business Media LLC,Germany,Ludovic Brèthes; Frédéric Lerasle; Patrick Danes; Mathias Fontmarty,,21,4,427,448,Sensory cue; Artificial intelligence; Mobile robot; Computer vision; Sensor fusion; Computer science; Particle filter; Eye tracking; BitTorrent tracker; Discriminative model; Robustness (computer science),,,,,https://link.springer.com/article/10.1007%2Fs00138-008-0174-7 http://homepages.laas.fr/lerasle/pdf/mva08.pdf https://dblp.uni-trier.de/db/journals/mva/mva21.html#BrethesLDF10 https://link.springer.com/content/pdf/10.1007%2Fs00138-008-0174-7.pdf https://rd.springer.com/article/10.1007/s00138-008-0174-7 https://homepages.laas.fr/lerasle/pdf/mva08.pdf http://dblp.uni-trier.de/db/journals/mva/mva21.html#BrethesLDF10,http://dx.doi.org/10.1007/s00138-008-0174-7,,10.1007/s00138-008-0174-7,2158264856,,0,003-744-037-897-247; 007-981-118-397-345; 008-004-151-750-554; 012-016-836-079-639; 012-535-551-238-514; 013-886-973-949-35X; 016-470-629-709-082; 017-759-306-864-09X; 019-183-007-691-217; 021-677-483-931-279; 027-039-156-687-338; 031-800-408-777-279; 037-829-308-226-557; 042-285-345-671-922; 043-109-382-116-411; 043-932-892-009-14X; 045-969-296-505-798; 050-591-553-710-880; 052-849-604-085-068; 054-146-432-294-358; 054-826-790-789-480; 055-287-898-944-047; 060-768-563-854-478; 061-797-239-711-378; 065-195-476-079-895; 068-310-842-612-582; 070-096-783-749-622; 076-124-457-836-767; 083-643-271-705-521; 087-302-963-041-965; 087-963-853-817-61X; 089-346-655-094-595; 093-305-729-075-535; 099-327-271-738-979; 112-796-075-588-310; 116-931-207-519-021; 125-471-933-258-147; 134-387-442-288-83X; 139-465-785-176-250; 146-731-124-708-03X; 147-129-230-312-367; 148-441-705-418-919; 171-670-501-915-879; 187-285-780-061-49X,13,false,,
017-108-666-837-168,Research advances of indoor navigation for blind people: A brief review of technological instrumentation,,2020,journal article,IEEE Instrumentation & Measurement Magazine,10946969; 19410123,Institute of Electrical and Electronics Engineers (IEEE),United States,Darius Plikynas; Arunas Zvironas; Marius Gudauskis; Andrius Budrionis; Povilas Daniušis; Ieva Sliesoraityte,"Blind persons need electronic traveling aid (ETA) solutions for better orientation and navigation in unfamiliar indoor environments, with embedded features for detection and recognition of both obstacles and desired destinations such as rooms, staircases, and elevators. Because the use of GPS for locational references is impractical indoors, the development of such navigation systems is challenging and requires a systematic review and evaluation of different technological approaches. Using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) method, we evaluated and compared current research papers that deal with the prototyping of assistive devices (visual sensory perception substitution with audio and haptic signals) for blind and visually impaired persons. We conducted an instructional assessment of selected indoor navigation prototypes using three main criteria: navigation technologies, sensors, and computer vision approaches. For the latter category, we conducted a separate systematic review, as papers in this research area primarily specialize in software computer vision solutions rather than hardware. The paper provides useful insights for researchers regarding technological instrumentation for the development of ETA solutions for blind and visually impaired (VI) persons in the field of indoor orientation and navigation.",23,4,22,32,Human–computer interaction; Instrumentation (computer programming); Haptic technology; Systematic review; Perception; Software; Field (computer science); Computer science; Global Positioning System; Orientation (computer vision),,,,,https://dblp.uni-trier.de/db/journals/imm/imm23.html#PlikynasZGBDS20 https://ieeexplore.ieee.org/document/9126068/,http://dx.doi.org/10.1109/mim.2020.9126068,,10.1109/mim.2020.9126068,3037395827,,0,001-722-850-820-058; 003-086-871-090-793; 003-787-428-155-830; 004-772-554-982-380; 007-907-758-104-429; 009-435-402-661-806; 009-853-748-682-331; 011-126-524-756-136; 013-212-129-187-029; 015-075-700-493-012; 019-215-610-025-597; 019-914-067-455-570; 020-998-360-866-973; 022-631-644-111-523; 023-438-345-162-875; 024-964-397-377-997; 026-931-024-930-691; 027-720-558-803-335; 029-360-136-216-567; 030-091-798-618-058; 032-538-345-541-781; 033-245-290-160-646; 033-627-073-872-114; 042-251-157-585-319; 042-893-530-917-399; 043-788-961-182-656; 048-523-147-281-068; 053-215-378-840-089; 055-495-282-581-758; 056-745-340-837-164; 057-847-555-860-541; 059-149-073-001-124; 061-205-550-296-347; 064-103-833-604-92X; 065-418-822-524-660; 065-770-367-239-317; 066-246-145-459-483; 067-894-235-988-508; 068-446-370-600-681; 068-526-647-480-345; 075-519-293-886-898; 077-455-969-276-663; 079-349-286-645-136; 079-763-049-744-61X; 081-871-676-661-821; 089-822-552-618-426; 089-827-360-713-19X; 091-900-147-932-211; 093-548-249-489-049; 094-097-235-517-666; 096-458-723-476-553; 097-395-382-637-910; 098-440-015-522-27X; 102-952-660-914-374; 127-070-672-662-621; 128-975-670-807-332; 134-226-557-899-385; 137-032-010-877-936; 147-118-674-595-78X; 157-306-463-851-764; 166-810-485-231-725; 168-330-006-032-201; 174-289-314-292-190; 175-854-191-542-73X; 179-237-001-293-456; 186-810-372-950-411,22,false,,
017-268-420-622-506,Machine-Learning-Based Device for Visually Impaired Person,2020-02-09,2020,book chapter,Advances in Intelligent Systems and Computing,21945357; 21945365,Springer Singapore,,Tuhina Priya; Kotla Sai Sravya; S. Umamaheswari,"The blind-assistive devices promote the detection of objects and alerting the user by a buzzer or an alarm. In this project, a camera is placed inside a cap which is used by the visually impaired person. The machine-learning algorithm is used for accurate detection of the object and provides an alarm. The ultrasonic sensor is used to measure the distance between the visually impaired person and the real-time object detected when the object is detected the alarm is actuated. Nowadays, the assistive devices include the involvement of both hardware and software section to assist the blind user. The proposed method for the visually impaired person aims to detect the object more accurately so that the visually impaired person can navigate to their full potential in real-time application.",,,79,88,Artificial intelligence; Buzzer; Software; ALARM; Visually impaired; Computer vision; Computer science; Object (computer science),,,,,https://link.springer.com/chapter/10.1007/978-981-15-0199-9_7,http://dx.doi.org/10.1007/978-981-15-0199-9_7,,10.1007/978-981-15-0199-9_7,3004556764,,0,004-766-408-593-108; 010-297-238-622-591; 013-056-435-015-274; 015-620-703-326-093; 037-238-441-886-162; 069-116-650-623-84X; 101-767-617-199-036; 109-259-898-739-94X; 115-263-538-545-630; 120-171-959-125-135; 146-084-945-839-717,3,false,,
017-368-838-377-139,Embedded Computer Vision for Object Recognition in Smart Devices for the Blind,2023-11-15,2023,conference proceedings article,2023 International Conference on Sustainable Communication Networks and Application (ICSCNA),,IEEE,,N Venkata Srikanth; Narra Prem Sai; Raji Pandurangan,"This study has proposed a novel method for assisting the visually impaired people by combining computer vision with cutting-edge technological instruments. Convolutional Neural Networks (CNNs) are primarily utilized for performing realtime object recognition. To address the object identification challenge, this study collects and preprocesses a large dataset, construct an efficient CNN architecture, optimize inference speed, and connect this system to smart devices. The results indicate significant gains in precision and throughput for practical applications. This innovation may significantly enhance the quality of life for the visually impaired by increasing their mobility. The research study has far-reaching implications for assistive technology and sheds light on the possibility of making the world more accessible for those with visual impairments. Long-term, intends to enhance the consumer product by enlarging the dataset, refining the model's precision, and investigating additional features. This research is a crucial first step in utilizing technology to provide equal opportunities to individuals with visual impairments.",,,,,Computer science; Convolutional neural network; Construct (python library); Object (grammar); Cognitive neuroscience of visual object recognition; Inference; Artificial intelligence; Identification (biology); Throughput; Computer vision; Machine vision; Enhanced Data Rates for GSM Evolution; Human–computer interaction; Telecommunications; Botany; Wireless; Biology; Programming language,,,,,,http://dx.doi.org/10.1109/icscna58489.2023.10370696,,10.1109/icscna58489.2023.10370696,,,0,004-071-983-923-934; 023-772-125-161-831; 033-554-664-404-916; 038-339-232-698-535; 038-847-248-738-782; 040-941-067-536-586; 063-518-361-601-226; 064-484-968-441-883; 068-274-091-408-752; 086-141-234-318-177; 112-901-104-231-647; 145-286-072-054-601; 175-893-859-445-457,0,false,,
017-387-981-396-440,A dataset for the recognition of obstacles on blind sidewalk,2021-08-16,2021,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Wu Tang; De-er Liu; Xiaoli Zhao; Zenghui Chen; Chen Zhao,,22,1,69,82,,,,,National Natural Science Foundation of China; National Natural Science Foundation of China; Natural Science Foundation of Jiangxi Province,,http://dx.doi.org/10.1007/s10209-021-00837-9,,10.1007/s10209-021-00837-9,,,0,000-051-707-082-100; 002-242-013-537-46X; 004-019-652-142-123; 004-193-609-931-550; 004-242-945-383-184; 005-225-964-313-490; 006-565-780-206-428; 010-684-117-336-855; 014-036-969-711-168; 015-967-911-639-302; 016-027-592-222-572; 016-658-296-028-629; 017-159-670-613-140; 017-289-190-110-902; 018-152-176-285-561; 020-916-460-373-883; 026-303-765-898-687; 031-218-334-653-826; 032-052-662-821-330; 041-584-324-491-598; 042-197-355-305-758; 045-309-399-228-849; 048-395-635-842-481; 049-317-239-158-314; 050-113-304-231-41X; 063-656-074-507-347; 064-043-611-038-916; 069-480-623-909-181; 071-773-804-778-45X; 075-465-285-579-483; 085-686-689-031-272; 102-122-613-011-188; 102-787-341-517-228; 113-452-723-216-812; 118-372-317-168-327; 123-748-458-842-892; 125-112-083-666-544; 127-671-470-361-947; 136-190-477-330-077; 149-162-653-144-382; 186-002-073-406-20X; 186-175-701-515-323,7,false,,
017-457-495-169-008,Optical Aircraft Positioning for Monitoring of the Integrated Navigation System during Landing Approach,,2019,journal article,Gyroscopy and Navigation,20751087; 20751109,Pleiades Publishing Ltd,Germany,Peter Hecker; Michael Angermann; Ulf Bestmann; Andreas Dekiert; S. Wolkow,,10,4,216,230,Systems engineering; Navigation system; Focus (computing); Computer science; Aviation,,,,,https://link.springer.com/article/10.1134/S2075108719040084,http://dx.doi.org/10.1134/s2075108719040084,,10.1134/s2075108719040084,2999585600,,2,000-977-152-347-169; 004-256-157-514-312; 006-757-928-199-428; 010-332-317-704-836; 010-405-777-952-616; 011-888-182-733-006; 014-070-923-568-051; 015-498-335-293-091; 018-892-415-027-396; 029-658-643-349-622; 029-867-281-964-978; 030-479-554-722-44X; 037-244-092-351-709; 042-584-913-649-61X; 045-292-257-728-831; 046-211-296-813-325; 050-421-072-808-420; 051-549-146-166-31X; 054-057-570-284-356; 054-531-493-861-484; 058-018-748-532-606; 058-584-994-083-857; 059-465-143-701-834; 064-711-326-093-925; 065-057-650-035-527; 066-104-243-527-175; 072-314-404-197-000; 080-087-360-945-97X; 081-603-338-843-349; 090-356-924-590-461; 094-068-009-896-845; 105-336-702-647-420; 108-854-805-412-746; 114-462-498-727-207; 126-278-206-019-707; 129-787-745-643-108; 133-752-773-908-059; 142-919-363-447-729; 148-476-292-925-225; 175-779-922-209-256; 178-571-039-933-479; 192-820-327-321-516,10,false,,
017-507-843-293-279,Computer Vision on Mars,2007-03-20,2007,journal article,International Journal of Computer Vision,09205691; 15731405,Springer Science and Business Media LLC,Netherlands,Larry Matthies; Mark Maimone; Andrew E. Johnson; Yang Cheng; Reg G. Willson; Carlos Y. Villalpando; Steve Goldberg; Andres Huertas; Andrew Neil Stein; Anelia Angelova,,75,1,67,92,Artificial intelligence; Collision avoidance (spacecraft); Spacecraft; Computer vision; Robotics; Computer science; Visual odometry; Stereopsis; Mars Exploration Program; Odometer; Image processing,,,,,https://dblp.uni-trier.de/db/journals/ijcv/ijcv75.html#MatthiesMJCWVGHSA07 https://dx.doi.org/10.1007/s11263-007-0046-z https://doi.org/10.1007/s11263-007-0046-z https://link.springer.com/article/10.1007/s11263-007-0046-z https://dl.acm.org/citation.cfm?id=1285522 https://dl.acm.org/doi/10.1007/s11263-007-0046-z https://www.ri.cmu.edu/pub_files/pub4/matthies_larry_2007_1/matthies_larry_2007_1.pdf http://dx.doi.org/10.1007/s11263-007-0046-z https://rd.springer.com/article/10.1007/s11263-007-0046-z,http://dx.doi.org/10.1007/s11263-007-0046-z,,10.1007/s11263-007-0046-z,1997568160,,6,000-808-439-351-985; 000-833-598-726-881; 002-725-652-744-795; 003-258-594-828-101; 003-646-042-190-271; 003-786-081-937-715; 009-259-983-940-650; 009-574-637-634-60X; 009-908-658-304-279; 015-226-492-949-868; 015-642-586-236-845; 018-307-494-247-740; 019-334-909-078-409; 019-362-778-436-517; 020-985-751-808-831; 021-311-500-600-230; 022-286-924-201-748; 025-521-147-729-746; 026-155-622-423-412; 026-389-937-620-56X; 027-466-612-985-512; 031-475-251-116-416; 035-129-460-100-131; 035-488-017-022-84X; 037-161-889-794-685; 039-782-735-688-956; 040-979-481-029-348; 041-251-117-571-661; 041-929-016-845-375; 043-605-465-154-934; 043-637-581-298-923; 043-931-219-537-802; 046-802-299-419-627; 052-455-450-082-494; 056-897-447-245-68X; 057-320-873-246-618; 058-009-976-442-827; 061-142-425-680-977; 062-862-937-049-945; 063-134-963-424-885; 063-911-102-895-873; 064-192-142-714-297; 064-810-689-733-139; 066-393-414-307-059; 067-842-821-404-478; 070-690-859-626-904; 071-322-624-009-343; 071-732-230-696-162; 071-738-893-977-552; 074-181-230-983-04X; 075-069-370-881-025; 075-443-874-151-517; 076-030-189-305-646; 078-118-171-312-599; 081-561-685-507-937; 082-391-007-325-197; 084-334-655-890-196; 085-783-096-643-219; 088-206-404-943-264; 089-078-612-647-93X; 090-302-910-864-245; 092-903-172-334-002; 093-050-180-655-88X; 094-738-067-799-250; 095-704-857-080-057; 096-965-989-836-006; 100-779-547-216-540; 103-928-235-146-85X; 104-039-036-721-803; 107-263-782-211-042; 110-167-280-695-858; 110-573-736-536-382; 110-829-567-762-878; 111-261-180-102-880; 112-941-086-353-603; 113-613-610-281-073; 116-968-484-849-378; 119-510-276-569-480; 120-588-671-187-744; 121-515-846-773-980; 123-142-523-178-152; 129-428-643-001-863; 129-927-523-515-698; 132-542-923-913-282; 134-858-706-455-952; 136-205-668-195-962; 136-677-306-583-277; 139-035-590-326-44X; 148-284-773-847-559; 148-975-926-020-839; 155-143-042-341-196; 156-197-477-017-010; 156-964-450-777-030; 167-043-859-163-432; 171-570-644-567-48X; 172-886-071-399-040; 177-489-235-670-961; 178-595-596-826-023; 190-520-062-006-256; 196-552-373-065-872,172,false,,
017-534-129-232-800,Blind Spot Obstacle Detection from Monocular Camera Images with Depth Cues Extracted by CNN,2018-11-26,2018,journal article,Automotive Innovation,20964250; 25228765,Springer Science and Business Media LLC,,Yuxiang Guo; Itsuo Kumazawa; Chuyo Kaku,"The images from a monocular camera can be processed to detect depth information regarding obstacles in the blind spot area captured by the side-view camera of a vehicle. The depth information is given as a classification result “near” or “far” when two blocks in the image are compared with respect to their distances and the depth information can be used for the purpose of blind spot area detection. In this paper, the proposed depth information is inferred from a combination of blur cues and texture cues. The depth information is estimated by comparing the features of two image blocks selected within a single image. A preliminary experiment demonstrates that a convolutional neural network (CNN) model trained by deep learning with a set of relatively ideal images achieves good accuracy. The same CNN model is applied to distinguish near and far obstacles according to a specified threshold in the vehicle blind spot area, and the promising results are obtained. The proposed method uses a standard blind spot camera and can improve safety without other additional sensing devices. Thus, the proposed approach has the potential to be applied in vehicular applications for the detection of objects in the driver’s blind spot.",1,4,362,373,Deep learning; Image (mathematics); Artificial intelligence; Set (abstract data type); Principal component analysis; Obstacle; Computer vision; Computer science; Depth perception; Blind spot; Convolutional neural network,,,,,https://link.springer.com/article/10.1007%2Fs42154-018-0036-6 https://link.springer.com/content/pdf/10.1007/s42154-018-0036-6.pdf,http://dx.doi.org/10.1007/s42154-018-0036-6,,10.1007/s42154-018-0036-6,2902276282,,0,009-961-620-068-234; 025-964-861-227-172; 031-502-282-110-309; 033-279-985-649-013; 035-078-919-654-406; 049-108-182-176-333; 055-653-048-792-46X; 061-720-909-498-372; 061-746-749-792-059; 079-539-668-726-182; 089-138-919-058-162; 148-921-062-232-864,9,true,cc-by,hybrid
017-571-875-650-919,BIBM Workshops - Detecting stairs and pedestrian crosswalks for the blind by RGBD camera,,2012,conference proceedings article,2012 IEEE International Conference on Bioinformatics and Biomedicine Workshops,,IEEE,,Shuihua Wang; Yingli Tian,"A computer vision-based wayfinding and navigation aid can improve the mobility of blind and visually impaired people to travel independently. In this paper, we develop a new framework to detect and recognize stairs and pedestrian crosswalks using a RGBD camera. Since both stairs and pedestrian crosswalks are featured by a group of parallel lines, we first apply Hough transform to extract the concurrent parallel lines based on the RGB channels. Then, the Depth channel is employed to further recognize pedestrian crosswalks, upstairs, and downstairs using support vector machine (SVM) classifiers. Furthermore, we estimate the distance between the camera and stairs for the blind users. The detection and recognition results on our collected dataset demonstrate that the effectiveness and efficiency of our proposed framework.",,,732,739,Support vector machine; Artificial intelligence; Hough transform; Object detection; Pedestrian; Stairs; Computer vision; Computer science; Feature extraction; Contextual image classification; Parallel,,,,,http://doi.ieeecomputersociety.org/10.1109/BIBMW.2012.6470227 http://dblp.uni-trier.de/db/conf/bibm/bibmw2012.html#WangT12 https://www.computer.org/csdl/pds/api/csdl/proceedings/download-article/12OmNzYeALN/pdf http://ieeexplore.ieee.org/document/6470227/ https://www.computer.org/csdl/proceedings-article/bibmw/2012/06470227/12OmNzYeALN https://dblp.uni-trier.de/db/conf/bibm/bibmw2012.html#WangT12 https://ieeexplore.ieee.org/document/6470227/,http://dx.doi.org/10.1109/bibmw.2012.6470227,,10.1109/bibmw.2012.6470227,2002859935,,7,016-408-213-077-520; 027-893-571-523-130; 058-923-977-741-380; 059-647-236-855-012; 062-957-059-042-10X; 063-161-240-623-745; 071-806-151-845-245; 099-150-824-995-115; 109-617-690-629-802; 116-976-189-445-151; 135-646-014-794-306; 148-405-057-283-922; 150-597-344-978-541; 168-052-814-589-107; 183-913-731-618-625; 195-842-744-142-232,55,false,,
017-698-009-416-534,Computational Methods for Exudates Detection and Macular Edema Estimation in Retinal Images: A Survey,2018-09-19,2018,journal article,Archives of Computational Methods in Engineering,11343060; 18861784,Springer Science and Business Media LLC,Spain,Muhammad Moazam Fraz; Maryam Badar; Asad Waqar Malik; Sarah Barman,,26,4,1193,1220,Optometry; Hypertensive retinopathy; Diabetic retinopathy; Macular edema; Retinal; Retinal image; Anatomical structures; Disease manifestation; Medicine; Segmentation,,,,,https://eprints.kingston.ac.uk/id/eprint/42046/ https://link.springer.com/article/10.1007/s11831-018-9281-4,http://dx.doi.org/10.1007/s11831-018-9281-4,,10.1007/s11831-018-9281-4,2890742214,,0,001-494-181-344-557; 002-189-396-855-237; 002-504-876-273-612; 004-535-303-649-971; 004-710-673-807-506; 006-596-852-768-262; 006-760-995-208-993; 006-772-243-869-199; 007-243-043-653-433; 007-490-908-040-82X; 008-126-266-858-587; 008-716-685-014-151; 010-890-695-564-734; 011-525-176-541-54X; 011-699-889-917-738; 012-186-124-281-873; 012-623-072-749-771; 012-815-410-750-350; 014-542-159-458-369; 015-257-596-674-32X; 015-978-855-801-953; 016-267-914-825-759; 016-335-676-256-539; 016-796-137-902-96X; 018-663-711-431-271; 018-835-513-079-324; 019-029-259-892-116; 020-755-240-561-327; 021-146-224-534-161; 021-258-019-739-359; 022-830-894-312-813; 023-015-580-985-324; 023-554-619-128-93X; 023-813-664-007-110; 024-055-044-793-372; 024-511-812-376-755; 027-269-071-292-947; 027-419-786-978-644; 027-938-117-168-323; 028-696-653-010-949; 029-046-498-775-352; 031-010-137-610-816; 032-352-757-363-533; 034-877-755-574-008; 035-519-275-193-78X; 037-551-804-931-921; 037-827-158-319-301; 038-094-347-842-733; 038-444-937-213-192; 038-568-245-363-178; 038-684-901-913-157; 039-674-359-839-766; 040-394-990-874-722; 042-235-522-698-523; 042-374-869-574-829; 043-319-105-168-710; 044-754-750-776-770; 045-757-694-292-869; 046-032-513-734-356; 047-095-207-629-632; 047-969-485-734-568; 049-865-716-199-512; 050-835-850-705-218; 051-608-571-504-225; 051-704-604-505-655; 052-016-974-406-862; 052-975-793-599-70X; 056-079-471-494-059; 056-643-183-526-911; 058-692-230-805-301; 058-868-068-459-725; 059-149-073-001-124; 059-406-173-396-439; 059-766-451-968-759; 060-755-509-845-340; 061-979-131-515-137; 065-449-968-674-540; 068-099-590-745-782; 069-480-623-909-181; 071-563-686-501-816; 074-097-650-866-235; 074-181-911-425-529; 074-943-749-330-430; 076-543-955-654-866; 076-544-353-064-986; 076-863-669-088-157; 077-216-949-221-923; 077-862-876-702-166; 079-027-086-340-045; 081-871-379-485-956; 082-217-934-374-826; 085-452-814-409-800; 085-998-220-698-654; 088-991-456-719-017; 090-451-289-525-147; 092-337-605-063-399; 095-727-071-343-144; 098-781-502-557-15X; 103-911-298-257-580; 103-929-107-208-520; 111-741-581-464-690; 114-321-519-144-723; 115-347-998-748-234; 124-175-189-789-456; 125-008-973-361-46X; 127-760-255-143-642; 130-745-014-036-435; 136-678-054-538-836; 137-439-299-101-358; 139-744-386-509-427; 145-038-154-640-556; 146-043-843-301-878; 146-628-158-233-671; 147-567-582-673-713; 148-775-059-372-089; 160-384-122-019-218; 182-929-586-980-598; 195-091-532-336-938; 195-737-682-443-982; 196-485-651-443-355,16,false,,
017-863-373-769-99X,Analysis and Implementation for a Walking Support System for Visually Impaired People,2011-07-01,2011,journal article,International Journal of Intelligent Mechatronics and Robotics,21561664; 21561656,IGI Global,United States,Eklas Hossain; Raisuddin Khan; Riza Muhida; Ahad Ali,"Visually impaired people are faced with challenges in detecting information about terrain. This paper presents a new walking support system for the blind to navigate without any assistance from others or using a guide cane. In this research, a belt, wearable around the waist, is equipped with four ultrasonic sensors and one sharp infrared sensor. Based on mathematical models, the specifications of the ultrasonic sensors are selected to identify optimum orientation of the sensors for detecting stairs and holes. These sensors are connected to a microcontroller and laptop for analyzing terrain. An algorithm capable of classifying various types of obstacles is developed. After successful tests using laptop, the microcontroller is used for the walking system, named 'Belt for Blind', to navigate their environment. The unit is also equipped with a servo motor and a buzzer to generate outputs that inform the user about the type of obstacle ahead. The device is light, cheap, and consumes less energy. However, this device is limited to standard pace of mobility and cannot differentiate between animate and inanimate obstacles. Further research is recommended to overcome these deficiencies to improve mobility of blind people.",1,3,46,62,Engineering; Wearable computer; Artificial intelligence; Terrain; Microcontroller; Obstacle; Pace; Buzzer; Laptop; Computer vision; Orientation (computer vision),,,,,https://www.igi-global.com/chapter/analysis-implementation-walking-support-system/76448 http://irep.iium.edu.my/4158/ https://dblp.uni-trier.de/db/journals/ijimr/ijimr1.html#HossainKMA11 https://dl.acm.org/doi/10.4018/ijimr.2011070104 https://doi.org/10.4018/ijimr.2011070104,http://dx.doi.org/10.4018/ijimr.2011070104,,10.4018/ijimr.2011070104,2044441319,,0,001-076-828-128-846; 007-527-228-704-510; 013-551-333-215-556; 013-847-555-426-159; 014-157-934-526-940; 020-678-057-737-391; 022-697-627-694-941; 030-127-369-356-741; 030-376-516-941-332; 031-799-548-880-483; 034-350-308-512-672; 045-039-665-546-894; 046-649-192-050-710; 053-266-220-901-486; 058-443-047-291-990; 060-257-784-475-928; 064-309-126-323-022; 065-992-777-809-97X; 073-722-974-726-137; 076-911-364-196-257; 078-736-735-476-395; 096-803-870-115-728; 098-811-589-762-176; 111-057-090-228-06X; 112-121-560-424-373; 125-501-372-179-565; 127-483-973-270-088; 129-233-008-155-337; 146-425-395-790-21X; 183-935-933-940-273,3,false,,
018-217-381-523-789,How Challenging is a Challenge? CEMS: a Challenge Evaluation Module for SLAM Visual Perception,2024-03-09,2024,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Xuhui Zhao; Zhi Gao; Hao Li; Hong Ji; Hong Yang; Chenyang Li; Hao Fang; Ben M. Chen,"<jats:title>Abstract</jats:title><jats:p>Despite promising SLAM research in both vision and robotics communities, which fundamentally sustains the autonomy of intelligent unmanned systems, visual challenges still threaten its robust operation severely. Existing SLAM methods usually focus on specific challenges and solve the problem with sophisticated enhancement or multi-modal fusion. However, they are basically limited to particular scenes with a non-quantitative understanding and awareness of challenges, resulting in a significant performance decline with poor generalization and(or) redundant computation with inflexible mechanisms. To push the frontier of visual SLAM, we propose a fully computational reliable evaluation module called CEMS (Challenge Evaluation Module for SLAM) for general visual perception based on a clear definition and systematic analysis. It decomposes various challenges into several common aspects and evaluates degradation with corresponding indicators. Extensive experiments demonstrate our feasibility and outperformance. The proposed module has a high consistency of 88.298% compared with annotation ground truth, and a strong correlation of 0.879 compared with SLAM tracking performance. Moreover, we show the prototype SLAM based on CEMS with better performance and the first comprehensive CET (Challenge Evaluation Table) for common SLAM datasets (EuRoC, KITTI, etc.) with objective and fair evaluations of various challenges. We make it available online to benefit the community on our website.</jats:p>",110,1,,,Computer science; Artificial intelligence; Consistency (knowledge bases); Simultaneous localization and mapping; Robotics; Visualization; Ground truth; Computer vision; Machine learning; Robot; Mobile robot,,,,National Natural Science Foundation of China Major Program; National Natural Science Foundation of China Major Program; Hubei Province Natural Science Foundation; Hubei Province Natural Science Foundation; Hubei Science and Technology Major Project; Hubei Science and Technology Major Project,https://link.springer.com/content/pdf/10.1007/s10846-024-02077-4.pdf https://doi.org/10.1007/s10846-024-02077-4,http://dx.doi.org/10.1007/s10846-024-02077-4,,10.1007/s10846-024-02077-4,,,0,000-931-985-270-609; 001-972-264-197-541; 003-679-975-983-67X; 006-287-896-333-464; 007-194-610-049-876; 007-831-437-996-843; 009-081-070-465-247; 015-416-439-509-860; 015-571-444-508-782; 017-575-182-761-622; 021-830-418-927-650; 023-341-788-052-498; 023-735-000-187-111; 024-538-632-076-325; 026-861-335-262-140; 027-652-817-448-613; 028-657-877-442-167; 030-710-903-797-116; 030-857-934-369-153; 032-119-305-821-862; 032-240-599-161-349; 033-876-006-219-278; 034-354-061-711-431; 034-894-880-999-667; 034-899-678-356-492; 036-279-751-554-416; 037-524-810-054-28X; 039-841-356-124-234; 041-003-596-369-969; 041-373-598-509-054; 047-326-510-583-783; 047-837-064-438-109; 048-510-019-754-753; 056-360-935-145-864; 060-141-480-164-106; 061-773-741-935-645; 062-688-820-159-55X; 066-026-078-004-344; 066-154-284-042-188; 069-186-819-600-197; 070-100-068-658-392; 072-221-189-739-176; 075-239-589-817-684; 078-093-064-868-569; 082-110-175-818-667; 085-859-436-036-764; 087-995-484-019-204; 094-577-932-024-915; 099-500-778-216-930; 099-508-323-333-213; 101-790-519-213-047; 121-214-820-186-578; 124-180-153-132-23X; 124-735-758-923-292; 124-869-908-217-89X; 134-784-798-046-187; 140-882-954-139-446; 146-941-439-409-750; 171-103-584-499-274; 174-050-973-447-665,4,true,cc-by,hybrid
018-262-121-105-137,Analysis of Navigation Assistants for Blind and Visually Impaired People: A Systematic Review,,2021,journal article,IEEE Access,21693536,Institute of Electrical and Electronics Engineers (IEEE),United States,Sulaiman Khan; Shah Nazir; Habib Ullah Khan,"Over the last few decades, the development in the field of navigation and routing devices has become a hindering task for the researchers to develop smart and intelligent guiding mechanism at indoor and outdoor locations for blind and visually impaired people (BVIPs). The existing research need to be analysed from a historical perception including early research on the first electronic travel aids to the use of modern artificial vision models for the navigation of BVIPs. Diverse approaches such as: e-cane or guide dog, infrared-based cane, laser based walker and many others are proposed for the navigation of BVIPs. But most of these techniques have limitations such as: infrared and ultrasonic based assistance has short range capacities for object detection. While laser based assistance can harm other people if it directly hit them on their eyes or any other part of the body. These trade-offs are critical to bring this technology in practice.To systematically assess, analyze, and identify the primary studies in this specialized field and provide an overview of the trends and empirical evidence in the proposed field. This systematic research work is performed by defining a set of relevant keywords, formulating four research questions, defining selection criteria for the articles, and synthesizing the empirical evidence in this area. Our pool of studies include 191 most relevant articles to the proposed field reported between 2011 and 2020 (a portion of 2020 is included). This systematic mapping will help the researchers, engineers, and practitioners to make more authentic decisions for finding gaps in the available navigation assistants and suggest a new and enhanced smart assistant application accordingly to ensure safety and accurate guidance of the BVIPs. This research work have several implications in particular the impact of reducing fatalities and major injuries of BVIPs.",9,,26712,26734,Set (psychology); Work (electrical); Data science; Perception; Task (project management); Field (computer science); Visualization; Computer science; Global Positioning System,,,,"Department of Accounting and Information Systems, College of Business and Economics, Qatar University, Doha, Qatar; Department of Computer Science, University of Swabi, Pakistan; Qatar National Library, Doha and Department of Accounting and Information System, Qatar University, Doha, Qatar",https://dblp.uni-trier.de/db/journals/access/access9.html#KhanNK21 https://ieeexplore.ieee.org/document/9328100 https://doi.org/10.1109/ACCESS.2021.3052415,http://dx.doi.org/10.1109/access.2021.3052415,,10.1109/access.2021.3052415,3134395502,,0,000-729-381-857-952; 001-308-499-154-495; 001-325-059-112-077; 001-672-715-224-680; 001-863-333-801-624; 002-037-819-294-847; 003-829-044-422-525; 005-088-603-765-979; 005-297-162-462-657; 006-150-323-853-406; 006-487-017-505-503; 006-557-756-014-856; 007-004-465-976-71X; 007-223-103-783-187; 007-437-510-247-350; 007-530-851-453-706; 007-573-197-769-65X; 007-725-872-437-764; 007-907-758-104-429; 008-139-948-663-237; 008-361-673-204-978; 008-367-942-736-25X; 008-684-469-671-014; 008-713-968-971-388; 009-097-030-325-625; 009-671-283-951-337; 009-690-618-758-576; 009-829-120-033-790; 010-252-997-204-631; 010-582-518-033-890; 011-821-531-136-685; 011-876-327-472-778; 012-081-521-655-479; 012-833-369-160-621; 013-108-056-949-247; 014-615-325-881-117; 015-075-700-493-012; 015-195-112-017-008; 015-291-279-210-205; 015-379-866-005-587; 015-440-630-508-33X; 015-944-524-719-546; 017-300-737-144-757; 018-709-039-252-240; 018-767-375-770-819; 018-828-594-955-588; 020-935-356-605-174; 021-025-067-076-564; 021-176-270-241-156; 022-031-347-734-67X; 022-940-138-590-719; 023-143-288-167-347; 023-210-552-095-483; 023-505-319-851-99X; 024-335-765-549-544; 024-429-002-344-847; 024-523-299-123-05X; 024-888-082-944-828; 025-028-410-791-784; 025-295-668-075-968; 025-415-238-630-208; 025-536-303-962-788; 025-764-110-104-300; 027-720-558-803-335; 027-934-550-270-689; 028-087-404-356-291; 028-683-545-086-724; 029-313-431-507-058; 029-360-136-216-567; 029-482-411-276-907; 029-885-082-233-530; 030-251-661-047-944; 030-440-500-148-114; 030-817-464-225-30X; 031-127-219-937-467; 031-221-517-989-821; 032-284-166-524-962; 032-768-175-764-600; 033-245-290-160-646; 033-457-095-812-412; 033-859-426-701-613; 034-936-259-439-98X; 035-155-677-944-748; 035-544-542-108-733; 036-337-450-605-489; 036-578-364-944-588; 036-779-046-415-85X; 037-411-892-552-299; 037-545-584-168-942; 037-877-656-330-779; 038-208-649-693-158; 039-085-681-603-182; 039-799-789-300-41X; 040-419-618-479-011; 040-533-527-422-721; 040-602-202-207-827; 040-940-319-245-132; 041-660-737-654-748; 043-376-650-343-976; 044-314-151-897-109; 044-632-900-761-983; 044-644-110-114-333; 045-111-552-339-892; 045-205-438-482-211; 045-957-607-388-923; 046-309-239-761-288; 046-554-537-599-919; 046-605-753-137-118; 046-625-623-299-790; 046-824-464-627-655; 047-356-385-281-782; 047-844-248-722-855; 049-139-218-880-325; 049-575-510-227-033; 049-870-006-718-994; 050-491-665-175-175; 050-620-337-235-317; 051-674-567-787-301; 051-861-879-703-203; 053-196-706-640-254; 053-567-501-093-090; 054-367-982-946-305; 055-759-390-517-581; 056-311-013-170-616; 056-357-389-437-52X; 056-404-390-495-54X; 056-734-407-356-215; 057-043-820-517-493; 057-565-643-418-627; 057-837-646-456-879; 057-847-555-860-541; 058-308-310-638-823; 058-780-408-093-023; 059-493-893-682-992; 061-348-708-228-909; 063-372-733-367-946; 064-521-070-547-235; 065-831-055-423-605; 065-989-043-835-282; 066-627-120-919-876; 066-880-999-231-966; 066-987-626-151-98X; 067-064-551-263-83X; 067-424-570-333-916; 068-163-377-582-711; 068-246-254-541-665; 069-706-555-672-053; 070-068-111-472-212; 070-116-981-693-266; 070-740-216-883-969; 070-765-623-587-543; 070-823-446-691-691; 070-965-792-440-573; 071-875-360-614-784; 072-018-820-086-231; 073-293-095-885-817; 074-089-299-077-609; 075-198-789-342-013; 075-693-006-853-959; 076-541-220-345-473; 076-752-025-952-569; 077-101-929-493-550; 078-946-722-239-45X; 080-148-076-219-168; 080-286-913-930-789; 080-304-413-097-782; 080-595-626-727-998; 081-162-236-525-226; 081-747-388-175-61X; 082-036-797-529-285; 083-275-795-865-394; 083-722-838-889-866; 084-183-891-000-305; 085-411-210-988-556; 085-612-945-410-967; 088-278-729-994-45X; 088-581-250-265-96X; 088-743-350-822-814; 090-723-743-421-935; 093-026-277-102-800; 093-281-588-152-037; 093-548-249-489-049; 097-009-336-212-593; 098-966-666-704-92X; 099-334-901-486-872; 100-575-932-125-341; 100-867-544-470-164; 101-173-483-983-547; 101-637-146-335-197; 101-976-408-993-63X; 102-100-961-622-631; 105-198-687-221-268; 105-451-873-515-258; 106-467-416-201-600; 107-659-830-473-823; 107-688-223-168-520; 108-525-198-891-337; 109-356-782-692-064; 109-879-745-469-237; 110-409-001-674-560; 111-772-107-673-112; 115-439-483-580-16X; 118-876-311-299-827; 119-517-530-171-868; 121-439-923-704-187; 122-728-061-908-190; 122-807-798-418-538; 124-851-601-621-358; 125-351-519-179-754; 126-391-967-174-60X; 129-622-838-722-010; 130-524-507-836-36X; 132-619-031-181-728; 134-692-988-441-892; 136-732-042-076-069; 136-794-910-171-095; 137-032-010-877-936; 138-956-519-021-897; 142-938-943-762-964; 142-968-657-337-219; 143-388-073-266-491; 143-858-441-690-063; 144-575-152-600-97X; 150-085-954-177-320; 150-659-697-905-891; 151-427-106-185-479; 153-471-712-074-630; 154-089-472-332-328; 156-120-400-966-933; 159-474-331-562-499; 160-616-914-721-582; 164-345-968-752-679; 164-630-824-349-584; 168-330-006-032-201; 170-582-125-506-257; 180-719-271-353-911; 182-230-054-803-698; 183-511-885-590-15X; 186-081-237-537-037; 189-270-541-373-235; 194-245-004-896-301,73,true,"CC BY, CC BY-NC-ND",gold
018-495-691-006-904,Intelligent perception and control for space robotics: Autonomous Satellite Rendezvous and Docking,2007-06-26,2007,journal article,Machine Vision and Applications,09328092; 14321769,Springer Science and Business Media LLC,Germany,Faisal Z. Qureshi; Demetri Terzopoulos,,19,3,141,161,Human–computer interaction; Machine learning; Robotic arm; Artificial intelligence; Cognition; Visual perception; Perception; Rendezvous; Robotic systems; Space robotics; Computer science; Machine vision,,,,,http://dx.doi.org/10.1007/s00138-007-0085-z https://link.springer.com/article/10.1007/s00138-007-0085-z http://cs.ucla.edu/~dt//papers/mva08/mva08.pdf https://dblp.uni-trier.de/db/journals/mva/mva19.html#QureshiT08 https://faculty.uoit.ca/qureshi/pubs/08-mva-j.pdf http://www.ai.rug.nl/~feldbrug/cogrobot/Intelligent%20perception%20and%20control%20for%20space%20robotics.pdf https://dx.doi.org/10.1007/s00138-007-0085-z https://www.cs.ucla.edu/~dt/papers/mva08/mva08.pdf http://faculty.uoit.ca/qureshi/pubs/08-mva-j.pdf https://dl.acm.org/doi/10.1007/s00138-007-0085-z,http://dx.doi.org/10.1007/s00138-007-0085-z,,10.1007/s00138-007-0085-z,1553140600,,0,000-289-809-657-817; 000-903-606-610-389; 003-376-198-814-469; 009-061-958-170-216; 015-451-854-109-972; 016-422-615-207-405; 019-647-300-706-820; 023-423-115-520-065; 025-889-586-624-837; 029-713-401-747-221; 032-201-070-226-577; 033-820-137-055-688; 038-779-998-597-227; 039-522-953-253-255; 046-932-112-127-874; 047-562-879-043-984; 050-721-440-297-527; 055-735-696-155-118; 057-511-449-198-719; 062-676-403-288-723; 081-245-180-170-609; 088-610-237-583-250; 092-047-318-751-687; 094-196-393-590-256; 098-036-019-031-895; 104-520-420-118-68X; 109-788-736-886-22X; 117-594-738-990-025; 117-870-636-418-771; 118-896-627-422-359; 129-260-261-279-425; 135-845-315-546-105; 136-890-310-755-96X; 152-380-657-152-922; 153-947-718-537-895; 164-779-673-141-334; 168-260-031-719-485; 177-909-268-638-632; 178-267-220-100-74X; 180-894-467-480-355,12,false,,
019-035-229-098-609,A computer vision framework for the analysis and interpretation of the cephalo-ocular behavior of drivers,2011-12-14,2011,journal article,Machine Vision and Applications,09328092; 14321769,Springer Science and Business Media LLC,Germany,Samy Metari; Florent Prel; Thierry Moszkowicz; Denis Laurendeau; Normand Teasdale; Steven S. Beauchemin; Martin Simoneau,,24,1,159,173,Haar-like features; Artificial intelligence; Visual search; Component-based software engineering; Software; Overtaking; Driving simulator; Computer vision; Computer science; Simulation; Blind spot; Robustness (computer science),,,,,https://dblp.uni-trier.de/db/journals/mva/mva24.html#MetariPMLTBS13 https://doi.org/10.1007/s00138-011-0381-5 https://link.springer.com/article/10.1007/s00138-011-0381-5 https://www.csd.uwo.ca/faculty/beau/PAPERS/mva-11.pdf http://doi.org/10.1007/s00138-011-0381-5,http://dx.doi.org/10.1007/s00138-011-0381-5,,10.1007/s00138-011-0381-5,1996204076,,0,006-499-842-015-430; 011-985-853-512-699; 012-016-836-079-639; 012-367-568-909-097; 016-834-308-590-249; 018-166-275-068-287; 033-364-231-006-368; 038-933-018-683-385; 043-022-952-629-736; 045-507-137-309-355; 053-124-107-553-711; 054-826-790-789-480; 058-041-313-038-33X; 061-247-703-838-369; 063-727-029-670-810; 066-346-561-791-539; 066-825-414-262-934; 069-468-812-769-147; 070-910-823-089-504; 071-953-309-257-962; 074-006-536-890-200; 081-257-574-173-560; 083-997-264-880-59X; 085-467-193-488-607; 096-403-654-729-669; 103-396-684-286-685; 112-581-648-464-641; 123-425-155-530-908; 133-282-899-329-163; 133-594-165-652-01X; 141-195-749-163-406; 146-375-240-703-81X; 148-897-069-516-010; 152-529-213-821-18X; 158-740-723-789-847; 159-275-182-206-711,3,false,,
019-147-046-636-589,A Smart Vision Based Navigation Aid for the Visually Impaired,2019-11-22,2019,journal article,Asian Journal of Research in Computer Science,25818260,Sciencedomain International,,Benjamin Kommey; Kumbong Herrman; Ernest Ofosu Addo,"<jats:p>Due to the ever increasing number of blind and visually impaired people in the world, there has been a great amount of research dedicated to the design of assistive technologies to support them. The various assistive technologies apply different techniques including laser, ultrasonic sensors and image processing. Autonomous navigation is a significant challenge for the visually impaired, it makes life uncomfortable for them and poses serious safety issues. In this paper we review the progress made so far in vision based systems and propose an approach for developing navigation aids through techniques used in other autonomous systems like self-driving vehicles. The proposed system uses a front camera to capture images and then produces commensurate guiding audio signals that allow the user freely move in their environment. An extra rear camera is included to allow the user to obtain more information about the scene. Care is taken however not to overload the user with information. The proposed method is tested both in indoor and outdoor scenes and is effective in notifying the user for any obstacles. The goal of this paper is to propose a model for and to develop subsystems for an intelligent, high performance, affordable and easy to use image based navigation aid for the visually impaired.</jats:p>",,,1,8,Human–computer interaction; Vision based; Navigation aid; Visually impaired; Computer science,,,,,https://www.journalajrcos.com/index.php/AJRCOS/article/view/30114 https://journalajrcos.com/index.php/AJRCOS/article/download/30114/56510,http://dx.doi.org/10.9734/ajrcos/2019/v4i330114,,10.9734/ajrcos/2019/v4i330114,2989844834,,0,,3,true,,gold
019-188-173-508-847,Smartphone-based food recognition system using multiple deep CNN models,2021-08-12,2021,journal article,Multimedia Tools and Applications,13807501; 15737721,Springer Science and Business Media LLC,Netherlands,Abdulnaser A. Fakhrou; Jayakanth Kunhoth; Somaya Al Maadeed,"People with blindness or low vision utilize mobile assistive tools for various applications such as object recognition, text recognition, etc. Most of the available applications are focused on recognizing generic objects. And they have not addressed the recognition of food dishes and fruit varieties. In this paper, we propose a smartphone-based system for recognizing the food dishes as well as fruits for children with visual impairments. The Smartphone application utilizes a trained deep CNN model for recognizing the food item from the real-time images. Furthermore, we develop a new deep convolutional neural network (CNN) model for food recognition using the fusion of two CNN architectures. The new deep CNN model is developed using the ensemble learning approach. The deep CNN food recognition model is trained on a customized food recognition dataset.The customized food recognition dataset consists of 29 varieties of food dishes and fruits. Moreover, we analyze the performance of multiple state of art deep CNN models for food recognition using the transfer learning approach. The ensemble model performed better than state of art CNN models and achieved a food recognition accuracy of 95.55 % in the customized food dataset. In addition to that, the proposed deep CNN model is evaluated in two publicly available food datasets to display its efficacy for food recognition tasks.",80,21,33011,33032,Machine learning; Ensemble forecasting; Transfer of learning; Artificial intelligence; Blindness; Food recognition; Deep cnn; Computer science; Ensemble learning; Cognitive neuroscience of visual object recognition; Convolutional neural network,,,,Qatar University; Qatar University,https://dblp.uni-trier.de/db/journals/mta/mta80.html#FakhrouKA21 https://link.springer.com/content/pdf/10.1007/s11042-021-11329-6.pdf https://link.springer.com/article/10.1007/s11042-021-11329-6,http://dx.doi.org/10.1007/s11042-021-11329-6,,10.1007/s11042-021-11329-6,3193979817,,0,001-777-255-149-616; 004-269-574-716-057; 004-306-253-497-690; 005-515-411-667-892; 007-404-308-406-134; 009-716-857-979-918; 016-343-427-901-322; 034-385-813-935-30X; 043-857-708-728-244; 045-298-655-695-753; 045-976-049-179-177; 050-429-230-208-874; 053-968-972-722-364; 061-038-230-856-231; 069-619-205-068-811; 074-199-903-520-265; 077-101-929-493-550; 079-310-940-223-758; 093-379-300-076-799; 094-702-872-703-173; 096-583-011-862-360; 097-468-019-228-228; 108-478-951-248-95X; 110-654-817-036-501; 114-536-786-109-103; 128-897-617-870-753; 129-383-784-414-462; 139-552-118-652-140; 140-302-802-648-686; 142-400-988-090-538; 142-513-404-636-236; 161-326-029-147-46X; 178-606-451-314-116,34,true,cc-by,hybrid
019-479-808-388-771,A Fully-Autonomous Aerial Robot for Search and Rescue Applications in Indoor Environments using Learning-Based Techniques,2018-07-03,2018,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Carlos Sampedro; Alejandro Rodriguez-Ramos; Hriday Bavle; Adrian Carrio; Paloma de la Puente; Pascual Campoy,"Search and Rescue (SAR) missions represent an important challenge in the robotics research field as they usually involve exceedingly variable-nature scenarios which require a high-level of autonomy and versatile decision-making capabilities. This challenge becomes even more relevant in the case of aerial robotic platforms owing to their limited payload and computational capabilities. In this paper, we present a fully-autonomous aerial robotic solution, for executing complex SAR missions in unstructured indoor environments. The proposed system is based on the combination of a complete hardware configuration and a flexible system architecture which allows the execution of high-level missions in a fully unsupervised manner (i.e. without human intervention). In order to obtain flexible and versatile behaviors from the proposed aerial robot, several learning-based capabilities have been integrated for target recognition and interaction. The target recognition capability includes a supervised learning classifier based on a computationally-efficient Convolutional Neural Network (CNN) model trained for target/background classification, while the capability to interact with the target for rescue operations introduces a novel Image-Based Visual Servoing (IBVS) algorithm which integrates a recent deep reinforcement learning method named Deep Deterministic Policy Gradients (DDPG). In order to train the aerial robot for performing IBVS tasks, a reinforcement learning framework has been developed, which integrates a deep reinforcement learning agent (e.g. DDPG) with a Gazebo-based simulator for aerial robotics. The proposed system has been validated in a wide range of simulation flights, using Gazebo and PX4 Software-In-The-Loop, and real flights in cluttered indoor environments, demonstrating the versatility of the proposed system in complex SAR missions.",95,2,601,627,Deep learning; Visual servoing; Supervised learning; Artificial intelligence; Search and rescue; Robotics; Computer science; Real-time computing; Convolutional neural network; Robot; Reinforcement learning,,,,,https://link.springer.com/article/10.1007/s10846-018-0898-1 https://dblp.uni-trier.de/db/journals/jirs/jirs95.html#SampedroRBCPC19 http://oa.upm.es/64148/ https://orbilu.uni.lu/handle/10993/47158 https://doi.org/10.1007/s10846-018-0898-1 https://dialnet.unirioja.es/servlet/articulo?codigo=7057526 https://digital.csic.es/handle/10261/215809,http://dx.doi.org/10.1007/s10846-018-0898-1,,10.1007/s10846-018-0898-1,2810169700,,1,000-358-250-056-880; 000-419-348-053-182; 001-332-650-636-033; 005-394-866-928-683; 005-827-930-577-779; 013-716-231-191-541; 021-954-968-302-721; 022-890-782-823-074; 023-438-345-162-875; 023-928-444-110-407; 025-151-529-700-089; 026-079-944-340-095; 027-749-616-022-764; 029-242-339-360-828; 030-902-177-577-771; 031-218-334-653-826; 033-360-258-344-721; 034-480-351-716-641; 043-367-199-132-423; 044-068-902-875-760; 047-727-195-203-863; 048-416-032-912-452; 054-111-647-666-251; 059-175-115-013-369; 059-465-725-495-954; 061-101-033-659-713; 062-502-512-653-486; 062-589-536-361-618; 066-133-135-091-645; 068-286-242-197-322; 077-659-768-520-37X; 084-510-831-812-921; 085-825-394-760-758; 092-543-059-316-439; 097-619-038-637-612; 105-072-310-689-393; 110-558-633-723-294; 112-701-308-953-936; 118-235-306-059-749; 119-979-929-190-67X; 120-944-152-032-327; 132-602-463-268-72X; 136-869-426-292-892; 141-077-134-230-230; 142-474-806-225-780; 154-194-340-708-025; 158-703-460-552-262; 162-550-235-020-227; 187-784-763-443-572; 195-091-532-336-938; 195-597-492-425-466,146,true,cc-by-nc-nd,green
019-626-082-551-88X,Guide-Me: Voice authenticated indoor user guidance system,2021-12-01,2021,conference proceedings article,"2021 IEEE 12th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)",,IEEE,,D. M. L. V Dissanayake; R. G. M. D. R. P Rajapaksha; U. P Prabhashawara; S. A. D. S.P Solanga; J. A. D. C. Anuradha Jayakody,"Due to a lack of knowledge about the building structure and possible impediments, the majority of blind persons require assistance when traveling through unknown regions. To solve this issue, this paper provides ""Guide-Me"" as a strategy for indoor navigation with optimum accessibility, usability, and security, decreasing obstacles that the user may meet when traveling through indoor surroundings. Because the intended audience for this research is blind or visually impaired persons, ""Guide-Me"" makes use of the user’s voice-based inputs. This paper also includes Bluetooth beacon integration for localization, a Smart stick with sensors for obstacle detection, a machine learning model for voice authentication, and an algorithm protocol for a secure connection between server and application Integration driven architecture to assist vision impaired in navigating the known and unknown indoor environment.",,,,,Computer science; Usability; Bluetooth; Obstacle; Authentication (law); Human–computer interaction; Protocol (science); Location tracking; Architecture; Wireless; Real-time computing; Computer security; Telecommunications; Medicine; Art; Visual arts; Alternative medicine; Pathology; Political science; Law,,,,,,http://dx.doi.org/10.1109/uemcon53757.2021.9666733,,10.1109/uemcon53757.2021.9666733,,,0,,0,false,,
020-081-309-753-936,An evaluation of EfficientDet for object detection used for indoor robots assistance navigation,2022-03-19,2022,journal article,Journal of Real-Time Image Processing,18618200; 18618219,Springer Science and Business Media LLC,Germany,Mouna Afif; Riadh Ayachi; Yahia Said; Mohamed Atri,,19,3,651,661,Computer science; Artificial intelligence; Object detection; Robot; Robotics; Robustness (evolution); Mobile robot; Machine vision; Implementation; Indoor positioning system; Computer vision; Cognitive neuroscience of visual object recognition; Real-time computing; Pruning; Embedded system; Object (grammar); Pattern recognition (psychology); Biochemistry; Chemistry; Accelerometer; Biology; Agronomy; Gene; Programming language; Operating system,,,,,,http://dx.doi.org/10.1007/s11554-022-01212-4,,10.1007/s11554-022-01212-4,,,0,000-575-646-477-703; 008-036-071-421-060; 015-786-862-042-194; 019-474-130-120-41X; 026-400-078-913-130; 026-994-702-158-53X; 034-181-142-547-052; 037-182-418-585-695; 041-058-139-703-974; 042-576-163-108-661; 044-472-558-807-885; 045-309-399-228-849; 047-040-726-932-715; 048-395-635-842-481; 061-579-383-511-500; 061-956-788-871-070; 073-216-818-844-578; 076-823-621-907-858; 096-729-849-252-522; 116-773-439-500-011; 147-818-711-392-410; 148-736-583-892-708,6,false,,
020-084-668-265-858,Monitoring Activity of Taking Medicine by Incorporating RFID and Video Analysis.,2013-02-07,2013,journal article,Network modeling and analysis in health informatics and bioinformatics,21926662; 21926670,Springer Nature,United States,Faiz M. Hasanuzzaman; Xiaodong Yang; Yingli Tian; Qingshan Liu; Elizabeth Capezuti,"In this paper, we present a new framework to monitor medication intake for elderly individuals by incorporating a video camera and radio frequency identification (RFID) sensors. The proposed framework can provide a key function for monitoring activities of daily living (ADLs) of elderly people at their own home. In an assistive environment, RFID tags are applied on medicine bottles located in a medicine cabinet so that each medicine bottle will have a unique ID. The description of the medicine data for each tag is manually input to a database. RFID readers will detect if any of these bottles are taken away from the medicine cabinet and identify the tag attached on the medicine bottle. A video camera is installed to continue monitoring the activity of taking medicine by integrating face detection and tracking, mouth detection, background subtraction, and activity detection. The preliminary results demonstrate that 100 % detection accuracy for identifying medicine bottles and promising results for monitoring activity of taking medicine.",2,2,61,70,Human–computer interaction; World Wide Web; Health informatics; Key (cryptography); Radio-frequency identification; Video camera; Medicine bottles; Medicine cabinet; Background subtraction; Computer science; Face detection,,,,NEI NIH HHS (R21 EY020990) United States,https://paperity.org/p/8703906/monitoring-activity-of-taking-medicine-by-incorporating-rfid-and-video-analysis https://link.springer.com/content/pdf/10.1007%2Fs13721-013-0025-y.pdf http://europepmc.org/articles/PMC3728181 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3728181/ https://dblp.uni-trier.de/db/journals/netmahib/netmahib2.html#HasanuzzamanYTLC13 https://link.springer.com/article/10.1007/s13721-013-0025-y https://link.springer.com/article/10.1007/s13721-013-0025-y/fulltext.html,http://dx.doi.org/10.1007/s13721-013-0025-y,23914344,10.1007/s13721-013-0025-y,2127720411,PMC3728181,0,001-333-723-225-011; 008-276-911-226-552; 011-433-961-967-302; 012-016-836-079-639; 012-367-568-909-097; 014-072-551-477-851; 020-358-012-142-690; 029-735-894-637-764; 032-133-232-035-617; 034-303-305-119-726; 034-350-308-512-672; 038-138-501-630-942; 043-022-952-629-736; 043-028-545-180-115; 043-348-573-940-487; 045-874-715-328-967; 047-146-552-070-70X; 048-298-689-305-353; 052-152-856-946-645; 053-124-107-553-711; 053-600-802-130-58X; 058-041-313-038-33X; 062-948-955-235-394; 069-468-812-769-147; 070-910-823-089-504; 071-953-309-257-962; 075-220-223-913-049; 079-520-235-014-120; 102-730-868-162-286; 109-509-373-703-249; 114-230-344-122-202; 117-311-904-055-92X; 125-922-638-229-327; 133-594-165-652-01X; 136-878-651-956-733; 141-139-127-112-456; 151-015-279-636-660; 154-822-066-020-598; 183-432-777-724-741; 187-572-604-127-113; 197-278-198-112-260,18,true,,green
020-091-917-670-404,Smart Gloves used for Blind Visually Impaired using Wearable Technology,2021-12-21,2021,journal article,Engineering and Scientific International Journal,23947187; 23947179,Innovative Research Developers and Publishers,,Satish N; Satish N,"<jats:p>In order to assist visually disabled persons, a research that significantly aids certain individuals in pacing more confidently has been suggested. The research proposes a smart walking glove that alerts visually disabled people about hazards and pits, and therefore the machine can assist them in walking with less injuries. It defines a more sophisticated navigational device for visually disabled people. It consists of a simple stroll fitted with sensors that provide environmental data. Global Positioning System (GPS) technology is combined with a microcontroller for those who wish to assist their loved ones in keeping track of them. Ultrasonic sensors, GPS receivers, vibrators, Peripheral Interface Controller (PIC) rulers, and batteries are all included in this statute. The traditional purpose of the system is to grant a convenience. The traditional purpose of the device is to provide a simple and effective tool for the inadvisable to solve their difficulties in everyday life. Technology and human lifestyles can no longer be isolated, and this has become a global issue. Is science, however, capable of assisting visually disabled people? The impediment between ben and ben is generally scored by blind people. For the Illogical people, mobility and action in accordance with pace and protection are described as men who are at ease in their surroundings unless they are counted among others. Cane or information dogs are commonly used to support the blind with their travel. However, there are some problems with the navigation support. Because the individual and as a result, use, the ferule grant small preview.; The ferule offers a small preview due to the human, and as a result, the consumer must be more vigilant in compliance with the speed than if the cell died slowly. Because of the knowledge dogs, educating and coordinating the dogs with illogical people is a challenging undertaking, but the end result is negligible.</jats:p>",8,4,,,Global Positioning System; Wearable computer; Disabled people; Computer science; Pace; Human–computer interaction; Simple (philosophy); Internet privacy; Visually impaired; Wearable technology; Embedded system; Psychology; Telecommunications; Applied psychology; Life style; Philosophy; Geodesy; Epistemology; Geography,,,,,http://irdp.info/download/3023/ https://doi.org/10.30726/esij/v8.i4.2021.84022,http://dx.doi.org/10.30726/esij/v8.i4.2021.84022,,10.30726/esij/v8.i4.2021.84022,,,0,,0,true,,gold
020-172-125-171-345,Real-time camera orientation estimation based on vanishing point tracking under Manhattan World assumption,2014-04-04,2014,journal article,Journal of Real-Time Image Processing,18618200; 18618219,Springer Science and Business Media LLC,Germany,Wael Elloumi; Sylvie Treuillet; Remy Leconge,,13,4,669,684,RANSAC; Euler angles; Artificial intelligence; Real image; Computer vision; Computer science; Cluster analysis; Computer graphics; Sampling (statistics); Robustness (computer science); Vanishing point,,,,,https://link.springer.com/article/10.1007/s11554-014-0419-9/fulltext.html https://doi.org/10.1007/s11554-014-0419-9 https://link.springer.com/article/10.1007/s11554-014-0419-9 https://dblp.uni-trier.de/db/journals/jrtip/jrtip13.html#ElloumiTL17,http://dx.doi.org/10.1007/s11554-014-0419-9,,10.1007/s11554-014-0419-9,1998378562,,1,000-407-039-337-220; 001-020-479-953-879; 013-089-871-422-440; 014-688-480-729-809; 017-446-312-279-885; 021-131-854-519-01X; 023-209-685-665-779; 028-303-905-295-179; 035-664-698-424-093; 035-697-369-273-125; 038-935-761-146-702; 041-714-218-844-321; 041-996-057-465-723; 043-084-632-735-576; 045-213-909-104-007; 046-572-302-718-265; 049-549-480-498-190; 069-147-115-918-381; 071-155-883-706-818; 083-070-756-258-525; 088-392-324-377-786; 099-219-885-828-866; 103-268-221-871-275; 114-913-233-800-210; 125-050-558-099-360; 126-321-344-350-554; 131-853-990-412-535; 137-917-177-415-483; 142-704-041-009-082; 144-698-142-202-097; 152-159-771-302-24X; 153-262-125-026-377; 163-706-836-255-755; 170-088-932-205-633,14,false,,
020-197-649-392-289,Evaluation of CT virtual intravascular endoscopy in the visualisation of aortic ostium in patients undergoing fenestrated stent grafts: a preliminary study,2007-06-19,2007,journal article,International Journal of Computer Assisted Radiology and Surgery,18616410; 18616429,Springer Science and Business Media LLC,Germany,Zhonghua Sun; Yvonne B. Allen; B. Fitzsimmons; David Ernest Hartley; Michael Lawrence-Brown,,2,S1,4,43,Radiology; Aortic aneurysm; Fenestration; Endoscopy; Stent; Aortic ostium; International congress; In patient; Three dimensional imaging; Medicine,,,,,http://espace.library.curtin.edu.au/R?func=dbin-jump-full&local_base=gen01-era02&object_id=20750 https://espace.curtin.edu/handle/20.500.11937/6188 https://espace.curtin.edu.au/handle/20.500.11937/6188,http://dx.doi.org/10.1007/s11548-007-0082-8,,10.1007/s11548-007-0082-8,1911514279,,0,,0,false,,
020-427-034-948-413,Visually impaired children: “coming to better terms”,2009-01-10,2009,journal article,Documenta ophthalmologica. Advances in ophthalmology,15732622; 00124486,Springer Science and Business Media LLC,Netherlands,Frans C. C. Riemslag,,119,1,1,7,Optometry; Developmental psychology; Psychology; Referral; Value (ethics); Rehabilitation; Vision disorder; Visual impairment; Congenital stationary night blindness; Terminology; Genetic counseling,,Albinism/diagnosis; Child; Diagnostic Errors; Humans; Night Blindness/congenital; Terminology as Topic; Vision Disorders/diagnosis,,,https://link.springer.com/article/10.1007%2Fs10633-008-9161-6 https://link.springer.com/content/pdf/10.1007%2Fs10633-008-9161-6.pdf https://www.ncbi.nlm.nih.gov/pubmed/19137348,http://dx.doi.org/10.1007/s10633-008-9161-6,19137348,10.1007/s10633-008-9161-6,2036078482,,0,001-414-666-772-91X; 001-753-026-857-276; 009-518-801-429-899; 010-897-073-556-07X; 016-367-857-370-10X; 020-969-615-938-738; 024-746-349-490-789; 033-485-172-310-95X; 039-782-725-847-394; 051-514-696-166-226; 059-457-798-407-10X; 061-105-002-079-191; 065-904-953-558-557; 068-558-700-705-811; 105-892-748-516-730; 111-155-738-692-914; 115-643-698-619-63X; 119-688-801-351-040,6,false,,
020-473-708-650-988,Traffic sign detection and recognition using RGSM and a novel feature extraction method,2021-04-21,2021,journal article,Peer-to-Peer Networking and Applications,19366442; 19366450,Springer Science and Business Media LLC,United States,M. Sudha; Galdis pushparathi,,14,4,2026,2037,Audio signal processing; Artificial intelligence; Pattern recognition; Mobile device; Jaccard index; Classifier (linguistics); Traffic sign; Computer science; Feature extraction; Audio signal; Traffic sign recognition,,,,,https://dblp.uni-trier.de/db/journals/ppna/ppna14.html#Sudhap21 https://doi.org/10.1007/s12083-021-01138-x https://link.springer.com/article/10.1007/s12083-021-01138-x,http://dx.doi.org/10.1007/s12083-021-01138-x,,10.1007/s12083-021-01138-x,3153632221,,0,006-843-517-588-796; 014-605-908-859-916; 019-061-465-135-730; 020-553-462-964-29X; 022-489-327-390-755; 023-609-534-604-437; 028-243-474-822-333; 028-585-134-519-792; 050-170-935-980-59X; 057-240-059-850-706; 060-601-840-135-881; 067-163-091-503-84X; 071-370-070-774-17X; 074-040-453-429-370; 076-308-391-841-921; 083-463-155-850-870; 084-291-901-009-425; 089-412-963-980-284; 094-876-724-085-705; 104-478-174-914-835; 115-784-825-983-100; 136-308-835-051-096,7,false,,
020-583-537-901-400,Robust real-time detection of multi-color markers on a cell phone,2011-06-03,2011,journal article,Journal of Real-Time Image Processing,18618200; 18618219,Springer Science and Business Media LLC,Germany,Homayoun Bagherinia; Roberto Manduchi,"We describe a fast algorithm to detect special multi-color markers with a camera cell phone. These color markers can be used for environmental labeling, for example, as a wayfinding aid for persons with visual impairment. Using a cascade of elemental detectors, robust detection is achieved at an extremely low computational cost. We also introduce a strategy to select surfaces for the marker that ensure very low specular reflection, thus facilitating color-based recognition.",8,2,207,223,Specular reflection; Pattern recognition (psychology); Artificial intelligence; Color constancy; Phone; Environmental labeling; Fast algorithm; Computer vision; Computer science; Computer graphics; Detector,,,,,http://www.tpbin.com/Uploads/Subjects/259f58f6-0133-45b4-bd2b-1649de5d3ccf.pdf https://escholarship.org/content/qt0g26h819/qt0g26h819.pdf?t=paxqe7 https://escholarship.org/uc/item/0g26h819 https://dblp.uni-trier.de/db/journals/jrtip/jrtip8.html#BagheriniaM13 https://link.springer.com/article/10.1007/s11554-011-0206-9 https://link.springer.com/content/pdf/10.1007%2Fs11554-011-0206-9.pdf http://users.soe.ucsc.edu/~manduchi/papers/JRTIP.pdf https://doi.org/10.1007/s11554-011-0206-9 https://www.tpbin.com/Uploads/Subjects/259f58f6-0133-45b4-bd2b-1649de5d3ccf.pdf,http://dx.doi.org/10.1007/s11554-011-0206-9,,10.1007/s11554-011-0206-9,2076045247,,8,001-567-495-856-445; 002-553-865-119-83X; 004-646-724-908-312; 005-991-142-819-380; 008-446-346-338-357; 010-187-670-334-591; 011-052-252-516-18X; 012-016-836-079-639; 012-080-878-782-417; 017-413-759-318-536; 020-358-012-142-690; 020-861-226-833-049; 026-726-113-728-919; 037-332-014-254-719; 040-669-338-024-938; 051-181-129-303-914; 054-772-476-662-552; 055-557-444-887-815; 062-219-110-203-470; 066-934-812-613-990; 092-642-014-398-50X; 099-820-000-852-453; 100-447-413-320-333; 118-719-054-138-205; 119-516-706-529-460; 126-915-683-383-48X; 129-243-738-783-182; 134-486-618-445-40X; 151-790-631-478-110; 173-428-838-013-421; 179-362-367-088-003; 191-715-714-073-961,23,true,,green
020-859-876-260-155,Assistive spectacles: A vision for the future,2020-04-09,2020,journal article,Social Theory & Health,14778211; 1477822x,Springer Science and Business Media LLC,United Kingdom,Steven Richardson; Thomas Abrams,,20,1,37,53,,,,,,,http://dx.doi.org/10.1057/s41285-020-00140-2,,10.1057/s41285-020-00140-2,,,0,012-595-669-983-450; 015-439-502-714-990; 017-602-924-353-347; 021-649-831-213-698; 022-612-917-125-856; 026-397-165-951-421; 030-611-556-012-439; 031-760-017-230-31X; 035-824-338-654-821; 049-034-496-128-159; 049-371-486-526-265; 049-518-833-605-966; 056-256-742-618-514; 056-501-097-456-431; 060-412-922-036-593; 069-345-943-504-376; 070-617-021-183-416; 072-035-000-934-826; 075-595-700-900-509; 084-629-850-747-836; 085-025-694-385-86X; 086-384-379-773-61X; 092-333-857-389-602; 099-150-317-608-133; 101-370-543-680-461; 143-998-271-402-798; 152-500-405-493-943; 154-684-902-390-429; 178-993-884-731-444; 192-829-948-558-058,1,false,,
020-998-360-866-973,ASSETS - Towards a Sign-Based Indoor Navigation System for People with Visual Impairments,2016-10-23,2016,journal article,ASSETS. Annual ACM Conference on Assistive Technologies,,,United States,Alejandro Rituerto; Giovanni Fusco; James M. Coughlan,"Navigation is a challenging task for many travelers with visual impairments. While a variety of GPS-enabled tools can provide wayfinding assistance in outdoor settings, GPS provides no useful localization information indoors. A variety of indoor navigation tools are being developed, but most of them require potentially costly physical infrastructure to be installed and maintained, or else the creation of detailed visual models of the environment. We report development of a new smartphone-based navigation aid, which combines inertial sensing, computer vision and floor plan information to estimate the user's location with no additional physical infrastructure and requiring only the locations of signs relative to the floor plan. A formative study was conducted with three blind volunteer participants demonstrating the feasibility of the approach and highlighting the areas needing improvement.",2016,,287,288,Navigational instrument; Human–computer interaction; Variety (cybernetics); Artificial intelligence; Mobile robot navigation; Navigation system; Task (project management); Computer vision; Computer science; Formative assessment; Global Positioning System; Floor plan,Navigation; blindness; low vision; wayfinding,,,ACL HHS (90RE5008) United States; ACL HHS (90RE5008),https://dl.acm.org/doi/pdf/10.1145/2982142.2982202 https://europepmc.org/articles/PMC5714555 https://dblp.uni-trier.de/db/conf/assets/assets2016.html#RituertoFC16 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5714555,http://dx.doi.org/10.1145/2982142.2982202,29214242,10.1145/2982142.2982202,2532929858,PMC5714555,0,013-056-435-015-274; 016-209-364-096-807; 026-684-493-399-893; 048-929-432-459-187; 102-976-506-139-695; 122-704-318-024-950; 133-846-312-833-522; 188-569-539-275-740,18,true,,green
021-076-303-008-716,Software for enhancing accessibility and fighting info-exclusion,2014-06-20,2014,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,João Barroso; Frode Eika Sandnes; Hugo Paredes; Leontios J. Hadjileontiadis; Paulo Martins,"In the innovative world of Information and Communication Technologies (ICT), there is a continual flow of new information and experiences engaging individuals in the digital era. The use of ICT is increasing and proving to be of greater importance in people’s everyday lives, accessing services to fulfill basic human needs. However, this increase in use and importance of ICT raises concerns to whether these technologies are fully accessible for all people, especially to disabled and elderly people. Disability can take many forms and can cover a wide range of impairments—some people have sensory disabilities covering difficulties with hearing, speech and sight; others are physically disabled and have problems with mobility; some suffer from cognitive impairments such as dyslexia and learning difficulties and there are those who battle with the debilitating effects of diseases such as arthritis and multiple sclerosis. Millions of people have disabilities that affect their use of the ICT due to the accessibility barriers that make it difficult or impossible for many people with disabilities to use them. Designing products that can be used by people with a wide range of abilities and disabilities, is a requirement in the years to come. A paradigmatic change in the software industry is needed to focus on the benefits of universal design. In fact, there is an increasing recognition that ICT accessibility can be a tool for commercial growth and the promotion of anti-discrimination legislation. Access to technology by disabled and elderly people is a major issue in allowing their integration in society. The growing global proportion of disabled and elderly people signifies that the urgency of this issue will not decrease in the years to come. This special issue comes after the successful organization of the 4th International Conference on Software Development for Enhancing Accessibility and Fighting Info-exclusion—DSAI 2012, which was held in Douro Region, Portugal (http://dsai2012.utad.pt/). This UAIS special issue is focused on research related to interfaces and interaction, assistive technologies, case studies and aging, taking also into account the target audience of the UAIS Journal. As a result, six (6) papers describing methodologies, systems, case studies and literature reviews from the area of universal access comprise the special issue. Marcio Martins, Antonio Cunha and Leonel Morgado’s ‘‘Usability test of 3D connection 3D mice versus keyboard?mouse in Second Life undertaken by people with motor disabilities due to medullary lesions’’ evaluates the usability of 3D controllers to navigate in 3D virtual worlds. The usability test was performed by 10 participants with motor disabilities due to medullary lesions on C5-D11 vertebrae. The main conclusion is that the most challenging tasks in the keyboard?mouse combination become less challenging using 3D mice and the least stable mouse, SpaceNavigator, originated the best feedback, contrary to originally expected results. The contribution by Margaretha Izzo, entitled ‘‘Universal Design for Learning: Enhancing Achievement of J. Barroso (&) H. Paredes P. Martins INESC TEC, Porto, Portugal e-mail: jbarroso@utad.pt",14,1,1,3,Internet privacy; Psychology; Usability; Maslow's hierarchy of needs; Legislation; Target audience; Metaverse; Information and Communications Technology; Promotion (rank); Universal design,,,,,https://link.springer.com/content/pdf/10.1007%2Fs10209-014-0359-y.pdf https://dblp.uni-trier.de/db/journals/uais/uais14.html#BarrosoSPHM15 https://link.springer.com/article/10.1007/s10209-014-0359-y https://link.springer.com/article/10.1007/s10209-014-0359-y/fulltext.html https://paperity.org/p/36812890/software-for-enhancing-accessibility-and-fighting-info-exclusion,http://dx.doi.org/10.1007/s10209-014-0359-y,,10.1007/s10209-014-0359-y,2006981697,,0,,3,true,,bronze
021-561-339-287-306,HRI (Companion) - Coupled Indoor Navigation for People Who Are Blind,,2018,conference proceedings article,Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction,,ACM,,Amal Nanavati; Xiang Zhi Tan; Aaron Steinfeld,"This paper presents our design of an autonomous navigation system for a mobile robot that guides people who are blind and low vision in indoor settings. It begins by presenting user studies that shaped our design of the system, moves on to describing our model of human-robot coupled motion, and concludes by describing our autonomous navigation system.",,,201,202,Human–computer interaction; Human–robot interaction; Mobile robot; Autonomous Navigation System; Low vision; User studies; Coupled motion; Computer science,,,,National Science Foundation,https://dl.acm.org/doi/pdf/10.1145/3173386.3176976 https://dblp.uni-trier.de/db/conf/hri/hri2018c.html#NanavatiTS18,http://dx.doi.org/10.1145/3173386.3176976,,10.1145/3173386.3176976,2792181177,,0,025-762-931-984-887; 028-460-767-882-226; 064-053-186-811-574; 098-462-674-213-822; 139-114-429-196-103,15,false,,
021-804-922-051-855,ICCHP (2) - Cognitive Evaluation of Haptic and Audio Feedback in Short Range Navigation Tasks,,2014,book chapter,Lecture Notes in Computer Science,03029743; 16113349,Springer International Publishing,Germany,Manuel Martinez; Angela Constantinescu; Boris Schauerte; Daniel Koester; Rainer Stiefelhagen,"Assistive navigation systems for the blind commonly use speech to convey directions to their users. However, this is problematic for short range navigation systems that need to provide fine but diligent guidance in order to avoid obstacles. For this task, we have compared haptic and audio feedback systems under the NASA-TLX protocol to analyze the additional cognitive load that they place on users. Both systems are able to guide the users through a test obstacle course. However, for white cane users, auditory feedback results in a 22 times higher cognitive load than haptic feedback. This discrepancy in cognitive load was not found on blindfolded users, thus we argue against evaluating navigation systems solely with blindfolded users.",8548,,128,135,Haptic technology; Auditory feedback; Task (project management); Obstacle course; Audio feedback; Computer science; Multimedia; Cognitive load; Protocol (object-oriented programming); Sonification,,,,,https://doi.org/10.1007/978-3-319-08599-9_20 http://doi.org/10.1007/978-3-319-08599-9_20 https://dblp.uni-trier.de/db/conf/icchp/icchp2014-2.html#MartinezCSKS14 https://cvhci.anthropomatik.kit.edu/~bschauer/pdf/martinez2014cognitive.pdf https://rd.springer.com/chapter/10.1007/978-3-319-08599-9_20 https://link.springer.com/chapter/10.1007/978-3-319-08599-9_20,http://dx.doi.org/10.1007/978-3-319-08599-9_20,,10.1007/978-3-319-08599-9_20,162712217,,0,011-797-116-120-363; 034-551-334-308-190; 038-858-720-835-740; 051-612-966-213-045; 073-928-284-197-323; 081-492-106-054-261; 120-413-407-974-701; 144-247-345-212-401; 157-217-076-872-704; 158-693-771-352-851,23,false,,
021-909-628-200-486,MIWAI - IntelliNavi: Navigation for Blind Based on Kinect and Machine Learning,,2014,book chapter,Lecture Notes in Computer Science,03029743; 16113349,Springer International Publishing,Germany,Alexy Bhowmick; Saurabh Prakash; Rukmani Bhagat; Vijay Prasad; Shyamanta M. Hazarika,"This paper presents a wearable navigation assistive system for the blind and the visually impaired built with off-the-shelf technology. Microsoft Kinect's on board depth sensor is used to extract Red, Green, Blue and Depth (RGB-D) data of the indoor environment. Speeded-Up Robust Features (SURF) and Bag-of-Visual-Words (BOVW) model is used to extract features and reduce generic indoor object detection into a machine learning problem. A Support Vector Machine classifier is used to classify scene objects and obstacles to issue critical real-time information to the user through an external aid (earphone) for safe navigation. We performed a user-study with blind-fold users to measure the efficiency of the overall framework.",,,172,183,Machine learning; Wearable computer; Artificial intelligence; Object detection; Measure (data warehouse); Support vector machine classifier; On board; Visually impaired; Computer vision; Computer science; Cognitive neuroscience of visual object recognition,,,,,http://dblp.uni-trier.de/db/conf/miwai/miwai2014.html#BhowmickPBPH14 https://www.researchgate.net/profile/Alexy_Bhowmick/publication/271328979_IntelliNavi__Navigation_for_Blind_Based_on_Kinect_and_Machine_Learning/links/55daaaa808aed6a199aaf540.pdf https://rd.springer.com/chapter/10.1007/978-3-319-13365-2_16 https://link.springer.com/chapter/10.1007/978-3-319-13365-2_16 https://link.springer.com/10.1007/978-3-319-13365-2_16,http://dx.doi.org/10.1007/978-3-319-13365-2_16,,10.1007/978-3-319-13365-2_16,214057527,,0,008-164-169-032-772; 010-719-927-108-990; 012-642-196-605-159; 014-387-823-436-92X; 015-522-377-591-609; 016-027-780-619-630; 022-489-327-390-755; 028-683-545-086-724; 031-896-755-578-767; 041-026-815-526-306; 042-101-500-666-105; 043-536-853-133-602; 051-612-966-213-045; 051-866-687-150-811; 069-441-532-275-214; 074-025-785-307-622; 075-903-419-661-27X; 094-332-505-126-057; 157-838-172-895-801; 192-623-080-180-332,20,false,,
021-922-733-063-432,Blind spot detection using vision for automotive applications,2008-10-01,2008,journal article,Journal of Zhejiang University-SCIENCE A,1673565x; 18621775,Zhejiang University Press,China,Miguel Angel Sotelo; José Barriga,,9,10,1369,1372,Engineering; Artificial intelligence; Optical flow; Vehicle detection; Blind spot detection; Computer vision; Automotive industry; Blind spot; Cluster analysis,,,,,http://www.robesafe.com/personal/sotelo/JZUS-BSD.pdf http://www.isislab.es/sotelo/JZUS-BSD.pdf https://link.springer.com/article/10.1631%2Fjzus.A0820111,http://dx.doi.org/10.1631/jzus.a0820111,,10.1631/jzus.a0820111,2046537727,,5,008-486-236-820-392; 018-804-371-707-740; 046-314-805-764-384,30,false,,
021-983-310-456-786,Feasibility Study: Towards a Robot-Assisted Gait Training in Ophthalmological Rehabilitation.,2023-09-24,2023,journal article,IEEE ... International Conference on Rehabilitation Robotics : [proceedings],19457901; 19457898,,United States,Andrea Scheidig; Robert Hartramph; Benjamin Schuetz; Steffen Mueller; Kathleen S Kunert; Johanna Lahne; Ute Oelschlegel; Ruediger Scheidig; Horst-Michael Gross,"The idea of using mobile assistance robots for gait training in rehabilitation has been increasingly explored in recent years due to the associated benefits. This paper describes how the previous results of research and praxis on gait training with a mobile assistance robot in orthopedic rehabilitation can be transferred to ophthalmic-related orientation and mobility training for blind and visually impaired people. To this end, the specific requirements for such orientation and mobility training are presented from a therapeutic perspective. Using sensory data, it is investigated how the analysis of training errors can be automated and transferred back to the training person. These pre-examinations are the prerequisite for any form of robot-assisted mobile gait training in ophthamological rehabilitation, which does not exist so far and which is expected to be of great benefit to these patients.",2023,,1,,,,"Humans; Gait; Robotics/methods; Feasibility Studies; Exercise Therapy/methods; Gait Disorders, Neurologic/rehabilitation",,,,http://dx.doi.org/10.1109/icorr58425.2023.10304760,37941232,10.1109/icorr58425.2023.10304760,,,0,000-226-476-969-012; 005-639-204-514-863; 006-142-640-704-084; 013-979-873-275-004; 016-210-984-622-992; 018-729-798-003-710; 028-303-905-295-179; 036-451-828-627-356; 041-825-579-352-017; 054-783-969-966-398; 064-565-225-829-769; 068-101-015-666-72X; 086-071-277-323-282; 087-630-779-165-231; 087-905-467-485-308; 090-807-670-343-54X; 091-675-074-481-190,0,false,,
022-132-958-209-471,"Evolution of a Prototype Lunar Rover: Addition of Laser-Based Hazard Detection, and Results from Field Trials in Lunar Analog Terrain",,1999,journal article,Autonomous Robots,09295593; 15737527,Springer Science and Business Media LLC,Netherlands,Eric Krotkov; Martial Hebert; Lars Henriksen; Paul Levin; Mark Maimone; Reid Simmons; James Teza,,7,2,119,130,Metre; Teleoperation; Terrain; Hazard (logic); Field (computer science); Computer science; Simulation; Laser; Stereopsis; Real-time computing,,,,,https://link.springer.com/article/10.1023/A%3A1008926000060 https://dblp.uni-trier.de/db/journals/arobots/arobots7.html#KrotkovHHLMST99 https://dialnet.unirioja.es/servlet/articulo?codigo=477355,http://dx.doi.org/10.1023/a:1008926000060,,10.1023/a:1008926000060,1539549010,,0,021-909-755-879-524; 030-132-534-232-846; 040-442-583-800-80X; 041-515-884-642-201; 043-122-606-283-97X; 077-537-067-520-558; 110-825-652-541-473; 132-087-609-479-527; 156-180-477-677-894; 161-822-042-758-452,6,false,,
022-489-327-390-755,"RGB-D image-based detection of stairs, pedestrian crosswalks and traffic signs",,2014,journal article,Journal of Visual Communication and Image Representation,10473203,Elsevier BV,United States,Shuihua Wang; Hangrong Pan; Chenyang Zhang; Yingli Tian,,25,2,263,272,Artificial intelligence; Hough transform; Pedestrian; Stairs; Image based; Visually impaired; Computer vision; Computer science; Cognitive neuroscience of visual object recognition; RGB color model; Parallel,,,,,http://dx.doi.org/10.1016/j.jvcir.2013.11.005 http://www-ee.ccny.cuny.edu/wwwn/yltian/Publications/JVCI-Stair-Traffic.pdf https://dblp.uni-trier.de/db/journals/jvcir/jvcir25.html#WangPZT14 https://dx.doi.org/10.1016/j.jvcir.2013.11.005 https://www.sciencedirect.com/science/article/pii/S1047320313001934 http://www.sciencedirect.com/science/article/pii/S1047320313001934 https://doi.org/10.1016/j.jvcir.2013.11.005,http://dx.doi.org/10.1016/j.jvcir.2013.11.005,,10.1016/j.jvcir.2013.11.005,2058807357,,0,001-328-618-806-096; 016-408-213-077-520; 016-755-097-340-737; 017-571-875-650-919; 023-438-345-162-875; 023-928-444-110-407; 024-417-401-364-679; 032-987-496-927-36X; 040-032-511-261-446; 043-056-567-014-861; 046-262-856-533-736; 048-216-416-258-772; 048-275-265-866-075; 059-647-236-855-012; 062-957-059-042-10X; 071-806-151-845-245; 076-133-810-427-53X; 079-738-770-506-639; 096-829-023-259-224; 098-418-441-499-649; 099-150-824-995-115; 108-396-749-438-718; 109-617-690-629-802; 116-976-189-445-151; 135-646-014-794-306; 145-898-655-711-084; 148-405-057-283-922; 150-597-344-978-541; 166-719-051-375-011; 168-052-814-589-107; 183-913-731-618-625; 195-842-744-142-232,133,false,,
022-574-286-201-415,Present state and future of Intelligent Space—Discussion on the implementation of RT in our environment—,2007-01-25,2007,journal article,Artificial Life and Robotics,14335298; 16147456,Springer Science and Business Media LLC,Germany,Hideki Hashimoto,,11,1,1,7,Human–computer interaction; Artificial intelligence; Intelligent sensor; System integration; Mobile robot; Scheme (programming language); Robotics; Computer science; Simulation; Machine vision; Intelligent decision support system; Robot,,,,,https://link.springer.com/article/10.1007%2Fs10015-006-0416-6 https://dblp.uni-trier.de/db/journals/alr/alr11.html#Hashimoto07,http://dx.doi.org/10.1007/s10015-006-0416-6,,10.1007/s10015-006-0416-6,2029233692,,0,015-262-673-948-109; 015-712-094-012-344; 025-389-593-165-033; 030-194-676-883-509; 030-559-295-446-128; 033-155-023-224-677; 033-364-231-006-368; 039-967-455-016-021; 051-153-057-550-036; 058-670-209-164-485; 059-343-986-930-218; 066-402-849-186-497; 071-114-038-314-314; 106-630-650-533-539,9,false,,
022-697-627-694-941,CHI Extended Abstracts - CyARM: an alternative aid device for blind persons,2005-04-02,2005,conference proceedings article,CHI '05 Extended Abstracts on Human Factors in Computing Systems,,ACM,,Kiyohide Ito; Makoto Okamoto; Junichi Akita; Tetsuo Ono; Ikuko Gyobu; Tomohito Takagi; Takahiro Hoshi; Yu Mishima,"With the concept of 'human-machine interface', designed especially for visually impaired persons, we have developed an electric aid device for use in guiding orientation and locomotion. The device, which we call CyARM, measures the distance between a person and an object with an ultrasonic sensor and transmits the distance information to the user's haptic sense. In this report, we will: (1) outline the concept of CyARM, (2) describe its mechanism, and (3) demonstrate three preliminary experiments that verify the usability of CyARM. We conducted the experiments in terms of detection of objects, detection of space, and tracking object movement. As a result of these experiments, we have concluded that CyARM is potentially effective for visually impaired persons. Our study will encourage the related studies of user interfaces, particularly focusing on electric aid devices that guide visually impaired persons in detecting their environment.",,,1483,1488,Human–computer interaction; Interface (computing); Haptic technology; Artificial intelligence; Usability; Blind persons; Computer vision; Computer science; Object (computer science); User interface; Orientation (computer vision),,,,,https://lib-repos.fun.ac.jp/dspace/handle/10445/6436 https://dblp.uni-trier.de/db/conf/chi/chi2005a.html#ItoOAOGTHM05 https://jglobal.jst.go.jp/detail?JGLOBAL_ID=201902206888576459 https://doi.org/10.1145/1056808.1056947,http://dx.doi.org/10.1145/1056808.1056947,,10.1145/1056808.1056947,1967590480,,0,052-999-204-914-967; 059-999-415-688-888; 183-935-933-940-273,73,false,,
022-864-089-807-819,A Depth Camera-Based Evaluation Method for Total Knee Arthroplasty (TKA) Simulation: Cross-Sectional Angle Measurement of 3D Printed Knee Joint,2024-08-14,2024,journal article,International Journal of Precision Engineering and Manufacturing,22347593; 20054602,Springer Science and Business Media LLC,,Jinwoo Jang; Minchae Kang; Min-Woo Han,,,,,,,,,,"National Research Foundation of Korea; National Research Foundation of Korea; National Research Foundation of Korea; Ministry of Trade, Industry and Energy",,http://dx.doi.org/10.1007/s12541-024-01102-8,,10.1007/s12541-024-01102-8,,,0,007-797-835-774-481; 009-598-847-112-70X; 009-946-543-750-196; 020-409-946-206-19X; 027-762-662-532-235; 046-478-469-619-938; 054-694-394-165-420; 058-576-104-179-866; 061-205-550-296-347; 063-899-020-685-355; 065-048-333-655-410; 075-528-145-503-079; 079-479-522-123-598; 105-411-659-473-193; 110-735-098-857-796; 113-258-446-191-418; 119-365-810-906-996; 126-047-458-827-630; 136-667-244-073-570; 137-466-477-113-604; 138-480-703-176-281; 152-683-203-274-229; 155-057-057-536-016; 162-681-774-455-537; 175-910-036-200-418; 182-352-535-753-607,0,false,,
022-930-616-916-623,READING AIDS FOR BLIND PEOPLE--A SURVEY OF PROGRESS WITH THE TECHNOLOGICAL AND HUMAN PROBLEMS.,,1964,journal article,Medical electronics & biological engineering,03689271; 17410444,Springer Science and Business Media LLC,United Kingdom,Patrick W. Nye,,2,3,247,264,Mathematics education; Psychology; Point (typography); Braille; Blindness; Acquired immunodeficiency syndrome (AIDS); Human physiology; Human engineering; Communication; Reading (process),AUDIO-VISUAL AIDS; BLINDNESS; HUMAN ENGINEERING; READING; TOUCH,Acquired Immunodeficiency Syndrome; Audiovisual Aids; Blindness; Ergonomics; Humans; Reading; Touch; Visually Impaired Persons,,,https://link.springer.com/article/10.1007%2FBF02474622 https://link.springer.com/10.1007/BF02474622 https://www.ncbi.nlm.nih.gov/pubmed/14199960,http://dx.doi.org/10.1007/bf02474622,14199960,10.1007/bf02474622,2066454939,,0,001-750-066-819-756; 008-239-196-998-779; 010-423-042-177-734; 015-898-193-973-835; 020-093-493-840-973; 023-093-874-154-191; 029-892-904-926-359; 029-935-850-107-936; 039-800-551-937-475; 055-807-088-113-125; 060-725-956-585-504; 068-459-561-210-486; 076-816-627-557-275; 084-968-172-923-920; 086-993-701-614-697; 089-897-727-084-742; 094-870-645-630-456; 101-254-133-519-239; 102-297-994-137-079; 117-962-266-568-243; 119-088-307-088-161; 129-233-008-155-337; 138-090-826-775-766; 164-938-335-748-294; 177-537-488-608-094; 178-890-368-837-435; 179-100-118-892-893,11,false,,
023-033-149-755-643,Employing Natural Terrain Semantics in Motion Planning for a Multi-Legged Robot,2018-05-22,2018,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Dominik Belter; Jan Wietrzykowski; Piotr Skrzypczyński,"This paper considers motion planning for a six-legged walking robot in rough terrain, considering both the geometry of the terrain and its semantic labeling. The semantic labels allow the robot to distinguish between different types of surfaces it can walk on, and identify areas that cannot be negotiated due to their physical nature. The proposed environment map provides to the planner information about the shape of the terrain, and the terrain class labels. Such labels as “wall” and “plant” denote areas that have to be avoided, whereas other labels, “grass”, “sand”, “concrete”, etc. represent negotiable areas of different properties. We test popular classification algorithms: Support Vector Machine and Random Trees in the task of producing proper terrain labeling from RGB-D data acquired by the robot. The motion planner uses the A∗ algorithm to guide the RRT-Connect method, which yields detailed motion plans for the multi-d.o.f. legged robot. As the A∗ planner takes into account the terrain semantic labels, the robot avoids areas which are potentially risky and chooses paths crossing mostly the preferred terrain types. We report experimental results that show the ability of the new approach to avoid areas that are considered risky for legged locomotion.",93,3,723,743,Statistical classification; Support vector machine; Semantics; Artificial intelligence; Terrain; Legged robot; Computer vision; Reflection mapping; Computer science; Motion planning; Robot,,,,Horizon 2020,https://link.springer.com/article/10.1007/s10846-018-0865-x https://dblp.uni-trier.de/db/journals/jirs/jirs93.html#BelterWS19 https://link.springer.com/content/pdf/10.1007/s10846-018-0865-x.pdf https://sin.put.poznan.pl/publications/details/i21102 https://rd.springer.com/article/10.1007/s10846-018-0865-x https://dialnet.unirioja.es/servlet/articulo?codigo=7053246,http://dx.doi.org/10.1007/s10846-018-0865-x,,10.1007/s10846-018-0865-x,2803780405,,0,000-013-362-655-665; 003-004-036-823-816; 005-271-933-739-779; 005-930-832-968-448; 006-776-134-027-29X; 008-555-513-100-921; 009-056-280-176-908; 011-805-082-562-987; 018-523-101-192-248; 018-717-731-797-343; 023-598-317-060-938; 023-928-444-110-407; 024-298-546-327-677; 028-616-425-167-589; 031-340-088-964-440; 031-479-838-845-114; 031-889-293-968-641; 037-761-853-244-728; 042-878-847-356-424; 043-122-606-283-97X; 043-617-517-381-847; 047-147-456-191-311; 049-178-577-170-191; 054-201-193-724-38X; 056-332-008-726-803; 057-587-111-510-745; 059-816-747-893-218; 060-284-834-111-30X; 063-655-869-624-040; 064-378-074-351-839; 074-954-072-879-803; 079-461-021-688-480; 090-949-101-257-012; 092-381-241-388-640; 105-772-514-857-974; 105-882-871-872-504; 108-533-141-273-589; 111-061-385-581-495; 113-297-357-779-414; 117-397-772-588-221; 123-498-859-171-509; 128-614-366-709-195; 133-600-480-333-025; 136-170-696-608-093; 141-850-126-151-124; 148-947-013-563-70X; 155-907-960-093-213; 156-309-549-841-265; 157-031-968-861-730; 157-506-403-437-672; 160-497-549-512-555; 160-717-087-533-483; 161-490-297-122-984; 171-236-552-582-339,38,true,cc-by,hybrid
023-093-874-154-191,Mobility and the blind—A survey,,1963,journal article,Medical Electronics & Biological Engineering,03689271; 17410444,Springer Science and Business Media LLC,United Kingdom,J. A. Leonard,,1,4,465,481,Instrumentation (computer programming); Data science; Theme (narrative); Population; Human physiology; Background information; Lack of knowledge; Field (computer science); Operations research; Computer Applications; Medicine,,,,,https://link.springer.com/article/10.1007/BF02474589,http://dx.doi.org/10.1007/bf02474589,,10.1007/bf02474589,2389036122,,0,009-142-401-900-87X; 018-156-012-221-906; 027-157-566-069-193; 041-515-335-426-266; 049-946-461-893-303; 054-843-170-429-403; 073-615-763-669-079; 091-390-327-547-903; 102-802-512-401-020; 114-977-441-333-55X; 125-487-916-057-673; 129-233-008-155-337; 129-771-561-453-522; 135-779-573-161-651,4,false,,
023-259-738-340-309,Assisting Visually Impaired People to Acquire Targets on a Large Wall-Mounted Display,2014-09-12,2014,journal article,Journal of Computer Science and Technology,10009000; 18604749,Springer Science and Business Media LLC,China,Kibum Kim; Xiangshi Ren,,29,5,825,836,Interface (computing); Haptic technology; Artificial intelligence; Steering law; Gesture; Public displays; Visually impaired; Computer vision; Audio feedback; Computer science,,,,,https://dx.doi.org/10.1007/s11390-014-1471-4 https://link.springer.com/article/10.1007/s11390-014-1471-4 http://dx.doi.org/10.1007/s11390-014-1471-4 https://link.springer.com/10.1007/s11390-014-1471-4 https://dblp.uni-trier.de/db/journals/jcst/jcst29.html#KimR14,http://dx.doi.org/10.1007/s11390-014-1471-4,,10.1007/s11390-014-1471-4,2119526689,,0,004-315-451-874-148; 010-677-416-831-957; 011-437-081-460-712; 012-782-941-724-284; 014-898-844-427-824; 015-353-517-576-087; 015-614-874-241-915; 016-353-889-913-511; 018-791-464-420-685; 021-803-980-874-222; 024-307-953-321-143; 025-367-855-177-142; 028-069-570-901-41X; 029-292-468-352-395; 031-663-214-026-68X; 034-350-308-512-672; 034-422-902-172-199; 034-522-273-201-214; 037-944-441-669-318; 040-660-979-093-641; 042-536-761-052-980; 044-208-645-237-793; 044-921-528-819-51X; 057-688-820-628-124; 058-981-433-915-800; 059-398-290-385-221; 064-713-411-409-049; 064-846-316-680-673; 070-673-353-909-769; 071-194-683-157-895; 082-117-472-083-689; 094-912-917-860-662; 095-912-134-058-223; 098-682-060-380-882; 102-989-911-812-33X; 107-127-281-529-486; 108-014-910-598-755; 109-712-612-856-474; 116-578-101-010-054; 117-705-559-634-091; 119-369-540-197-963; 127-131-032-743-85X; 132-088-761-241-200; 145-570-430-225-668; 150-038-697-296-385; 153-336-059-614-988; 165-703-939-503-001; 172-283-905-829-287; 187-121-409-756-675,2,false,,
023-309-209-821-636,An AI-Based Visual Aid With Integrated Reading Assistant for the Completely Blind,,2020,journal article,IEEE Transactions on Human-Machine Systems,21682291; 21682305,Institute of Electrical and Electronics Engineers (IEEE),United States,Muiz Ahmed Khan; Pias Paul; Mahmudur Rashid; Mainul Hossain; Atiqur Rahman Ahad,"Blindness prevents a person from gaining knowledge of the surrounding environment and makes unassisted navigation, object recognition, obstacle avoidance, and reading tasks a major challenge. In this work, we propose a novel visual aid system for the completely blind. Because of its low cost, compact size, and ease-of-integration, Raspberry Pi 3 Model B+ has been used to demonstrate the functionality of the proposed prototype. The design incorporates a camera and sensors for obstacle avoidance and advanced image processing algorithms for object detection. The distance between the user and the obstacle is measured by the camera as well as ultrasonic sensors. The system includes an integrated reading assistant, in the form of the image-to-text converter, followed by an auditory feedback. The entire setup is lightweight and portable and can be mounted onto a regular pair of eyeglasses, without any additional cost and complexity. Experiments are carried out with 60 completely blind individuals to evaluate the performance of the proposed device with respect to the traditional white cane. The evaluations are performed in controlled environments that mimic real-world scenarios encountered by a blind person. Results show that the proposed device, as compared with the white cane, enables greater accessibility, comfort, and ease of navigation for the visually impaired.",50,6,507,517,Digital image processing; Wearable computer; Artificial intelligence; Object detection; Obstacle; Auditory feedback; Computer vision; Computer science; Reading (process); Cognitive neuroscience of visual object recognition; Obstacle avoidance,,,,,https://dblp.uni-trier.de/db/journals/thms/thms50.html#KhanPRHA20 https://ieeexplore.ieee.org/document/9234074 https://doi.org/10.1109/THMS.2020.3027534,http://dx.doi.org/10.1109/thms.2020.3027534,,10.1109/thms.2020.3027534,3093871982,,3,003-755-844-432-28X; 007-223-103-783-187; 007-305-095-446-752; 010-461-705-230-16X; 013-056-435-015-274; 018-170-211-594-394; 018-843-508-574-144; 027-720-558-803-335; 029-927-540-699-18X; 031-218-334-653-826; 032-661-597-680-28X; 038-326-446-864-583; 039-970-463-405-198; 040-057-279-881-544; 040-671-340-841-091; 053-567-501-093-090; 055-495-282-581-758; 059-644-292-671-783; 061-933-888-557-079; 062-901-330-040-847; 066-246-145-459-483; 066-622-591-843-254; 067-149-711-104-938; 069-480-623-909-181; 070-606-003-984-347; 074-023-912-138-181; 077-260-829-014-223; 086-299-160-068-514; 088-022-615-544-97X; 088-618-108-481-991; 098-926-109-358-108; 101-490-736-383-339; 102-067-991-252-334; 104-208-613-793-423; 109-877-722-576-68X; 112-365-549-669-557; 118-533-294-109-774; 120-891-067-806-838; 123-890-691-474-106; 132-379-634-262-121; 145-457-820-380-343; 158-651-279-838-117,77,false,,
023-423-721-018-788,Low Cost Visual Support System for Challenged People,2022-03-25,2022,conference proceedings article,2022 International Conference on Smart Technologies and Systems for Next Generation Computing (ICSTSN),,IEEE,,Christo Ananth; Stalin Jacob; Jenifer Darling Rosita; M. S. Muthuraman; T. Ananth Kumar,"People who are visually impaired have a hard time navigating their surroundings, recognizing objects, and avoiding hazards on their own since they do not know what is going on in their immediate surroundings. We have devised a new method of delivering assistance to people who are blind in their quest to improve their vision. An affordable, compact, and easy-to-use Raspberry Pi 3 Model B+ was chosen to demonstrate how the proposed prototype works. All of this is made possible by a camera and sensors and the most modern image processing algorithmic methods available. A camera and ultrasonic sensors are utilized to measure the distance between the user and the object. Using a global positioning system (GPS), it is also possible to track down people who are blind or have impaired vision (GPS). Consider the possibility of making this a for-profit product. Small and light, it may be fitted to a regular pair of spectacles without incurring additional fees or posing any further difficulties.",,,,,Global Positioning System; Computer science; Computer vision; Raspberry pi; Artificial intelligence; Track (disk drive); Object (grammar); Computer graphics (images); Computer security; Internet of Things; Telecommunications; Operating system,,,,,https://zenodo.org/records/6784329/files/S9%20-%20IEEE.pdf https://zenodo.org/record/6784329,http://dx.doi.org/10.1109/icstsn53084.2022.9761312,,10.1109/icstsn53084.2022.9761312,,,0,007-305-095-446-752; 010-461-705-230-16X; 040-057-279-881-544; 040-351-685-890-777; 040-671-340-841-091; 066-622-591-843-254; 067-149-711-104-938; 070-606-003-984-347; 077-260-829-014-223; 080-148-076-219-168; 109-877-722-576-68X; 187-559-896-608-974,0,true,,green
023-585-819-983-801,Visually Impaired People Navigation System using Sensors and Neural Network,2022-11-17,2022,conference proceedings article,2022 IEEE 3rd International Conference on Human-Machine Systems (ICHMS),,IEEE,,Hakar Mohsin Saber; Nawzad Kameran Al-Salihi; Rebaz Mohammed Dler Omer,"Without a personal assistant or a trained dog, it is difficult and somehow risky for visually impaired individuals to navigate indoors and outdoors. Navigation for individuals with visual impairment is now an active area of research, and numerous systems have been created. Developing a reliable and robust system will give the visually impaired more awareness about their surroundings, provides more independence in their lives, and reduces the risks they might face. This paper proposes the design and implementation of a prototype for a wearable system that enables blind and visually impaired individuals to navigate easily and safely by using computer vision and deep learning to identify objects and sensors to detect obstacles. Using an attached camera, the system uses image processing to recognize objects around the user. It identifies the objects based on a trained deep learning dataset and audibly communicates the object names to the user. Ultrasonic sensors detect nearby obstructions and alert the user by vibrating. Upon request, a Global Positioning System (GPS) receiver transmits the user’s location via Short Message Service (SMS) to a caregiver. All the sensors and devices are connected to and controlled by an Arduino Mega microcontroller that has been programmed with efficient and dependable algorithms to perform tasks precisely and rapidly. Object recognition and reading are performed using a computer equipped with the necessary MATLAB software and libraries.",,,,,Computer science; Wearable computer; Arduino; Global Positioning System; Computer vision; Artificial intelligence; Microcontroller; Object (grammar); Software; Object detection; Visually impaired; Human–computer interaction; Real-time computing; Embedded system; Telecommunications; Segmentation; Programming language,,,,,,http://dx.doi.org/10.1109/ichms56717.2022.9980818,,10.1109/ichms56717.2022.9980818,,,0,013-691-140-907-374; 036-723-065-534-309; 056-167-810-394-870; 060-195-177-866-619; 077-703-665-866-285; 084-295-211-596-034; 099-512-892-836-162; 151-015-279-636-660,0,false,,
023-927-354-803-82X,Developing a navigation aid for the frail and visually impaired,2004-04-22,2004,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Pontus Engelbrektsson; I. C. Karlsson; Blaithin Gallagher; H Hunter; Helen Petrie; Ann-Marie O'neill,,3,3,194,201,Human–computer interaction; User requirements document; Navigational aid; Preference; Computer communication networks; Navigation aid; User involvement; Visually impaired; Development team; Computer science; Multimedia,,,,,https://link.springer.com/content/pdf/10.1007%2Fs10209-003-0088-0.pdf https://dblp.uni-trier.de/db/journals/uais/uais3.html#EngelbrektssonKGHPO04 https://link.springer.com/article/10.1007/s10209-003-0088-0 https://core.ac.uk/display/19335602 https://link.springer.com/article/10.1007/s10209-003-0088-0/fulltext.html https://uhra.herts.ac.uk/handle/2299/12486?show=full,http://dx.doi.org/10.1007/s10209-003-0088-0,,10.1007/s10209-003-0088-0,2024652338,,0,002-719-403-070-979; 004-050-161-682-622; 030-127-369-356-741; 032-214-485-927-271; 061-427-972-568-158; 080-391-868-175-650; 106-638-424-625-060; 111-781-420-483-07X; 117-384-261-049-696; 153-333-361-393-465; 159-682-971-316-719,14,false,,
024-078-722-706-920,Swarm cognition on off-road autonomous robots,2011-01-04,2011,journal article,Swarm Intelligence,19353812; 19353820,Springer Science and Business Media LLC,United States,Pedro Santana; Luis M. Correia,,5,1,45,72,Artificial intelligence; Social robot; Set (psychology); Action selection; Swarm behaviour; Poison control; Computer science; Robot; Robustness (computer science); Process (computing),,,,,http://www.di.fc.ul.pt/~lcorreia/papers/SI-2011.pdf https://dx.doi.org/10.1007/s11721-010-0051-7 https://dblp.uni-trier.de/db/journals/swarm/swarm5.html#SantanaC11 https://link.springer.com/article/10.1007%2Fs11721-010-0051-7 https://www.safetylit.org/citations/index.php?fuseaction=citations.viewdetails&citationIds[]=citjournalarticle_327279_19 https://ciencia.iscte-iul.pt/publications/swarm-cognition-on-off-road-autonomous-robots/48232 https://doi.org/10.1007/s11721-010-0051-7,http://dx.doi.org/10.1007/s11721-010-0051-7,,10.1007/s11721-010-0051-7,2064027113,,0,004-468-353-805-250; 006-310-088-623-140; 007-082-413-775-709; 008-402-333-165-819; 008-902-659-017-868; 009-999-316-540-294; 011-809-196-228-352; 012-122-421-394-388; 012-948-925-059-744; 015-280-961-459-747; 015-998-839-756-88X; 016-491-434-353-912; 017-086-729-253-510; 017-358-441-295-590; 018-335-113-850-895; 018-398-556-281-554; 018-766-556-286-546; 018-797-826-483-905; 022-356-142-009-533; 023-699-297-636-356; 024-918-884-334-743; 026-053-633-577-014; 026-103-828-042-653; 026-670-048-440-483; 027-055-613-982-480; 028-534-331-559-929; 028-825-707-138-656; 032-031-717-495-93X; 032-095-708-725-45X; 032-905-474-283-59X; 033-820-137-055-688; 033-899-843-077-248; 035-008-126-143-979; 035-785-918-328-521; 039-589-355-245-34X; 040-349-439-548-754; 040-353-772-705-073; 040-693-536-445-987; 042-326-794-367-908; 042-540-922-383-492; 042-871-541-663-235; 042-940-841-127-553; 043-020-833-842-287; 043-375-846-935-987; 044-430-610-597-817; 044-600-082-150-219; 046-956-438-132-67X; 047-350-750-889-925; 048-303-380-897-139; 048-966-876-633-863; 050-252-163-363-223; 052-752-674-258-648; 053-600-802-130-58X; 056-580-646-032-980; 057-522-555-268-534; 059-363-699-172-571; 059-424-379-092-456; 063-075-100-682-793; 067-748-443-896-549; 067-881-905-249-614; 069-217-390-348-37X; 069-307-423-143-311; 075-310-654-995-866; 076-180-740-369-507; 077-691-710-636-652; 079-798-963-517-400; 079-801-836-939-701; 084-654-594-902-124; 086-754-320-670-558; 087-077-553-083-806; 087-442-125-858-743; 088-608-291-916-762; 093-166-174-058-389; 094-613-443-664-968; 096-717-380-007-985; 097-945-462-042-689; 098-059-732-445-50X; 099-010-640-263-062; 099-257-558-904-849; 101-526-872-465-699; 102-025-338-771-363; 102-555-702-781-751; 102-795-687-978-136; 103-921-127-603-281; 109-939-134-166-300; 111-153-388-482-837; 111-981-341-314-610; 115-653-336-582-496; 117-919-407-529-384; 118-876-499-634-788; 119-065-370-922-065; 121-152-948-863-661; 122-530-330-429-435; 123-454-429-164-558; 124-949-120-644-298; 125-658-057-387-001; 125-750-862-754-419; 129-027-117-519-286; 129-236-787-962-245; 130-400-546-217-196; 134-129-530-852-822; 135-915-928-756-579; 138-867-413-493-138; 148-975-926-020-839; 154-620-922-152-652; 155-316-953-902-305; 158-136-902-647-747; 158-669-768-538-153; 159-497-085-828-504; 167-986-102-692-598; 170-588-192-900-729; 172-436-918-636-323; 177-684-987-958-646; 178-053-678-929-45X; 187-430-824-185-768; 188-328-418-736-160; 197-086-172-179-014; 197-391-290-583-502,12,false,,
024-294-829-318-369,Real-time camera-based eye gaze tracking using convolutional neural network: a case study on social media website,2022-03-30,2022,journal article,Virtual Reality,13594338; 14349957,Springer Science and Business Media LLC,United Kingdom,Nandini Modi; Jaiteg Singh,"Eye gaze tracking plays an important role in various fields including, human computer interaction, virtual and augmented reality and in identifying effective marketing solutions in affective manner. This paper addresses real-time eye gaze estimation problem using low resolution ordinary camera available in almost every desktop environment as opposed to gaze tracking technologies requiring costly equipment and infrared light sources. In this research, a camera based non-invasive technique has been proposed for tracking and recording gaze points. Further, the proposed framework was used to analyze gaze behavior of users on advertisements displayed on social media website. Eye gaze fixations data of 32 participants were recorded, and gaze patterns were plotted using Heat maps. In addition, the gaze driven interface was designed for virtual interaction tasks to assess the performance, and usability of our proposed framework.",26,4,1489,1506,Gaze; Computer science; Eye tracking; Convolutional neural network; Usability; Computer vision; Artificial intelligence; Tracking (education); Virtual reality; Human–computer interaction; Social media; Augmented reality; Interface (matter); World Wide Web; Psychology; Pedagogy; Bubble; Maximum bubble pressure method; Parallel computing,,,,,,http://dx.doi.org/10.1007/s10055-022-00642-6,,10.1007/s10055-022-00642-6,,,0,002-778-017-341-048; 002-872-584-212-737; 002-954-070-312-035; 004-796-234-924-41X; 005-519-248-057-493; 005-819-212-318-860; 006-120-658-099-567; 006-483-529-728-171; 012-016-836-079-639; 012-753-654-891-407; 013-389-570-056-568; 013-433-032-598-605; 013-666-239-055-453; 015-686-951-830-338; 016-564-669-763-419; 018-170-876-560-869; 018-885-434-390-101; 019-407-021-424-553; 020-120-450-915-403; 022-556-950-285-17X; 022-887-240-944-261; 023-568-716-739-692; 024-207-469-370-383; 026-545-388-130-104; 028-283-750-627-320; 030-543-144-121-135; 034-188-793-805-176; 037-250-011-698-26X; 037-680-354-860-214; 041-199-133-509-803; 041-758-781-840-13X; 044-755-372-560-458; 046-273-777-781-639; 046-720-536-227-321; 050-241-758-924-444; 051-223-032-244-244; 052-546-712-148-638; 054-193-961-388-503; 057-357-969-026-475; 058-898-421-628-858; 059-046-088-816-148; 059-516-784-609-752; 063-869-662-972-922; 065-508-704-391-675; 065-753-024-977-540; 066-690-989-074-015; 070-473-642-580-14X; 071-124-060-346-821; 073-731-663-271-571; 074-431-085-819-380; 075-002-857-962-461; 076-590-119-209-785; 077-017-064-424-844; 078-557-323-607-906; 082-510-445-954-566; 084-604-612-731-619; 085-005-837-677-184; 085-433-213-997-578; 089-405-517-921-916; 092-593-028-115-066; 094-738-418-837-90X; 096-700-802-027-169; 100-050-470-794-353; 103-532-852-381-344; 104-065-232-063-077; 104-785-476-135-722; 107-151-553-552-816; 107-155-464-567-002; 108-966-718-380-877; 113-434-367-003-466; 114-594-185-974-973; 120-874-389-982-303; 121-254-101-219-325; 126-591-545-356-397; 128-921-756-889-064; 133-787-094-681-761; 138-380-505-128-862; 144-805-054-715-731; 150-529-557-166-497; 152-242-582-522-291; 179-333-007-503-995,10,true,"CC BY, CC BY-NC-ND",gold
024-739-196-747-639,"Early Cognitive Vision: Using Gestalt-Laws for Task-Dependent, Active Image-Processing",,2004,journal article,Natural Computing,15677818,Springer Science and Business Media LLC,Netherlands,Florentin Wörgötter; Norbert Krüger; Nicolas Pugeault; Dirk Calow; Markus Lappe; Karl Pauwels; Marc M. Van Hulle; Sovira Tan; Alan Johnston,,3,3,293,321,Data processing; Artificial intelligence; Human visual system model; Theory of computation; Gestalt psychology; Context (language use); Task (project management); Focus (computing); Computer science; Image processing,,,,,https://dblp.uni-trier.de/db/journals/nc/nc3.html#WorgotterKPCLPHTJ04 https://link.springer.com/article/10.1023/B:NACO.0000036817.38320.fe https://doi.org/10.1023/B:NACO.0000036817.38320.fe https://epubs.surrey.ac.uk/832854/ https://surrey.eprints-hosting.org/832854/ https://vbn.aau.dk/da/publications/early-cognitive-vision-using-gestaltlaws-for-taskdependent-active-imageprocessing(ff57f980-0020-11da-b4d5-000ea68e967b).html,http://dx.doi.org/10.1023/b:naco.0000036817.38320.fe,,10.1023/b:naco.0000036817.38320.fe,2097738643,,0,000-128-011-343-23X; 000-799-610-858-54X; 001-879-576-279-881; 003-395-136-268-38X; 005-991-033-899-326; 006-036-341-859-546; 006-074-356-932-184; 008-267-305-714-543; 008-888-068-282-215; 009-087-343-735-888; 009-105-656-849-356; 010-136-267-298-180; 011-269-612-405-174; 011-923-664-272-223; 012-356-595-703-349; 012-986-084-330-946; 013-147-882-489-360; 013-210-560-033-888; 013-343-081-660-090; 014-033-964-160-949; 014-369-707-150-503; 018-989-769-655-163; 020-242-654-198-480; 022-516-595-102-765; 025-297-070-127-910; 027-639-898-678-608; 027-877-665-848-93X; 028-707-038-739-456; 029-860-069-360-470; 031-674-262-405-581; 031-945-870-801-336; 032-429-243-799-113; 033-541-936-845-588; 033-953-512-735-343; 034-058-966-607-421; 034-671-129-156-112; 035-893-192-898-282; 036-662-138-868-590; 036-979-691-570-357; 037-180-708-932-81X; 039-646-951-770-00X; 042-407-695-822-340; 046-872-240-102-549; 048-580-763-918-115; 049-336-882-076-346; 050-380-657-505-582; 052-937-033-108-405; 054-689-512-080-190; 058-102-771-876-02X; 058-565-870-655-768; 059-000-052-889-827; 059-019-144-646-38X; 059-363-699-172-571; 063-396-694-108-824; 067-524-887-806-556; 072-154-881-766-085; 075-235-698-460-292; 077-223-748-392-772; 080-341-675-852-365; 083-532-467-034-940; 086-075-458-006-452; 086-577-975-238-980; 086-626-028-584-445; 094-304-750-339-904; 095-665-904-135-788; 097-125-102-136-004; 099-324-217-540-618; 099-786-935-500-852; 100-277-587-352-731; 101-985-298-118-790; 104-981-696-480-69X; 108-633-749-835-63X; 109-939-207-975-716; 110-975-130-163-122; 116-102-340-293-860; 117-522-806-752-692; 118-695-812-560-197; 125-324-585-582-353; 125-561-741-226-254; 125-750-862-754-419; 129-435-154-530-663; 131-956-341-311-736; 138-985-646-826-439; 147-156-299-805-358; 166-525-432-633-12X; 177-437-110-661-16X; 188-328-418-736-160,21,false,,
024-873-718-242-898,The role of head movements in the discrimination of 2-D shape by blind echolocation experts.,2014-05-30,2014,journal article,"Attention, perception & psychophysics",1943393x; 19433921,Springer Science and Business Media LLC,United States,Jennifer L. Milne; Melvyn A. Goodale; Lore Thaler,"Similar to certain bats and dolphins, some blind humans can use sound echoes to perceive their silent surroundings. By producing an auditory signal (e.g., a tongue click) and listening to the returning echoes, these individuals can obtain information about their environment, such as the size, distance, and density of objects. Past research has also hinted at the possibility that blind individuals may be able to use echolocation to gather information about 2-D surface shape, with definite results pending. Thus, here we investigated people’s ability to use echolocation to identify the 2-D shape (contour) of objects. We also investigated the role played by head movements—that is, exploratory movements of the head while echolocating—because anecdotal evidence suggests that head movements might be beneficial for shape identification. To this end, we compared the performance of six expert echolocators to that of ten blind nonecholocators and ten blindfolded sighted controls in a shape identification task, with and without head movements. We found that the expert echolocators could use echoes to determine the shapes of the objects with exceptional accuracy when they were allowed to make head movements, but that their performance dropped to chance level when they had to remain still. Neither blind nor blindfolded sighted controls performed above chance, regardless of head movements. Our results show not only that experts can use echolocation to successfully identify 2-D shape, but also that head movements made while echolocating are necessary for the correct identification of 2-D shape.",76,6,1828,1837,Psychology; Human echolocation; Active listening; Head (linguistics); Speech recognition; Head movements; Surface shape; Auditory signal; Communication; Identification (information),,"Adult; Analysis of Variance; Blindness/physiopathology; Discrimination, Psychological/physiology; Female; Form Perception/physiology; Head Movements/physiology; Humans; Male; Middle Aged; Sound Localization/physiology; Young Adult",,,https://www.questia.com/library/journal/1P3-3414254111/the-role-of-head-movements-in-the-discrimination-of https://core.ac.uk/display/42124361 http://www.ncbi.nlm.nih.gov/pubmed/24874262 http://dro.dur.ac.uk/13260/ https://dro.dur.ac.uk/13260/ https://europepmc.org/article/MED/24874262 https://link.springer.com/article/10.3758%2Fs13414-014-0695-2 https://link.springer.com/article/10.3758/s13414-014-0695-2/fulltext.html https://core.ac.uk/download/pdf/42124361.pdf,http://dx.doi.org/10.3758/s13414-014-0695-2,24874262,10.3758/s13414-014-0695-2,2146590246,,0,000-099-414-952-654; 003-090-809-378-974; 008-699-608-597-718; 008-803-922-664-664; 013-044-071-585-533; 014-790-339-235-851; 018-156-012-221-906; 019-388-133-816-34X; 019-562-955-392-946; 021-905-014-073-760; 022-190-377-474-977; 023-502-780-994-231; 024-061-714-411-11X; 027-823-915-833-13X; 029-378-341-938-070; 029-732-441-045-455; 031-696-287-727-760; 040-310-362-293-336; 043-147-295-324-586; 045-951-778-792-536; 049-404-712-409-293; 049-435-675-206-981; 051-194-122-391-125; 052-154-971-754-242; 052-617-534-049-850; 053-670-979-455-324; 053-677-703-598-959; 055-222-157-783-963; 065-794-300-944-51X; 074-879-921-488-069; 078-277-260-226-121; 080-411-167-059-855; 087-467-128-486-149; 089-671-332-491-261; 090-204-103-677-989; 106-603-492-993-738; 117-298-983-720-612; 136-847-062-786-471; 167-722-532-112-363; 173-658-701-117-40X,56,true,,green
024-909-531-097-696,Image Based High-Level Control System Design for Steering and Controlling of an Active Capsule Endoscope,2018-11-15,2018,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Mehrnaz Aghanouri; Ali Ghaffari; Nasim Dadashi Serej,,94,1,115,134,Artificial intelligence; Control system; Sensitivity (control systems); Endoscope; Capsule endoscopy; Capsule Endoscopes; Computer vision; Computer science; Control theory; Motion planning; Orientation (computer vision),,,,,https://dblp.uni-trier.de/db/journals/jirs/jirs94.html#AghanouriGS19 https://link.springer.com/article/10.1007/s10846-018-0956-8 https://dialnet.unirioja.es/servlet/articulo?codigo=7054010,http://dx.doi.org/10.1007/s10846-018-0956-8,,10.1007/s10846-018-0956-8,2900613432,,0,003-819-868-526-694; 004-394-099-326-115; 006-352-258-521-813; 006-996-309-742-733; 008-870-431-397-661; 010-700-630-050-484; 011-189-595-287-283; 012-407-764-840-465; 022-443-190-106-579; 023-223-414-687-731; 023-762-316-649-275; 030-168-395-480-309; 034-681-157-791-008; 037-593-797-504-248; 038-474-938-942-263; 038-639-889-810-557; 043-257-569-932-325; 045-565-041-480-612; 051-532-812-280-261; 053-606-875-592-336; 058-233-099-726-030; 062-095-371-617-020; 071-981-725-854-465; 074-761-444-291-285; 077-704-709-203-163; 086-966-292-024-763; 087-192-050-079-470; 087-303-021-062-265; 088-064-107-237-776; 091-977-692-564-317; 104-047-815-146-473; 171-721-605-898-505; 186-412-044-017-67X,7,false,,
024-998-487-288-58X,A Prototype Navigation System for Guiding Blind People Indoors using NXT Mindstorms,2013-09-15,2013,journal article,International Journal of Online and Biomedical Engineering (iJOE),26268493,International Association of Online Engineering (IAOE),,Tareq Alhmiedat; Anas Abutaleb; Gassan Samara,"People with visual impairment face enormous difficulties in terms of their mobility as they do not have enough information about their location and orientation with respect to traffic and obstacles on their route. Visually impaired people can navigate unknown areas by relying on the assistance of canes, other people, or specially trained guide dogs. The traditional ways of using guide dogs and long-cane only help to avoid obstacles, not to know where they are. The research presented in this paper introduces a mobile assistant navigation prototype to locate and direct blind people indoors. Since most of the existing navigation systems developed so far for blind people employ a complex conjunction of positioning systems, video cameras, location-based and image processing algorithms, we designed an affordable low-cost prototype navigation system for orienting and tracking the position of blind people in complex environments. The prototype system is based on the inertial navigation system and experiments have been performed on NXT Mindstorms platform.",9,5,52,58,Digital image processing; Engineering; Inertial navigation system; Artificial intelligence; Navigation system; Visual impairment; Visually impaired; Robotics; Multimedia; Orientation (computer vision),,,,,https://dblp.uni-trier.de/db/journals/ijoe/ijoe9.html#AlhmiedatAS13 https://online-journals.org/index.php/i-joe/article/view/2848,http://dx.doi.org/10.3991/ijoe.v9i5.2848,,10.3991/ijoe.v9i5.2848,2019293222,,0,,20,true,cc-by,gold
025-006-043-180-14X,Soft computing based intention reading techniques as a means of human-robot interaction for human centered system,2003-01-01,2003,journal article,"Soft Computing - A Fusion of Foundations, Methodologies and Applications",14327643; 14337479,Springer Science and Business Media LLC,Germany,Dae-Jin Kim; Won-Kyung Song; Jeong-Su Han; Zeungnam Bien,,7,3,160,166,Visual servoing; Soft computing; Human–robot interaction; Robotic arm; Rehabilitation robotics; Rehabilitation engineering; Computer science; Multimedia; Reading (process); Robot,,,,,https://dblp.uni-trier.de/db/journals/soco/soco7.html#KimSHB03 https://link.springer.com/article/10.1007/s00500-002-0204-8,http://dx.doi.org/10.1007/s00500-002-0204-8,,10.1007/s00500-002-0204-8,2089486134,,0,000-608-850-645-16X; 001-028-934-996-982; 029-834-213-403-529; 034-394-878-628-655; 034-746-692-227-753; 052-546-972-692-656; 057-445-129-129-957; 071-401-223-822-662; 099-962-787-695-650; 107-530-066-500-98X; 119-212-472-276-217; 127-262-195-545-620; 167-168-613-182-894,16,false,,
025-124-933-362-663,Design and analysis of an infrared range sensor system for floor-state estimation,2011-06-18,2011,journal article,Journal of Mechanical Science and Technology,1738494x; 19763824,Springer Science and Business Media LLC,South Korea,Min-Young Lee; Sooyong Lee,,25,4,1043,1050,Artificial intelligence; Infrared; Covariance; Range (statistics); Obstacle; User assistance; Sensor system; Computer vision; Engineering design process; Computer science; Simulation; State (computer science),,,,,http://dspace.kci.go.kr/handle/kci/44155 https://link.springer.com/article/10.1007%2Fs12206-011-0141-5 http://www.j-mst.org/On_line/admin/files/26-J2010-207.pdf,http://dx.doi.org/10.1007/s12206-011-0141-5,,10.1007/s12206-011-0141-5,2045889818,,0,010-937-704-727-204; 013-551-333-215-556; 031-612-744-612-651; 053-203-187-560-385; 056-773-999-189-634; 078-736-735-476-395; 176-619-480-167-936,6,false,,
025-308-865-365-403,A non-contact method of capturing low-resolution text for OCR,2003-04-22,2003,journal article,Pattern Analysis & Applications,14337541; 1433755x,Springer Science and Business Media LLC,Germany,Majid Mirmehdi; Paul Clark; J. Lam,,6,1,12,21,Zoom; Speech synthesis; Pattern recognition (psychology); Artificial intelligence; Point (typography); Context (language use); Computer vision; Computer science; Optical character recognition; Reading (process); Document layout analysis,,,,,https://link.springer.com/content/pdf/10.1007%2Fs10044-002-0172-8.pdf https://link.springer.com/article/10.1007%2Fs10044-002-0172-8 https://dblp.uni-trier.de/db/journals/paa/paa6.html#MirmehdiCL03 http://research-information.bristol.ac.uk/en/publications/a-noncontact-method-of-capturing-lowresolution-text-for-ocr(d73e6a95-1bb3-4599-85df-f0e1f6e7b98d)/export.html,http://dx.doi.org/10.1007/s10044-002-0172-8,,10.1007/s10044-002-0172-8,2070298661,,5,000-400-274-350-642; 007-126-665-243-778; 017-647-917-539-986; 017-878-275-793-47X; 035-811-889-928-945; 044-778-342-000-021; 045-101-061-573-466; 049-023-956-892-77X; 054-772-476-662-552; 089-571-989-504-866; 090-542-957-883-681; 111-438-359-443-90X; 127-182-952-102-289; 127-867-917-940-828; 139-571-348-270-440; 161-770-090-219-734,8,false,,
025-396-537-963-175,Transorbital target localization with augmented ophthalmologic surgical endoscopy.,2014-09-12,2014,journal article,International journal of computer assisted radiology and surgery,18616429; 18616410,Springer Science and Business Media LLC,Germany,Michael P. DeLisi; Louise A. Mawn; Robert L. Galloway,,10,7,1141,1148,Neurovascular bundle; Radiology; Surgery; Orbit (anatomy); Endoscope; Optic neuropathy; Endoscopy; Reduction (orthopedic surgery); Transorbital; Surgical endoscopy; Medicine,,Animals; Endoscopy/methods; Microspheres; Ophthalmologic Surgical Procedures/methods; Orbit/surgery; Swine,,,https://dblp.uni-trier.de/db/journals/cars/cars10.html#DeLisiMG15 https://www.ncbi.nlm.nih.gov/pubmed/25213269 http://dblp.uni-trier.de/db/journals/cars/cars10.html#DeLisiMG15 https://link.springer.com/article/10.1007/s11548-014-1112-y,http://dx.doi.org/10.1007/s11548-014-1112-y,25213269,10.1007/s11548-014-1112-y,2131077918,,0,000-895-207-597-415; 003-463-986-216-844; 006-727-164-791-466; 012-761-868-975-163; 013-113-258-434-766; 028-433-020-827-727; 033-343-812-597-706; 044-955-257-166-814; 049-311-906-044-568; 050-813-000-715-279; 054-936-335-836-983; 062-876-094-169-566; 066-724-553-901-30X; 078-845-834-134-529; 088-211-838-462-157; 091-742-453-050-902; 093-850-327-786-194; 098-520-300-485-789; 100-133-223-308-163; 126-154-313-712-457; 168-926-887-730-460,0,false,,
025-516-052-268-557,ICCV Workshops - Hierarchical Feature Degradation Based Blind Image Quality Assessment,,2017,conference proceedings article,2017 IEEE International Conference on Computer Vision Workshops (ICCVW),,IEEE,,Jinjian Wu; Jichen Zeng; Yongxu Liu; Guangming Shi; Weisi Lin,"Though blind image quality assessment (BIQA) is highly demanded for many image processing systems, it is extremely difficult for BIQA to accurately predict the quality without the guide of the reference image. In this paper, we introduce a novel BIQA method with hierarchical feature degradation (HFD). Since the human brain presents hierarchical procedure for visual recognition, we suggest that different levels of distortion generate different degradations on hierarchical features, and propose to consider the degradations on both the low and high level features for quality assessment. Inspired by the orientation selectivity (OS) mechanism in the primary visual cortex, an OS based local visual structure is designed for low-level visual content extraction. Meanwhile, according to the feature integration function of deep neural networks, the deep semantics is extracted with the residual network for high-level visual content representation. Next, by analyzing the degradation on both the local structure and the deep semantics, a HFD based memory (prior knowledge) is learned to represent the generalized quality degradation. Finally, with the guidance of the HFD based memory, a novel HFD-BIQA model is built. Experimental results on the publicly available databases demonstrate the quality prediction accuracy of the proposed HFD-BIQA, and verify that the HFD-BIQA performs highly consistent with the subjective perception1.",,,510,517,Distortion; Artificial intelligence; Visual perception; Pattern recognition; Visualization; Computer science; Feature extraction; Image quality; Feature (computer vision); Orientation (computer vision); Image processing,,,,,https://www.computer.org/csdl/proceedings-article/iccvw/2017/1034a510/12OmNA0MZ5p http://doi.ieeecomputersociety.org/10.1109/ICCVW.2017.67 https://dblp.uni-trier.de/db/conf/iccvw/iccvw2017.html#WuZLSL17 https://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w9/Wu_Hierarchical_Feature_Degradation_ICCV_2017_paper.pdf,http://dx.doi.org/10.1109/iccvw.2017.67,,10.1109/iccvw.2017.67,2766563094,,0,005-541-341-726-672; 010-037-102-410-599; 011-927-414-739-394; 015-571-444-508-782; 020-233-013-143-936; 020-657-228-113-186; 023-928-444-110-407; 024-573-819-555-688; 035-832-275-159-187; 039-135-942-929-563; 044-095-303-626-459; 045-275-469-338-021; 049-539-415-883-181; 050-228-675-562-752; 055-463-893-774-499; 057-097-165-669-995; 058-329-909-149-682; 059-149-073-001-124; 060-834-533-443-304; 067-098-374-632-301; 078-986-222-652-017; 092-404-655-489-459; 092-892-579-103-029; 093-607-645-060-356; 100-483-570-027-198; 113-998-151-721-614; 121-456-195-261-238; 166-860-861-757-073,23,false,,
025-764-110-104-300,Low Cost GPS and GSM Based Navigational Aid for Visually Impaired People,2016-08-19,2016,journal article,Wireless Personal Communications,09296212; 1572834x,Springer Science and Business Media LLC,Netherlands,Rahul Dhod; Gurmohan Singh; Gagandeep Singh; Manjit Kaur,,92,4,1575,1589,Human–computer interaction; Embedded system; Obstacle; Navigational aid; Visually impaired; Computer science; Wireless; GSM; Global Positioning System,,,,,https://link.springer.com/article/10.1007/s11277-016-3622-0 http://dblp.uni-trier.de/db/journals/wpc/wpc92.html#DhodSSK17 https://dblp.uni-trier.de/db/journals/wpc/wpc92.html#DhodSSK17 https://link.springer.com/article/10.1007/s11277-016-3622-0/fulltext.html,http://dx.doi.org/10.1007/s11277-016-3622-0,,10.1007/s11277-016-3622-0,2515092963,,0,001-076-828-128-846; 013-056-435-015-274; 013-551-333-215-556; 014-157-934-526-940; 022-697-627-694-941; 030-127-369-356-741; 030-238-764-985-187; 034-350-308-512-672; 042-273-874-330-949; 045-039-665-546-894; 047-969-307-899-765; 064-309-126-323-022; 078-806-921-040-22X; 085-848-747-356-69X; 096-803-870-115-728; 107-675-088-325-585; 111-057-090-228-06X; 112-849-097-976-249; 125-501-372-179-565; 142-120-275-133-554; 144-507-062-057-649; 195-842-744-142-232,24,false,,
025-908-636-926-149,"Planetary Rover Developments Supporting Mars Exploration, Sample Return and Future Human-Robotic Colonization",,2003,journal article,Autonomous Robots,09295593; 15737527,Springer Science and Business Media LLC,Netherlands,Paul S. Schenker; Terry Huntsberger; Paolo Pirjanian; Eric T. Baumgartner; Edward Tunstel,,14,2,103,126,Control reconfiguration; Artificial intelligence; Terrain; Systems engineering; Integrated design; Rendezvous; Robotics; Computer science; Simulation; Exploration of Mars; Mars Exploration Program; Robot,,,,,https://dl.acm.org/doi/10.1023/A%3A1022271301244 https://ntrs.nasa.gov/search.jsp?R=20060031643 https://dblp.uni-trier.de/db/journals/arobots/arobots14.html#SchenkerHPBT03 https://link.springer.com/article/10.1023/A%3A1022271301244 https://www-robotics.jpl.nasa.gov/publications/Terrance_Huntsberger/Huntsberger_SpaceRobotics.pdf,http://dx.doi.org/10.1023/a:1022271301244,,10.1023/a:1022271301244,1482949338,,0,005-170-045-595-348; 007-116-209-767-026; 011-377-335-026-237; 019-482-497-918-954; 019-607-628-727-549; 019-946-899-240-210; 022-087-869-900-164; 023-724-632-264-679; 028-879-975-765-983; 032-887-916-929-508; 033-647-413-209-975; 043-679-593-527-637; 045-031-939-169-357; 049-374-085-398-235; 049-786-317-102-984; 050-387-185-181-517; 050-975-757-910-254; 066-472-961-918-09X; 068-453-360-771-263; 070-418-355-269-111; 083-284-947-902-137; 085-336-726-736-766; 089-322-708-053-230; 098-946-449-132-306; 103-166-807-675-979; 105-685-159-183-50X; 107-079-541-409-728; 112-222-410-905-502; 114-024-583-019-133; 116-918-545-600-715; 117-671-240-095-847; 123-192-450-175-991; 125-164-703-772-490; 130-911-005-247-426; 141-307-011-595-847; 147-746-527-747-867; 148-910-505-219-80X; 150-338-102-593-124; 156-917-600-467-651; 170-570-088-864-697,155,false,,
025-975-586-108-673,"Importance of Drive-by-Wire Technology for Providing Green, Accessible Transportation",2013-06-17,2013,journal article,Auto Tech Review,22503390,Springer Fachmedien Wiesbaden GmbH,,Arghya Sardar; Suresh Babu Muttana,,2,6,18,23,Engineering; Automotive engineering; Transport engineering; Drive by wire,,,,,https://link.springer.com/10.1365/s40112-013-0337-4 https://link.springer.com/article/10.1365/s40112-013-0337-4,http://dx.doi.org/10.1365/s40112-013-0337-4,,10.1365/s40112-013-0337-4,2007048346,,0,,0,false,,
026-086-575-095-703,Personalizable edge services for Web accessibility,2007-09-29,2007,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Ugo Erra; Gennaro Iaccarino; Delfina Malandrino; Vittorio Scarano,,6,3,285,306,Web modeling; World Wide Web; Web standards; Web Accessibility Initiative; Web Content Accessibility Guidelines; Web service; Computer science; Multimedia; Services computing; Web navigation; Web development,,,,,https://link.springer.com/article/10.1007/s10209-007-0091-y https://dblp.uni-trier.de/db/journals/uais/uais6.html#ErraIMS07 https://www.iris.unisa.it/handle/11386/1742487 https://core.ac.uk/display/54617349 https://rd.springer.com/article/10.1007/s10209-007-0091-y https://dx.doi.org/10.1007/s10209-007-0091-y,http://dx.doi.org/10.1007/s10209-007-0091-y,,10.1007/s10209-007-0091-y,2160029765,,23,004-147-397-637-60X; 007-482-404-669-974; 009-916-186-540-15X; 012-463-970-936-341; 015-875-185-014-632; 021-045-241-629-178; 025-492-135-073-970; 025-563-184-853-047; 027-496-668-132-065; 028-971-110-975-158; 029-676-370-985-285; 030-866-901-994-678; 037-833-056-567-030; 054-653-474-483-663; 063-538-905-972-69X; 065-028-506-334-941; 070-440-328-679-546; 091-052-886-349-995; 097-790-327-676-300; 102-488-105-327-772; 134-467-113-370-998; 137-745-287-089-131; 155-814-428-762-174; 160-361-350-885-044; 179-470-402-846-761,20,false,,
026-641-095-976-245,À propos de la conception de systèmes d’aide aux déplacements des déficients visuels,,2003,journal article,Annales Des Télécommunications,00034347; 19589395,Springer Science and Business Media LLC,Germany,Edwige E. Pissaloux,,58,5,905,917,Art; Humanities; Computer communication networks,,,,,https://dblp.uni-trier.de/db/journals/adt/adt58.html#Pissaloux03 https://link.springer.com/article/10.1007/BF03001538,http://dx.doi.org/10.1007/bf03001538,,10.1007/bf03001538,67219276,,0,065-651-375-706-251; 065-992-777-809-97X; 088-470-436-866-921; 106-630-650-533-539; 125-612-444-830-743; 133-457-684-144-980; 139-579-071-159-307,0,false,,
026-785-969-159-821,An expert system for the recommendation of visual aids,,1991,journal article,Journal of Intelligent and Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Julia E. Hodges; Jose L. Cordova; William H. Graves; Charles R. Talor,,4,2,99,107,Legal expert system; Subject-matter expert; Frame (networking); Knowledge representation and reasoning; Structure (mathematical logic); Focus (computing); Computer science; Knowledge base; Multimedia; Expert system,,,,,https://link.springer.com/article/10.1007/BF00440414 https://dblp.uni-trier.de/db/journals/jirs/jirs4.html#HodgesCGT91,http://dx.doi.org/10.1007/bf00440414,,10.1007/bf00440414,2081770795,,0,016-003-331-004-001; 026-318-942-206-103,4,false,,
026-954-721-989-025,PCA-based image recognition of braille blocks for guiding the visually handicapped,2012-11-29,2012,journal article,International Journal of Precision Engineering and Manufacturing,22347593; 20054602,Springer Science and Business Media LLC,,Sang-Jun Park; Dongwon Shin,,13,12,2115,2120,Brightness; Artificial intelligence; Principal component analysis; Intensity mapping; Braille; Block (programming); Learning set; Computer vision; Computer science; Feature vector; Orientation (computer vision),,,,,https://link.springer.com/article/10.1007%2Fs12541-012-0280-3,http://dx.doi.org/10.1007/s12541-012-0280-3,,10.1007/s12541-012-0280-3,2084664965,,0,010-104-987-820-313; 019-183-007-691-217; 030-420-854-855-803; 064-309-126-323-022; 065-992-777-809-97X; 071-413-130-603-274; 071-780-738-841-021; 086-647-424-463-752; 098-811-589-762-176; 124-706-969-167-105; 127-024-526-942-206; 134-183-127-172-912,2,false,,
026-980-421-817-327,Designing an integrated driver assistance system using image sensors,2012-01-03,2012,journal article,Journal of Intelligent Manufacturing,09565515; 15728145,Springer Science and Business Media LLC,Netherlands,Muhammad Akhlaq; Tarek R. Sheltami; Bo Helgeson; Elhadi M. Shakshuki,,23,6,2109,2132,Engineering; Embedded system; Smart system; Distraction; MATLAB; Context awareness; Digital image; Intelligent transportation system; Image sensor; Real-time computing; Advanced driver assistance systems,,,,,http://www.diva-portal.org/smash/record.jsf?pid=diva2:834783 https://dblp.uni-trier.de/db/journals/jim/jim23.html#AkhlaqSHS12 https://rd.springer.com/article/10.1007/s10845-011-0618-1 https://link.springer.com/article/10.1007/s10845-011-0618-1,http://dx.doi.org/10.1007/s10845-011-0618-1,,10.1007/s10845-011-0618-1,2003086425,,0,002-309-302-189-960; 003-935-439-273-260; 004-104-322-944-234; 005-360-277-591-597; 006-296-749-368-696; 007-042-921-016-163; 007-776-982-538-910; 008-157-181-917-244; 008-486-236-820-392; 008-659-209-257-138; 009-362-278-392-105; 010-930-501-704-684; 013-347-567-622-594; 015-119-310-307-968; 016-803-263-289-118; 018-513-955-335-303; 019-720-840-756-19X; 019-901-856-470-922; 023-082-908-511-684; 023-499-588-430-044; 024-356-070-911-466; 024-947-794-794-941; 028-929-655-372-665; 030-327-659-173-894; 031-336-137-150-67X; 031-649-123-328-796; 032-049-576-301-286; 034-639-120-159-501; 034-699-454-102-913; 036-975-699-382-301; 037-128-094-432-782; 038-438-530-415-584; 038-512-739-950-273; 038-680-923-681-467; 038-811-286-384-327; 038-933-018-683-385; 040-569-117-833-40X; 041-298-732-657-641; 042-463-405-427-77X; 043-454-206-606-566; 043-969-749-558-006; 044-799-089-859-491; 044-840-050-517-816; 047-027-757-857-109; 051-984-206-966-631; 052-020-473-533-937; 053-459-971-976-615; 054-232-409-592-585; 063-317-464-990-702; 065-968-473-448-853; 066-356-220-108-745; 076-892-580-449-561; 087-497-479-579-459; 087-931-189-714-420; 090-419-928-994-32X; 090-657-979-235-538; 097-272-944-784-550; 102-067-616-633-127; 105-375-780-356-145; 108-092-592-695-683; 108-724-939-563-042; 109-310-008-291-831; 114-174-628-565-52X; 115-580-320-388-563; 117-771-520-499-756; 118-096-144-285-85X; 119-880-727-074-101; 121-065-744-894-882; 122-338-889-863-881; 129-686-390-631-083; 133-960-589-827-861; 134-798-501-375-99X; 134-911-290-732-02X; 138-745-908-104-258; 145-283-041-744-411; 151-076-456-966-089; 157-199-107-367-887; 167-508-480-300-145; 180-006-958-446-911; 183-886-000-690-542; 188-042-372-198-323; 196-125-630-441-386,34,false,,
027-026-193-850-672,Autonomous Path Guiding Robot for Visually Impaired People,2018-08-12,2018,book chapter,Cognitive Informatics and Soft Computing,21945357; 21945365,Springer Singapore,,Rajesh Kannan Megalingam; Souraj Vishnu; Vishnu Sasikumar; Sajikumar Sreekumar,"This paper introduces an intelligent and efficient path guidance robot to assist the visually impaired people in their movements. This is a novel device for the replacement of strenuous guide dogs. The robot has the capability to move along multiple paths and then remember as well as retrace all of them, thus making it a perfect substitute for a guide dog which is often a luxury for the ninety percent of blind people living in low income settings. The robot is capable to guide the user to travel places which cannot be traced using GPS. Since most of the navigation systems that are developed so far for the blind people employ a complex conjunction of positioning systems, video cameras, location mapping and image processing algorithms, we have designed an affordable low-cost prototype navigation system which serves the purpose of guiding the visually impaired people both in indoor as well as outdoor environment.",,,257,266,Digital image processing; Artificial intelligence; Conjunction (grammar); Navigation system; PATH (variable); Visually impaired; Computer vision; Computer science; Global Positioning System; Robot,,,,,https://www.amrita.edu/publication/autonomous-path-guiding-robot-visually-impaired-people https://link.springer.com/chapter/10.1007%2F978-981-13-0617-4_25,http://dx.doi.org/10.1007/978-981-13-0617-4_25,,10.1007/978-981-13-0617-4_25,2886343384,,0,000-631-028-807-828; 003-710-720-057-407; 012-465-128-799-874; 017-942-160-984-27X; 037-192-825-161-047; 041-571-183-213-822; 072-015-966-174-329; 182-230-054-803-698; 187-275-448-283-374,22,false,,
027-265-410-975-307,The Sputnik of servgoods: Autonomous vehicles,2017-01-24,2017,journal article,Journal of Systems Science and Systems Engineering,10043756; 18619576,Springer Science and Business Media LLC,China,James M. Tien,,26,2,133,162,Government; Space Age; Liability; Object (philosophy); Computer security; Computer science; Product (business); Big data; Analytics; Cloud computing,,,,,https://link.springer.com/article/10.1007%2Fs11518-016-5325-1 https://link.springer.com/content/pdf/10.1007%2Fs11518-016-5325-1.pdf https://miami.pure.elsevier.com/en/publications/the-sputnik-of-servgoods-autonomous-vehicles,http://dx.doi.org/10.1007/s11518-016-5325-1,,10.1007/s11518-016-5325-1,2569480220,,0,006-092-442-468-524; 012-135-449-187-631; 027-530-022-852-481; 040-685-824-975-739; 041-409-654-388-00X; 059-798-611-737-287; 061-646-630-453-211; 066-428-082-047-138; 072-664-224-164-623; 073-644-095-327-681; 082-395-796-469-166; 084-038-419-848-758; 084-215-090-111-834; 093-459-245-992-988; 093-530-739-304-785; 103-740-785-309-604; 108-126-010-323-293; 130-109-783-341-840; 141-474-491-140-806; 151-196-327-319-359; 163-556-661-292-94X; 164-864-083-753-978; 165-999-988-248-09X; 175-958-633-444-202; 178-538-595-763-648; 194-176-472-098-806,11,false,,
027-282-377-751-540,3D computational modeling and perceptual analysis of kinetic depth effects,2020-08-13,2020,journal article,Computational Visual Media,20960433; 20960662,Springer Science and Business Media LLC,,Meng Yao Cui; Shao-Ping Lu; Miao Wang; Yong-Liang Yang; Yu-Kun Lai; Paul L. Rosin,"Humans have the ability to perceive kinetic depth effects, i.e., to perceived 3D shapes from 2D projections of rotating 3D objects. This process is based on a variety of visual cues such as lighting and shading effects. However, when such cues are weak or missing, perception can become faulty, as demonstrated by the famous silhouette illusion example of the spinning dancer. Inspired by this, we establish objective and subjective evaluation models of rotated 3D objects by taking their projected 2D images as input. We investigate five different cues: ambient luminance, shading, rotation speed, perspective, and color difference between the objects and background. In the objective evaluation model, we first apply 3D reconstruction algorithms to obtain an objective reconstruction quality metric, and then use quadratic stepwise regression analysis to determine weights of depth cues to represent the reconstruction quality. In the subjective evaluation model, we use a comprehensive user study to reveal correlations with reaction time and accuracy, rotation speed, and perspective. The two evaluation models are generally consistent, and potentially of benefit to inter-disciplinary research into visual perception and 3D reconstruction.",6,3,265,277,3D reconstruction; Sensory cue; Perspective (graphical); Artificial intelligence; Visual perception; Illusion; Spinning Dancer; Computer vision; Computer science; Depth perception; Silhouette,,,,,https://dc.tsinghuajournals.com/computational-visual-media/vol6/iss3/6/ https://link.springer.com/content/pdf/10.1007/s41095-020-0180-x.pdf https://link.springer.com/article/10.1007/s41095-020-0180-x https://dc.tsinghuajournals.com/cgi/viewcontent.cgi?article=1170&context=computational-visual-media https://orca.cardiff.ac.uk/132758/ http://cvm.tsinghuajournals.com/EN/10.1007/s41095-020-0180-x https://doi.org/10.1007/s41095-020-0180-x https://researchportal.bath.ac.uk/en/publications/3d-computational-modeling-and-perceptual-analysis-of-kinetic-dept https://dblp.uni-trier.de/db/journals/cvm/cvm6.html#CuiLWYLR20 https://core.ac.uk/download/326510248.pdf,http://dx.doi.org/10.1007/s41095-020-0180-x,,10.1007/s41095-020-0180-x,3049547888,,0,002-587-689-671-62X; 004-984-348-887-07X; 012-663-606-013-213; 014-968-555-027-518; 015-026-851-843-189; 016-623-273-815-531; 018-011-911-093-728; 019-029-293-990-844; 021-337-171-485-491; 021-737-391-292-299; 025-783-475-188-803; 026-497-274-553-674; 029-348-073-345-182; 029-749-293-194-096; 030-582-037-721-807; 033-281-633-159-195; 033-674-642-692-466; 038-136-854-088-483; 055-373-548-345-192; 060-229-581-889-452; 061-022-305-604-797; 066-222-453-348-586; 066-504-323-243-917; 070-589-878-860-804; 072-479-578-691-75X; 073-177-982-251-126; 080-697-760-473-535; 083-651-086-825-644; 085-196-930-884-625; 087-542-937-287-315; 088-636-439-389-750; 088-796-347-789-952; 089-080-953-267-487; 090-006-448-355-144; 093-368-020-210-013; 094-760-869-422-19X; 095-718-951-656-152; 102-555-702-781-751; 135-432-562-789-58X; 151-308-057-007-059; 185-365-960-543-960; 190-420-028-536-52X,2,true,cc-by,gold
027-443-617-768-793,Visual computation of egomotion using an image interpolation technique,1996-04-01,1996,journal article,Biological cybernetics,03401200; 14320770,Springer Science and Business Media LLC,Germany,Javaan Chahl; Mandyam V. Srinivasan,,74,5,405,411,Trajectory; Robotic arm; Artificial intelligence; Translation (geometry); Mobile robot; Image scaling; Computer vision; Mathematics; Rotation (mathematics); Robot; Orientation (computer vision); Computation; Orientation (vector space); Position (finance); Translation (biology); Interpolation (computer graphics); Computer science; Algorithm; Image (mathematics); Geometry; Physics; Biochemistry; Chemistry; Finance; Astronomy; Messenger RNA; Economics; Gene,,"Algorithms; Image Processing, Computer-Assisted; Motion; Photography/instrumentation; Robotics/instrumentation; Rotation",,,https://link.springer.com/article/10.1007/BF00206707 https://link.springer.com/10.1007/BF00206707 http://dx.doi.org/10.1007/BF00206707 https://espace.library.uq.edu.au/view/UQ:696283 https://dx.doi.org/10.1007/BF00206707 https://dblp.uni-trier.de/db/journals/bc/bc74.html#ChahlS96 https://www.ncbi.nlm.nih.gov/pubmed/8991456,http://dx.doi.org/10.1007/bf00206707,8991456,10.1007/bf00206707; 10.1007/s004220050252,2054388983,,1,010-680-379-649-730; 035-864-289-678-662; 039-437-250-893-17X; 058-253-338-790-418; 078-358-255-284-970; 080-346-984-505-909; 094-502-018-466-993; 120-725-983-514-773; 146-209-904-730-586; 152-578-931-624-421,46,false,,
027-588-207-376-882,Computer Vision and Iot Based Smart System for Visually Impaired People,2021-01-28,2021,conference proceedings article,"2021 11th International Conference on Cloud Computing, Data Science & Engineering (Confluence)",,IEEE,,Sneha Rao; Vishwa Mohan Singh,"For people who are visually impaired, navigation is a challenge they encounter on a daily basis. For the same, use of walking sticks have become a common practice. Although, there are a lot of limitation of just relying on a blind stick. A more suitable method will alert the user about the nature of the obstacle and should also be of assistance in guiding the user to their location. In this paper, we propose an architecture of an assistance system revolving around a shoe that employees IoT devices and sensors along with computer vision algorithms to provide functionalities like obstacle detection, avoidance and navigation. The method uses a smartphone based voice assistance and guides the user with appropriate haptic feedback calculated using various sensors and actuators.",,,,,Human–computer interaction; Architecture; Haptic technology; Smart system; Intelligent sensor; Obstacle; Computer vision algorithms; Visually impaired; Computer science; Internet of Things,,,,,http://xplorestaging.ieee.org/ielx7/9377009/9376876/09377120.pdf?arnumber=9377120 http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=9377120,http://dx.doi.org/10.1109/confluence51648.2021.9377120,,10.1109/confluence51648.2021.9377120,3139111832,,0,021-502-194-797-048; 030-440-500-148-114; 068-027-842-398-818; 073-778-972-169-624; 097-781-233-042-273; 170-362-243-033-062; 199-850-563-701-627,21,false,,
027-652-211-846-887,Multimedia platform for mathematics’ interactive learning accessible to blind people,2017-03-08,2017,journal article,Multimedia Tools and Applications,13807501; 15737721,Springer Science and Business Media LLC,Netherlands,Michał Maćkowski; Piotr Brzoza; Marek Żabka; Dominik Spinczyk,"Nowadays, the math learning is an important step in developing professional carriers in technical and economic sciences. Increasing the number of e-learning tools used in universities courses can reduce the potential barrier of access to mathematical knowledge, but most of them are not accessible for impaired students. Moreover, classical printed math books include little explicit instructional information about structural information interpretations. Taking into account these barriers the article presents the developed method used for creating interactive steps of decomposed math’s exercise solution and alternative description of math formulas accessible for the blind. The elements of proposed methodology: generation of state machine, design and presentation of transition conditions, generating the presentation layer and a typical usage by a blind user are presented. A set of rules for describing mathematical formulas were proposed after consultation with mathematicians and teachers of blind people. The application was developed as web application. The graphical interface of presented application was designed using PHP and JavaScript technologies. The collection of prepared exercises include about 240 prepared exercises from different areas of mathematics and 60 selected exercises including alternative description layer. About 1000 students and about 40 impaired students, from 6 faculties of the university use this platform during math courses for both self and class learning. The defined rules were used to read aloud mathematical formulas to the visually impaired people with a different level of mathematical knowledge. The results confirmed good understanding of mathematical formulas by using prepared alternative description.",77,5,6191,6208,Class (computer programming); Interactive Learning; Areas of mathematics; Graphical user interface; Computer science; Multimedia; Distance education,,,,Seventh Framework Programme,https://dl.acm.org/doi/10.1007/s11042-017-4526-z https://link.springer.com/article/10.1007/s11042-017-4526-z https://paperity.org/p/79304054/multimedia-platform-for-mathematics-interactive-learning-accessible-to-blind-people https://core.ac.uk/display/81555435 https://link.springer.com/content/pdf/10.1007%2Fs11042-017-4526-z.pdf https://www.mendeley.com/catalogue/4158665d-d9cc-3968-bdb4-fe1366107f58/ https://doi.org/10.1007/s11042-017-4526-z https://link.springer.com/article/10.1007/s11042-017-4526-z/fulltext.html https://dblp.uni-trier.de/db/journals/mta/mta77.html#MackowskiBZS18,http://dx.doi.org/10.1007/s11042-017-4526-z,,10.1007/s11042-017-4526-z,2592635712,,0,000-886-897-971-982; 003-003-000-182-952; 007-370-584-440-440; 012-440-048-958-301; 020-531-291-882-582; 020-836-942-335-660; 029-960-402-986-636; 033-827-462-556-277; 040-259-696-681-119; 046-801-162-988-146; 048-860-102-902-323; 049-407-453-569-463; 061-715-593-814-992; 066-283-648-500-444; 072-055-443-301-436; 075-056-001-292-281; 084-890-732-543-110; 087-212-416-528-521; 096-095-385-702-326; 101-637-283-239-900; 121-451-223-558-491; 135-607-800-607-983,26,true,cc-by,hybrid
027-682-239-914-13X,Navigation Enabling Application for Blind People,2022-02-15,2022,book chapter,Proceedings of International Conference on Industrial Instrumentation and Control,18761100; 18761119,Springer Nature Singapore,Germany,Atindra Nath Sarkar; null Jayita Saha; Pamela Das; Ritu Kumari; Tulika Chakraborty; Naiwrita Dey,"This paper proposes an assistive navigation application system to help visually impaired persons with independent travel in indoor. The proposed system is implemented in two parts: first, the obstacle detection is done by object identification using image processing and the images are stored in cloud further. You only look once (YOLO) version 3 which is a fully convolutional neural network (FCNN) is used for object identification. Twenty classes are considered with 2600 number of image samples. The application assembly is developed in android studio in Java platform. The application is named as “NETRA”. The blind person has to be logged in to the application beforehand. Later with text-to-speech conversion, the blind individual will be navigated. In the present work, the object identification using image processing and the overall application assembly is shown and discussed.",,,451,464,Computer science; Computer vision; Artificial intelligence; Identification (biology); Obstacle; Android (operating system); Object (grammar); Java; Convolutional neural network; Cloud computing; Image processing; Cognitive neuroscience of visual object recognition; Image (mathematics); Operating system; Geography; Botany; Archaeology; Biology,,,,,,http://dx.doi.org/10.1007/978-981-16-7011-4_43,,10.1007/978-981-16-7011-4_43,,,0,008-663-015-733-53X; 043-022-952-629-736; 062-128-353-400-109; 066-133-135-091-645; 110-762-686-137-414; 120-637-170-499-844; 131-503-960-053-375,1,false,,
027-934-550-270-689,Mobile phone based indoor navigation system for blind and visually impaired people: VUK — Visionless supporting frameworK,,2018,conference proceedings article,2018 19th International Carpathian Control Conference (ICCC),,IEEE,,Laszlo Arvai,"The urban mobility for blind and visually impaired people is a challenging task. Using public transportation, subways or visiting complex buildings like shopping malls is virtually impossible without some kind of support. Beside the conventional helpers the advanced sensory and computational power of a recent smart phone make it ideal candidate as a navigation aid for blind people. To take the full advantage of this device a carefully designed and implemented software system is required where the special user needs, man-machine interfaces and state of the art indoor localization method needs to be combined. This paper introduces the architecture of an indoor navigation system specially designed for blind and visually impaired people, explains main components, interfaces and briefly presents important algorithms and methods.",,,,,Human–computer interaction; Architecture; Software system; Mobile phone; Navigation system; Task (project management); Visually impaired; Computer science; State (computer science); Public transport,,,,,http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=8399660 http://xplorestaging.ieee.org/ielx7/8387527/8399518/08399660.pdf?arnumber=8399660,http://dx.doi.org/10.1109/carpathiancc.2018.8399660,,10.1109/carpathiancc.2018.8399660,2810587908,,0,011-552-293-977-610; 034-491-291-990-166; 037-649-514-992-799; 039-166-902-859-638; 045-610-565-774-275; 091-885-681-937-220; 092-520-794-435-061; 103-490-387-033-105; 184-689-690-593-603,11,false,,
028-193-380-963-060,A closer look onto breast density with weakly supervised dense-tissue masks,2019-05-18,2019,conference proceedings,International journal of computer assisted radiology and surgery,18616429; 18616410,Springer Science and Business Media LLC,Germany,Mickael Tardy; Bruno Scheffer; Diana Mateus,This work focuses on the automatic quantification of the breast density from digital mammography imaging. Using only categorical image-wise labels we train a model capable of predicting continuous density percentage as well as providing a pixel wise support frit for the dense region. In particular we propose a weakly supervised loss linking the density percentage to the mask size.,14,Suppl 1,11,194,Deep learning; Dense connective tissue; Artificial intelligence; Pixel; Pattern recognition; Digital mammography; Continuous density; Breast density; Computer science; Categorical variable; Cancer; Radiology; Transfer of learning; Biopsy; Optical coherence tomography; Lesion; Laser Microscopy; Early detection; Upper aerodigestive tract; Gold standard (test); Medicine; Medical physics,,,,,http://ui.adsabs.harvard.edu/abs/2019arXiv190711860T/abstract https://hal.archives-ouvertes.fr/hal-02565263 https://openreview.net/forum?id=SkgCGCBEqN https://openreview.net/pdf?id=SkgCGCBEqN,http://dx.doi.org/10.1007/s11548-019-01969-3,31104256,10.1007/s11548-019-01969-3,3100502148; 2914458205,,1,003-905-411-095-055; 014-442-325-816-54X,5,true,,green
028-460-767-882-226,Robot-assisted shopping for the blind: issues in spatial cognition and product selection,2008-03-11,2008,journal article,Intelligent Service Robotics,18612776; 18612784,Springer Science and Business Media LLC,Germany,Chaitanya Gharpure; Vladimir Kulyukin,,1,3,237,251,Product (category theory); Human–robot interaction; Haptic technology; Spatial cognition; Product selection; Assistive robotics; Computer science; Multimedia; Robot,,,,,https://link.springer.com/article/10.1007/s11370-008-0020-9 https://dblp.uni-trier.de/db/journals/isrob/isrob1.html#GharpureK08 https://doi.org/10.1007/s11370-008-0020-9,http://dx.doi.org/10.1007/s11370-008-0020-9,,10.1007/s11370-008-0020-9,2051490227,,0,002-201-032-015-180; 007-300-790-339-390; 011-797-116-120-363; 015-195-550-005-330; 015-623-086-767-734; 016-810-631-838-447; 022-335-702-913-705; 026-080-233-493-464; 027-846-890-041-014; 030-127-369-356-741; 032-132-714-120-297; 036-022-688-541-298; 040-653-161-580-237; 044-479-924-573-444; 048-298-689-305-353; 053-872-980-515-457; 057-083-293-597-247; 058-612-859-869-772; 061-691-497-102-297; 069-419-248-108-995; 069-781-103-164-947; 073-407-269-569-770; 074-160-301-853-013; 081-927-652-906-388; 086-720-900-772-539; 088-673-691-914-820; 091-729-545-811-67X; 094-763-081-817-982; 103-565-903-319-200; 103-769-217-393-19X; 104-694-342-259-021; 107-887-647-672-757; 118-713-134-426-98X; 125-892-623-875-209; 135-646-014-794-306; 140-644-534-290-218; 150-204-276-810-957; 152-700-026-822-07X; 154-421-405-824-607; 155-680-283-716-538; 176-328-435-633-379,65,false,,
028-628-140-541-774,Computer vision-based object recognition for the visually impaired in an indoors environment: a survey,2013-10-15,2013,journal article,The Visual Computer,01782789; 14322315,Springer Science and Business Media LLC,Germany,Rabia Jafri; Syed Abid Ali; Hamid R. Arabnia; Shameem Fatima,,30,11,1197,1222,Artificial intelligence; Visually impaired; Computer vision; Focus (computing); Computer science; Multimedia; Resource (project management); Cognitive neuroscience of visual object recognition; Computer graphics,,,,,https://doi.org/10.1007/s00371-013-0886-1 https://www.infona.pl/resource/bwmeta1.element.springer-a47a61d6-eb88-3e45-b32e-a073a2d8314f https://link.springer.com/article/10.1007/s00371-013-0886-1 https://dl.acm.org/citation.cfm?id=2684518 https://dblp.uni-trier.de/db/journals/vc/vc30.html#JafriAAF14 https://dl.acm.org/doi/10.1007/s00371-013-0886-1,http://dx.doi.org/10.1007/s00371-013-0886-1,,10.1007/s00371-013-0886-1,1969113967,,2,001-076-828-128-846; 002-038-817-804-326; 002-724-634-490-58X; 004-809-996-572-967; 009-128-554-117-48X; 011-214-253-974-00X; 012-642-196-605-159; 013-000-098-593-934; 013-056-435-015-274; 013-731-981-813-544; 013-847-555-426-159; 014-013-566-005-427; 014-157-934-526-940; 014-279-588-505-48X; 019-116-681-732-308; 019-514-465-332-477; 019-932-725-468-242; 020-397-774-916-201; 021-185-871-895-205; 024-081-579-088-96X; 026-105-720-767-301; 029-382-620-854-808; 030-440-500-148-114; 030-707-222-425-440; 030-869-182-891-085; 031-258-275-252-008; 031-827-665-840-574; 032-133-232-035-617; 032-786-641-709-377; 034-090-269-366-482; 034-163-350-435-371; 034-303-305-119-726; 037-852-576-689-916; 038-218-082-706-921; 038-225-377-218-605; 038-858-720-835-740; 044-202-560-313-315; 045-298-655-695-753; 045-547-305-745-073; 045-568-616-540-617; 045-673-862-191-277; 045-874-715-328-967; 047-081-444-387-991; 047-126-935-003-311; 047-272-694-013-898; 047-749-445-596-16X; 049-098-087-244-565; 049-106-304-885-412; 050-861-552-634-369; 051-612-966-213-045; 053-196-706-640-254; 053-968-972-722-364; 055-188-042-679-385; 057-849-677-847-379; 058-942-174-334-646; 065-049-774-625-65X; 065-992-777-809-97X; 068-337-577-140-376; 071-473-658-566-47X; 071-806-151-845-245; 071-941-689-773-599; 072-727-441-158-997; 074-117-698-720-580; 074-324-981-337-749; 076-186-513-442-29X; 077-303-325-289-693; 079-520-235-014-120; 080-087-555-076-33X; 084-418-827-681-971; 085-248-504-750-327; 088-026-110-992-900; 088-499-742-459-856; 090-170-384-821-699; 090-884-510-770-832; 090-924-053-435-345; 092-391-305-190-481; 093-487-016-608-661; 096-167-405-097-291; 098-116-610-021-176; 098-440-015-522-27X; 099-112-199-553-604; 104-620-643-412-024; 109-509-373-703-249; 110-650-958-929-874; 116-189-372-609-852; 117-320-268-093-486; 120-762-486-035-141; 124-597-558-957-962; 125-069-333-407-609; 125-215-400-077-068; 127-483-973-270-088; 128-897-617-870-753; 130-487-670-305-075; 133-457-684-144-980; 136-975-330-574-149; 138-240-748-829-95X; 141-789-666-022-796; 144-454-158-356-436; 146-818-241-205-847; 151-015-279-636-660; 153-704-670-049-000; 155-503-280-718-987; 156-066-619-903-086; 157-840-160-077-863; 159-112-312-446-873; 159-903-694-495-777; 160-234-441-041-937; 165-315-873-244-89X; 168-511-383-336-908; 175-130-600-114-960; 175-391-077-920-292; 181-797-068-816-79X; 183-617-660-742-453; 189-205-044-796-965; 198-094-749-849-956,113,false,,
028-658-887-298-730,Visually Guided Search Behavior during Walking in Insects with Different Habitat Utilization Strategies,2019-12-18,2019,journal article,Journal of Insect Behavior,08927553; 15728889,Springer Science and Business Media LLC,United States,Karl Kral,,32,4,290,305,Entomology; Homing (biology); Cognitive psychology; Habitat; Visual field; Landmark; Nest site; Animal ecology; Visually guided; Biology,,,,,https://link.springer.com/article/10.1007/s10905-019-09735-8,http://dx.doi.org/10.1007/s10905-019-09735-8,,10.1007/s10905-019-09735-8,2995883311,,0,000-550-018-701-064; 000-567-331-424-455; 000-606-905-620-017; 002-857-356-088-273; 005-186-755-777-951; 006-616-909-542-420; 007-545-564-798-885; 008-543-899-349-309; 010-613-832-231-43X; 010-658-020-119-687; 012-919-063-159-879; 013-458-941-552-517; 016-105-773-670-017; 016-389-447-190-605; 016-531-346-737-303; 017-760-711-955-479; 017-917-722-388-046; 018-055-262-864-338; 019-173-728-342-43X; 019-494-265-133-160; 020-182-341-403-197; 021-174-962-547-069; 021-226-616-538-858; 024-591-158-685-344; 026-350-134-217-39X; 026-492-475-056-495; 029-095-014-148-231; 029-853-682-970-095; 030-243-795-351-888; 030-261-334-601-185; 030-351-456-708-924; 030-688-517-426-052; 030-799-077-690-125; 031-692-492-061-592; 032-353-739-264-982; 033-958-344-571-871; 034-135-331-637-324; 034-257-835-187-334; 034-332-641-912-769; 035-864-289-678-662; 036-505-123-639-673; 036-749-600-123-903; 038-004-216-045-476; 038-270-861-527-728; 039-774-835-545-944; 041-932-801-200-764; 043-941-618-129-311; 044-302-466-082-272; 044-511-182-910-700; 048-036-750-750-326; 048-192-085-977-479; 049-178-911-031-358; 051-009-162-453-792; 053-978-380-681-131; 059-007-331-864-034; 059-174-265-195-36X; 062-223-717-006-619; 064-533-587-679-976; 065-638-265-118-196; 068-886-660-729-132; 070-773-352-956-828; 071-416-954-167-093; 072-057-233-458-24X; 072-305-413-834-53X; 074-052-352-175-031; 075-394-965-046-757; 076-581-828-964-957; 083-783-303-108-540; 085-870-240-974-078; 086-374-701-646-610; 086-693-792-127-776; 091-601-974-225-377; 093-159-363-459-426; 094-775-166-323-090; 097-385-645-266-965; 098-404-722-909-280; 099-028-243-091-223; 103-855-775-555-30X; 104-818-761-317-013; 106-265-811-173-19X; 112-157-082-428-103; 114-639-784-194-414; 119-022-596-099-171; 119-735-594-595-85X; 124-835-656-018-951; 127-949-455-887-928; 131-295-043-007-490; 133-582-607-577-339; 135-039-657-320-978; 135-919-776-861-481; 138-142-282-286-359; 138-340-029-270-406; 140-103-063-791-355; 142-667-502-035-25X; 148-406-472-838-90X; 148-981-098-139-332; 152-790-391-864-851; 155-492-496-570-36X; 161-854-404-824-477; 163-380-100-757-501; 163-524-925-370-993; 174-756-114-035-341,5,false,,
028-671-432-434-245,Real-time pedestrian crossing lights detection algorithm for the visually impaired,2017-12-15,2017,journal article,Multimedia Tools and Applications,13807501; 15737721,Springer Science and Business Media LLC,Netherlands,Ruiqi Cheng; Kaiwei Wang; Kailun Yang; Ningbo Long; Jian Bai; Dong Liu,,77,16,20651,20671,Algorithm; Ground truth; Pedestrian crossing; Visually impaired; Frame rate; Computer science; Precision and recall; Image processing; Robustness (computer science),,,,,https://link.springer.com/article/10.1007/s11042-017-5472-5 https://doi.org/10.1007/s11042-017-5472-5 https://dblp.uni-trier.de/db/journals/mta/mta77.html#ChengWYLBL18,http://dx.doi.org/10.1007/s11042-017-5472-5,,10.1007/s11042-017-5472-5,2771992117,,0,006-349-410-091-743; 022-940-138-590-719; 023-438-345-162-875; 035-395-409-643-765; 035-569-236-178-312; 037-163-533-301-156; 043-536-853-133-602; 047-676-233-635-740; 049-467-020-706-025; 051-851-442-391-236; 061-612-681-218-646; 068-446-370-600-681; 071-306-486-880-383; 071-583-782-423-553; 085-094-607-137-825; 088-827-492-737-349; 092-264-803-418-192; 097-741-796-821-207; 118-861-810-500-060; 141-265-786-504-629; 146-788-878-955-093,39,false,,
029-207-411-037-846,A study on the attention of people with low vision to accessibility guidance signs,2023-10-26,2023,journal article,Journal on Multimodal User Interfaces,17837677; 17838738,Springer Science and Business Media LLC,Germany,Weitao Jiang; Bingxin Zhang; Ruiqi Sun; Dong Zhang; Shan Hu,,18,1,87,101,Computer science; Low vision; Computer vision; Human–computer interaction; Artificial intelligence; Visual attention; Optometry; Perception; Psychology; Medicine; Neuroscience,,,,,,http://dx.doi.org/10.1007/s12193-023-00417-6,,10.1007/s12193-023-00417-6,,,0,000-470-497-733-126; 001-514-212-254-243; 002-641-654-624-887; 002-704-194-668-787; 003-682-075-177-450; 004-804-915-515-678; 006-548-617-871-060; 007-305-095-446-752; 007-530-851-453-706; 007-569-563-602-55X; 008-361-673-204-978; 012-081-521-655-479; 013-056-435-015-274; 013-456-414-125-610; 016-289-010-228-596; 018-843-508-574-144; 020-801-848-755-431; 020-967-107-016-770; 025-225-530-473-716; 027-224-850-444-998; 030-440-500-148-114; 030-777-084-514-78X; 042-352-331-255-503; 042-893-530-917-399; 043-020-833-842-287; 048-809-024-427-323; 055-495-691-087-785; 057-847-555-860-541; 058-536-827-846-304; 059-424-379-092-456; 061-505-355-706-499; 062-891-822-362-766; 063-953-563-813-469; 065-477-975-652-508; 065-561-146-681-012; 066-037-919-140-345; 067-083-846-597-187; 068-526-647-480-345; 072-700-686-208-18X; 073-315-728-261-014; 075-519-293-886-898; 075-937-982-151-194; 078-896-244-502-504; 079-293-749-274-790; 080-506-961-914-97X; 082-094-559-634-464; 085-564-340-397-815; 088-026-691-471-176; 088-776-105-175-191; 091-315-273-827-829; 097-740-040-292-443; 112-025-284-175-226; 118-479-191-922-776; 119-882-574-664-723; 137-032-010-877-936; 142-099-375-656-981; 158-472-077-766-166; 159-711-382-862-033; 179-386-041-365-484; 181-496-802-713-578,0,false,,
029-327-214-479-561,ICOST - Mobile Assistive Application for Blind People in Indoor Navigation.,2020-06-23,2020,book chapter,Lecture Notes in Computer Science,03029743; 16113349,Springer International Publishing,Germany,Hanen Jabnoun; Mohammad Abu Hashish; Faouzi Benzarti,"Navigation is an important human task that needs the human sense of vision. In this context, recent technologies developments provide technical assistance to support the visually impaired in their daily tasks and improve their quality of life. In this paper, we present a mobile assistive application called “GuiderMoi” that retrieves information about directions using color targets and identifies the next orientation for the visually impaired. In order to avoid the failure in detection and the inaccurate tracking caused by the mobile camera, the proposed method based on the CamShift algorithm aims to introduce better location and identification of color targets. Tests were conduct in natural indoor scene. The results depending on the distance and the angle of view, defined the accurate values to have a highest rate of target recognition. This work has perspectives for this such as implicating the augmented reality and the intelligent navigation based on machine learning and real-time processing.",,,395,403,Human–computer interaction; Augmented reality; Context (language use); Task (project management); Mobile camera; Human sense; Computer science; Identification (information); Orientation (computer vision); Angle of view,,,,,https://link.springer.com/chapter/10.1007/978-3-030-51517-1_36 https://rd.springer.com/chapter/10.1007/978-3-030-51517-1_36 https://dblp.uni-trier.de/db/conf/icost/icost2020.html#JabnounHB20,http://dx.doi.org/10.1007/978-3-030-51517-1_36,,10.1007/978-3-030-51517-1_36,3037267321,,0,013-847-555-426-159; 016-209-364-096-807; 022-265-938-732-803; 025-330-815-941-573; 028-078-246-285-461; 031-258-275-252-008; 045-673-862-191-277; 047-749-445-596-16X; 048-603-106-043-151; 057-847-555-860-541; 065-819-195-728-230; 068-858-525-190-391; 072-995-846-715-537; 081-763-967-118-804; 091-792-890-326-861; 100-676-794-926-943; 102-076-221-044-20X; 121-197-907-645-173,3,true,cc-by,hybrid
029-360-136-216-567,RGB-D camera based wearable navigation system for the visually impaired,,2016,journal article,Computer Vision and Image Understanding,10773142; 1090235x,Elsevier BV,United States,Young Ho Lee; Gerard Medioni,,149,,3,20,Wearable computer; Artificial intelligence; Shortest path problem; Navigation system; Orientation and Mobility; Computer vision; Computer science; Turn-by-turn navigation; Occupancy grid mapping; Global Positioning System; Obstacle avoidance,,,,USAMRMC; TATRC,https://www.sciencedirect.com/science/article/abs/pii/S1077314216300248 https://dx.doi.org/10.1016/j.cviu.2016.03.019 https://dl.acm.org/doi/10.1016/j.cviu.2016.03.019 http://dx.doi.org/10.1016/j.cviu.2016.03.019 http://www.sciencedirect.com/science/article/pii/S1077314216300248 https://dblp.uni-trier.de/db/journals/cviu/cviu149.html#LeeM16,http://dx.doi.org/10.1016/j.cviu.2016.03.019,,10.1016/j.cviu.2016.03.019,2315531387,,1,004-756-256-314-376; 007-781-062-370-947; 010-292-874-026-413; 010-719-927-108-990; 013-650-518-339-057; 014-478-034-924-123; 015-016-116-643-819; 024-369-065-483-244; 025-278-629-527-832; 028-683-545-086-724; 031-612-744-612-651; 033-119-829-457-834; 034-551-334-308-190; 037-163-533-301-156; 040-898-046-412-436; 042-878-847-356-424; 044-239-148-413-612; 045-205-438-482-211; 045-321-254-881-478; 048-603-106-043-151; 049-098-087-244-565; 051-766-223-654-722; 055-113-311-832-987; 059-482-749-135-704; 060-029-581-945-26X; 065-992-777-809-97X; 068-409-552-839-615; 068-581-156-196-413; 072-481-987-766-954; 074-941-207-321-047; 079-132-025-467-063; 086-269-797-595-47X; 101-969-663-263-663; 128-296-357-928-766; 138-774-270-601-871; 158-098-789-138-937; 170-387-846-090-147; 176-619-480-167-936; 183-816-558-497-844; 184-515-171-522-125; 188-569-539-275-740; 195-842-744-142-232,111,false,,
029-420-088-068-863,Towards Understanding Sensory Substitution for Accessible Visualization: An Interview Study.,2021-12-24,2021,journal article,IEEE transactions on visualization and computer graphics,19410506; 10772626; 21609306,Institute of Electrical and Electronics Engineers (IEEE),United States,Pramod Chundury; Biswaksen Patnaik; Yasmin Reyazuddin; Christine Tang; Jonathan Lazar; Niklas Elmqvist,"For all its potential in supporting data analysis, particularly in exploratory situations, visualization also creates barriers: accessibility for blind and visually impaired individuals. Regardless of how effective a visualization is, providing equal access for blind users requires a paradigm shift for the visualization research community. To enact such a shift, it is not sufficient to treat visualization accessibility as merely another technical problem to overcome. Instead, supporting the millions of blind and visually impaired users around the world who have equally valid needs for data analysis as sighted individuals requires a respectful, equitable, and holistic approach that includes all users from the onset. In this paper, we draw on accessibility research methodologies to make inroads towards such an approach. We first identify the people who have specific insight into how blind people perceive the world: orientation and mobility (O and M) experts, who are instructors that teach blind individuals how to navigate the physical world using non-visual senses.",28,1,1,1,Human–computer interaction; Orientation and Mobility; Interview study; Visually impaired; Research community; Visualization; Computer science; Paradigm shift; Sensory substitution,,"Blindness; Computer Graphics; Humans; Touch; Vision, Ocular; Visually Impaired Persons",,,https://www.ncbi.nlm.nih.gov/pubmed/34587061 http://xplorestaging.ieee.org/ielx7/2945/4359476/09552177.pdf?arnumber=9552177,http://dx.doi.org/10.1109/tvcg.2021.3114829,34587061,10.1109/tvcg.2021.3114829,3202733979,,0,001-386-840-011-038; 001-858-271-877-642; 002-408-516-508-721; 003-055-508-972-538; 008-612-209-509-827; 010-907-397-613-292; 011-305-212-011-315; 012-337-599-402-232; 012-904-269-079-450; 015-837-338-452-266; 015-978-126-687-61X; 016-497-255-140-37X; 017-950-146-555-129; 018-106-046-766-271; 021-347-185-987-360; 024-093-010-731-859; 025-089-652-467-417; 027-033-143-017-928; 028-040-318-504-122; 030-609-509-752-124; 032-287-797-629-991; 033-016-061-226-219; 034-804-042-689-182; 036-596-448-417-207; 036-895-827-642-260; 039-283-948-801-805; 042-381-074-344-865; 044-188-694-002-231; 044-356-060-642-288; 046-136-788-502-673; 046-991-476-786-467; 048-327-566-365-049; 048-379-892-594-458; 049-011-708-214-023; 049-621-063-588-981; 057-054-513-506-464; 057-507-711-999-65X; 061-070-177-754-892; 063-373-121-028-155; 064-566-939-497-334; 065-989-043-835-282; 067-668-678-671-547; 068-163-377-582-711; 074-200-490-341-888; 075-224-265-138-093; 079-090-335-218-377; 082-067-592-876-541; 082-953-778-110-938; 083-946-520-667-692; 085-729-350-827-949; 086-346-752-533-976; 086-585-229-048-691; 087-993-101-734-747; 089-897-727-084-742; 091-724-246-682-414; 092-909-589-521-361; 093-289-409-439-363; 093-668-999-939-359; 094-763-081-817-982; 096-575-415-162-897; 096-786-004-864-560; 097-504-966-869-336; 098-995-012-561-715; 103-469-568-032-463; 107-649-285-251-520; 109-912-280-570-475; 119-967-251-516-956; 123-331-904-432-815; 127-388-252-926-686; 133-684-734-618-794; 144-837-804-461-424; 145-976-029-007-525; 152-678-363-991-650; 162-855-474-582-208; 164-938-335-748-294; 175-536-843-064-11X; 178-575-219-743-304; 179-044-661-384-950,25,false,,
029-446-194-754-619,Temperature-based alternate perception method for human-motion detection with visually impaired user applications,2017-10-25,2017,journal article,International Journal of Intelligent Robotics and Applications,23665971; 2366598x,Springer Science and Business Media LLC,,Jiaoying Jiang; Kok-Meng Lee; Jingjing Ji,,1,4,383,398,Motion (physics); Artificial intelligence; Image resolution; Perception; Motion detection; Computer vision; Field (computer science); Computer science; Image processing; Robustness (computer science); Noise (video),,,,,https://link.springer.com/content/pdf/10.1007%2Fs41315-017-0035-5.pdf https://link.springer.com/article/10.1007%2Fs41315-017-0035-5 https://rd.springer.com/article/10.1007/s41315-017-0035-5 https://dblp.uni-trier.de/db/journals/ijira/ijira1.html#JiangLJ17,http://dx.doi.org/10.1007/s41315-017-0035-5,,10.1007/s41315-017-0035-5,2767186153,,0,002-990-910-547-637; 005-303-872-644-71X; 007-981-118-397-345; 010-812-080-754-343; 013-056-435-015-274; 017-315-906-883-131; 017-334-081-311-633; 020-008-624-420-698; 021-802-904-479-912; 021-920-311-828-213; 021-954-968-302-721; 027-720-558-803-335; 035-048-525-263-042; 036-879-436-371-23X; 041-925-549-498-232; 046-009-910-350-481; 066-622-591-843-254; 072-253-529-023-163; 090-179-534-344-332; 091-463-785-359-330; 111-829-235-998-290; 116-132-992-923-503; 167-451-513-429-347; 186-432-344-459-742,1,false,,
029-574-265-837-670,Eyes-free environmental awareness for navigation,2011-11-23,2011,journal article,Journal on Multimodal User Interfaces,17837677; 17838738,Springer Science and Business Media LLC,Germany,Dalia El-Shimy; Florian Grond; Adriana Olmos; Jeremy R. Cooperstock,,5,3,131,141,Human–computer interaction; Point of interest; Mobile device; Rendering (computer graphics); Data resources; Visual attention; Target population; Computer science; Multimedia; Public transport; Radar,,,,,https://doi.org/10.1007/s12193-011-0065-5 https://dblp.uni-trier.de/db/journals/jmui/jmui5.html#El-ShimyGOC12 https://link.springer.com/article/10.1007/s12193-011-0065-5 https://rd.springer.com/article/10.1007/s12193-011-0065-5,http://dx.doi.org/10.1007/s12193-011-0065-5,,10.1007/s12193-011-0065-5,2033823570,,3,006-291-424-106-783; 029-495-302-838-088; 046-827-791-624-340; 057-083-293-597-247; 067-217-663-836-852; 071-220-737-240-480; 073-407-269-569-770; 080-877-996-625-003; 093-532-644-161-514; 113-925-305-248-157; 116-976-189-445-151; 129-887-764-479-233; 192-588-056-613-897,12,false,,
029-785-009-752-993,The SmartVision navigation prototype for the blind,,2010,,,,,,J. M. H. du Buf; João Barroso; João M. F. Rodrigues; Hugo Paredes; Miguel Farrajota; Hugo Fernandes; João José; Victor Teixeira; Mário Saleiro,"The goal of the project ""SmartVision: active vision for the blind"" is to develop a small and portable but intelligent and reliable system for assisting the blind and visually impaired while navigating autonomously, both outdoor and indoor. In this paper we present an overview of the prototype, design issues, and its different modules which integrate a GIS with GPS, Wi-Fi, RFID tags and computer vision. The prototype addresses global navigation by following known landmarks, local navigation with path tracking and obstacle avoidance, and object recognition. The system does not replace the white cane, but extends it beyond its reach. The userfriendly interface consists of a 4-button hand-held box, a vibration actuator in the handle of the cane, and speech synthesis. A future version may also employ active RFID tags for marking navigation landmarks, and speech recognition may complement speech synthesis.",,,,,Human–computer interaction; Interface (computing); Speech synthesis; Actuator; Active vision; Computer science; Global Positioning System; User Friendly; Cognitive neuroscience of visual object recognition; Obstacle avoidance,,,,,,,,,832248642,,0,000-241-731-826-744; 019-775-185-370-343; 024-542-562-950-823; 055-699-335-878-366; 056-773-999-189-634; 057-890-353-263-937; 065-882-712-564-29X; 068-446-370-600-681; 077-703-665-866-285; 078-191-959-374-104; 080-869-027-189-863; 095-077-362-337-471; 125-215-400-077-068; 130-552-169-471-000; 140-557-040-326-561; 148-768-859-064-775; 171-967-210-532-103; 172-478-666-678-88X; 181-390-807-704-755,7,false,,
029-850-495-079-869,Localization and Guidance in RAMPE/INFOMOVILLE-an interactive system of assistance for Blind travelers,,2009,conference proceedings article,2009 Second International Conference on the Applications of Digital Information and Web Technologies,,IEEE,,Jinane Sayah; Genevieve Baudoin; Olivier Venard; Bachar El Hassan,"We present in this paper the RAMPE/INFOMOVILLE system-an interactive Man-Machine-Interface for the assistance, information, orientation and security of persons having visual or/and auditive handicap in public transports. We desrcibe this system, its components and operation. The current version doesn't include a localization solution to guide the visually impaired person and assist him/her to reach the public transport station. A solution is proposed to address the needs of this application in terms of Guidance and Localization. We do base on the WiFi Positioning System currently very wide spread and easy to use in localizing assets and people but that presents, however, some disadvantages highlighted in the body of this paper. Some modifications were brought to remedy these drawbacks in order to have a proprietary and adequate solution for our application. This solution is tested, an experiment is done and the results shown.",,,243,249,Human–computer interaction; Human–machine interface; Positioning system; Biomedical communication; Visually impaired; Computer security; Computer science; Public transport; Global Positioning System; Orientation (computer vision); Mobile telephony,,,,,http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.ieee-000005273868 https://ieeexplore.ieee.org/document/5273868/,http://dx.doi.org/10.1109/icadiwt.2009.5273868,,10.1109/icadiwt.2009.5273868,2111299834,,0,002-370-584-649-058; 045-500-897-718-061; 094-346-512-675-81X; 130-630-523-476-738,4,false,,
029-889-561-083-355,"An Onboard Monocular Vision System for Autonomous Takeoff, Hovering and Landing of a Micro Aerial Vehicle",2012-09-13,2012,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Shaowu Yang; Sebastian Scherer; Andreas Zell,"In this paper, we present an onboard monocular vision system for autonomous takeoff, hovering and landing of a Micro Aerial Vehicle (MAV). Since pose information with metric scale is critical for autonomous flight of a MAV, we present a novel solution to six degrees of freedom (DOF) pose estimation. It is based on a single image of a typical landing pad which consists of the letter ""H"" surrounded by a circle. A vision algorithm for robust and real-time landing pad recognition is implemented. Then the 5 DOF pose is estimated from the elliptic projection of the circle by using projective geometry. The remaining geometric ambiguity is resolved by incorporating the gravity vector estimated by the inertial measurement unit (IMU). The last degree of freedom pose, yaw angle of the MAV, is estimated from the ellipse fitted from the letter ""H"". The efficiency of the presented vision system is demonstrated comprehensively by comparing it to ground truth data provided by a tracking system and by using its pose estimates as control inputs to autonomous flights of a quadrotor.",69,1,499,515,Six degrees of freedom; Tracking system; Artificial intelligence; Monocular vision; Takeoff; Computer vision; Computer science; Pose; Machine vision; Ellipse; Inertial measurement unit,,,,,https://link.springer.com/content/pdf/10.1007%2Fs10846-012-9749-7.pdf https://dialnet.unirioja.es/servlet/articulo?codigo=4106000 https://dx.doi.org/10.1007/s10846-012-9749-7 http://dx.doi.org/10.1007/s10846-012-9749-7 https://dblp.uni-trier.de/db/journals/jirs/jirs69.html#YangSZ13,http://dx.doi.org/10.1007/s10846-012-9749-7,,10.1007/s10846-012-9749-7,1997522477,,1,001-085-723-265-187; 004-711-578-270-397; 018-866-305-444-915; 021-700-740-826-30X; 024-131-470-445-452; 024-183-730-384-217; 025-151-529-700-089; 026-684-536-513-319; 027-333-425-165-261; 041-825-464-367-446; 043-999-955-569-475; 045-846-418-626-169; 055-211-858-566-79X; 074-324-981-337-749; 075-359-662-961-890; 078-394-132-758-716; 079-756-801-541-445; 090-695-523-126-170; 096-654-150-407-072; 099-230-453-362-975; 134-019-508-188-593; 144-488-569-314-324; 146-572-777-410-227; 151-489-947-969-554; 151-849-867-548-22X,155,true,,green
030-006-119-435-809,A Smart System for Obstacle Detection to Assist Visually Impaired in Navigating Autonomously Using Machine Learning Approach,2023-02-10,2023,other,Artificial Intelligence Applications and Reconfigurable Architectures,,Wiley,,Vijay Dabhade; Dnyaneshwar Dhawalshankh; Anuradha Thakare; Maithili Kulkarni; Priyanka Ambekar,"Obstacle detection is a popular approach for detecting barriers in the region of a subject. Visual impairment affects a large number of people around the world. Visually impaired or blind people encounter numerous challenges in their daily lives; the white cane is still the most widely used instrument for obstacle detection; in an unfamiliar environment, they rely entirely on other people to reach their desired goal. This will allow blind people to navigate independently without the use of any aids by utilizing object detection systems that detect objects over a period of time. The proposed system in this article uses machine learning algorithms to see objects through the camera and then uses audio output to teach blind people about the item and its location. Obstacle detection approaches are discussed that can create and develop a system which will assist visually impaired people in navigating autonomously.",,,137,149,Obstacle; Computer science; Artificial intelligence; Computer vision; Visually impaired; Human–computer interaction; Object detection; Object (grammar); Pattern recognition (psychology); Geography; Archaeology,,,,,,http://dx.doi.org/10.1002/9781119857891.ch7,,10.1002/9781119857891.ch7,,,0,003-251-716-880-820; 040-941-067-536-586; 049-289-745-367-746; 101-172-604-809-357; 101-637-146-335-197,0,false,,
030-127-369-356-741,The GuideCane-applying mobile robot technologies to assist the visually impaired,,2001,journal article,"IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans",10834427,Institute of Electrical and Electronics Engineers (IEEE),United States,I. Ulrich; Johann Borenstein,"The GuideCane is a device designed to help blind or visually impaired users navigate safely and quickly among obstacles and other hazards. During operation, the user pushes the lightweight GuideCane forward. When the GuideCane's ultrasonic sensors detect an obstacle, the embedded computer determines a suitable direction of motion that steers the GuideCane and the user around it. The steering action results in a very noticeable force felt in the handle, which easily guides the user without any conscious effort on his/her part.",31,2,131,136,Motion (physics); Human–computer interaction; Motion control; Artificial intelligence; Mobile robot; Object detection; Obstacle; Action (philosophy); Motion detection; Computer vision; Computer science; Motion planning,,,,,https://dl.acm.org/doi/10.1109/3468.911370 https://dx.doi.org/10.1109/3468.911370 https://dblp.uni-trier.de/db/journals/tsmc/tsmca31.html#UlrichB01 http://dx.doi.org/10.1109/3468.911370 https://ieeexplore.ieee.org/document/911370/ http://cs.unc.edu/~welch/class/mobility/papers/GuideCane.pdf https://doi.org/10.1109/3468.911370 https://cs.unc.edu/~welch/class/mobility/papers/GuideCane.pdf https://ieeexplore.ieee.org/abstract/document/911370 http://www-personal.umich.edu/~johannb/Papers/paper72.pdf https://core.ac.uk/display/22936136,http://dx.doi.org/10.1109/3468.911370,,10.1109/3468.911370,2120436369,,12,010-306-378-600-804; 012-220-148-772-614; 025-278-629-527-832; 031-612-744-612-651; 045-321-254-881-478; 046-649-192-050-710; 058-443-047-291-990; 065-992-777-809-97X; 073-722-974-726-137; 078-736-735-476-395; 094-003-193-394-783; 096-803-870-115-728; 127-129-010-898-639; 135-646-014-794-306,363,false,,
030-251-661-047-944,Obstacle and Fall Detection to Guide the Visually Impaired People with Real Time Monitoring,2020-06-27,2020,journal article,SN Computer Science,2662995x; 26618907,Springer Science and Business Media LLC,,Mohammad Marufur Rahman; Milon Islam; Shishir Ahmmed; Saeed Anwar Khan,,1,4,1,10,Human–computer interaction; Microcontroller; Obstacle; SAFER; Visually impaired; Computer science; Bluetooth; Data transmission; Accelerometer; Track (disk drive),,,,,https://link.springer.com/article/10.1007/s42979-020-00231-x https://dblp.uni-trier.de/db/journals/sncs/sncs1.html#RahmanIAK20 https://doi.org/10.1007/s42979-020-00231-x https://link.springer.com/content/pdf/10.1007/s42979-020-00231-x.pdf,http://dx.doi.org/10.1007/s42979-020-00231-x,,10.1007/s42979-020-00231-x,3038116466,,0,002-841-567-358-302; 011-199-288-144-505; 012-081-521-655-479; 016-663-644-145-618; 018-843-508-574-144; 025-358-618-965-546; 026-330-055-991-696; 031-532-771-330-967; 034-519-495-322-081; 037-168-339-082-036; 037-437-225-866-951; 038-326-446-864-583; 038-920-552-577-440; 040-533-527-422-721; 045-581-684-504-286; 053-334-747-148-744; 054-151-572-436-553; 056-311-013-170-616; 059-964-780-360-565; 081-187-105-109-848; 088-639-813-788-238; 089-923-728-985-127; 098-061-809-426-183; 106-587-878-038-509; 109-525-288-959-67X; 124-599-469-135-595; 131-569-136-823-927; 134-771-655-317-362; 151-039-447-100-109; 155-030-852-892-781; 157-392-555-293-070,37,false,,
030-286-183-359-731,Vision-based outdoor navigation of self-driving car using lane detection,2021-08-04,2021,journal article,International Journal of Information Technology,25112104; 25112112,Springer Science and Business Media LLC,,Amit Kumar; Tejeshwar Saini; Pratik B. Pandey; Apoorv Agarwal; Anand Agrawal; Basant Agarwal,,14,1,215,227,,,,,,,http://dx.doi.org/10.1007/s41870-021-00747-2,,10.1007/s41870-021-00747-2,,,0,004-246-362-529-936; 005-099-309-092-236; 007-121-877-859-459; 022-861-447-976-687; 025-374-473-982-598; 036-405-187-046-676; 038-066-445-222-759; 038-687-694-666-66X; 039-419-637-407-827; 051-796-842-432-544; 058-975-761-287-208; 082-907-939-330-513; 106-644-239-684-326; 108-052-682-036-44X; 109-388-875-800-796; 164-484-623-576-26X; 168-461-201-923-712,7,false,,
030-476-253-175-901,Two-stage multi-sensor fusion positioning system with seamless switching for cooperative mobile robot and manipulator system,2023-03-24,2023,journal article,International Journal of Intelligent Robotics and Applications,23665971; 2366598x,Springer Science and Business Media LLC,,Manman Yang; Erfu Yang,"<jats:title>Abstract</jats:title><jats:p>The stoppage of a mobile platform is generally scheduled to feed parts for machines on production lines, such as fenced industrial robotic manipulators. A non-stop mobile robotic part feeding system can contribute to production efficiency and flexibility but contains several challenging tasks. For example, the industrial robotic manipulator needs to perceive the positions of the mobile robot accurately and robustly before grasping the supplies when the mobile robot moves around. Thus, based on the relative distance between the two robots, an interaction mode of the integrated robotic system consisting of a fixed robotic manipulator and a mobile robot is developed for robotic interaction. In order to accurately and robustly perceive the positions of a mobile robot, two different positioning approaches for the robotic manipulator positioning mobile robot in an indoor environment are utilised. One approach is ultrasonic sensors fused with inertia measurement units (IMU) by extended Kalman filter (EKF). Furthermore, an outlier rejection mechanism is implemented to escape outliers from ultrasonic measurement. Another positioning approach is achieved by detecting an ArUco marker with visual sensor. Lastly, a positioning switching strategy according to the visual sensor state allows the robotic manipulator to reposition the mobile robot seamlessly. According to the static experiments, EKF-based positioning approach fusing IMU with ultrasonic sensor can export high-accuracy (the root mean square error is 0.04 m) and high-precision (the standard deviation is 0.0033 m) in positioning while keeping a high update frequency of 181.9 HZ in static positioning. Evaluations through dynamic experiments demonstrate that the proposed positioning system can suppress the positioning drifts over time in comparison with wheel encoder-based positioning method. The two-stage repositioning strategy can support the robotic manipulator to identify the positions of the mobile robot robustly, even in the case when the visual sensor is occluded.</jats:p>",7,2,275,290,Mobile robot; Inertial measurement unit; Extended Kalman filter; Robot; Mobile manipulator; Artificial intelligence; Computer science; Computer vision; Engineering; Real-time computing; Kalman filter,,,,University of Strathclyde,https://link.springer.com/content/pdf/10.1007/s41315-023-00276-0.pdf https://doi.org/10.1007/s41315-023-00276-0,http://dx.doi.org/10.1007/s41315-023-00276-0,,10.1007/s41315-023-00276-0,,,0,000-940-865-319-935; 001-360-928-241-406; 003-086-277-037-121; 003-164-727-900-596; 005-310-089-320-380; 009-176-798-477-680; 010-568-233-244-108; 010-789-697-450-515; 012-885-764-079-828; 015-417-609-922-215; 016-397-597-572-350; 019-594-650-520-499; 031-858-835-547-293; 037-217-375-088-417; 040-041-171-157-486; 041-601-720-630-715; 042-087-595-941-393; 048-929-432-459-187; 051-456-288-660-847; 051-537-926-980-189; 053-012-804-131-462; 055-179-633-388-841; 059-194-705-001-839; 059-710-021-134-559; 060-607-760-589-565; 061-637-392-210-18X; 065-775-243-288-910; 070-190-466-392-720; 071-007-737-126-060; 074-324-981-337-749; 074-779-523-533-014; 076-039-142-893-576; 077-400-889-867-043; 084-012-215-914-519; 094-808-250-069-740; 098-275-158-792-377; 103-628-725-173-821; 109-153-341-414-333; 113-774-383-120-161; 115-179-430-205-341; 120-924-509-703-755; 141-620-853-723-631; 161-510-637-764-891; 170-527-378-819-823; 184-446-291-593-247; 184-512-764-577-97X,2,true,cc-by,hybrid
030-672-855-411-885,Effective 3-D shape discrimination survives retinal blur.,,2010,journal article,"Attention, perception & psychophysics",1943393x; 19433921,Springer Science and Business Media LLC,United States,J. Farley Norman; Amanda M. Beers; Jessica Holmin; Andalexandria M. Boswell,"A single experiment evaluated observers' ability to visually discriminate 3-D object shape, where the 3-D structure was defined by motion, texture, Lambertian shading, and occluding contours. The observers' vision was degraded to varying degrees by blurring the experimental stimuli, using 2.0-, 2.5-, and 3.0-diopter convex lenses. The lenses reduced the observers' acuity from -0.091 LogMAR (in the no-blur conditions) to 0.924 LogMAR (in the conditions with the most blur; 3.0-diopter lenses). This visual degradation, although producing severe reductions in visual acuity, had only small (but significant) effects on the observers' ability to discriminate 3-D shape. The observers' shape discrimination performance was facilitated by the objects' rotation in depth, regardless of the presence or absence of blur. Our results indicate that accurate global shape discrimination survives a considerable amount of retinal blur.",72,6,1569,1575,Psychophysics; Artificial intelligence; Psychology; Three-dimensional space; Dioptre; Visual acuity; Perceptual Distortion; Computer vision; Communication; Motion perception; Depth perception; Rotation (mathematics),,"Adult; Attention; Contrast Sensitivity; Depth Perception; Discrimination, Psychological; Female; Humans; Male; Motion Perception; Pattern Recognition, Visual; Perceptual Distortion; Perceptual Masking; Psychophysics; Visual Acuity; Young Adult",,,https://wku.edu/psychological-sciences/labs/vision_and_haptics/downloads/app_2010_norman.pdf https://europepmc.org/article/MED/20675801 https://link.springer.com/article/10.3758/APP.72.6.1569 https://pubmed.ncbi.nlm.nih.gov/20675801/ https://link.springer.com/content/pdf/10.3758%2FAPP.72.6.1569.pdf https://www.ncbi.nlm.nih.gov/pubmed/20675801 https://www.questia.com/library/journal/1P3-2118686171/effective-3-d-shape-discrimination-survives-retinal,http://dx.doi.org/10.3758/app.72.6.1569,20675801,10.3758/app.72.6.1569,2051693436,,0,001-187-202-225-726; 005-267-934-727-591; 008-930-517-663-715; 009-312-498-088-417; 010-820-916-838-53X; 011-281-366-598-469; 011-794-691-105-596; 012-169-752-310-765; 016-106-807-352-645; 016-446-523-653-893; 016-619-057-100-898; 017-922-626-201-583; 020-316-578-013-139; 023-868-017-679-314; 024-103-501-336-50X; 025-744-184-003-133; 027-974-129-788-376; 032-706-904-383-987; 033-951-283-712-439; 034-006-259-819-746; 035-302-682-286-53X; 037-407-702-820-052; 038-791-609-905-823; 041-202-813-542-222; 042-589-118-688-655; 043-976-781-903-418; 045-575-130-074-017; 045-841-151-550-288; 047-763-877-447-468; 048-847-407-631-687; 052-649-568-416-926; 056-588-768-386-927; 058-074-142-866-309; 060-583-490-208-174; 067-161-445-138-730; 067-646-869-814-97X; 067-719-246-453-156; 069-321-419-009-233; 070-560-964-936-239; 070-589-878-860-804; 073-939-444-258-951; 075-479-005-417-98X; 092-299-763-830-587; 098-787-243-958-780; 099-621-927-232-426; 099-844-079-120-736; 100-354-298-375-44X; 102-166-716-112-859; 104-541-196-925-817; 115-551-561-678-163; 116-901-368-488-273; 133-769-195-367-967; 135-626-099-504-962; 136-099-693-870-126; 138-864-654-742-242; 141-161-713-336-920; 143-517-746-330-098; 159-194-489-256-073; 174-848-941-567-235; 185-333-747-621-492,12,true,,bronze
030-710-512-990-541,Survey on Blind Assist System using ML and Image Processing,2024-05-22,2024,journal article,"International Journal of Advanced Research in Science, Communication and Technology",25819429,Naksh Solutions,,null Prof. Deepu G; null Adarsh N M; null Bhuvan R; null Darshan A; null Darshan R,"<jats:p>This paper presents a Blind Assist System (BAS) leveraging Machine Learning (ML) and Image Processing (IP) techniques to enhance the autonomy and safety of visually impaired individuals. The system utilizes a convolutional neural network (CNN) to process real-time image inputs from a wearable camera device. Through ML classification, it identifies objects, obstacles, and environmental cues. The IP module further refines data, providing depth perception and spatial awareness. Leveraging ML's adaptability, the system continuously learns and improves its recognition accuracy. Integration with auditory feedback facilitates intuitive interaction, conveying vital information to users. In evaluations, the BAS demonstrates promising results in aiding navigation and increasing users' independence. The fusion of ML and IP offers a robust solution for empowering the visually impaired in navigating complex environments</jats:p>",,,797,799,Computer science; Computer vision; Image processing; Artificial intelligence; Image (mathematics),,,,,,http://dx.doi.org/10.48175/ijarsct-18390,,10.48175/ijarsct-18390,,,0,001-220-627-881-288; 024-095-146-218-164; 087-105-444-728-025; 139-149-263-291-329,0,true,,bronze
030-846-671-006-321,"A Robotic Guide for Blind People. Part 1. A Multi-National Survey of the Attitudes, Requirements and Preferences of Potential End-Users",,2010,journal article,Applied Bionics and Biomechanics,11762322; 17542103,Hindawi Limited,Egypt,Marion A. Hersh; Michael A. Johnson,"<jats:p>This paper reports the results of a multi-national survey in several different countries on the attitudes, requirements and preferences of blind and visually impaired people for a robotic guide. The survey is introduced by a brief overview of existing work on robotic travel aids and other mobile robotic devices. The questionnaire comprises three sections on personal information about respondents, existing use of mobility and navigation devices and the functions and other features of a robotic guide. The survey found that respondents were very interested in the robotic guide having a number of different functions and being useful in a wide range of circumstances. They considered the robot's appearance to be very important but did not like any of the proposed designs. From their comments, respondents wanted the robot to be discreet and inconspicuous, small, light weight and portable, easy to use, robust to damage, require minimal maintenance, have a long life and a long battery life.</jats:p>",7,4,277,288,Robot; Human–computer interaction; Work (physics); Mobile robot; Questionnaire; Visually impaired; Engineering; Computer science; Simulation; Artificial intelligence; Mechanical engineering; Social science; Sociology,,,,,https://downloads.hindawi.com/journals/abb/2010/252609.pdf https://doi.org/10.1155/2010/252609,http://dx.doi.org/10.1155/2010/252609,,10.1155/2010/252609,,,0,,12,true,cc-by,gold
031-355-067-335-217,MSD-NAS: multi-scale dense neural architecture search for real-time pedestrian lane detection,2023-08-11,2023,journal article,Applied Intelligence,0924669x; 15737497,Springer Science and Business Media LLC,Netherlands,Sui Paul Ang; Son Lam Phung; Soan T. M. Duong; Abdesselam Bouzerdoum,"<jats:title>Abstract</jats:title><jats:p>Accurate detection of pedestrian lanes is a crucial criterion for vision-impaired people to navigate freely and safely. The current deep learning methods have achieved reasonable accuracy at this task. However, they lack practicality for real-time pedestrian lane detection due to non-optimal accuracy, speed, and model size trade-off. Hence, an optimized deep neural network (DNN) for pedestrian lane detection is required. Designing a DNN from scratch is a laborious task that requires significant experience and time. This paper proposes a novel neural architecture search (NAS) algorithm, named MSD-NAS, to automate this laborious task. The proposed method designs an optimized deep network with multi-scale input branches, allowing the derived network to utilize local and global contexts for predictions. The search is also performed in a large and generic space that includes many existing hand-designed network architectures as candidates. To further boost performance, we propose a Short-term Visual Memory mechanism to improve information facilitation within the derived networks. Evaluated on the PLVP3 dataset of 10,000 images, the DNN designed by MSD-NAS achieves state-of-the-art accuracy (0.9781) and mIoU (0.9542), while being 20.16 times faster and 2.56 times smaller than the current best deep learning model.</jats:p>",53,21,25787,25801,Computer science; Pedestrian detection; Task (project management); Artificial neural network; Artificial intelligence; Deep learning; Pedestrian; Scratch; Scale (ratio); Key (lock); Machine learning; Real-time computing; Pattern recognition (psychology); Physics; Computer security; Management; Quantum mechanics; Transport engineering; Engineering; Economics; Operating system,,,,The University of Wollongong,https://link.springer.com/content/pdf/10.1007/s10489-023-04682-6.pdf https://doi.org/10.1007/s10489-023-04682-6,http://dx.doi.org/10.1007/s10489-023-04682-6,,10.1007/s10489-023-04682-6,,,0,002-649-617-859-886; 003-813-258-014-256; 003-819-868-526-694; 004-634-886-716-286; 015-705-835-654-083; 020-233-013-143-936; 024-088-945-875-704; 026-005-792-695-329; 027-893-571-523-130; 030-014-279-775-853; 030-540-246-658-722; 037-643-649-976-393; 038-702-890-586-942; 041-058-139-703-974; 041-314-165-476-904; 049-551-659-753-473; 065-924-516-942-647; 073-476-694-087-771; 075-196-660-819-74X; 076-853-538-573-049; 081-618-070-928-596; 099-582-031-353-605; 104-258-969-696-371; 116-237-864-366-387; 124-326-712-632-53X; 144-738-058-499-258; 165-770-407-696-973; 167-131-435-704-717,1,true,cc-by,hybrid
031-523-631-032-525,Intelligent Mobility System for Improving the Blind Pedestrian Independent Behavior in Unknown Outdoor Environments,2022-09-12,2022,journal article,Human Behavior and Emerging Technologies,25781863,Hindawi Limited,,Armida González-Lorence; Ángel C. Navarrete-Fernández; Rosana Ayala-Landeros; Juan E. Soto-Osornio; José G. Ayala-Landeros,"<jats:p>The second cause of disability in Mexico is visual impairment; 26% population is blind. Even though blind people maintain a lower quality’s life than sighted people, there is not enough social interest to develop comprehensive solutions that improve it. Although many voice-activated emerging technologies use artificial intelligence to get human-machine communication through intelligent virtual assistants, such as Alexa, Siri, Google Home, and Cortana, among others, in reality, there have not been developed specific tools just for blind people that help them to improve their independent behavior because they always depend on other people in doing their daily tasks, more even when they need to move around unknown environments for them. This document reports the development of an autonomous mobility system to be operated by blind people. By audio reconstruction, it is possible to dictate, to the blind pedestrian, in real-time, the presence of traffic lights, crosswalks, and information about their current location. This system employs real-time computer vision tools, artificial intelligence, audio playback systems, and location systems, and it improves the independent behavior of blind people because they could move through unknown environments without the assistance of any sighted person, giving them greater independence and consequently increasing their life’s quality.</jats:p>",2022,,1,18,Pedestrian; Computer science; Independence (probability theory); Population; Human–computer interaction; Independent living; Quality (philosophy); Artificial intelligence; Engineering; Transport engineering; Medicine; Gerontology; Philosophy; Statistics; Mathematics; Environmental health; Epistemology,,,,Tecnológico Nacional de México,https://downloads.hindawi.com/journals/hbet/2022/4943457.pdf https://doi.org/10.1155/2022/4943457,http://dx.doi.org/10.1155/2022/4943457,,10.1155/2022/4943457,,,0,012-443-289-150-18X; 024-727-157-978-129; 031-374-595-067-408; 031-766-621-087-588; 044-439-044-658-160; 083-444-228-320-480; 087-943-654-324-953; 095-830-617-026-366; 101-688-820-641-330; 108-456-749-455-544; 116-942-038-911-535; 158-478-300-316-123,1,true,cc-by,gold
031-643-110-129-889,Representation of locomotor space by the blind,,1987,journal article,Perception & psychophysics,00315117; 15325962,Springer Science and Business Media LLC,United States,Claude Veraart; Mc. Wanetdefalque,"Representation of locomotor space by early- and late-blind subjects and by blindfolded sighted subjects was studied within a perimeter where the direction and distance of landmarks had to be located. Subjects were guided along routes to be explored, both with and without the use of an ultrasonic echolocating prosthesis that enabled object localization. Without the prosthesis, early-blind subjects’ performance was worse than that of visually experienced subjects, both in direction and in distance assessments. With the help of the prosthesis, early- and late-blind subjects’ performance improved, especially in distance assessments; late-blinds’ performance remained better than that of early-blinds. These results suggest that early-blinds’ spatial representation would be the most impaired on routes requiring the mastering of euclidean concepts.",42,2,132,139,Psychology; Representation (systemics); Space (commercial competition); Spatial representation; Visual experience; Communication; Perimeter; Physical medicine and rehabilitation,,"Adult; Blindness/psychology; Humans; Locomotion; Middle Aged; Orientation; Sensory Aids; Set, Psychology",,,https://dial.uclouvain.be/pr/boreal/object/boreal:54085 https://link.springer.com/content/pdf/10.3758%2FBF03210501.pdf https://www.ncbi.nlm.nih.gov/pubmed/3627933 https://rd.springer.com/article/10.3758/BF03210501 https://link.springer.com/article/10.3758/BF03210501 https://paperity.org/p/21212945/representation-of-locomotor-space-by-the-blind,http://dx.doi.org/10.3758/bf03210501,3627933,10.3758/bf03210501,2040321387,,0,000-009-769-121-794; 006-813-235-552-771; 014-420-329-953-341; 018-945-737-932-328; 027-041-318-480-587; 028-076-277-298-063; 028-849-969-520-798; 035-423-604-175-86X; 037-389-881-152-996; 041-405-394-062-207; 044-393-938-213-848; 044-494-480-311-659; 045-669-898-745-284; 048-177-902-521-397; 048-492-566-170-152; 049-904-804-407-819; 056-508-377-103-499; 059-834-781-644-29X; 063-367-551-819-079; 067-892-061-809-776; 070-935-505-662-732; 074-086-205-822-708; 086-288-728-578-303; 088-475-092-198-781; 111-915-513-334-111; 116-220-833-791-435; 119-128-798-252-90X; 135-646-014-794-306; 165-329-143-165-257; 168-273-041-841-030; 170-362-816-166-701; 192-136-128-163-163,35,true,,bronze
031-768-944-008-214,The effect of non-visual preview upon the walking speed of visually impaired people,,1986,journal article,Ergonomics,00140139; 13665847,Informa UK Limited,United Kingdom,D D Clark-Carter; A.D. Heyes; C. I. Howarth,"Abstract Blind pedestrians lack preview of their environment and as a consequence they frequently adopt a slow walking speed. Electronic mobility aids have been designed to increase preview, and thus thinking time, over and above that provided by a long cane, but their ranges have been chosen on somewhat arbitrary grounds. Previous work on the effects of varying preview upon mobility have used artificial indoor environments. In addition, they have used either sight or an aid with a complicated display, the Swedish Laser Cane, as the medium through which to present the preview. The present experiment provided subjects with varying amounts of preview via an electronic mobility aid, the Sonic Pathfinder, as they walked a simple outdoor route. Their walking speed was recorded and found to increase significantly with increased preview. The range of the Sonic Pathfinder has now been increased on the strength of this experiment.",29,12,1575,1581,Engineering; Sight; Work (physics); Pathfinder; Mobility aid; Sensory Aid; Poison control; Visually impaired; Simulation; Preferred walking speed,,Adult; Blindness/rehabilitation; Computers; Humans; Locomotion; Microcomputers; Middle Aged; Sensory Aids,,,https://pubmed.ncbi.nlm.nih.gov/3816749/ https://www.tandfonline.com/doi/abs/10.1080/00140138608967270 https://europepmc.org/article/MED/3816749 https://www.ncbi.nlm.nih.gov/pubmed/3816749 https://www.safetylit.org/citations/index.php?fuseaction=citations.viewdetails&citationIds[]=citjournalarticle_28618_6,http://dx.doi.org/10.1080/00140138608967270,3816749,10.1080/00140138608967270,1981372451,,1,024-369-065-483-244; 030-376-516-941-332; 049-869-871-125-95X; 094-876-518-116-363,43,false,,
031-854-757-653-77X,A Smart City Assistive Infrastructure for the Blind and Visually Impaired People: A Thin Client Concept,2018-12-01,2018,,,,,,Dmytro Zubov,"The World Health Organization pointed out that over 285 million people worldwide suffer from loss of vision and blindness, and that the number could drop drastically in just a few years. About 90 % of the blind and visually impaired (B&VI) live at a low income that means these people cannot buy the expensive assistive devices for the spatial cognition. In this work, the new concept of the smart city assistive infrastructure with distributed server-client architecture is presented for the B&VI using the inclusive smart assistive component that interacts with other subsystems of the smart city (smart buildings, smart mobility, smart energy, etc.) via IoT protocols such as MQTT. The main constituents of thin client are as follows: Raspberry Pi 3 B board with camera for the objects detection / recognition, ultrasonic sensor(s) HC-SR04 for the obstacles identification on the short-range distance up to 5 m, GPS module for the global navigation, iBeacon Bluetooth low energy proximity sensing software, MQTT IoT protocol for the mutual communication of clients, Python multithread application, Raspbian OS. The thin client hardware is of affordable price USD 70. The objects detection and recognition are implemented on the thin clients via the Histogram of Oriented Gradients with Euclidean distance classifier (HOG+EDC). The design of the recognition models, the file hosting of the training images and the knowledge base with the recognition rules are done on server(s). The modified Viola-Jones fast face detector with the combination of features “eye” and “nose” is proposed to speed up the image processing, but its detection rate is not 100 %. Hence, it can be applied only with the subsequent recognition using HOG+EDC.",9,4,25,37,Human–computer interaction; MQTT; Histogram of oriented gradients; Thin client; Smart city; iBeacon; Computer science; Python (programming language); Global Positioning System; Building automation,,,,,https://www.brain.edusoft.ro/index.php/brain/article/view/862 http://www.edusoft.ro/brain/index.php/brain/article/view/862/1001 https://www.brain.edusoft.ro/index.php/brain/article/download/862/1001 https://www.edusoft.ro/brain/index.php/brain/article/download/862/1001 https://www.edusoft.ro/brain/index.php/brain/article/view/862/1001,https://www.brain.edusoft.ro/index.php/brain/article/view/862,,,2906001280,,0,,3,false,,
031-881-912-891-799,A Survey of Vision Aids for the Blind,,2006,conference proceedings article,2006 6th World Congress on Intelligent Control and Automation,,IEEE,,Jihong Liu; Xiaoye Sun,"The development of effective user interfaces, appropriate sensors, and information processing techniques make it is possible to enable the blind to achieve additional perception of the environment. Since the beginning of the 1970's, the research of vision aids for the visually impaired people has been broadly extended. After introducing the traditional methods for guiding blind, two typical modes of mobility aid are presented in this paper. One called as ETA (short for Electronic Travel Aids), bases on the other natural senses of the blind, such as hearing, touch, smell, feeling and etc, focusing on Meijer's vOICe system which is based on the blind's sensitive auditory senses and ENVS which is by means of haptic feedback. The other technique is artificial vision, using surgical methods of implanting visual prosthesis in the blind's healthy retina, cortex, or optic nerve. The prosthesis generates electrical impulse and evokes the perception of points of light in the patients' visual cortex. The first type is non-wounded for the blind while the second type is wounded for the blind. Besides these, comparisons between the above two techniques and the related researches have also been indicated in this paper.",1,,4312,4316,Haptic technology; Phosphene; Artificial intelligence; Perception; Retina; Visual prosthesis; Visual cortex; Optic nerve; Vision aid; Computer vision; Computer science; Information processing; User interface,,,,,http://ieeexplore.ieee.org/document/1713189/ http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.ieee-000001713189,http://dx.doi.org/10.1109/wcica.2006.1713189,,10.1109/wcica.2006.1713189,2136629934,,0,002-491-826-057-153; 010-104-987-820-313; 013-350-108-535-78X; 013-847-555-426-159; 015-332-638-696-162; 019-004-727-294-846; 026-203-424-274-042; 032-786-641-709-377; 034-196-646-262-503; 039-232-446-202-853; 055-132-919-152-975; 063-048-719-705-828; 078-681-532-069-655; 084-375-521-719-722; 091-337-913-895-465; 096-453-528-988-403; 097-450-938-016-597; 114-076-647-919-961; 115-418-484-287-800; 122-625-345-463-697; 123-501-344-440-241; 129-105-758-487-473; 144-057-603-881-791; 148-187-541-317-212; 185-783-077-691-122; 192-108-002-700-287,15,false,,
032-264-664-120-313,An Accessible Agent for Blind's Self-Localization and Navigational Assistance,2023-01-01,2023,preprint,,,Elsevier BV,,Murtaza Hanif; Tauqir Ahmad; Muhammad Aslam; Muhammad Waseem wasi,,,,,,Computer science; Human–computer interaction; Artificial intelligence,,,,,,http://dx.doi.org/10.2139/ssrn.4415344,,10.2139/ssrn.4415344,,,0,,0,false,,
032-304-772-464-854,A Robotic Assistant for Disabled Chess Players in Competitive Games,2023-11-01,2023,journal article,International Journal of Social Robotics,18754791; 18754805,Springer Science and Business Media LLC,Germany,Luca Pozzi; Silvia Guerini; Stefano Arrigoni; Alessandra Pedrocchi; Marta Gandolla,"<jats:title>Abstract</jats:title><jats:p>Access to regular sports competitions is often precluded for disabled people. Chess, which has been recognized as a sport by the International Olympic Committee in 1999, is a rare exception. Nevertheless, to compete in official tournaments, people suffering from a high level of motor impairment must rely on the assistance of a person to move their pieces on the chessboard, under their indications. This can result in a reduction of the feeling of independence and self-esteem. In this work, a service robot is employed as an assistant for competitive chess players, moving pieces on a standard chessboard for competitions, and adhering to the rules of the international chess federation (e.g. not relying on a custom sensorized chess-set). The robot is controlled through an intuitive graphical user interface. The user interface can be navigated with easy-to-use devices, such as a mouse, a touchpad, or a commodity joystick for motion-impaired people (Ottobock calibratable Mini joystick). An effective framework for the opponent’s move identification from RGB-D images is proposed and used to keep track of the live game situation. The application is implemented in ROS on a PAL Robotics TIAGo robot, a service robot with a 7 degrees-of-freedom arm, an extensible torso, and a re-orientable RGB-D camera. The robustness of the application is tested by reproducing six famous chess games several times on a standard wooden competition chessboard, making TIAGo play on behalf of the player with white or black pieces, alternatively. The application is properly working without the need of operator intervention in the <jats:inline-formula><jats:alternatives><jats:tex-math>$$91.6\%$$</jats:tex-math><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"">;                   <mml:mrow>;                     <mml:mn>91.6</mml:mn>;                     <mml:mo>%</mml:mo>;                   </mml:mrow>;                 </mml:math></jats:alternatives></jats:inline-formula> of the performed moves. The proposed approach successfully opens the door to independent competitive chess playing for motor disabled people in official tournaments.; </jats:p>",16,1,173,183,Joystick; Computer science; Artificial intelligence; Robotics; Human–computer interaction; Robot; Interface (matter); Computer vision; Simulation; Bubble; Maximum bubble pressure method; Parallel computing,,,,Politecnico di Milano,https://link.springer.com/content/pdf/10.1007/s12369-023-01069-y.pdf https://doi.org/10.1007/s12369-023-01069-y,http://dx.doi.org/10.1007/s12369-023-01069-y,,10.1007/s12369-023-01069-y,,,0,004-807-794-236-309; 005-640-899-589-033; 010-037-102-410-599; 010-099-297-277-858; 016-562-551-828-943; 036-438-698-072-754; 040-257-413-379-317; 046-070-490-207-12X; 047-149-658-907-09X; 060-708-028-448-991; 062-770-495-703-795; 066-594-304-839-143; 077-639-617-452-690; 104-235-515-261-718; 108-798-403-484-84X; 111-966-570-313-237; 114-054-345-865-112; 125-657-648-517-862; 136-954-485-001-932,0,true,cc-by,hybrid
032-324-729-101-562,Arabic glove-talk (AGT): A communication aid for vocally impaired,,1998,journal article,Pattern Analysis and Applications,14337541; 1433755x,Springer Science and Business Media LLC,Germany,A. S. Tolba; A. N. Abu-Rezq,,1,4,218,230,Artificial intelligence; Gesture recognition; Test set; Natural language processing; Gesture; Vocabulary; Words per minute; Real-time operating system; Speech recognition; Computer science; Perceptron; Classifier (UML),,,,,https://doi.org/10.1007/BF01234769 https://dblp.uni-trier.de/db/journals/paa/paa1.html#TolbaA98 https://link.springer.com/article/10.1007/BF01234769,http://dx.doi.org/10.1007/bf01234769,,10.1007/bf01234769,2053492115,,0,001-997-549-046-983; 008-966-681-572-481; 010-702-472-697-491; 013-170-230-065-795; 030-657-837-237-70X; 036-353-853-607-710; 041-013-612-851-886; 050-968-987-433-072; 070-614-734-367-473; 072-560-908-506-00X; 079-513-442-135-819; 108-256-033-208-170; 122-512-653-536-260; 141-620-079-561-206; 143-609-128-203-348; 155-992-277-294-765; 178-302-032-960-735,10,false,,
032-661-597-680-28X,Design and Construction of Electronic Aid for Visually Impaired People,,2018,journal article,IEEE Transactions on Human-Machine Systems,21682291; 21682305,Institute of Electrical and Electronics Engineers (IEEE),United States,Kailas Patil; Qaidjohar Jawadwala; Felix Che Shu,"The NavGuide is a novel electronic device to assist visually impaired people with obstacle free path-finding. The highlight of the NavGuide system is that it provides simplified information on the surrounding environment and deduces priority information without causing information overload. The priority information is provided to the user through vibration and audio feedback mechanisms. The proof-of-concept device consists of a low power embedded system with ultrasonic sensors, vibration motors, and a battery. To test the effectiveness of the NavGuide system in daily-life mobility of visually impaired people, we performed an evaluation using 70 blind people of the “school & home for the blind.” All evaluations were performed in controlled, real-world test environments with the NavGuide and traditional white cane. The evaluation results show that NavGuide is a useful aid in the detection of obstacles, wet floors, and ascending staircases and its performance is better than that of a white cane.",48,2,172,182,Human–computer interaction; Information overload; Obstacle; Headphones; White cane; Visually impaired; Audio feedback; Computer science,,,,,http://ieeexplore.ieee.org/document/8291597 https://ieeexplore.ieee.org/abstract/document/8291597/ https://dblp.uni-trier.de/db/journals/thms/thms48.html#PatilJS18,http://dx.doi.org/10.1109/thms.2018.2799588,,10.1109/thms.2018.2799588,2792444113,,0,001-030-959-147-058; 002-990-910-547-637; 003-983-620-977-111; 005-303-872-644-71X; 006-917-922-806-186; 008-361-673-204-978; 010-506-038-184-791; 013-056-435-015-274; 013-847-555-426-159; 020-581-037-678-426; 022-697-627-694-941; 023-290-902-886-991; 024-964-397-377-997; 027-720-558-803-335; 030-127-369-356-741; 037-491-194-003-965; 039-970-463-405-198; 047-186-625-334-857; 058-443-047-291-990; 064-309-126-323-022; 066-037-919-140-345; 066-526-177-885-125; 066-622-591-843-254; 067-428-787-994-238; 072-157-487-137-540; 077-124-590-119-309; 096-803-870-115-728; 099-072-913-456-358; 106-778-908-131-221; 109-525-288-959-67X; 112-365-549-669-557; 144-452-403-048-99X; 191-862-890-760-752,88,false,,
032-710-775-514-283,Quadrotor Autonomous Approaching and Landing on a Vessel Deck,2017-12-26,2017,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Liyang Wang; Xiaoli Bai,,92,1,125,143,Monocular vision; Marine engineering; Deck; Sea state; Two stages; Computer science; Robustness (computer science),,,,,https://dl.acm.org/doi/10.1007/s10846-017-0757-5 https://dialnet.unirioja.es/servlet/articulo?codigo=6660498 https://www.researchwithrutgers.com/en/publications/quadrotor-autonomous-approaching-and-landing-on-a-vessel-deck https://link.springer.com/article/10.1007/s10846-017-0757-5 https://dblp.uni-trier.de/db/journals/jirs/jirs92.html#WangB18,http://dx.doi.org/10.1007/s10846-017-0757-5,,10.1007/s10846-017-0757-5,2776254938,,0,000-962-323-956-005; 001-616-494-472-966; 025-192-618-273-833; 026-777-712-409-557; 026-995-598-276-601; 029-556-951-774-068; 035-065-202-206-793; 039-610-166-736-311; 040-553-422-081-869; 043-472-319-942-346; 046-472-719-473-728; 052-225-250-328-388; 075-359-662-961-890; 076-279-156-776-517; 087-433-624-807-196; 091-082-569-511-562; 098-418-797-798-017; 100-224-199-218-558; 101-281-217-757-246; 109-465-246-894-618; 128-130-913-855-423; 132-017-908-348-048; 133-313-307-653-883; 154-804-735-848-263; 170-542-994-320-079; 176-082-400-512-895; 199-860-361-832-513,36,false,,
032-786-641-709-377,Role of object identification in sonification system for visually impaired,,,conference proceedings article,TENCON 2003. Conference on Convergent Technologies for Asia-Pacific Region,,Allied Publishers Pvt. Ltd,,R. Nagarajan; Sazali Yaacob; G. Sainarayanan,"The role of object identification in a sonification system for navigation assistance to the visually impaired (NAVI) is discussed. The developed system includes a single board processing system (SBPS), vision sensor mounted on headgear and stereo earphones. The vision sensor captures the vision information in front of the blind user. The captured image is processed to identify the object in the image. Object identification is achieved by a real time image processing methodology using fuzzy algorithms. The processed image is mapped onto stereo acoustic patterns and transferred to the stereo earphones in the system. Blind individuals were trained with NAVI system and tested for obstacle identification. Suggestions from the blind volunteers regarding the pleasantness and discrimination of the sound pattern were incorporated in the prototype. With object identification, the discrimination of object and background by sound is found to be easier compared to sound produced from the unprocessed image.",2,,735,739,Engineering; Artificial intelligence; Visual perception; Fuzzy control system; Computer vision; Fuzzy logic; Object (computer science); Cognitive neuroscience of visual object recognition; Identification (information); Sonification; Image processing,,,,,http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1273276,http://dx.doi.org/10.1109/tencon.2003.1273276,,10.1109/tencon.2003.1273276,2123013445,,1,010-025-512-242-323; 013-847-555-426-159; 033-536-677-191-914; 044-080-619-911-182; 045-673-862-191-277; 118-643-017-360-933; 129-400-209-376-57X,21,false,,
032-827-793-108-755,ROBIO - Skin stroking haptic feedback glove for assisting blinds in navigation,,2017,conference proceedings article,2017 IEEE International Conference on Robotics and Biomimetics (ROBIO),,IEEE,,Kashan Aqeel; Urooj Naveed; Faarah Fatima; Farah Haq; Muhammad Arshad; Ammar Abbas; Muhammad Nabeel; Muhammad Khurram,"Visually impaired or blind people are unable to assess or analyze obstacles and tasks they wishes to perform. Lack of visual feedback has kept their lives restricted. Therefore, numerous techniques are available for sensing hurdles and irregularities in their paths. The real challenge is to develop a fast and spontaneous actuating technique. From manual white cane to Electronic Traveling Aid (ETA) and haptic feedback assistive bracelet, each has its own importance in the field. In this paper, two techniques of haptic feedback are explored namely, Vibrotactile and Skin Stroking. Skin stroking is a novel technique to be incorporated in haptic feedback systems, it is a painless and comfortable form of a tactile display. Requirements of this system is not limited to guidance and navigation purposes only that is why this device was experimented in maneuvering guidance, rotational and translational motion of hands. Vibrotactile and Skin stroking mechanism were connected with an android application via bluetooth which controls the motion of subjects throughout the experiments. The results gathered from experiments signifies that directional differentiating cues were better while using skin stroking haptic feedback mechanism and more can be expected from this field in the future.",,,177,182,Motion (physics); Human–computer interaction; Haptic technology; Tactile display; Translational motion; White cane; Android application; Visually impaired; Computer science; Bluetooth,,,,,https://dblp.uni-trier.de/db/conf/robio/robio2017.html#AqeelNFHAANK17,http://dx.doi.org/10.1109/robio.2017.8324414,,10.1109/robio.2017.8324414,2791926865,,0,000-319-559-523-254; 001-627-496-270-093; 014-329-213-294-121; 019-925-883-027-48X; 021-417-679-849-671; 030-440-500-148-114; 043-780-576-473-688; 051-126-094-178-401; 052-184-321-673-384; 059-450-510-502-422; 065-386-584-236-327; 079-254-506-729-947; 090-875-229-117-677; 121-327-768-465-334; 130-552-169-471-000,0,false,,
032-962-519-122-863,Redefining robot based technologies for elderly people assistance: a survey,,2016,journal article,"Journal of Robotics, Networking and Artificial Life",23526386,ALife Robotics Corporation Ltd.,,Luigi Pagliarini; Henrik Hautop Lund,"We analyse the state of the art of hi-tech and robot based technologies in terms of Assistive Technology for all patients and, in particular, elderly people assistance and everyday activities aid. We focus on different aspects and characteristics of these tools, such as playfulness, invasiveness, learning-speed, efficiency, short and long-term effect, active vs. passive, etc. We do so by showing the most important existing examples, and by taking into account all the possible factors that might help researchers when thinking of developing appropriate technologies for elderly care, as well as, for their relative assistance personnel. Indeed, while in rehabilitation robotics, a major role is played by the human–machine interface (HMI) used to gather the patient’s intent from biological signals, and convert them into control signals for the robotic artefacts, surprisingly, decades of research have not yet declared what the optimal HMI is in this context [1]. Further, there is an urgent need to clarify how various technologies can be a goal or an approach for preventive, rehabilitative and assistive interaction. Therefore, we try to make a first step towards a redefinition of Robotics Assistive Technology.",3,1,28,32,Human–computer interaction; Interface (computing); Artificial intelligence; Rehabilitation robotics; Control (management); Context (language use); Elderly people; Robotics; Focus (computing); Computer science; Simulation; Robot,,,,,https://core.ac.uk/display/141514025 https://dblp.uni-trier.de/db/journals/jrnal/jrnal3.html#PagliariniL16 https://www.atlantis-press.com/php/paper-details.php?id=25856333 https://core.ac.uk/download/141514025.pdf,http://dx.doi.org/10.2991/jrnal.2016.3.1.7,,10.2991/jrnal.2016.3.1.7,2465188030,,0,008-804-105-000-272; 016-989-374-023-089; 037-491-194-003-965; 048-838-233-675-422; 070-226-177-777-781; 080-284-509-638-680; 142-145-178-030-546; 152-761-194-033-749; 189-292-516-992-250,1,true,cc-by-nc,gold
033-133-982-983-671,ICCE-TW - The Design Concept of Intelligent Guide Cane,,2019,conference proceedings article,2019 IEEE International Conference on Consumer Electronics - Taiwan (ICCE-TW),,IEEE,,Chi-You Wei; Bo-Yi Li; Hsin-Wen Wei; Wei-Tsong Lee,"Some people are congenial blindness or visually impaired while the others are caused by acquired factors. One reason that results in poor vision or blindness is age. As people get old, the eyesight of people is destined to deteriorate. Normally, people with poor vision can only live in a familiar environment to ensure their own safety. The mobility of people with impaired vision is restricted and they may live in inconvenient life. Therefore, in this study, we aim to provide a design concept of novel intelligent guide cane to enhance the mobility of visually impaired people. The proposed design is to add some functions to the original guide cane and connect the intelligent guide cane with hand-held device. Since the hand-held device that is widely used in today and has a considerable amount of computing, the proposed design can act as an auxiliary machine similar to guide dogs or better than guide dogs. The proposed design aims to provide the functions such as route planning, navigation, obstacle detection, and object recognition. With the proposed design, the mobility of visually impaired people can be greatly improved.",,,1,2,Human–computer interaction; Blindness; Impaired Vision; Poor vision; Computer science; Cognitive neuroscience of visual object recognition,,,,,https://doi.org/10.1109/ICCE-TW46550.2019.8991709 http://xplorestaging.ieee.org/ielx7/8966968/8991681/08991709.pdf?arnumber=8991709 http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=8991709 http://dblp.uni-trier.de/db/conf/icce-tw/icce-tw2019.html#WeiLWL19,http://dx.doi.org/10.1109/icce-tw46550.2019.8991709,,10.1109/icce-tw46550.2019.8991709,3005613076,,0,132-602-463-268-72X,1,false,,
033-216-575-654-532,Enabling Social Interaction: A Face Recognition System for Visually Impaired People using OpenCV,2024-08-02,2024,journal article,FUOYE Journal of Engineering and Technology,25790617; 25790625,African Journals Online (AJOL),,Adedayo A. Sobowale; T. A. Abdul-Hameed; Peace O. Sobowale; Bolaji V. Johnson,"<jats:p>Over the last few decades, there have been substantial developments in a variety of domains, including computer science, artificial intelligence, and machine learning, which has accelerated the evolution of intelligent systems. Examples include speech recognition system, face recognition. This research work developed a method to assist blind and visually impaired people in the aforementioned forms of social interactions in order to address all these deficiencies. A facial recognition system for visually impaired people is a technology designed to help individuals with vision impairment identify people through their facial features. This device can recognize faces of individuals by silently broadcasting their names over speakers using face recognition technology. The system uses a camera to capture an image of a person's face, which is then processed to extract key features such as the eyes, nose, and mouth. The extracted features were compared to a database of known faces to identify the person in the image. Blind and visually impaired individuals encounter significant challenges in identifying people during social interactions. Traditional methods like speech recognition might not be reliable in all situations, such as with silent group members. This social isolation can hinder their participation in professional and educational settings. This research proposes a novel face recognition system to address these challenges. The system utilizes a camera to capture a person's face. Key facial features are then extracted using the Open Computer Vision Library (OpenCV). These features are compared against a pre-enrolled database of known faces for identification. Upon successful recognition, the system discreetly announces the person's name through audio output. This system empowers visually impaired individuals to navigate social interactions more confidently. By providing a reliable method for facial recognition, the system promotes greater social inclusion and participation in various environments. The model shows excellent performance, consistently achieving high accuracy, peaking at 89.1% on the 69th epoch, and consistently maintaining high validation accuracy, reaching 91.2% in the 67th epoch, indicating its ability to function effectively.</jats:p>",9,2,203,212,,,,,,,http://dx.doi.org/10.4314/fuoyejet.v9i2.8,,10.4314/fuoyejet.v9i2.8,,,0,,0,false,,
033-490-815-544-255,A Deep Learning-Based Object Detection System for Blind People,2021-06-08,2021,book chapter,Lecture Notes in Networks and Systems,23673370; 23673389,Springer Singapore,,Kalam Swathi; Bandi Vamsi; Nakka Thirupathi Rao,"Visual impairment is one of the top disabilities among men and women across the world of all ages. Object detection is the primary task for them, and it can be implemented by deep learning techniques. Earlier implementation techniques involve in object detection with a strategy of single labeling. The proposed model uses classification techniques which reduce the recognize time of multi-objects with best time complexities and can help the visually impaired people in assisting the accurate navigation, in both indoor and outdoor circumstances. The proposed hybrid model is a combination of U-Net with base as residual network (ResNet) which improves accuracy in detection of objects in indoor and outdoor for visually impaired people.",,,223,231,Deep learning; Artificial intelligence; Object detection; Task (project management); Visual impairment; Residual neural network; Hybrid model; Visually impaired; Computer vision; Computer science,,,,,https://link.springer.com/chapter/10.1007/978-981-16-1773-7_18,http://dx.doi.org/10.1007/978-981-16-1773-7_18,,10.1007/978-981-16-1773-7_18,3166505645,,0,001-065-278-896-176; 005-252-618-071-904; 011-297-761-538-710; 017-334-081-311-633; 025-522-385-395-133; 026-431-038-380-626; 029-290-669-441-328; 037-182-418-585-695; 045-988-503-295-660; 048-395-635-842-481; 051-970-317-843-151; 064-792-519-477-340; 083-744-521-989-268; 098-483-601-734-013; 164-332-313-558-557,4,false,,
033-597-364-447-682,Vision4All — A Deep Learning Fashion Assistance Solution For Blinds,2022-05-27,2022,conference proceedings article,2022 5th International Conference on Artificial Intelligence and Big Data (ICAIBD),,IEEE,,Laila Khalid; Wei Gong,"Fashion is an industry that never appears to slow down, and it would be incredible if the blind could participate in this growing trend. By 2050, nearly 120 million individuals are expected to be vision-impaired. In this paper, the proposed Vision4All model assists users in identifying colors, clothing categories, textures, fabric, style, graphic, and text-based content on clothes. We enhanced FashionNet, a deep model that learns clothing characteristics by predicting garment qualities and categories together. To improve prediction accuracy, the ResNet34 architecture replaced the obsolete VGG16 design. A Fine-Grained multilabel classification model was trained by tackling the noisy data problem for attribute prediction. For identifying the range of graphical content printed on clothes, we use Pythia's modular re-implementation of the bottom-up, top-down approach. Our solution allows users to navigate through speech eliminating the requirement for users to rely on their vision. Vision4All is the first complete solution to align with Fashion assistance for the visually impaired.",,,,,Clothing; Computer science; Modular design; Deep learning; Artificial intelligence; Architecture; Human–computer interaction; Clothing industry; Style (visual arts); Machine learning; Visual arts; Art; Archaeology; History; Operating system,,,,,,http://dx.doi.org/10.1109/icaibd55127.2022.9820475,,10.1109/icaibd55127.2022.9820475,,,0,004-269-574-716-057; 004-306-253-497-690; 018-193-483-515-989; 019-827-712-091-154; 020-195-149-955-98X; 020-233-013-143-936; 021-643-768-856-975; 037-376-150-587-539; 064-287-678-182-518; 124-385-928-276-562; 127-768-322-146-779,1,false,,
033-667-793-251-363,AI-based level detection and optimisation of assistive robot maneuverability,2021-10-28,2021,book,,,,,Siti Fauziah Toha; Ahmad Syahrin Idris; Abdur Razzaq Abd Halim,"Assistive devices for blind and visually impaired people are one of the technologies that famous among the researchers. There are many devices has been developed by the researcher in order to aid the visually impaired people to move around. One of the challenges faces by the blind and visually impaired people is stair. Using traditional method, the present of the staircase cannot be detected in a safe distance. Rovision is one of the devices that has the capability to guide the user to move to desired place without hitting any obstacle or object. In mobile robot, sensors play important roles in guiding the robot by sending the data of the surrounding for the robot to execute the action. Camera and ultrasonic sensor are two sensors that use in the Rovision to navigate the robot in safe distance while detecting the staircase. Image processing is one of the best methods in detecting staircase. It has a capability to learn by its own using the training dataset. By training the dataset, the system be able to identify the staircase and the position inside the camera frame to help Rovision to maneuver and guide the user safely. Rovision also use ultrasonic sensors to avoid obstacle in surrounding in order to have clear and safe path for the visually impaired person. This AI-Based Level Detection and Optimisation of Assistive Robot Maneuverability book will be useful for postgraduate students as well as final year undergraduate students researching on robotics area especially using the latest python software with focus on artificial intelligence techniques.",,,,,Human–computer interaction; Frame (networking); Artificial intelligence; Mobile robot; Obstacle; Software; Robotics; Focus (computing); Computer science; Python (programming language); Robot,,,,,http://irep.iium.edu.my/93365/,http://irep.iium.edu.my/93365/,,,3214246856,,0,,0,false,,
033-857-565-750-272,Divergent stereo in autonomous navigation: from bees to robots,,1995,journal article,International Journal of Computer Vision,09205691; 15731405,Springer Science and Business Media LLC,Netherlands,José Santos-Victor; Giulio Sandini; Francesca Curotto; Stefano Garibaldi,,14,2,159,177,Motion (physics); Pattern recognition (psychology); Artificial intelligence; Optical flow; Mobile robot; Navigation system; Computer vision; Robotics; Computer science; Robot; Image processing,,,,,https://link.springer.com/article/10.1007%2FBF01418981 https://dx.doi.org/10.1007/BF01418981 https://doi.org/10.1007/BF01418981 http://dx.doi.org/10.1007/BF01418981 https://dblp.uni-trier.de/db/journals/ijcv/ijcv14.html#Santos-VictorSCG95,http://dx.doi.org/10.1007/bf01418981,,10.1007/bf01418981,2065206922,,0,006-513-407-206-329; 007-545-564-798-885; 020-637-948-609-616; 020-705-971-244-311; 021-976-651-490-733; 029-725-405-930-021; 033-928-519-356-675; 041-581-209-746-782; 047-523-142-144-615; 051-033-537-088-833; 052-519-409-193-168; 070-438-568-091-28X; 093-973-416-463-979; 094-820-238-226-506; 096-108-733-130-409; 109-939-134-166-300; 131-735-429-494-427; 145-032-090-481-201; 176-072-305-433-209; 196-138-616-232-177,156,false,,
033-882-659-494-091,A Vision-Based Automatic Landing Method for Fixed-Wing UAVs,2009-10-23,2009,journal article,Journal of Intelligent and Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Sungsik Huh; David Hyunchul Shim,,57,1,217,231,Visual servoing; Engineering; Position (vector); Artificial intelligence; Terminal (electronics); Airbag; Airplane; Computer vision; Moment (mathematics); Machine vision; Global Positioning System,,,,,https://rd.springer.com/chapter/10.1007/978-90-481-8764-5_11 https://dl.acm.org/doi/10.1007/s10846-009-9382-2 https://doi.org/10.1007/s10846-009-9382-2 https://link.springer.com/article/10.1007/s10846-009-9382-2 https://dialnet.unirioja.es/servlet/articulo?codigo=3115885 https://link.springer.com/article/10.1007/s10846-009-9382-2?view=classic https://dblp.uni-trier.de/db/journals/jirs/jirs57.html#HuhS10 https://dl.acm.org/citation.cfm?id=1666352 https://koasas.kaist.ac.kr/handle/10203/158496,http://dx.doi.org/10.1007/s10846-009-9382-2,,10.1007/s10846-009-9382-2,1990613079,,2,012-609-331-097-501; 012-657-537-842-77X; 014-275-388-582-714; 021-273-133-036-257; 057-760-674-638-431; 077-038-147-021-306; 092-978-697-053-816; 099-230-453-362-975; 109-643-390-237-566; 148-842-678-129-894,53,false,,
033-947-592-266-127,HAPTICS - Evaluation of rotational and directional vibration patterns on a tactile belt for guiding visually impaired people,,2014,conference proceedings article,2014 IEEE Haptics Symposium (HAPTICS),,IEEE,,Akansel Cosgun; E. Akin Sisbot; Henrik I. Christensen,"We present the design of a vibro tactile belt and an evaluation study of intuitive vibration patterns for providing navigation assistance to blind people. Encoding directions with haptic interfaces is a common practice for outdoor navigation assistance but it is insufficient in cluttered indoor environments where fine maneuvers are needed. We consider rotational motions in addition to directional in our application. In a usability study with 15 subjects, we evaluate the recognition accuracy and reaction times of vibration patterns that include 1) directional and 2) rotational motion. Our results show that the directional pattern of two intermittent pulses was preferred by most subjects, even though it had slightly more recognition error than patterns with continuous vibrations. Rotational patterns were recognized by subjects with almost perfect accuracy. Average reaction time to all vibration patterns varied between 1 and 2 seconds. Our tactile belt design was found comfortable, but it was also found slightly noisy.",,,367,370,Encoding (memory); Haptic technology; Pattern recognition (psychology); Artificial intelligence; Usability; Vibration; Navigation assistance; Visually impaired; Computer vision; Computer science; Rotation around a fixed axis,,,,,http://ieeexplore.ieee.org/abstract/document/6775483 https://dblp.uni-trier.de/db/conf/haptics/haptics2014.html#CosgunSC14 https://ieeexplore.ieee.org/abstract/document/6775483 https://research.monash.edu/en/publications/evaluation-of-rotational-and-directional-vibration-patterns-on-a-,http://dx.doi.org/10.1109/haptics.2014.6775483,,10.1109/haptics.2014.6775483,1965712840,,0,003-274-959-651-784; 003-506-530-057-475; 029-270-236-495-701; 034-422-902-172-199; 067-100-400-949-987; 073-676-742-362-288; 078-431-662-636-994; 085-309-270-374-774; 094-827-772-601-128; 121-170-590-802-897; 159-164-194-012-568; 185-085-881-079-518,38,false,,
034-677-153-864-773,Wayfinding Design and Accessibility,2015-09-23,2015,journal article,GSTF Journal of Engineering Technology,2251371x,Global Science and Technology Forum,,Roberto de Paolis; Silvia Guerini,,3,2,,,,,,,,,http://dx.doi.org/10.7603/s40707-014-0012-4,,10.7603/s40707-014-0012-4,,,0,,0,false,,
034-742-688-109-756,Fuzzy clustering in vision recognition applied in NAVI,,,conference proceedings article,2002 Annual Meeting of the North American Fuzzy Information Processing Society Proceedings. NAFIPS-FLINT 2002 (Cat. No. 02TH8622),,IEEE,,R. Nagarajan; Sazali Yaacob; G. Sainarayanan,"In this paper a system for navigation assistance for visually impaired (NAVI) is presented. The system includes a single board processing system (UPS), vision sensor mounted headgear and stereo earphones. The image of environment is captured by the vision sensor. The image is then processed by a novel real time image processing methodology using the fuzzy clustering algorithm. The proposed methodology incorporates certain human vision properties for clear representation of the environment. The image processed is mapped to a stereo acoustic pattern and transferred to the user's earphones. The sound produced varies with the gray value and orientation of the obstacle in front. Blind individuals are trained with NAVI and tested for obstacle identification. Suggestions from the blind volunteers regarding pleasantness and discrimination of sound pattern were also incorporated in the prototype. Certain improvements for faster convergence of clustering are also considered.",,,261,266,Audio signal processing; Artificial intelligence; Fuzzy clustering; Obstacle; Computer vision; Computer science; Fuzzy set; Cluster analysis; Identification (information); Orientation (computer vision); Image processing,,,,,https://ieeexplore.ieee.org/document/1018066/,http://dx.doi.org/10.1109/nafips.2002.1018066,,10.1109/nafips.2002.1018066,2170596413,,0,007-062-686-399-544; 010-025-512-242-323; 013-847-555-426-159; 045-673-862-191-277; 066-374-042-552-286; 074-324-981-337-749; 085-836-945-086-485; 179-906-777-760-723,7,false,,
035-111-835-027-909,Ultrasonic Spectacles for Blind Person,2023-05-12,2023,journal article,International Journal of Innovative Research in Engineering,25828746,Fifth Dimension Research Publications,,Keerthana T; Kumari Aarju; Deepa. K; Karpagavalli C; Vigneshwari S,"<jats:p>The ultrasonic spatial code line for blind people using Node MCU, web camera, ultrasonic sensor, and output of speaker is an innovative system designed to aid visually impaired individuals in navigating their environment independently and safely. The system works by using a combination of hardware components and software algorithms to provide real-time spatial feedback to the user. The system consists of a Node MCU microcontroller, which acts as the central processing unit and controls the flow of data between the various components. A web camera is used to capture a live video feed of the user's surroundings, which is then processed using image recognition algorithms to identify and locate obstacles in the environment. An ultrasonic sensor is used to measure the distance between the user and any obstacles detected in the video feed. This information is then used to generate a tactile feedback map that is transmitted to the user via a speaker output. The tactile feedback map consists of a series of vibrations that correspond to the location and distance of the obstacles in the user's environment. The ultrasonic spatial code line is a critical need for blind individuals who struggle with navigating their environment independently. The system provides a practical solution that can help improve their quality of life by enabling them to move around more safely and confidently. Additionally, the system is relatively low-cost and easy to implement, making it accessible to a broader range of individuals. Key Word: Electronic Travel Aid – Visual Impairment- Machine learning – Navigation Aid - Mobility – Ultrasonic spectacles – Blind navigation;</jats:p>",,,219,221,Computer science; Microcontroller; Ultrasonic sensor; Node (physics); Computer vision; Artificial intelligence; Code (set theory); Software; Speech synthesis; Real-time computing; Computer hardware; Engineering; Physics; Structural engineering; Set (abstract data type); Acoustics; Programming language,,,,,,http://dx.doi.org/10.59256/ijire.2023040377,,10.59256/ijire.2023040377,,,0,,0,false,,
035-146-961-988-954,Surgical robotics and instrumentation,2013-05-10,2013,journal article,International Journal of Computer Assisted Radiology and Surgery,18616410; 18616429,Springer Science and Business Media LLC,Germany,Philip Nicolai; Thorsten Brennecke; Mirko Kunze; Luzie Schreiter; Tim Beyl; Y. Zhang; Julien Mintenbeck; Jörg Raczkowsky; Heinz Wörn,,8,1,127,139,Instrumentation (computer programming); Engineering; Systems engineering; Surgical robotics,,,,,https://publikationen.bibliothek.kit.edu/1000038597,http://dx.doi.org/10.1007/s11548-013-0861-3,,10.1007/s11548-013-0861-3,2971321990,,0,002-727-550-540-352; 012-971-158-716-111; 025-797-923-312-511; 052-548-878-710-132; 079-220-109-865-528; 143-388-866-651-007,1,false,,
035-185-985-584-737,16th Annual conference of the international society for computer aided surgery,2012-05-17,2012,journal article,International Journal of Computer Assisted Radiology and Surgery,18616410; 18616429,Springer Science and Business Media LLC,Germany,,,7,S1,369,465,Medicine; Computer science; General surgery,,,,,,http://dx.doi.org/10.1007/s11548-012-0737-y,,10.1007/s11548-012-0737-y,,,0,,3,false,,
035-313-478-601-436,Obstruction Avoidance Framework for Indoor and Outdoor Navigation to Enhance Mobility of Visually Impaired Person,2023-04-20,2023,conference proceedings article,"2023 International Conference on Computational Intelligence, Communication Technology and Networking (CICTN)",,IEEE,,Tanya Tanya; Saurabh Singh; Subrat Tripathi; Shivani Kapoor; Amanpreet Singh Saini,"Blindness and visual impairments in computer vision are among the top ten disabilities affecting people, with India having the largest percentage of visually impaired people worldwide. An object detection and recognition framework is being created to aid in helping these people navigate and remain safe in their surroundings. This system uses a camera to capture an image and feed it as input to the software. The Single Shot MultiBox Detector (SSD) architecture is employed for object detection, utilizing deep neural networks to achieve accurate results. The input image is processed using the COCO datasets, which act as the system's training data and are predefined in the Tensor Flow framework. About 90% of objects in the actual world have features in these datasets, and depth estimation is used to calculate distance. The software then produces an audio output with the assistance of voice packages. Python is used to build the system because it has many built-in packages and libraries that make complicated code easier to understand. As a result, writing programmes with fewer lines of code is simpler.",,,,,Computer science; Python (programming language); Artificial intelligence; Computer vision; Software; Blindness; Visually impaired; Object detection; Optical flow; Code (set theory); Human–computer interaction; Image (mathematics); Segmentation; Programming language; Medicine; Set (abstract data type); Optometry,,,,,,http://dx.doi.org/10.1109/cictn57981.2023.10140455,,10.1109/cictn57981.2023.10140455,,,0,017-042-971-176-619; 027-682-239-914-13X; 033-554-664-404-916; 105-095-514-938-373; 106-587-878-038-509; 112-566-962-059-235; 149-393-755-739-676; 152-127-960-081-400; 165-422-564-417-041; 171-057-734-143-202; 179-067-951-617-101; 199-788-200-071-609,1,false,,
035-505-616-217-747,Artificial potential functions based camera movements and visual behaviors in attentive robots,2011-08-11,2011,journal article,Autonomous Robots,09295593; 15737527,Springer Science and Business Media LLC,Netherlands,Ozgur Erkent; H. Isil Bozma,,32,1,15,34,Smooth pursuit; Motion (physics); Visual servoing; Saccadic masking; Automaton; Artificial intelligence; Novelty; Computer vision; Computer science; Dynamical systems theory; Robot,,,,,https://link.springer.com/article/10.1007%2Fs10514-011-9240-5 https://dblp.uni-trier.de/db/journals/arobots/arobots32.html#ErkentB12 https://rd.springer.com/content/pdf/10.1007%2Fs10514-011-9240-5.pdf,http://dx.doi.org/10.1007/s10514-011-9240-5,,10.1007/s10514-011-9240-5,2092955338,,0,002-819-820-329-441; 003-099-733-308-048; 006-149-952-440-649; 007-622-955-326-056; 008-197-123-503-47X; 009-483-175-409-761; 014-145-947-109-187; 014-268-266-061-165; 015-722-657-916-101; 015-998-839-756-88X; 017-030-680-005-985; 017-560-889-530-834; 021-345-144-727-301; 022-265-058-486-317; 023-370-064-023-846; 023-548-656-777-161; 023-919-131-089-315; 025-309-865-441-247; 027-184-489-316-181; 035-785-918-328-521; 035-890-066-979-90X; 036-125-126-588-483; 040-304-494-609-514; 040-975-968-953-251; 043-020-833-842-287; 046-081-607-258-531; 047-175-420-949-563; 048-730-954-645-480; 049-149-448-212-233; 053-038-343-836-384; 053-752-920-804-699; 055-014-100-192-154; 057-553-587-549-164; 059-995-930-296-805; 060-080-925-286-207; 060-319-594-246-789; 060-437-552-797-221; 066-845-831-097-771; 067-001-628-699-132; 067-023-626-670-51X; 067-255-511-386-538; 067-668-236-111-761; 068-993-734-562-560; 069-217-390-348-37X; 072-901-031-588-936; 076-497-631-058-676; 078-833-748-452-944; 087-442-125-858-743; 092-283-280-765-606; 096-542-114-349-227; 096-717-380-007-985; 097-231-783-172-092; 097-967-098-624-46X; 100-797-551-662-34X; 101-029-620-275-04X; 103-639-032-104-472; 105-273-779-648-91X; 106-352-705-102-836; 109-605-302-092-268; 110-564-232-978-510; 114-859-821-305-481; 117-133-276-301-317; 127-829-253-650-862; 133-400-954-185-137; 134-129-530-852-822; 134-608-951-601-675; 135-845-315-546-105; 139-677-647-646-036; 139-855-442-331-11X; 149-378-392-762-467; 150-015-862-946-547; 161-376-313-914-721; 173-099-375-692-143; 176-567-213-407-630,7,false,,
035-506-792-102-812,Estimating camera parameters from starry night photographs,2020-11-06,2020,journal article,Computational Visual Media,20960433; 20960662,Springer Science and Business Media LLC,,Naoto Ishikawa; Yoshinori Dobashi,"We propose an efficient, specific method for estimating camera parameters from a single starry night image. Such an image consists of a collection of disks representing stars, so traditional estimation methods for common pictures do not work. Our method uses a database, a star catalog, that stores the positions of stars on the celestial sphere. Our method computes magnitudes (i.e., brightnesses) of stars in the input image and uses them to find the corresponding stars in the star catalog. Camera parameters can then be estimated by a simple geometric calculation. Our method is over ten times faster and more accurate than a previous method.",6,4,445,454,A* search algorithm; Stars; Artificial intelligence; Celestial sphere; Astrophotography; Computer vision; Computer science; Star catalogue; Pattern matching; Constellation; Computer graphics,,,,,https://link.springer.com/article/10.1007/s41095-020-0198-0 http://cvm.tsinghuajournals.com/EN/10.1007/s41095-020-0198-0 https://link.springer.com/content/pdf/10.1007/s41095-020-0198-0.pdf https://doi.org/10.1007/s41095-020-0198-0 https://dblp.uni-trier.de/db/journals/cvm/cvm6.html#IshikawaD20 https://www.firstacademics.com/article/10.1007/s41095-020-0198-0 https://eprints.lib.hokudai.ac.jp/dspace/handle/2115/82075,http://dx.doi.org/10.1007/s41095-020-0198-0,,10.1007/s41095-020-0198-0,3097953759,,0,002-772-785-956-814; 006-854-381-125-162; 021-714-157-778-918; 028-303-905-295-179; 038-055-831-499-04X; 049-283-340-824-999; 052-680-377-730-600; 061-176-969-004-876; 064-890-070-028-869; 068-955-186-300-764; 070-195-968-155-402; 071-982-848-783-059; 074-626-718-145-110; 077-496-824-833-976; 081-613-857-062-99X; 086-011-836-617-70X; 087-242-738-300-041; 088-890-703-294-405; 104-887-486-795-49X; 105-985-948-410-597; 114-058-237-547-334; 119-198-581-005-848; 123-085-521-342-627; 146-128-713-708-816; 159-383-244-984-833; 162-053-743-682-369; 176-136-239-718-272,0,true,cc-by,gold
035-826-945-197-860,Multi-Robot Exploration in Wireless Environments,2012-04-26,2012,journal article,Cognitive Computation,18669956; 18669964,Springer Science and Business Media LLC,Germany,Anshika Pal; Ritu Tiwari; Anupam Shukla,,4,4,526,542,Wireless network; Distributed computing; Communications protocol; Mobile robot; Mobile robot navigation; Interaction protocol; Computer science; Wireless; Robot; Mobile ad hoc network,,,,,https://link.springer.com/article/10.1007/s12559-012-9142-7/fulltext.html https://link.springer.com/article/10.1007/s12559-012-9142-7 https://link.springer.com/content/pdf/10.1007%2Fs12559-012-9142-7.pdf,http://dx.doi.org/10.1007/s12559-012-9142-7,,10.1007/s12559-012-9142-7,1973084318,,0,003-622-848-273-002; 012-068-899-532-319; 016-559-281-047-399; 019-643-795-395-80X; 023-842-565-185-037; 024-311-129-393-677; 025-079-446-503-115; 031-901-989-328-529; 033-071-263-883-710; 035-689-623-011-869; 041-317-722-448-37X; 046-146-590-965-081; 050-929-418-514-858; 052-353-933-555-973; 052-561-654-431-958; 053-629-969-804-688; 054-213-458-012-555; 058-811-259-191-613; 059-261-928-043-160; 059-838-321-363-540; 063-179-842-524-336; 064-053-186-811-574; 071-957-292-698-97X; 072-211-545-989-421; 074-114-810-653-477; 074-566-285-876-521; 077-986-052-800-104; 081-001-965-572-922; 091-078-709-243-028; 092-089-063-945-775; 093-595-402-120-03X; 094-732-695-115-895; 097-218-957-636-090; 098-416-326-090-910; 102-493-145-730-167; 102-613-586-876-528; 118-175-107-260-675; 124-110-560-799-978; 131-830-246-564-846; 145-804-086-994-546; 151-659-555-170-056; 152-994-736-300-459; 154-987-956-854-610; 155-745-621-981-76X; 156-759-174-255-558,14,false,,
036-043-861-867-834,A 3D Computer Vision-Guided Robotic Companion for Non-Contact Human Assistance and Rehabilitation,2020-09-21,2020,journal article,Journal of intelligent & robotic systems,15730409; 09210296,Springer Science and Business Media LLC,Netherlands,Tao Shen; Rayhan Afsar; He Zhang; Cang Ye; Xiangrong Shen,"With the rapid aging of the U.S. population, the mobility impairment is becoming a more and more challenging issue that affects a large number of individuals. The research presented in this paper aims at helping the mobility-challenged individuals with a novel robotic companion, which is a walker-type mobile robot capable of accompanying the human user and keeping user at the center for protection and possible power assistance. The robotic companion is equipped with a 3D computer vision system, which provides a unique capability of sensing the human-robot relative position/orientation without physical contact or the need for wearable sensors. As such, the robotic companion enables the user to walk freely with minimum disturbance to his/her normal gait, relieving the user from the physical and cognitive loads associated with the use of traditional assistive devices. For the development of the robotic companion, the authors designed and fabricated a low-cost, differentially steered mobile robotic platform, and also developed a unique image processing system to extract the position/orientation information from the 3D camera-captured images. Furthermore, an advanced motion control system was developed for the robotic companion, which provides novel solutions to the unique challenges such as sway reduction and noise reduction in digital differentiation. To quantify the performance, component and system-level experimentation was conducted, and the results demonstrated that robotic companion and its key components function as desired and the system is expected to reduce the user load and improve the user mobility in real-world scenarios.",100,3,911,923,Motion control; Wearable computer; Artificial intelligence; Mobile robot; Key (cryptography); Population; Computer vision; Computer science; Component (UML); Orientation (computer vision); Image processing,,,,NINR NIH HHS (R01 NR016151) United States,https://doi.org/10.1007/s10846-020-01258-1 https://dialnet.unirioja.es/servlet/articulo?codigo=7819158 https://europepmc.org/article/MED/33776207 http://doi.org/10.1007/s10846-020-01258-1 https://www.ncbi.nlm.nih.gov/pubmed/33776207 https://pubmed.ncbi.nlm.nih.gov/33776207/ https://link.springer.com/article/10.1007/s10846-020-01258-1 https://dblp.uni-trier.de/db/journals/jirs/jirs100.html#ShenAZYS20,http://dx.doi.org/10.1007/s10846-020-01258-1,33776207,10.1007/s10846-020-01258-1,3087247447,PMC7990042,0,003-788-872-941-98X; 003-940-066-092-112; 007-381-264-100-950; 013-769-784-215-602; 016-144-296-568-90X; 016-397-781-226-597; 019-100-674-376-820; 020-503-248-292-220; 021-742-137-073-847; 024-801-865-678-402; 028-355-201-727-652; 031-321-776-637-634; 031-340-088-964-440; 034-901-989-083-065; 037-966-150-573-567; 040-151-128-084-899; 041-763-166-386-234; 046-003-043-896-867; 048-981-722-695-207; 050-489-372-109-454; 054-729-367-254-142; 055-118-890-810-901; 069-273-676-118-998; 069-523-876-223-951; 075-511-167-215-130; 077-018-374-860-718; 080-403-334-364-161; 081-903-032-858-93X; 087-181-732-370-085; 089-068-421-285-041; 096-013-117-377-764; 102-443-499-451-493; 106-527-885-911-500; 111-260-356-453-39X; 123-324-175-118-985; 150-595-881-944-663; 153-606-108-662-781; 158-909-333-285-572,5,true,,green
036-071-747-081-676,Guide Me: Recognition and Servoing on Mobiles,2018-01-31,2018,journal article,Arabian Journal for Science and Engineering,2193567x; 21914281; 13198025,Springer Science and Business Media LLC,Saudi Arabia,Abdulhafez Abdulhafez; Gaurav Harit,,43,12,7359,7372,Artificial intelligence; Reliability (computer networking); Matching (graph theory); Thesaurus (information retrieval); Mobile phone; Computer vision; Computer science; Computation; Object (computer science); Cognitive neuroscience of visual object recognition; Convergence (routing),,,,,https://link.springer.com/article/10.1007/s13369-018-3084-7 https://jglobal.jst.go.jp/en/detail?JGLOBAL_ID=201902288964487635,http://dx.doi.org/10.1007/s13369-018-3084-7,,10.1007/s13369-018-3084-7,2785500319,,0,000-786-716-359-893; 002-234-863-117-357; 002-345-873-500-677; 003-031-964-861-122; 008-620-985-087-567; 010-557-331-864-382; 016-539-862-398-535; 023-925-368-922-548; 025-513-762-128-619; 025-862-355-055-075; 032-633-708-641-366; 046-703-334-644-632; 047-727-195-203-863; 050-828-593-256-726; 051-817-341-806-456; 058-916-069-385-979; 059-874-975-599-390; 061-677-802-154-042; 071-583-782-423-553; 074-199-903-520-265; 077-602-265-835-620; 084-729-807-470-497; 086-083-592-775-973; 099-121-947-089-398; 102-786-306-974-437; 103-023-625-535-563; 114-140-860-480-410; 114-905-235-118-928; 116-971-646-450-632; 118-145-550-416-139; 120-273-856-497-267; 141-132-265-070-694; 175-494-915-435-125; 192-751-999-674-131,0,false,,
036-166-855-358-929,Artificial mosaics,2005-06-09,2005,journal article,The Visual Computer,01782789; 14328726; 14322315,Springer Science and Business Media LLC,Germany,Gianpiero Di Blasi; Giovanni Gallo,,21,6,373,383,Image (mathematics); Computer graphics (images); Raster graphics; Simple (abstract algebra); Quality (business); Along edge; Mosaic (geodemography); Computer science; Computer graphics; Image processing,,,,,https://link.springer.com/article/10.1007/s00371-005-0292-4 https://link.springer.com/content/pdf/10.1007%2Fs00371-005-0292-4.pdf,http://dx.doi.org/10.1007/s00371-005-0292-4,,10.1007/s00371-005-0292-4,3188012383,,0,006-781-366-308-839; 016-896-353-266-476; 021-291-234-767-592; 021-502-842-217-487; 036-973-617-888-116; 052-158-521-354-848; 053-524-100-900-867; 076-198-255-092-415; 077-691-867-193-757; 155-632-268-139-764; 190-341-124-346-931,52,false,,
036-920-253-203-542,Towards reliable object detection in noisy images,2017-12-09,2017,journal article,Pattern Recognition and Image Analysis,10546618; 15556212,Pleiades Publishing Ltd,United States,Sergey Milyaev; Ivan Laptev,,27,4,713,722,Image compression; Artificial intelligence; Noise reduction; Object detection; Pedestrian detection; Computer vision; Image noise; Computer science; Image quality; Cognitive neuroscience of visual object recognition; Convolutional neural network,,,,,https://dl.acm.org/doi/10.1134/S1054661817040149 https://link.springer.com/article/10.1134/S1054661817040149,http://dx.doi.org/10.1134/s1054661817040149,,10.1134/s1054661817040149,2774297389,,0,003-499-035-235-702; 004-193-609-931-550; 007-316-191-175-159; 007-557-606-137-195; 010-037-102-410-599; 010-759-689-155-412; 015-620-703-326-093; 022-127-283-176-758; 023-438-345-162-875; 024-881-309-381-23X; 034-073-120-160-844; 040-218-035-484-010; 041-036-823-153-65X; 066-133-135-091-645; 069-137-512-638-317; 085-507-633-373-250; 094-760-470-146-509; 171-744-559-076-587; 195-091-532-336-938,21,false,,
037-039-198-474-114,Finding objects for assisting blind people,2013-02-07,2013,journal article,Network modeling and analysis in health informatics and bioinformatics,21926662; 21926670,Springer Nature,United States,Chucai Yi; Roberto W. Flores; Ricardo Chincha; Yingli Tian,"Computer vision technology has been widely used for blind assistance, such as navigation and way finding. However, few camera-based systems are developed for helping blind or visually impaired people to find daily necessities. In this paper, we propose a prototype system of blind-assistant object finding by camera-based network and matching-based recognition. We collect a dataset of daily necessities and apply Speeded-Up Robust Features and Scale Invariant Feature Transform feature descriptors to perform object recognition. Experimental results demonstrate the effectiveness of our prototype system.",2,2,71,79,Machine learning; Artificial intelligence; Way finding; Visually impaired; Computer vision; Computer science; Scale-invariant feature transform; Object (computer science); Cognitive neuroscience of visual object recognition; Feature (computer vision),,,,NEI NIH HHS (R21 EY020990) United States,https://paperity.org/p/8908544/finding-objects-for-assisting-blind-people https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3719410/ http://europepmc.org/articles/PMC3719410 https://link.springer.com/10.1007/s13721-013-0026-x https://link.springer.com/article/10.1007/s13721-013-0026-x https://dblp.uni-trier.de/db/journals/netmahib/netmahib2.html#YiFCT13 https://link.springer.com/content/pdf/10.1007%2Fs13721-013-0026-x.pdf,http://dx.doi.org/10.1007/s13721-013-0026-x,23894729,10.1007/s13721-013-0026-x,1998144823,PMC3719410,10,002-469-564-380-629; 005-861-492-610-995; 015-049-788-401-629; 017-062-246-464-560; 021-272-916-231-150; 034-350-308-512-672; 039-533-788-125-463; 044-202-560-313-315; 046-684-991-081-664; 055-382-135-850-769; 068-279-882-269-210; 068-446-370-600-681; 075-520-682-747-021; 093-265-408-337-289; 094-513-668-877-229; 096-014-228-878-398; 098-440-015-522-27X; 119-147-559-161-430; 120-786-413-189-971; 124-597-558-957-962; 133-207-039-243-212; 148-641-826-351-69X; 189-691-486-505-033,25,true,,green
037-133-995-895-465,Scale Invariant Feature Transform on the Sphere: Theory and Applications,2011-11-10,2011,journal article,International Journal of Computer Vision,09205691; 15731405,Springer Science and Business Media LLC,Netherlands,Javier Cruz-Mota; Iva Bogdanova; Benoît Paquier; Michel Bierlaire; Jean-Philippe Thiran,"A SIFT algorithm in spherical coordinates for omnidirectional images is proposed. This algorithm can generate two types of local descriptors, Local Spherical Descriptors and Local Planar Descriptors. With the first ones, point matching between two omnidirectional images can be performed, and with the second ones, the same matching process can be done but between omnidirectional and planar images. Furthermore, a planar to spherical mapping is introduced and an algorithm for its estimation is given. This mapping allows to extract objects from an omnidirectional image given their SIFT descriptors in a planar image. Several experiments, confirming the promising and accurate performance of the system, are conducted.",98,2,217,241,Spherical coordinate system; Planar; Artificial intelligence; Pattern recognition; Matching (graph theory); Point set registration; Omnidirectional antenna; Object detection; Computer vision; Mathematics; Feature extraction; Scale-invariant feature transform,,,,,https://doc.rero.ch/record/314293 https://infoscience.epfl.ch/record/170321 https://dx.doi.org/10.1007/s11263-011-0505-4 https://link.springer.com/article/10.1007%2Fs11263-011-0505-4 https://www.tpbin.com/Uploads/Subjects/e284e938-0dfe-411f-b6c6-1cad0f5cc9dd.pdf http://www.tpbin.com/Uploads/Subjects/e284e938-0dfe-411f-b6c6-1cad0f5cc9dd.pdf http://dx.doi.org/10.1007/s11263-011-0505-4 https://dblp.uni-trier.de/db/journals/ijcv/ijcv98.html#Cruz-MotaBPBT12 https://core.ac.uk/download/147975814.pdf,http://dx.doi.org/10.1007/s11263-011-0505-4,,10.1007/s11263-011-0505-4,2157653924,,1,002-724-634-490-58X; 005-000-267-388-326; 005-597-562-783-614; 008-215-888-545-387; 011-450-474-079-329; 012-642-196-605-159; 015-215-743-243-429; 016-575-037-768-178; 017-019-672-760-676; 021-033-546-707-589; 026-066-303-554-074; 027-110-881-564-946; 028-303-905-295-179; 028-412-594-347-797; 029-061-861-419-041; 033-245-685-094-882; 037-379-927-843-110; 041-003-807-260-374; 046-668-544-844-797; 054-573-724-656-644; 068-344-965-124-736; 069-552-164-534-05X; 074-920-703-064-454; 076-881-990-743-807; 079-499-077-131-386; 084-679-921-430-947; 087-911-891-613-904; 098-440-015-522-27X; 100-321-806-722-989; 105-204-888-195-359; 108-135-700-298-624; 113-758-486-360-782; 117-241-275-478-979; 117-589-640-323-914; 123-916-387-179-404; 128-785-454-931-518; 130-461-950-828-57X; 130-681-299-817-98X; 135-920-960-607-545; 137-423-666-102-293; 139-282-589-508-244; 139-519-941-168-377; 146-296-596-852-060; 146-772-866-616-83X; 149-382-334-560-48X; 155-066-611-554-516; 166-011-740-159-900; 174-260-789-699-77X; 191-424-642-484-955,130,true,,green
037-163-533-301-156,CVPR Workshops - Visual Navigation Aid for the Blind in Dynamic Environments,,2014,conference proceedings article,2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops,,IEEE,,Tung-Sing Leung; Gerard Medioni,"We describe a robust method to estimate egomotion in highly dynamic environments. Our application is a head mounted stereo system designed to help the visually impaired navigate. Instead of computing egomotion from 3D point correspondences in consecutive frames, we propose to find the ground plane, then decompose the 6DoF egomotion into the motion of the ground plane, and a planar motion on the ground plane. The ground plane is estimated at each frame by analysis of the disparity array. Next, we estimate the normal to the ground plane. This is done either from the visual data, or from the IMU reading. We evaluate the results on both synthetic and real scenes, and compare the results of the direct, 6 DoF estimate with our plane-based approach, with and without the IMU. We conclude that the egomotion estimation using this new approach produces significantly better results, both in simulation and on real data sets.",,,579,586,Frame (networking); Artificial intelligence; Motion (geometry); Plane (geometry); Point (geometry); Computer vision; Computer science; Visual odometry; Inertial measurement unit; Ground plane,,,,,https://dblp.uni-trier.de/db/conf/cvpr/cvprw2014.html#LeungM14 http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload=true&arnumber=6910038 http://dx.doi.org/10.1109/CVPRW.2014.89 https://ieeexplore.ieee.org/document/6910038/ https://openaccess.thecvf.com/content_cvpr_workshops_2014/W16/html/Leung_Visual_Navigation_Aid_2014_CVPR_paper.html https://www.cv-foundation.org/openaccess/content_cvpr_workshops_2014/W16/html/Leung_Visual_Navigation_Aid_2014_CVPR_paper.html https://doi.org/10.1109/CVPRW.2014.89 https://dx.doi.org/10.1109/CVPRW.2014.89 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6910038 https://www.cv-foundation.org/openaccess/content_cvpr_workshops_2014/W16/papers/Leung_Visual_Navigation_Aid_2014_CVPR_paper.pdf,http://dx.doi.org/10.1109/cvprw.2014.89,,10.1109/cvprw.2014.89,1969683776,,0,004-511-416-923-897; 005-857-282-882-43X; 007-391-459-034-77X; 008-587-112-913-346; 014-478-034-924-123; 016-160-128-737-997; 023-612-297-764-660; 028-279-887-573-296; 028-282-127-740-669; 028-303-905-295-179; 028-847-878-700-92X; 043-056-567-014-861; 044-925-465-549-740; 049-921-334-736-611; 052-219-096-473-93X; 057-379-894-626-392; 061-199-231-534-098; 107-262-279-999-325; 115-278-034-310-151; 128-907-188-513-761; 144-774-070-048-044; 170-387-846-090-147; 183-177-134-821-382,50,true,,green
037-182-418-585-695,Deep Learning Based Application for Indoor Scene Recognition,2020-03-20,2020,journal article,Neural Processing Letters,13704621; 1573773x,Springer Science and Business Media LLC,Netherlands,Mouna Afif; Riadh Ayachi; Yahia Said; Mohamed Atri,,51,3,2827,2837,Deep learning; Computational intelligence; Artificial intelligence; Key (cryptography); Computer vision; Field (computer science); Computer science; Component (UML); Artificial neural network; Object (computer science); Convolutional neural network,,,,,https://doi.org/10.1007/s11063-020-10231-w https://dblp.uni-trier.de/db/journals/npl/npl51.html#AfifASA20 https://link.springer.com/article/10.1007/s11063-020-10231-w,http://dx.doi.org/10.1007/s11063-020-10231-w,,10.1007/s11063-020-10231-w,3013004221,,0,000-589-329-683-885; 002-491-759-024-334; 002-508-484-172-239; 003-456-545-749-041; 004-269-574-716-057; 004-337-183-941-849; 006-639-160-290-55X; 010-067-991-372-664; 016-456-940-706-916; 020-233-013-143-936; 025-522-385-395-133; 026-363-993-267-742; 033-239-723-330-575; 040-209-930-304-077; 044-396-615-675-996; 045-309-399-228-849; 048-254-076-918-785; 049-317-239-158-314; 049-921-334-736-611; 061-579-383-511-500; 064-657-750-690-046; 069-480-623-909-181; 070-002-546-287-735; 080-922-196-287-325; 115-725-112-822-171; 120-437-599-272-427; 132-618-624-119-87X; 137-638-549-224-812; 140-394-829-785-809; 180-515-374-647-90X,44,false,,
037-378-081-417-550,Artificial intelligence: looking though the Pygmalion Lens,2018-10-13,2018,journal article,AI & SOCIETY,09515666; 14355655,Springer Science and Business Media LLC,Germany,Karamjit S. Gill,,33,4,459,465,Optometry; Sociology; Lens (geology),,,,,https://link.springer.com/article/10.1007/s00146-018-0866-0/fulltext.html https://link.springer.com/article/10.1007/s00146-018-0866-0 https://dblp.uni-trier.de/db/journals/ais/ais33.html#Gill18 https://link.springer.com/content/pdf/10.1007%2Fs00146-018-0866-0.pdf https://doi.org/10.1007/s00146-018-0866-0,http://dx.doi.org/10.1007/s00146-018-0866-0,,10.1007/s00146-018-0866-0,2896504051,,0,003-712-004-091-961; 027-132-581-923-377; 110-264-581-829-228; 147-733-898-004-648,10,true,cc0,hybrid
037-478-972-598-058,ADAGSS: Automatic Dataset Generation for Semantic Segmentation,2020-06-08,2020,conference proceedings,International journal of computer assisted radiology and surgery,18616429; 18616410,Springer Science and Business Media LLC,Germany,L. Palladino; Bogdan Mihai Maris; Paolo Fiorini,,15,Suppl 1,214,214,Deep learning; Artificial intelligence; Pattern recognition; Computer science; Contextual image classification; Segmentation,,,,,https://iris.univr.it/handle/11562/1038621?mode=full.1005,http://dx.doi.org/10.1007/s11548-020-02171-6,32514840,10.1007/s11548-020-02171-6,3167674282,PMC7276970,1,,11,true,implied-oa,green
037-545-584-168-942,Novel indoor navigation system for visually impaired and blind people,,2015,conference proceedings article,2015 International Conference on Applied Research in Computer Science and Engineering (ICAR),,IEEE,,Kabalan Chaccour; Georges Badr,"Visual impairment or vision loss is the main reason for reduced mobility in humans. Visually Impaired (VI) people require continuous assistance during their movement. Researchers and scientists have put many solutions to assist those people in the activities of their daily living (ADL). The current research delivers a novel indoor navigation system for visually impaired people. The proposed solution is designed for indoor use only (house, office, companies, etc.). It provides the visually impaired person the ability to navigate without any other hardware assistance. The proposed system architecture uses a network of IP cameras installed at the ceiling of each room. A remote processing system analyzes-by computer vision algorithms-photos taken from the environment in order to inform the subject about his location and reacts accordingly to deliver the adequate assistance. A guidance algorithm helps him reach his destination using a simple interactive mobile application installed on his smart phone. The proof of concept prototype was designed with one camera on top of a wooden floor model to simulate the system. Results showed good reliability in indoor navigation and obstacles avoidance.",,,1,5,Proof of concept; Engineering; Reliability (computer networking); Mobile robot navigation; SIMPLE (military communications protocol); Ceiling (cloud); Navigation system; Visual impairment; Multimedia; Systems architecture,,,,,http://ieeexplore.ieee.org/document/7338143/ https://ieeexplore.ieee.org/document/7338143/,http://dx.doi.org/10.1109/arcse.2015.7338143,,10.1109/arcse.2015.7338143,2178444645,,1,000-631-028-807-828; 001-076-828-128-846; 013-056-435-015-274; 013-551-333-215-556; 013-678-295-106-102; 013-847-555-426-159; 014-157-934-526-940; 015-998-086-943-276; 022-697-627-694-941; 030-127-369-356-741; 031-799-548-880-483; 034-350-308-512-672; 064-309-126-323-022; 066-374-042-552-286; 066-526-177-885-125; 098-811-589-762-176,10,false,,
037-808-561-986-047,Orientation technology for indoor travel by persons with multiple disabilities.,2009-08-20,2009,journal article,Cognitive processing,16124790; 16124782,Springer Science and Business Media LLC,Germany,Giulio E. Lancioni; Mark F. O’Reilly; Nirbhay N. Singh; Jeff Sigafoos; Doretta Oliva,,10,2,244,246,Psychology; Cognitive psychology; Corrective feedback; Orientation (mental); Multiple disabilities; Intellectual disability; Behavioural sciences,,Cues; Disabled Persons; Feedback; Humans; Movement/physiology; Orientation/physiology; Space Perception/physiology; User-Computer Interface,,,http://www.ncbi.nlm.nih.gov/pubmed/19693578 https://link.springer.com/article/10.1007%2Fs10339-009-0293-8 https://europepmc.org/article/MED/19693578 https://core.ac.uk/display/54146627,http://dx.doi.org/10.1007/s10339-009-0293-8,19693578,10.1007/s10339-009-0293-8,2031678472,,0,003-573-584-084-065; 004-486-421-176-513; 015-682-230-414-125; 018-438-138-864-259; 023-224-288-961-254; 027-948-422-229-187; 032-074-682-006-824; 096-495-338-140-79X; 108-804-858-522-070; 115-165-824-255-336; 118-150-218-037-598; 120-896-719-918-943,3,false,,
037-941-949-319-200,Positioning of logistics and warehousing automated guided vehicle based on improved LSTM network,2021-08-27,2021,journal article,International Journal of System Assurance Engineering and Management,09756809; 09764348,Springer Science and Business Media LLC,India,Tingting Yan,,14,2,509,518,,,,,,,http://dx.doi.org/10.1007/s13198-021-01243-3,,10.1007/s13198-021-01243-3,,,0,001-393-303-556-548; 003-918-784-729-834; 006-567-689-415-244; 023-593-592-900-943; 024-155-742-936-930; 030-852-649-696-750; 033-554-759-481-789; 038-157-514-257-749; 038-680-731-564-573; 055-489-677-669-294; 061-568-408-045-208; 086-859-570-530-698; 090-233-411-784-463; 092-662-153-967-352; 119-401-758-081-717; 132-221-510-583-687; 135-266-700-824-830; 140-700-368-294-646; 154-214-770-809-26X,8,false,,
037-999-038-682-270,Real-time Powered Wheelchair Assistive Navigation System Based on Intelligent Semantic Segmentation for Visually Impaired Users,2022-12-05,2022,conference proceedings article,2022 IEEE 5th International Conference on Image Processing Applications and Systems (IPAS),,IEEE,,Elhassan Mohamed; Konstantinos Sirlantzis; Gareth Howells,"People with movement disabilities may find powered wheelchair driving a challenging task due to their comorbidities. Certain visually impaired persons with mobility disabilities are not prescribed a powered wheelchair because of their sight condition. However, powered wheelchairs are essential to the majority of these disabled users for commuting and social interaction. It is vital for their independence and wellbeing. In this paper, we propose to use a semantic segmentation (SS) system based on deep learning algorithms to provide environmental cues and information to visually impaired wheelchair users to aid with the navigation process. The system classifies the objects of the indoor environment and presents the annotated output on a display customised to the user's condition. The user can select a target object, for which the system can display the estimated distance from the current position of the wheelchair. The system runs in real-time, using a depth camera installed on the wheelchair, and it displays the scene in front of the wheelchair with every pixel annotated with distinguishable colour to represent the different components of the environment along with the distance to the target object. Our system has been designed, implemented and deployed on a real powered wheelchair for practical evaluation. The proposed system helped the users to estimate more accurately the distance to the target objects with a relative error of 19.8% and 18.4% for the conditions of a) semi-neglect and b) short-sightedness, respectively, compared to errors of 47.8% and 5.6% without the SS system. In our experiments, healthy participants were put in simulated conditions representing the above visual impairments using instruments commonly used in medical research for this purpose. Finally, our system helps to visualise, on the display, hidden areas of the environment and blind spots that visually impaired users would not be able to see without it.",,,,,Wheelchair; Computer science; Computer vision; Artificial intelligence; Process (computing); Object (grammar); Human–computer interaction; Task (project management); Visually impaired; Engineering; Systems engineering; World Wide Web; Operating system,,,,European Regional Development Fund,,http://dx.doi.org/10.1109/ipas55744.2022.10053051,,10.1109/ipas55744.2022.10053051,,,0,004-379-232-856-209; 006-164-429-411-873; 011-672-570-240-817; 016-865-629-176-892; 031-579-907-516-006; 033-358-073-208-173; 044-912-780-668-165; 065-852-775-788-445; 097-841-829-926-151; 101-925-778-820-461; 122-349-500-732-888; 151-839-380-579-156,1,false,,
038-091-995-640-343,"Blind aid stick: Hurdle recognition, simulated perception, android integrated voice based cooperation via GPS along with panic alert system",,2017,conference proceedings article,2017 International Conference on Nascent Technologies in Engineering (ICNTE),,IEEE,,Akshay Salil Arora; Vishakha Gaikwad,"Evolution of technology has always been endeavored with making daily life simple. With a fast paced life everybody today is harnessing the benefits of technology except some parts of the society. One of them is the visually impaired who have to rely on others for travelling and other activities. This paper aims at providing one such theoretical model which incorporates the latest technologies to provide efficient and smart electronic aid to the blind. We have used IR sensors along with ultrasonic range finder circuit for hurdle detection. Bluetooth module which along with GPS technology and an Android application for blind, will provide voice assistance to desired location and in panic situations will send SMS alert to registered mobile numbers The basic objective of the system is to provide a convenient and easy navigation aid for unsighted which helps in artificial vision by providing information about the environmental scenario of static and dynamic objects around them.",,,1,3,Technological evolution; Human–computer interaction; Engineering; Perception; Alert system; Computer security; Bluetooth; Android (operating system); Global Positioning System; Humanoid robot; Mobile telephony,,,,,https://ieeexplore.ieee.org/document/7947957/,http://dx.doi.org/10.1109/icnte.2017.7947957,,10.1109/icnte.2017.7947957,2691713013,,0,138-877-582-114-134,7,false,,
038-208-649-693-158,Navigation system for visually impaired people,,2015,conference proceedings article,"2015 International Conference on Computation of Power, Energy, Information and Communication (ICCPEIC)",,IEEE,,Chaitali Kishor Lakde; Prakash S. Prasad,"Navigation assistance for visually impaired (NAVI) refers to systems that are able to assist or guide people with vision loss, ranging from partially sighted to totally blind, by means of sound commands. Many researchers are working to assist visually impaired people in different ways like voice based assistance, ultrasonic based assistance, camera based assistance and in some advance way researchers are trying to give transplantation of real eyes with robotic eyes which can capable enough to plot the real image over patient retina using some biomedical technologies. In other way creating a fusion of sensing technology and voice based guidance system some of the products were developed which could give better result than individual technology. There are some limitation in system like obstacle detection which could not see the object but detection the object and camera based system can't work properly in different light level so the proposed system is a fusion of color sensing sensor and the obstacle sensor along with the voice based assistance system. The main idea of the proposed system to make person aware of path he is walking and also the obstacle in the path.",,,0093,0098,Engineering; Artificial intelligence; Plot (graphics); Real image; Guidance system; Obstacle; Navigation system; PATH (variable); Transplantation; Computer vision; Object (computer science),,,,,http://ieeexplore.ieee.org/document/7259447 https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7259447,http://dx.doi.org/10.1109/iccpeic.2015.7259447,,10.1109/iccpeic.2015.7259447,1531679706,,0,008-361-673-204-978; 009-690-618-758-576; 036-096-374-435-654; 061-190-872-413-287; 062-957-059-042-10X; 094-907-387-316-750; 096-803-870-115-728; 118-876-311-299-827; 124-599-469-135-595; 127-483-973-270-088; 141-489-442-115-69X; 142-528-534-500-945; 166-178-392-045-45X; 183-935-933-940-273,39,false,,
038-215-574-557-690,ICIRA (3) - Semantic Segmentation and Topological Mapping of Floor Plans,2021-10-18,2021,book chapter,Intelligent Robotics and Applications,03029743; 16113349,Springer International Publishing,Germany,Ke Liu; Ran Huang,"When visually impaired people walk in an unknown indoor environment, it is crucial to build a topological semantic map from the captured floor plan for navigation purposes. This paper proposes a topological mapping method from the floor plan model based on deep learning semantic segmentation. The topological semantic map can be used for assistive blind navigation purposes in unknown indoor environments. A deep learning network is developed for semantic segmentation, and disturbances such as image rotation, color transformation and Gaussian noises are taken into consideration in the training to enhance the robustness. With the semantic segmentation result as input, a topological semantic mapping algorithm is then proposed based on the graph theory. Experiments are presented to demonstrate the effectiveness of the proposed mapping method.",,,378,389,Deep learning; Artificial intelligence; Gaussian; Topological map; Semantic mapping; Computer vision; Computer science; Floor plan; Graph theory; Segmentation; Robustness (computer science),,,,,https://link.springer.com/chapter/10.1007/978-3-030-89134-3_35 https://doi.org/10.1007/978-3-030-89134-3_35,http://dx.doi.org/10.1007/978-3-030-89134-3_35,,10.1007/978-3-030-89134-3_35,3207116141,,0,003-622-912-291-951; 007-061-826-385-781; 007-223-103-783-187; 010-024-573-442-471; 015-339-975-520-952; 050-434-829-910-623; 051-425-281-185-199; 058-777-343-134-690; 072-627-929-080-048; 074-324-981-337-749; 076-625-999-439-462; 084-611-184-976-85X; 089-545-984-661-011; 097-352-846-189-352; 105-728-065-131-351; 109-344-937-413-608; 122-680-431-279-185; 135-257-210-494-328; 150-330-533-222-433; 153-335-030-464-82X; 171-233-043-187-425; 192-697-687-929-866,0,false,,
038-319-130-947-299,Dual Objective Based Navigation Assistance tothe Blind and Visually Impaired,,2014,journal article,International Journal of Innovative Research in Computer and Communication Engineering,23209798,,,Archana R Priyadarshini,"The visually impaired and the blind lead a challenging life. ETA’s play a vital role in their navigation .In this paper, canny edge detection, morphological operations and Bounding Box are performed for obstacle detection technique. Later depth map creation process is done. The deviation of estimated Depth map with the reference depth map can be used to get the spatial information of the environment which can be given to the visually impaired. This paper provides outdoor(i.e. by GPS modem) as well as indoor(by Machine Vision) navigation assistance by two objectives such as sound as well as voice and can detect any number of obstacles. Indoor navigation is done using Image Processing methodology and Outdoor navigation is done through the use of GPS modem.",2,5,4335,4342,Depth map; Minimum bounding box; Canny edge detector; Artificial intelligence; Mobile robot navigation; Obstacle; Computer vision; Computer science; Simulation; Machine vision; Global Positioning System; Image processing,,,,,https://www.rroij.com/open-access/dual-objective-based-navigation-assistance-tothe-blind-and-visually-impaired.pdf https://www.rroij.com/open-access/dual-objective-based-navigation-assistance-tothe-blind-and-visually-impaired-.php?aid=46169,https://www.rroij.com/open-access/dual-objective-based-navigation-assistance-tothe-blind-and-visually-impaired-.php?aid=46169,,,2187724894,,0,008-214-681-885-646; 013-056-435-015-274; 016-554-897-381-94X; 017-446-312-279-885; 028-723-698-021-212; 053-583-192-599-759; 056-167-810-394-870; 057-890-353-263-937; 064-309-126-323-022; 088-551-209-391-094; 096-803-870-115-728; 120-413-407-974-701; 135-442-888-799-760; 153-471-712-074-630,2,false,,
038-319-648-870-089,Nanofibril scaffold assisted MEMS artificial hydrogel neuromasts for enhanced sensitivity flow sensing.,2016-01-14,2016,journal article,Scientific reports,20452322,Springer Science and Business Media LLC,United Kingdom,Ajay Giri Prakash Kottapalli; Meghali Bora; Mohsen Asadnia; Jianmin Miao; Subbu S. Venkatraman; Michael S. Triantafyllou,"We present the development and testing of superficial neuromast-inspired flow sensors that also attain high sensitivity and resolution through a biomimetic hyaulronic acid-based hydrogel cupula dressing. The inspiration comes from the spatially distributed neuromasts of the blind cavefish that live in completely dark undersea caves; the sensors enable the fish to form three-dimensional flow and object maps, enabling them to maneuver efficiently in cluttered environments. A canopy shaped electrospun nanofibril scaffold, inspired by the cupular fibrils, assists the drop-casting process allowing the formation of a prolate spheroid-shaped artificial cupula. Rheological and nanoindentation characterizations showed that the Young’s modulus of the artificial cupula closely matches the biological cupula (10–100 Pa). A comparative experimental study conducted to evaluate the sensitivities of the naked hair cell sensor and the cupula-dressed sensor in sensing steady-state flows demonstrated a sensitivity enhancement by 3.5–5 times due to the presence of hydrogel cupula. The novel strategies of sensor development presented in this report are applicable to the design and fabrication of other biomimetic sensors as well. The developed sensors can be used in the navigation and maneuvering of underwater robots, but can also find applications in biomedical and microfluidic devices.",6,1,19336,19336,Biomedical engineering; Biomimetics; Materials science; Prolate spheroid; Sensitivity (control systems); Enhanced sensitivity; Microelectromechanical systems; Bioinformatics; Flow (psychology); Microfluidics; Scaffold,,"Animals; Biomimetics; Biosensing Techniques; Elastic Modulus; Fishes; Hydrodynamics; Hydrogel, Polyethylene Glycol Dimethacrylate/chemistry; Micro-Electrical-Mechanical Systems; Nanofibers/chemistry","Hydrogel, Polyethylene Glycol Dimethacrylate",,http://dspace.mit.edu/handle/1721.1/101891 https://dx.doi.org/10.1038/srep19336 https://core.ac.uk/display/80938705 https://ui.adsabs.harvard.edu/abs/2016NatSR...619336K/abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4725914/ https://www.nature.com/articles/srep19336.pdf https://www.nature.com/articles/srep19336 http://dx.doi.org/10.1038/srep19336 https://researchers.mq.edu.au/en/publications/nanofibril-scaffold-assisted-mems-artificial-hydrogel-neuromasts- http://europepmc.org/articles/PMC4725914 https://dr.ntu.edu.sg/handle/10220/46699,http://dx.doi.org/10.1038/srep19336,26763299,10.1038/srep19336,2239268002,PMC4725914,1,005-091-841-462-372; 009-884-098-210-990; 010-066-596-574-165; 010-806-874-159-596; 011-870-491-880-629; 012-656-394-613-990; 017-433-107-889-622; 023-673-726-815-649; 024-124-534-530-985; 027-065-593-144-939; 027-422-387-233-546; 027-832-755-605-836; 028-556-317-384-314; 029-312-579-856-321; 029-655-844-641-087; 032-621-647-172-608; 036-034-817-916-577; 036-683-841-830-95X; 038-760-986-670-933; 040-285-044-786-598; 042-490-551-008-948; 045-682-785-605-943; 049-368-634-299-571; 051-327-098-854-746; 054-445-842-896-554; 057-596-301-485-354; 058-621-101-650-341; 059-501-664-211-510; 065-332-459-604-326; 065-989-577-074-940; 076-522-386-603-635; 076-597-070-505-373; 079-521-061-003-716; 086-146-553-346-776; 087-289-322-467-911; 095-825-045-251-210; 103-064-327-861-169; 110-277-706-889-092; 118-978-860-174-063; 122-859-837-628-269; 123-268-100-189-515; 124-678-178-454-014; 129-648-387-845-446; 129-839-083-305-813; 130-635-508-487-191; 153-634-135-833-257,90,true,"CC BY, CC BY-NC-ND",gold
038-326-446-864-583,An Astute Assistive Device for Mobility and Object Recognition for Visually Impaired People,,2019,journal article,IEEE Transactions on Human-Machine Systems,21682291; 21682305,Institute of Electrical and Electronics Engineers (IEEE),United States,Vidula Meshram; Kailas Patil; Vishal Meshram; Felix Che Shu,"To provide autonomous navigation and orientation to visually impaired people, this article proposes a new electronic assistive device called the NavCane. The device helps people find obstacle-free paths in both indoor and outdoor settings. The NavCane also aids in the recognition of objects in an indoor setting. The advantage of the NavCane device is that it provides priority information about obstacles in the path without causing information overload. The priority information deduced by the system is transmitted to the user using tactile and auditory communication methods. Unlike existing electronic travel assistance systems which are limited to obstacle detection and path finding, the NavCane also helps users by recognizing objects in known indoor settings. The developed prototype is low cost and as a low power embedded device for obstacle detection and obstacle identification, it is an alternative to machine vision systems. It has a radio-frequency identification reader, ultrasonic sensors, a global system for mobile communication module, a global positioning system module, vibration motors, a gyroscope, a wet floor sensor, and a battery. To test the usefulness of the NavCane in mundane commuting, object recognition, and rehabilitation for visually impaired people, we assessed it with the help of 80 visually impaired people from a blind school and a home for elderly people. All the assessments were executed in controlled indoor and outdoor test environments with both a NavCane and a white cane. The experimental results show that the NavCane is an effective device for detecting of obstacles, ascending and descending staircases, navigating wet floors, and object recognition in environments that are known and unknown to the user. In addition, our evaluation results indicate that the NavCane improves the performance of obstacle-free navigation compared to a white cane.",49,5,449,460,Human–computer interaction; Information overload; Obstacle; Home for elderly; Visualization; Computer science; Global Positioning System; Cognitive neuroscience of visual object recognition; Identification (information); Mobile telephony,,,,,https://ieeexplore.ieee.org/document/8801898/ https://dblp.uni-trier.de/db/journals/thms/thms49.html#MeshramPMS19 https://doi.org/10.1109/THMS.2019.2931745,http://dx.doi.org/10.1109/thms.2019.2931745,,10.1109/thms.2019.2931745,2968097540,,0,006-917-922-806-186; 012-006-431-706-99X; 013-847-555-426-159; 017-334-081-311-633; 022-697-627-694-941; 024-627-299-891-290; 024-964-397-377-997; 026-015-545-733-278; 030-127-369-356-741; 031-160-271-128-821; 032-661-597-680-28X; 036-829-576-592-290; 037-491-194-003-965; 043-279-114-043-81X; 049-878-204-983-752; 058-443-047-291-990; 061-933-888-557-079; 064-309-126-323-022; 064-521-070-547-235; 066-037-919-140-345; 066-246-145-459-483; 066-622-591-843-254; 070-590-602-617-000; 084-824-110-981-874; 096-803-870-115-728; 109-525-288-959-67X; 114-278-670-675-93X; 135-996-783-502-179,76,false,,
038-528-106-733-734,Embodied architecture design : On people-centered design of visuo-locomotive cognitive experiences,2018-08-23,2018,journal article,Cognitive processing,16124790; 16124782,Springer Science and Business Media LLC,Germany,Mehul Bhatt,"This presentation focusses on the analysis and design of human-centered, embodied, cognitive user experiences from the perspectives of spatial cognition and computation, artificial intelligence, an ...",19,Suppl 1,34,67,Embodied cognition; Psychology; Cognition; Cognitive science; Presentation; Spatial cognition; Architecture design; Cognitive user; Perception; Training (civil); Action (philosophy); Coaching; Conversation analysis; Athletes; Visual impairment; Climbing; Physical medicine and rehabilitation; Film theory; Interface (Java); Context (language use); Media studies; Human–computer interaction; Shopping mall; Virtual model; Computer science; Image (mathematics); Cognitive psychology; Qualitative research; Movement (music); Confluence; Structure (mathematical logic); Style (sociolinguistics); Home environment; Mental rotation; Tracing; Personality factors; Developmental psychology; Age related; Space (commercial competition); SIMMON; Decision points; Affect (psychology); Theoretical computer science; Path (graph theory); Neuroscience,,,,,http://www.diva-portal.org/smash/record.jsf?pid=diva2:1246885,http://dx.doi.org/10.1007/s10339-018-0884-3,30141090,10.1007/s10339-018-0884-3,3006387761; 2899426951; 2899180340; 2941731638; 2899199117; 3043796959; 2919853709; 2898981178; 3108284018; 3124366525; 2889723454; 2899344632,,0,,11,true,,green
038-529-429-962-974,Review on LiDAR-Based Navigation Systems for the Visually Impaired,2023-04-11,2023,journal article,SN Computer Science,26618907; 2662995x,Springer Science and Business Media LLC,,Mayuri Jain; Warish Patel,,4,4,,,Lidar; Visually impaired; Computer science; Field (mathematics); Navigation system; Human–computer interaction; Artificial intelligence; Remote sensing; Geography; Mathematics; Pure mathematics,,,,,,http://dx.doi.org/10.1007/s42979-023-01735-y,,10.1007/s42979-023-01735-y,,,0,010-371-675-659-125; 013-501-956-117-041; 020-373-201-301-684; 025-764-110-104-300; 034-028-229-282-033; 035-363-747-813-775; 051-282-858-761-782; 058-013-370-483-44X; 065-553-420-193-637; 087-732-317-182-149; 094-689-032-052-829; 110-831-712-365-194; 111-772-107-673-112; 118-540-529-844-833; 143-676-420-241-712; 147-339-380-863-434; 151-729-719-938-22X; 152-283-069-199-842; 176-339-454-003-376,8,false,,
038-718-341-933-688,On the semantics of object-oriented landmark recognition,2012-03-27,2012,journal article,Pattern Recognition and Image Analysis,10546618; 15556212,Pleiades Publishing Ltd,United States,Eckart Michaelsen; K. Jäger; D. Roschkowski; L. Doktorski; Michael Arens,,22,1,44,53,Human–computer interaction; Pattern recognition (psychology); Artificial intelligence; Semantics (computer science); Structure (mathematical logic); Landmark; Salient; Hallucinating; Computer science; Probabilistic logic; Object-oriented programming,,,,,https://link.springer.com/content/pdf/10.1134%2FS1054661812010270.pdf https://link.springer.com/article/10.1134/S1054661812010270,http://dx.doi.org/10.1134/s1054661812010270,,10.1134/s1054661812010270,2043673917,,0,001-393-030-870-573; 004-861-251-500-609; 010-996-437-918-603; 024-295-505-134-963; 025-748-967-357-550; 040-626-546-480-866; 048-381-833-662-29X; 056-217-932-345-519; 059-930-393-216-111; 068-655-710-732-471; 077-966-742-446-626; 093-070-307-802-620; 093-788-082-926-771; 101-280-654-413-826; 117-915-045-084-710; 119-801-597-064-088; 153-915-900-248-989; 181-288-006-356-294,3,false,,
038-830-538-463-958,Tactual displays for wearable computing,,1997,journal article,Personal Technologies,09492054; 16174917,Springer Science and Business Media LLC,,Hong Z. Tan; Alex Pentland,,1,4,225,230,Human–computer interaction; Mobile computing; Wearable computer; Geometric pattern; Computer science; Sensory system,,,,,https://dx.doi.org/10.1007/BF01682025 https://link.springer.com/article/10.1007/BF01682025 https://dblp.uni-trier.de/db/journals/puc/puc1.html#TanP97 http://dx.doi.org/10.1007/BF01682025,http://dx.doi.org/10.1007/bf01682025,,10.1007/bf01682025,3021341353,,10,017-138-975-350-457; 034-359-419-027-582; 080-803-430-855-617; 094-274-735-709-079; 098-116-610-021-176; 100-921-219-584-031; 114-777-477-981-296; 125-275-740-243-004; 141-898-369-927-997; 189-096-453-967-235,82,false,,
038-877-611-717-132,"An efficient, dense and long-range marker system for the guidance of the visually impaired",2020-08-18,2020,journal article,Machine Vision and Applications,09328092; 14321769,Springer Science and Business Media LLC,Germany,Juan Manuel Sáez; Miguel Angel Lozano; Francisco Escolano; Javier Pita Lozano,"In this paper, we address the problem of making a mobile/smartphone camera sensitive to distant fiducial markers. To this end, we carefully design a novel visual marker that is both dense and readable from large distances. The main novelty of the proposed marker is the combination of a quaternary color-based coding system with robust methods for reading the color patterns included in each frame once it is detected. These patterns include a CRC whose length grows linearly, whereas that of the message grows quadratically. Our experiments show that the proposed bundle marker-vision algorithm outperforms the alternatives in terms of distance and angle and also that it is very robust to changes in lighting conditions, thus making it a good intelligent system for guiding people with visual impairments in their day to day use of public transportation systems.",31,7,1,10,Frame (networking); Pattern recognition (psychology); Artificial intelligence; Fiducial marker; Range (mathematics); Bundle; Novelty; Visually impaired; Computer vision; Computer science; Reading (process),,,,Ministerio de Economía y Competitividad; Instituto de Fomento de la Región de Murcia; Ministerio de Economía y Competitividad,http://rua.ua.es/dspace/handle/10045/109040 https://rua.ua.es/dspace/bitstream/10045/109040/5/Saez_etal_2020_MachineVisionApplicat_preprint.pdf https://link.springer.com/article/10.1007/s00138-020-01097-y https://dblp.uni-trier.de/db/journals/mva/mva31.html#SaezLEL20,http://dx.doi.org/10.1007/s00138-020-01097-y,,10.1007/s00138-020-01097-y,3071393346,,0,048-446-984-045-429; 048-929-432-459-187; 062-370-727-285-236; 087-368-473-909-76X; 104-798-809-774-155; 123-468-327-110-468; 151-790-631-478-110; 173-428-838-013-421; 177-752-447-398-90X,1,true,,green
038-987-143-577-008,Spring 2012 Colloquium,,2012,,,,,,YingLi Tian,"Recent technology developments in computer vision, digital cameras, and portable computers make it possible to develop practical computer vision-based algorithms to help blind persons independently explore unfamiliar environments and improve the quality of their daily life. In this talk, I will introduce the research conducted in CCNY Media Lab for applying computer vision technologies to assist people who are visually impaired including indoor navigation and wayfinding, banknote recognition, and clothing pattern recognition.",,,,,Engineering; Pattern recognition (psychology); Quality (business); Banknote; Blind persons; Visually impaired; Multimedia; Clothing,,,,,,,,,2185430824,,0,,0,false,,
039-228-966-267-018,Real-Time Sign Detection for Accessible Indoor Navigation.,,2021,journal article,Journal on technology and persons with disabilities : ... Annual International Technology and Persons with Disabilities Conference,23304219,,United States,Seyed Ali Cheraghi; Giovanni Fusco; James M. Coughlan,"Indoor navigation is a major challenge for people with visual impairments, who often lack access to visual cues such as informational signs, landmarks and structural features that people with normal vision rely on for wayfinding. We describe a new approach to recognizing and analyzing informational signs, such as Exit and restroom signs, in a building. This approach will be incorporated in iNavigate, a smartphone app we are developing, that provides accessible indoor navigation assistance. The app combines a digital map of the environment with computer vision and inertial sensing to estimate the user's location on the map in real time. Our new approach can recognize and analyze any sign from a small number of training images, and multiple types of signs can be processed simultaneously in each video frame. Moreover, in addition to estimating the distance to each detected sign, we can also estimate the approximate sign orientation (indicating if the sign is viewed head-on or obliquely), which improves the localization performance in challenging conditions. We evaluate the performance of our approach on four sign types distributed among multiple floors of an office building.",9,,125,139,Sensory cue; Frame (networking); Sign (mathematics); Artificial intelligence; Information sign; Navigation assistance; Sign detection; Smartphone app; Computer vision; Computer science; Orientation (computer vision),Accessibility; Blindness; Low Vision; Navigation; Visually Impaired; Wayfinding,,,ACL HHS (90RE5024) United States; NEI NIH HHS (R01 EY029033) United States,https://www.ncbi.nlm.nih.gov/pubmed/34350305,https://www.ncbi.nlm.nih.gov/pubmed/34350305,34350305,,3190648415,PMC8331194,0,005-642-690-871-424; 020-322-728-476-317; 034-222-058-985-816,2,true,,unknown
039-365-451-848-535,Automatic switching between speech and non-speech: adaptive auditory feedback in desktop assistance for the visually impaired,2019-10-16,2019,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Muhammad Shoaib; Ibrar Hussain; Hamid Turab Mirza,,19,4,813,823,Human–computer interaction; Social relation; Auditory feedback; Visual impairment; Computer communication networks; Task completion; Action awareness; Visually impaired; Computer science,,,,"Higher Education Commission, Pakistan",https://link.springer.com/article/10.1007/s10209-019-00696-5 https://dblp.uni-trier.de/db/journals/uais/uais19.html#ShoaibHM20 https://doi.org/10.1007/s10209-019-00696-5,http://dx.doi.org/10.1007/s10209-019-00696-5,,10.1007/s10209-019-00696-5,2981307579,,0,001-500-232-053-716; 003-959-914-086-066; 005-528-943-687-045; 006-748-214-673-904; 010-506-038-184-791; 011-679-964-026-452; 013-185-503-943-920; 014-984-968-440-511; 015-978-126-687-61X; 016-068-376-695-077; 024-093-010-731-859; 026-122-730-982-43X; 030-875-763-516-321; 030-981-598-132-584; 037-057-813-299-303; 037-318-752-257-857; 045-599-200-378-144; 046-983-906-128-801; 049-293-834-043-374; 050-887-509-736-365; 052-327-947-140-876; 053-458-084-921-840; 053-794-028-526-128; 055-323-413-595-244; 056-092-375-535-400; 056-149-199-381-856; 057-083-293-597-247; 060-172-836-014-347; 061-611-750-218-70X; 064-557-621-468-053; 073-407-269-569-770; 080-162-454-888-432; 086-346-817-733-742; 087-139-068-243-205; 100-082-897-349-538; 101-737-585-999-487; 104-615-442-592-026; 125-892-623-875-209; 128-636-187-564-08X; 138-660-124-562-223; 142-780-526-545-11X; 151-430-767-813-177; 172-245-959-896-429; 194-046-489-693-526,4,false,,
039-383-177-065-800,Exploiting social partners in robot learning,2010-07-07,2010,journal article,Autonomous Robots,09295593; 15737527,Springer Science and Business Media LLC,Netherlands,Maya Cakmak; Nick DePalma; Rosa I. Arriaga; Andrea L. Thomaz,,29,3,309,329,Observational learning; Human–computer interaction; Imitative learning; Artificial intelligence; Robot learning; Synchronous learning; Cognitive imitation; Experiential learning; Computer science; Cooperative learning; Social learning,,,,,https://dl.acm.org/doi/10.1007/s10514-010-9197-9 http://core.ac.uk/display/4750787 https://www.cc.gatech.edu/social-machines/papers/cakmak10_auro_social.pdf http://dx.doi.org/10.1007/s10514-010-9197-9 https://link.springer.com/article/10.1007/s10514-010-9197-9 https://dblp.uni-trier.de/db/journals/arobots/arobots29.html#CakmakDAT10 https://dx.doi.org/10.1007/s10514-010-9197-9,http://dx.doi.org/10.1007/s10514-010-9197-9,,10.1007/s10514-010-9197-9,2049323762,,0,001-776-574-595-256; 003-978-064-785-082; 013-216-514-323-507; 017-241-108-806-87X; 017-611-503-753-021; 018-244-819-438-217; 018-635-872-077-682; 018-880-621-748-970; 021-454-501-063-041; 023-399-291-021-620; 023-928-444-110-407; 029-360-116-636-572; 032-756-035-163-119; 033-302-839-149-486; 036-565-959-520-36X; 036-761-028-781-420; 037-767-936-701-813; 040-074-800-562-646; 041-570-427-629-978; 042-781-618-906-41X; 058-175-271-162-837; 058-312-759-859-660; 065-041-623-854-598; 068-711-073-877-603; 094-190-096-594-042; 094-738-913-897-838; 095-088-995-942-296; 095-382-017-778-795; 102-701-829-108-92X; 103-854-178-667-181; 107-390-229-971-232; 111-608-853-695-482; 113-719-046-550-302; 115-213-241-178-790; 126-033-011-353-726; 129-664-851-810-563; 132-967-294-463-103; 136-304-038-583-729; 137-320-213-389-496; 140-116-225-706-68X; 147-276-036-034-770; 153-726-731-458-236; 163-479-403-050-247; 168-164-123-915-440; 172-573-079-472-196; 176-842-001-783-46X; 185-104-217-369-69X; 194-544-558-766-859,37,false,,
039-389-642-291-14X,Design and Research on Guide Blind Device Based on User Experience,2020-09-29,2020,book chapter,Man-Machine-Environment System Engineering,18761100; 18761119,Springer Singapore,Germany,Zhang Jianyi; Shi Xinyu; Xinqin Jin; Li Fengfeng; Xin Chen,"Facing the problem that the existing guide blind devices cannot fully meet the needs of visually impaired users, taking the guide blind device as study object, basing on user experience theory, studies the needs of visually impaired users and the future design direction of guide blind devices. Conduct user interviews and external observation experiments for visually impaired users, build user emotional experience maps, understand the road factors that affect visually impaired travel and the true needs of visually impaired users for guide blind devices. Transform user needs into practical and feasible function implementation directions, and explore suitable functions and interaction methods of guiding blind devices. The research results show that the future guide equipment should meet the needs of users at multiple levels and achieve simple operation, lightweight volume, and interactive humanization.",,,1029,1036,Human–computer interaction; User experience design; User needs; Visually impaired; Computer science; Function (engineering),,,,,https://link.springer.com/chapter/10.1007/978-981-15-6978-4_118 https://link.springer.com/content/pdf/10.1007%2F978-981-15-6978-4_118.pdf,http://dx.doi.org/10.1007/978-981-15-6978-4_118,,10.1007/978-981-15-6978-4_118,3091398126,,0,027-698-865-547-337; 037-437-923-022-326; 041-823-170-427-088; 045-516-564-886-186; 054-879-961-055-393; 075-519-293-886-898; 084-031-713-999-539; 100-766-731-883-498; 106-484-868-965-067; 143-411-709-737-731,0,false,,
039-441-335-410-36X,Revolutionizing Blind Navigation through AI Voices,2024-05-01,2024,journal article,International Journal of Innovative Research in Engineering,25828746,Fifth Dimension Research Publications,,Maheswari J; Sowmiya B; Sowmiya K; Thirisha S; Vishnupriya G,"<jats:p>The project titled ""Revolutionizing Blind Navigation through AI Voices"" presents a novel approach to aid visually impaired individuals in navigating their surroundings independently and safely. By harnessing the power of artificial intelligence (AI) and voice technology, our system provides real-time guidance and assistance to users. Through the integration of advanced object detection algorithms, such as YOLO (You Only Look Once), our solution enables accurate detection and recognition of various objects in the environment. The detected objects are then translated into audio instructions, delivered through AI-generated voices, to help users understand their surroundings and navigate effectively. Moreover, the system is seamlessly integrated with a web interface using Flask, allowing for remote control and interaction. Our project contributes to the advancement of assistive technologies for the visually impaired, offering a user-friendly and reliable solution for enhanced mobility and independence.</jats:p>",,,303,307,Computer science,,,,,,http://dx.doi.org/10.59256/ijire.20240502042,,10.59256/ijire.20240502042,,,0,,0,false,,
039-444-900-531-951,"CARS 2022—Computer Assisted Radiology and Surgery Proceedings of the 36th International Congress and Exhibition Tokyo, Japan, June 7–11, 2022",2022-05-25,2022,journal article,International Journal of Computer Assisted Radiology and Surgery,18616429; 18616410,Springer Science and Business Media LLC,Germany,,"Reflections on the CARS 2022 Theme of ''Intelligent Technologies for Precision Diagnosis and Therapy'' and on Model Guided MedicineEvery year on occasion of the CARS Congress, we are facing very similar questions, such as: 1.What are the issues in health care and specifically in the domain of CARS and what previous work can be built on? 2. Why are new concepts needed?3. How can possible new concepts, for example, towards a Model Guided Medicine (MGM), be realised?4. Who are the stakeholders and beneficiaries of possible MGM realisations? 5.When can the different stages of MGM solution concepts be realized?In the following Preface of the IJCARS proceedings, an attempt is being made to address and to shine some light on the above questions in the context of the scope of IJCARS and the CARS 2022 Congress in Tokyo and beyond.Int J CARS (2022) 17 (Suppl 1):S1-S147",17,S1,1,147,Exhibition; Medicine; General surgery; Medical physics; Gerontology; Art history; History,,,,,https://link.springer.com/content/pdf/10.1007/s11548-022-02635-x.pdf https://doi.org/10.1007/s11548-022-02635-x,http://dx.doi.org/10.1007/s11548-022-02635-x,,10.1007/s11548-022-02635-x,,,0,,3,true,,bronze
039-597-217-579-464,A Blur-SURE-Based Approach to Kernel Estimation for Motion Deblurring,2019-06-17,2019,journal article,Pattern Recognition and Image Analysis,10546618; 15556212,Pleiades Publishing Ltd,United States,Jing Li,,29,2,240,251,Mean squared error; Artificial intelligence; Deconvolution; Wiener filter; Deblurring; Motion blur; Computer vision; Kernel (image processing); Computer science; Kernel density estimation; Image processing,,,,,https://dl.acm.org/doi/10.1134/S1054661819010164 https://link.springer.com/content/pdf/10.1134%2FS1054661819010164.pdf https://link.springer.com/article/10.1134%2FS1054661819010164,http://dx.doi.org/10.1134/s1054661819010164,,10.1134/s1054661819010164,2949389262,,0,009-273-095-819-618; 009-943-137-532-333; 014-825-182-604-061; 017-928-327-742-707; 019-082-028-766-211; 020-789-787-255-281; 023-818-755-614-840; 025-961-197-939-919; 030-484-573-252-283; 035-734-779-742-166; 038-609-532-994-616; 048-271-447-668-880; 049-148-350-841-512; 055-875-369-089-859; 059-135-988-645-377; 059-458-646-431-869; 060-989-075-894-306; 063-273-669-549-628; 063-810-742-684-078; 070-782-557-015-890; 074-371-884-935-220; 076-652-370-404-047; 079-188-651-567-232; 083-366-896-780-334; 083-781-768-661-654; 083-791-643-309-779; 089-116-090-132-019; 089-814-321-600-683; 091-061-049-086-352; 099-887-414-014-093; 100-210-615-680-825; 103-705-491-285-762; 113-608-678-694-96X; 119-959-351-961-168; 123-059-196-194-931; 124-858-613-126-41X; 127-511-557-118-760; 133-884-931-657-481; 137-286-857-915-803; 140-065-464-159-321; 141-339-895-303-585; 141-895-302-880-473; 147-425-178-361-845; 165-465-087-696-120; 166-767-687-045-233; 169-227-601-108-021,1,false,,
039-853-000-175-960,Design and implementation of an educational game considering issues for visually impaired people inclusion,2020-01-10,2020,journal article,Smart Learning Environments,21967091,Springer Science and Business Media LLC,,Luiz Valério Neto; Paulo Henrique Fontoura; Rogério Augusto Bordini; Joice Lee Otsuka; Delano Medeiros Beder,"In recent years there has been an increase in research focusing on the effectiveness of using video games as educational digital resources that can contribute to the learning process at different levels, which has also subsidised the development of educational games. However, these games are mostly visual and not accessible to people with visual impairments. As an educational resource, it is essential that the design of educational games be conducted not only with a focus on the balance between playful and educational aspects, but also with a focus on including the largest number of people. This article, therefore, aims to describe the design, implementation and evaluation processes of an accessible version of the educational game Em Busca do Santo Grau, based on EduGameAccess – a set of recommendations that integrates educational, playability and accessibility aspects for people with visual impairments.",7,1,1,16,Inclusion (education); Psychology; Set (psychology); Educational game; Educational resources; Digital resources; Visually impaired; Focus (computing); Process (engineering); Applied psychology,,,,,https://slejournal.springeropen.com/articles/10.1186/s40561-019-0103-4 https://doi.org/10.1186/s40561-019-0103-4 https://link.springer.com/content/pdf/10.1186/s40561-019-0103-4.pdf https://link.springer.com/article/10.1186/s40561-019-0103-4 https://dblp.uni-trier.de/db/journals/sle/sle7.html#NetoJBOB20,http://dx.doi.org/10.1186/s40561-019-0103-4,,10.1186/s40561-019-0103-4,3013711696,,0,003-485-284-705-210; 030-836-841-055-718; 031-445-197-810-898; 056-785-290-779-280; 072-359-347-978-247; 072-958-947-849-451; 087-452-847-569-31X; 117-131-791-199-573; 117-855-876-226-546; 126-007-292-397-020; 126-858-629-361-222; 138-148-139-576-473; 138-293-790-502-159; 163-492-242-581-149; 194-825-643-560-776,13,true,cc-by,gold
040-004-702-906-51X,An enhanced text detection technique for the visually impaired to read text,2016-09-20,2016,journal article,Information Systems Frontiers,13873326; 15729419,Springer Science and Business Media LLC,Netherlands,S. P. Faustina Joan; S. Valli,,19,5,1039,1056,AdaBoost; Artificial intelligence; Histogram of oriented gradients; Pattern recognition; Computer science; C4.5 algorithm; Feature (computer vision); Pyramid (image processing); Feature vector; Decision tree; Maximally stable extremal regions,,,,,https://link.springer.com/article/10.1007/s10796-016-9699-x https://ideas.repec.org/a/spr/infosf/v19y2017i5d10.1007_s10796-016-9699-x.html https://doi.org/10.1007/s10796-016-9699-x https://dblp.uni-trier.de/db/journals/isf/isf19.html#JoanV17 http://dblp.uni-trier.de/db/journals/isf/isf19.html#JoanV17,http://dx.doi.org/10.1007/s10796-016-9699-x,,10.1007/s10796-016-9699-x,2521748545,,0,000-424-243-390-605; 003-062-844-426-208; 003-422-049-970-833; 004-766-408-593-108; 006-917-922-806-186; 007-975-066-276-195; 011-138-607-318-885; 012-098-482-977-213; 013-035-213-027-859; 013-107-381-709-939; 013-617-471-416-275; 017-656-390-558-009; 017-901-534-952-205; 018-857-357-001-433; 019-669-475-341-760; 020-594-432-429-086; 020-706-181-739-971; 022-787-516-343-160; 024-605-466-691-126; 025-291-742-510-860; 026-039-073-216-921; 026-287-947-734-205; 029-061-861-419-041; 029-571-852-301-56X; 031-967-529-225-926; 033-046-578-767-111; 033-125-857-673-092; 038-275-115-980-363; 041-566-224-917-775; 041-754-772-638-492; 053-058-977-099-038; 053-730-729-535-024; 060-971-273-896-77X; 063-995-730-775-355; 064-088-973-362-974; 069-304-109-881-11X; 074-324-981-337-749; 076-948-397-104-039; 086-818-192-257-557; 089-731-195-432-880; 094-199-667-687-471; 098-283-568-177-14X; 114-888-284-970-47X; 125-452-392-794-583; 137-320-213-389-496; 140-895-288-214-960; 151-205-906-804-115; 177-912-501-207-482; 194-960-478-337-184,16,false,,
040-086-249-347-765,Design of barrier free travel system for the visually impaired,2021-05-28,2021,conference proceedings article,2021 IEEE International Conference on Artificial Intelligence and Industrial Design (AIID),,IEEE,,Chen Shenjin; Zhan Chuanchun,"The use of intelligent perception technology and mobile Internet technology allows visually impaired people to perceive their surroundings through smart phone, and integrates intelligent perception, precise positioning and navigation, virtual blind roads and cloud services to provide barrier-free travel services for visually impaired people. Through the deployment of the blind information infrastructure to provide the visually impaired with real-time bus travel and guidance services, the experimental results show that the method of intelligent sensing and processing technology for the visually impaired is innovative and effectively solves the problem. The problem of autonomous travel of the visually impaired has important practical significance to enhance the travel experience of the visually impaired in a typical travel place.",,,,,Software deployment; Human–computer interaction; Perception; Smart phone; Mobile internet; Travel services; Visually impaired; Information infrastructure; Computer science; Cloud computing,,,,Guangdong Science and Technology Research Plan,http://xplorestaging.ieee.org/ielx7/9456450/9456451/09456567.pdf?arnumber=9456567 http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=9456567,http://dx.doi.org/10.1109/aiid51893.2021.9456567,,10.1109/aiid51893.2021.9456567,3175835988,,0,078-110-903-949-071; 145-087-817-109-709,0,false,,
040-416-017-338-383,TransEffiVisNet – an image captioning architecture for auditory assistance for the visually impaired,2024-08-23,2024,journal article,Multimedia Tools and Applications,15737721; 13807501,Springer Science and Business Media LLC,Netherlands,Harshitha R; Lakshmipriya B; Vallidevi Krishnamurthy,,,,,,,,,,,,http://dx.doi.org/10.1007/s11042-024-20036-x,,10.1007/s11042-024-20036-x,,,0,011-245-784-875-854; 014-465-009-961-165; 015-002-013-293-070; 037-965-005-631-252; 047-695-238-380-568; 048-683-153-209-970; 051-915-945-716-485; 053-700-234-472-626; 060-297-744-874-883; 060-836-818-792-304; 068-531-315-403-18X; 069-480-623-909-181; 074-315-241-834-681; 075-675-028-905-212; 096-971-264-708-34X; 099-628-218-933-379; 129-215-473-312-060; 144-809-633-990-912; 147-022-564-211-101; 163-837-935-935-772; 171-189-370-546-346; 185-616-751-096-539; 190-374-352-003-237; 191-306-047-040-375; 199-847-148-267-897; 199-970-788-377-860,0,false,,
040-521-394-212-756,The control of locomotion when vision is reduced or missing,,1991,journal article,Adaptability of Human Gait - Implications for the Control of Locomotion,01664115,Elsevier,Netherlands,Gunnar Jansson,,78,,333,357,Human–computer interaction; Haptic technology; Psychology; Human echolocation; Peripheral vision; Perception; Vision disorder; Communication; Sound localization; Identification (information); Adaptation (computer science),,,,,https://www.sciencedirect.com/science/article/abs/pii/S0166411508607487 https://www.sciencedirect.com/science/article/pii/S0166411508607487#!,http://dx.doi.org/10.1016/s0166-4115(08)60748-7,,10.1016/s0166-4115(08)60748-7,26846930,,0,002-018-804-147-816; 005-366-603-685-423; 006-951-527-221-208; 006-956-729-410-739; 008-699-608-597-718; 012-613-947-324-057; 013-676-346-750-020; 014-015-592-708-563; 015-476-928-897-846; 020-585-547-019-445; 022-451-509-729-605; 023-805-265-682-303; 026-048-436-398-74X; 026-487-079-058-070; 030-603-300-377-143; 031-696-287-727-760; 031-732-949-639-838; 032-673-891-617-482; 037-228-290-519-744; 038-701-910-154-502; 040-379-539-078-726; 040-436-249-619-801; 042-417-680-035-896; 044-356-991-436-714; 049-813-454-087-275; 053-393-602-843-041; 054-999-963-477-65X; 056-508-377-103-499; 059-737-477-705-831; 059-790-375-568-242; 061-405-445-106-456; 063-367-551-819-079; 063-502-048-341-310; 063-523-491-864-813; 064-524-510-925-302; 066-297-751-248-221; 069-295-460-897-563; 069-849-702-694-040; 073-887-428-945-848; 074-086-205-822-708; 088-649-730-609-803; 090-758-269-328-935; 091-703-645-244-267; 093-461-303-846-878; 094-763-081-817-982; 098-116-610-021-176; 102-555-702-781-751; 105-370-958-808-282; 110-184-795-861-813; 120-818-636-822-918; 122-658-452-595-434; 122-678-886-400-129; 135-646-014-794-306; 138-043-249-236-372; 147-739-709-420-130; 156-827-733-532-673; 173-276-340-664-163; 175-683-777-173-843; 183-935-933-940-273,1,false,,
040-646-180-311-790,Camera Based Indoor Object Detection and Distance Estimation Framework for Assistive Mobility,2022-12-02,2022,conference proceedings article,"2022 IEEE International Conference on Service Operations and Logistics, and Informatics (SOLI)",,IEEE,,Vivek Kumar Paswan; Ayesha Choudhary,"In this paper, we propose a novel, real-time, deep learning, and computer vision-based indoor object detection and distance estimation framework for assistive mobility. Blind and visually impaired people find it difficult to deal with indoor objects and their location in their daily life. The emerging deep learning technologies can help them to do this task efficiently and conveniently by providing them with information about indoor objects present in their surroundings. In our proposed framework, we have trained the recent YOLOv7 model for indoor object detection. We have used the bounding box parameters to estimate the distance of the detected object from the user. The information on detected objects and estimated distance have been provided to visually impaired users through audio feedback. Our proposed framework will assist visually impaired people in making informed decisions and make them more confident and better prepared during their navigation in an indoor environment.",,,,,Computer science; Object (grammar); Task (project management); Artificial intelligence; Object detection; Bounding overwatch; Computer vision; Minimum bounding box; Estimation; Deep learning; Human–computer interaction; Image (mathematics); Pattern recognition (psychology); Engineering; Systems engineering,,,,,,http://dx.doi.org/10.1109/soli57430.2022.10294458,,10.1109/soli57430.2022.10294458,,,0,004-269-574-716-057; 008-927-524-692-138; 012-370-328-340-986; 015-601-294-792-734; 021-326-533-295-625; 023-309-209-821-636; 048-254-076-918-785; 049-317-239-158-314; 066-133-135-091-645; 069-480-623-909-181; 085-507-633-373-250; 092-452-392-581-46X; 094-072-138-016-877; 098-926-109-358-108; 121-707-574-802-675; 139-597-916-281-136; 165-556-947-058-430; 187-202-562-453-851,1,false,,
040-940-319-245-132,PerCom - Smartphone-based Indoor Localization for Blind Navigation across Building Complexes,,2018,conference proceedings article,2018 IEEE International Conference on Pervasive Computing and Communications (PerCom),,IEEE,,Masayuki Murata; Dragan Ahmetovic; Daisuke Sato; Hironobu Takagi; Kris M. Kitani; Chieko Asakawa,"Continuous and accurate smartphone-based localization is a promising technology for supporting independent mobility of people with visual impairments. However, despite extensive research on indoor localization techniques, they are still not ready for deployment in large and complex environments, like shopping malls and hospitals, where navigation assistance is needed. To achieve accurate, continuous, and real-time localization with smartphones in such environments, we present a series of key techniques enhancing a probabilistic localization algorithm. The algorithm is designed for smartphones and employs inertial sensors on a mobile device and Received Signal Strength (RSS) from Bluetooth Low Energy (BLE) beacons. We evaluate the proposed system in a 21,000 ${\text{m}^{2}}$ shopping mall which includes three multi-story buildings and a large open underground passageway. Experiments in this space validate the effect of the proposed technologies to improve localization accuracy. Field experiments with visually impaired participants confirm the practical performance of the proposed system in realistic use cases.",,,1,10,Beacon; Mobile computing; Mobile device; RSS; Visualization; Computer science; Probabilistic logic; Bluetooth; Inertial measurement unit; Real-time computing,,,,,https://air.unimi.it/handle/2434/774331 https://iris.unito.it/bitstream/2318/1684399/1/1570401483.pdf https://ieeexplore.ieee.org/document/8444593 https://dblp.uni-trier.de/db/conf/percom/percom2018.html#MurataASTKA18 https://doi.org/10.1109/PERCOM.2018.8444593 https://www.computer.org/csdl/proceedings-article/percom/2018/08444593/13bd1tl2olX,http://dx.doi.org/10.1109/percom.2018.8444593,,10.1109/percom.2018.8444593,2888465049,,6,001-448-544-148-575; 001-996-430-142-340; 005-132-898-043-994; 006-615-644-763-134; 009-927-794-055-32X; 011-369-981-296-055; 014-151-331-306-036; 015-733-840-903-484; 016-732-725-225-210; 017-434-368-515-180; 018-249-047-846-554; 019-336-739-884-465; 019-811-761-679-740; 019-828-744-033-99X; 022-818-790-792-275; 024-340-152-801-993; 026-684-493-399-893; 031-410-405-104-518; 037-893-410-565-455; 044-901-027-282-816; 049-485-542-422-096; 052-058-164-372-424; 055-660-300-282-556; 056-660-722-902-221; 057-110-853-820-322; 057-847-555-860-541; 063-200-348-257-985; 064-722-056-462-519; 065-032-907-680-425; 071-473-658-566-47X; 079-433-787-480-087; 081-871-676-661-821; 085-639-580-887-462; 088-305-233-913-186; 097-499-459-534-124; 102-976-506-139-695; 105-810-803-790-416; 139-114-429-196-103; 153-544-008-246-78X; 165-253-140-454-32X; 188-569-539-275-740; 189-029-312-730-111,80,true,,green
040-995-936-965-942,Iterative Design of Sonification Techniques to Support People with Visual Impairments in Obstacle Avoidance,2021-10-15,2021,journal article,ACM Transactions on Accessible Computing,19367228; 19367236,Association for Computing Machinery (ACM),United States,Giorgio Presti; Dragan Ahmetovic; Mattia Ducci; Cristian Bernareggi; Luca A. Ludovico; Adriano Baratè; Federico Avanzini; Sergio Mascetti,"Obstacle avoidance is a major challenge during independent mobility for blind or visually impaired (BVI) people. Typically, BVI people can only perceive obstacles at a short distance (about 1 m, in case they are using the white cane), and some obstacles are hard to detect (e.g., those elevated from the ground), or should not be hit by the white cane (e.g., a standing person). A solution to these problems can be found in recent computer-vision techniques that can run on mobile and wearable devices to detect obstacles at a distance. However, in addition to detecting obstacles, it is also necessary to convey information about them in real time. This contribution presents WatchOut, a sonification technique for conveying real-time information about the main properties of an obstacle to a BVI person, who can then use this additional feedback to safely navigate in the environment. WatchOut was designed with a user-centered approach, involving four iterations of online listening tests with BVI participants in order to define, improve and evaluate the sonification technique, eventually obtaining an almost perfect recognition accuracy. WatchOut was also implemented and tested as a module of a mobile app that detects obstacles using state-of-the-art computer vision technology. Results show that the system is considered usable and can guide the users to avoid more than 85% of the obstacles.",14,4,1,27,Iterative design; Human–computer interaction; Active listening; Obstacle; USable; Computer science; Turn-by-turn navigation; Wearable technology; Obstacle avoidance; Sonification,,,,,https://dlnext.acm.org/doi/abs/10.1145/3470649 https://dblp.uni-trier.de/db/journals/taccess/taccess14.html#PrestiADBLBAM21 https://dl.acm.org/doi/10.1145/3470649,http://dx.doi.org/10.1145/3470649,,10.1145/3470649,3205220448,,0,000-319-559-523-254; 000-501-534-285-334; 001-386-840-011-038; 002-391-643-288-41X; 004-825-665-828-964; 006-350-086-710-765; 007-223-103-783-187; 009-142-401-900-87X; 009-824-797-627-547; 010-719-927-108-990; 012-081-521-655-479; 012-344-135-607-729; 013-056-435-015-274; 013-847-555-426-159; 014-157-934-526-940; 016-577-116-408-663; 017-411-280-443-272; 017-499-516-520-553; 020-678-057-737-391; 022-697-627-694-941; 023-868-294-867-153; 024-335-765-549-544; 025-306-351-240-479; 025-764-110-104-300; 026-418-488-997-838; 028-174-049-936-114; 028-996-789-272-782; 029-313-431-507-058; 029-482-411-276-907; 030-127-369-356-741; 030-807-170-587-365; 031-799-548-880-483; 034-350-308-512-672; 034-936-259-439-98X; 035-101-435-747-318; 035-395-409-643-765; 035-544-542-108-733; 036-280-949-293-766; 036-552-223-342-289; 036-732-101-738-89X; 042-317-340-997-142; 043-409-565-062-30X; 051-065-738-489-461; 060-257-784-475-928; 063-367-028-428-831; 063-373-121-028-155; 064-043-611-038-916; 064-121-100-029-787; 064-309-126-323-022; 064-311-641-824-246; 066-246-145-459-483; 066-526-177-885-125; 068-526-647-480-345; 071-473-658-566-47X; 075-050-583-490-589; 076-911-364-196-257; 077-838-265-920-228; 078-370-310-843-504; 082-060-789-804-822; 083-870-431-489-961; 086-212-029-513-470; 090-825-602-051-026; 093-548-249-489-049; 096-733-124-330-978; 097-345-009-472-569; 098-811-589-762-176; 098-910-337-444-079; 104-535-881-798-843; 105-790-942-783-392; 110-020-005-480-063; 112-121-560-424-373; 113-713-658-788-357; 120-200-241-666-098; 129-948-693-450-519; 132-271-253-341-075; 133-041-170-002-740; 134-571-420-308-465; 134-788-672-466-192; 143-470-545-989-309; 167-673-301-993-329; 173-164-658-058-36X; 176-619-480-167-936; 183-935-933-940-273,8,false,,
041-902-102-240-495,CVPR Workshops - Digital Sign System for Indoor Wayfinding for the Visually Impaired,,,conference proceedings article,2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05) - Workshops,,IEEE,,Bosco S. Tjan; Paul J. Beckmann; Rudrava Roy; Nicholas A. Giudice; Gordon E. Legge,"Mobility challenges and independent travel are major concerns for blind and visually impaired pedestrians [1][2]. Navigation and wayfinding in unfamiliar indoor environments are particularly challenging because blind pedestrians do not have ready access to building maps, signs and other orienting devices. The development of assistive technologies to aid wayfinding is hampered by the lack of a reliable and costefficient method for providing location information in an indoor environment. Here we describe the design and implementation of a digital sign system based on low-cost passive retro-reflective tags printed with specially designed patterns that can be readily detected and identified by a handheld camera and machine-vision system. Performance of the prototype showed the tag detection/recognition system could cope with the real-world environment of a typical building.",3,,30,30,Sign system; Mobile device; Visually impaired; Computer science; Multimedia; Global Positioning System,,,,,http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.ieee-000001565327 https://ieeexplore.ieee.org/document/1565327/ http://eye.psych.umn.edu/groups/gellab/Tjanetal2005.pdf http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1565327 https://experts.umn.edu/en/publications/digital-sign-system-for-indoor-wayfinding-for-the-visually-impair https://dblp.uni-trier.de/db/conf/cvpr/cvprw2005.html#TjanBRGL05 http://vision.psych.umn.edu/users/beckmann/Pubs/2005TjanBeckmannRoyGiudiceLegge.pdf,http://dx.doi.org/10.1109/cvpr.2005.442,,10.1109/cvpr.2005.442,2097397260,,56,012-081-190-916-004; 013-678-295-106-102; 020-397-774-916-201; 058-468-795-176-403; 084-679-921-430-947; 087-272-456-866-28X; 099-027-071-247-96X; 106-052-130-152-052; 160-589-213-299-680,46,false,,
041-997-100-015-950,An Obstacle Detection Method for Visually Impaired Persons by Ground Plane Removal Using Speeded-Up Robust Features and Gray Level Co-Occurrence Matrix,2018-06-16,2018,journal article,Pattern Recognition and Image Analysis,10546618; 15556212,Pleiades Publishing Ltd,United States,Anish Jindal; N. Aggarwal; Savita Gupta,,28,2,288,300,Active contour model; Co-occurrence matrix; Frame (networking); Pattern recognition (psychology); Artificial intelligence; Monocular vision; Region of interest; Obstacle; Computer vision; Computer science; Ground plane,,,,,https://link.springer.com/article/10.1134/S1054661818020086 https://link.springer.com/content/pdf/10.1134/S1054661818020086.pdf,http://dx.doi.org/10.1134/s1054661818020086,,10.1134/s1054661818020086,2808401252,,0,000-371-951-472-939; 004-207-209-558-755; 007-789-415-690-409; 008-508-113-612-045; 011-189-595-287-283; 012-642-196-605-159; 014-166-020-318-269; 016-634-713-992-616; 023-201-676-730-026; 025-339-027-375-532; 026-605-724-543-082; 031-335-943-685-981; 033-487-066-269-526; 046-692-739-345-656; 048-466-855-122-107; 049-272-582-766-388; 051-886-600-723-161; 066-719-615-803-849; 075-703-478-713-600; 079-634-556-042-742; 082-089-436-591-437; 098-440-015-522-27X; 105-633-704-675-645; 119-839-507-625-077; 137-320-213-389-496; 147-450-294-979-33X; 186-412-044-017-67X,17,false,,
042-001-176-242-862,Design and Testing of a Low-Cost Robotic Wheelchair Prototype,,1995,journal article,Autonomous Robots,09295593; 15737527,Springer Science and Business Media LLC,Netherlands,David P. Miller; Marc G. Slack,,2,1,77,88,Human–computer interaction; Input device; Variety (cybernetics); Control (management); Wheelchair; Behavior control; Assistive robotics; Computer science; Simulation; Moment (mathematics); Operator (computer programming),,,,,http://dpm.kipr.org/papers/wheelchair94.pdf http://www.kipr.org/papers/wheelchair94.pdf https://link.springer.com/content/pdf/10.1007/BF00735440.pdf https://dx.doi.org/10.1007/BF00735440 https://dblp.uni-trier.de/db/journals/arobots/arobots2.html#MillerS95 https://link.springer.com/article/10.1007/BF00735440 https://doi.org/10.1007/BF00735440 http://dx.doi.org/10.1007/BF00735440,http://dx.doi.org/10.1007/bf00735440,,10.1007/bf00735440,2081883647,,1,004-421-506-103-252; 025-317-590-222-001; 047-245-657-442-411; 052-813-708-509-617; 058-052-006-395-306; 081-398-492-851-371; 084-534-682-944-803; 088-741-583-567-691; 114-396-077-325-16X; 189-042-622-891-152,171,false,,
042-156-772-128-560,Blind-environment interaction through voice augmented objects,2014-07-01,2014,journal article,Journal on Multimodal User Interfaces,17837677; 17838738,Springer Science and Business Media LLC,Germany,Rosen Ivanov,,8,4,345,365,Human–computer interaction; Interface (computing); Mobile phone; Phone; Identity (object-oriented programming); Service (business); Mobile service; Computer science; Multimedia; Object (computer science); Identification (information),,,,,https://dblp.uni-trier.de/db/journals/jmui/jmui8.html#Ivanov14 https://link.springer.com/article/10.1007/s12193-014-0166-z https://doi.org/10.1007/s12193-014-0166-z https://link.springer.com/article/10.1007/s12193-014-0166-z/fulltext.html https://rd.springer.com/article/10.1007/s12193-014-0166-z,http://dx.doi.org/10.1007/s12193-014-0166-z,,10.1007/s12193-014-0166-z,2089042468,,0,002-673-692-954-242; 003-634-369-751-95X; 006-256-120-472-553; 007-062-648-671-48X; 021-727-126-739-396; 022-680-541-071-136; 024-928-941-647-252; 026-062-137-058-607; 028-114-702-513-751; 034-468-880-826-73X; 037-039-198-474-114; 040-937-014-635-095; 047-126-935-003-311; 047-716-780-907-104; 051-114-198-678-47X; 053-933-872-331-104; 061-659-353-743-36X; 066-085-922-766-441; 069-304-099-084-626; 071-473-658-566-47X; 074-065-804-709-480; 083-924-881-693-987; 088-349-822-238-208; 091-208-738-197-094; 094-034-394-970-857; 107-447-086-119-951; 117-735-069-214-331; 119-129-743-376-061; 130-162-071-412-471; 142-148-469-325-890; 152-468-966-715-043; 158-042-657-559-956; 169-982-725-966-924,11,false,,
042-222-969-039-177,HCI (10) - Feature Detection Applied to Context-Aware Blind Guidance Support,2015-07-18,2015,book chapter,Universal Access in Human-Computer Interaction. Access to the Human Environment and Culture,03029743; 16113349,Springer International Publishing,Germany,Hugo Fernandes; André Sousa; Hugo Paredes; Vitor Filipe; João Barroso,"Human beings have developed a number of evolutionary mechanisms that allows the distinction between different objects and the triggering of events based on their perception of reality. Visual impairment has a significant impact on individuals’ quality of life, including their ability to work and to develop personal relationships as they often feel cut off people and things around them, due to their impairment. The need for assistive technologies has long been a constant in the daily lives of people with visual impairments, and will remain a constant in future years. Cognitive mapping is of extreme importance for individuals in terms of creating a conceptual model of the surrounding space and objects around them, thereby supporting their interaction with the physical environment. This work describes the use of computer vision techniques, namely feature detectors and descriptors, to detect objects in the scene and help contextualize the user within the surrounding space, enhancing their mobility, navigation and cognitive mapping of a new environment.",,,129,138,Human–computer interaction; Conceptual model; Artificial intelligence; Cognitive map; Perception; Space (commercial competition); Quality of life (healthcare); Context (language use); Visual impairment; Computer science; Orientation (computer vision),,,,,https://doi.org/10.1007/978-3-319-20687-5_13 https://link.springer.com/chapter/10.1007/978-3-319-20687-5_13 https://rd.springer.com/chapter/10.1007/978-3-319-20687-5_13 https://link.springer.com/content/pdf/10.1007%2F978-3-319-20687-5_13.pdf,http://dx.doi.org/10.1007/978-3-319-20687-5_13,,10.1007/978-3-319-20687-5_13,964990232,,0,006-322-344-260-050; 027-271-424-765-037; 039-079-110-624-640; 040-660-173-610-646; 049-592-217-207-783; 050-606-134-980-99X; 057-793-015-492-337; 068-446-370-600-681; 079-145-231-538-435; 090-884-510-770-832; 097-390-623-922-438; 099-643-392-623-259; 107-983-224-543-339; 125-867-883-586-61X; 144-045-024-507-975; 151-015-279-636-660; 192-751-999-674-131,5,false,,
042-418-744-828-17X,SII - Novel design of a social mobile robot for the blind disabilities,,2013,conference proceedings article,Proceedings of the 2013 IEEE/SICE International Symposium on System Integration,,IEEE,,Min-Fan Ricky Lee; Fu Hsin Steven Chiu; Chen Zhuo,"World Health Organization estimated that over 1 billion people are living with some form of disability. Some 110 to 190 million people live with significant disabilities. The number of guide dogs is far lower than the demand and only few visually impaired have access to qualified guidance dogs. This paper present an autonomous mobile robot system aimed for the service to the blind. The system includes a proof-of-concept appearance design as the human machine interface that consists of a stick and main body enveloping the mobile robot platform. The main body contains four modules as sensing, navigation, remote control and social platform. The appearance has a high quality texture demonstrated human factors engineering concepts emphasizing user-friendly interface. The stick has rotation joint design fitting the average human height. The length and angle of the stick is adjustable to user's height and can be used in separate from or combination with the main body. The system combines artificial intelligence, sensing, and navigation system that allows blind people to move in a free state.",,,161,166,Interface (computing); Engineering; Human–robot interaction; Artificial intelligence; Social robot; Personal robot; Mobile robot; Mobile robot navigation; Navigation system; Computer vision; Robot control,,,,,https://ieeexplore.ieee.org/document/6776710/ http://ieeexplore.ieee.org/document/6776710/ https://dblp.uni-trier.de/db/conf/sii/sii2013.html#LeeCZ13,http://dx.doi.org/10.1109/sii.2013.6776710,,10.1109/sii.2013.6776710,2015269757,,0,005-557-745-418-218; 033-274-142-035-790; 035-421-745-808-967; 054-923-233-224-609; 065-744-119-333-639; 089-844-069-007-796; 100-961-921-176-077; 103-726-621-742-996,6,false,,
042-515-590-831-61X,Enabling visually impaired people to learn three-dimensional tactile graphics with a 3DOF haptic mouse.,2021-09-25,2021,journal article,Journal of neuroengineering and rehabilitation,17430003,Springer Science and Business Media LLC,United Kingdom,Mariacarla Memeo; Marco Jacono; Giulio Sandini; Luca Brayda,"BACKGROUND In this work, we present a novel sensory substitution system that enables to learn three dimensional digital information via touch when vision is unavailable. The system is based on a mouse-shaped device, designed to jointly perceive, with one finger only, local tactile height and inclination cues of arbitrary scalar fields. The device hosts a tactile actuator with three degrees of freedom: elevation, roll and pitch. The actuator approximates the tactile interaction with a plane tangential to the contact point between the finger and the field. Spatial information can therefore be mentally constructed by integrating local and global tactile cues: the actuator provides local cues, whereas proprioception associated with the mouse motion provides the global cues. METHODS The efficacy of the system is measured by a virtual/real object-matching task. Twenty-four gender and age-matched participants (one blind and one blindfolded sighted group) matched a tactile dictionary of virtual objects with their 3D-printed solid version. The exploration of the virtual objects happened in three conditions, i.e., with isolated or combined height and inclination cues. We investigated the performance and the mental cost of approximating virtual objects in these tactile conditions. RESULTS In both groups, elevation and inclination cues were sufficient to recognize the tactile dictionary, but their combination worked at best. The presence of elevation decreased a subjective estimate of mental effort. Interestingly, only visually impaired participants were aware of their performance and were able to predict it. CONCLUSIONS The proposed technology could facilitate the learning of science, engineering and mathematics in absence of vision, being also an industrial low-cost solution to make graphical user interfaces accessible for people with vision loss.",18,1,146,,Motion (physics); Sensory cue; Haptic technology; Artificial intelligence; Actuator; Visual impairment; Computer vision; Graphical user interface; Cognitive neuroscience of visual object recognition; Sensory substitution,Geometry; Haptics; Object recognition; Visual impairment; Workload,Animals; Blindness; Humans; Learning; Mice; Touch; Touch Perception; Visually Impaired Persons,,Istituto Italiano di Tecnologia; Fondazione Vodafone Italia,https://jneuroengrehab.biomedcentral.com/articles/10.1186/s12984-021-00935-y https://www.ncbi.nlm.nih.gov/pubmed/34563218,http://dx.doi.org/10.1186/s12984-021-00935-y,34563218,10.1186/s12984-021-00935-y,3203444133,PMC8467032,0,000-185-521-073-031; 002-024-707-552-722; 003-695-340-121-507; 004-128-555-882-691; 006-200-706-413-742; 006-566-700-510-281; 006-787-352-436-925; 009-942-546-945-134; 011-490-807-289-452; 011-797-116-120-363; 012-878-058-553-582; 014-724-332-426-203; 015-846-077-402-539; 018-518-352-822-025; 019-432-135-759-276; 019-469-092-868-059; 022-933-780-836-944; 024-634-684-799-445; 027-049-361-368-415; 027-182-526-166-09X; 027-953-911-731-422; 031-294-750-698-550; 034-871-196-904-906; 035-441-997-223-023; 035-888-237-105-391; 037-589-872-932-823; 037-730-305-798-381; 038-056-295-016-642; 038-404-952-723-311; 041-357-004-723-943; 043-380-401-330-631; 047-182-660-757-048; 047-257-750-224-150; 050-000-324-440-795; 051-537-926-980-189; 053-329-063-158-769; 055-539-164-934-497; 056-928-161-457-918; 058-800-703-440-142; 061-417-285-837-580; 061-701-724-403-169; 066-108-334-095-572; 066-714-254-050-091; 073-731-689-315-691; 074-569-999-581-323; 078-492-249-831-891; 088-773-975-481-885; 089-486-948-113-74X; 094-518-265-457-361; 095-050-257-657-510; 096-050-254-751-567; 097-026-507-709-903; 097-761-177-558-348; 102-053-786-877-485; 108-042-417-369-404; 121-327-768-465-334; 121-689-354-640-786; 126-850-560-853-336; 132-418-470-493-669; 142-249-427-420-593; 142-309-322-901-640; 147-799-262-488-343; 167-350-926-593-804,2,true,"CC BY, CC0",gold
042-600-261-176-614,Are Saudi Arabian banks' mobile applications accessible for blind or partially sighted users?: a customers' perspective and evaluation,2024-01-19,2024,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Asmaa Alayed,"<jats:title>Abstract</jats:title><jats:p>This study investigated the accessibility problems and barriers faced by visually impaired Arabic speakers when using banking mobile applications. The sample included mobile applications for the top three banks in Saudi Arabia in terms of their customer base, including applications for AlRajhi, SNB, and Riyad bank. The focus was on the three most popular e-services provided by those applications. Using a mixed method approach, accessibility was inspected manually according to current Web Content Accessibility Guidelines (WCAG 2.1). In addition, in-depth usability testing for those applications was conducted with 12 blind Arabic-speaking users. The results revealed that the accessibility level for all sample applications did not conform to WCAG 2.1, which reflected the low accessibility level provided by these applications. However, AlRajhi’s application was the best among the tested applications. In manual inspection, it had the lowest number of violations of success criteria compared to SNB’s and Riyad’s applications. It also achieved the best results in usability testing in terms of effectiveness, efficiency and satisfaction compared to the other two applications. The overall results showed that the current situation of accessibility of Saudi banking applications is not very satisfactory in terms of number of violations to WCAG 2.1 success criteria. The most violated principle by all the applications was operable, with around 37% of total violations, followed by perceivable, then robust and understandable.</jats:p>",,,,,Usability; Computer science; Arabic; Sample (material); Perspective (graphical); Mobile banking; Focus (optics); Multimedia; World Wide Web; Human–computer interaction; Artificial intelligence; Philosophy; Linguistics; Chemistry; Physics; Chromatography; Optics,,,,,https://link.springer.com/content/pdf/10.1007/s10209-023-01082-y.pdf https://doi.org/10.1007/s10209-023-01082-y,http://dx.doi.org/10.1007/s10209-023-01082-y,,10.1007/s10209-023-01082-y,,,0,001-698-658-889-648; 003-255-394-036-069; 008-617-463-761-182; 010-646-641-896-202; 011-259-586-617-640; 011-592-937-357-262; 013-199-390-745-668; 031-694-195-813-742; 034-584-872-901-021; 041-653-104-103-457; 045-574-401-436-063; 048-000-048-627-42X; 057-243-312-135-655; 078-266-636-231-356; 088-450-585-345-574; 100-157-520-980-972; 106-145-725-024-028; 124-867-809-263-365; 160-294-729-073-836; 165-441-050-562-229; 172-680-715-488-812; 172-980-073-969-941; 174-205-266-759-823,0,true,cc-by,hybrid
042-627-460-556-265,Embedded Blind Aid System Based on µClinux,,2008,journal article,Computer Engineering,10003428,Shanghai Jisuanji Xuehui/Shanghai Computer Society,China,Yang Chao; Zhao Qun-fei,"This paper proposes a solution using embedded system to provide navigation for the blind or visually impaired persons.Integrating with the high performance fix-point DSP ADSP-BF533 and video codec SAA7113,a hardware platform using as image processing is designed while recognition algorithms implemented on ported embedded operation system and optimizing methods are given.Results of experiment suggest that this system has good real-time property and recognition accuracy,which can serve the blind’s need for walking outdoors independently.",,,,,Porting; Embedded system; Codec; Computer science; Digital signal processing; Property (programming); Computer hardware; Image processing,,,,,https://en.cnki.com.cn/Article_en/CJFDTOTAL-JSJC200824101.htm,https://en.cnki.com.cn/Article_en/CJFDTOTAL-JSJC200824101.htm,,,2375177306,,0,065-992-777-809-97X; 193-102-666-478-817,0,false,,
042-777-177-469-047,HIS - OCR signage recognition with skew & slant correction for visually impaired people,,2011,conference proceedings article,2011 11th International Conference on Hybrid Intelligent Systems (HIS),,IEEE,,Intan Fariza Bt Hairuman; Oi-Mean Foong,"It is a challenge for visually impaired people (VIPs) to navigate independently whenever they attempt to find their way in unfamiliar buildings searching for amenities (i.e. exits, ladies/gents toilets) even with a walking stick or a guide dog. Camera-based computer vision systems have the potential to assist VIPs in independent navigation or way finding in unfamiliar places. To leverage on previous research of Signage Recognition Framework which could only recognize public signage with slanted angle less than30°, an improved OCR signage recognition model with skew and slant correction in public signage is presented. The proposed OCR method consists of Canny edge detection algorithm, Hough Transformation and Shearing Transformation were used to detect and correct skewed and slanted images. The proposed model would capture a public signage image, compare the image in the database using template matching algorithm and convert to machine readable text in a text file. The text will then be processed by Microsoft Speech Application Program Interface (SAPI) speech synthesizer and translated to voice as output. Experiments were conducted on 5 blind folded subjects to test the performance of the model. The proposed OCR recognition model has achieved satisfactory recognition rate of 82.7%.",,,306,310,Signage; Canny edge detector; Speech synthesis; Artificial intelligence; Template matching; Edge detection; Hough transform; Speech recognition; Walking stick; Computer vision; Computer science; Optical character recognition,,,,,http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.ieee-000006122123 https://doi.org/10.1109/HIS.2011.6122123 https://dblp.uni-trier.de/db/conf/his/his2011.html#HairumanF11 https://ieeexplore.ieee.org/document/6122123/ http://ieeexplore.ieee.org/document/6122123/ http://eprints.utp.edu.my/8008/,http://dx.doi.org/10.1109/his.2011.6122123,,10.1109/his.2011.6122123,1996341870,,1,003-046-584-583-29X; 013-282-878-691-023; 020-546-253-708-470; 033-609-469-125-132; 038-218-082-706-921; 046-836-556-289-225; 049-924-516-928-378; 054-810-334-553-240; 062-246-626-650-11X; 064-084-634-794-723; 067-893-845-010-65X; 071-067-675-936-790; 104-363-062-318-895; 116-538-743-642-778; 132-608-007-041-238; 158-837-510-050-652,15,false,,
042-840-067-918-103,Imagining the future of bioimage analysis,2016-12-07,2016,journal article,Nature biotechnology,15461696; 10870156,Springer Science and Business Media LLC,United Kingdom,Erik Meijering; Anne E. Carpenter; Hanchuan Peng; Fred A. Hamprecht; Jean-Christophe Olivo-Marin,,34,12,1250,1255,Perspective (graphical); Data science; Software; Computer science,,"Acceleration; Humans; Image Processing, Computer-Assisted; Imagination; Vision, Ocular",,,https://europepmc.org/article/MED/27926723 https://www.nature.com/articles/nbt.3722.pdf https://repub.eur.nl/pub/94881 https://www.ncbi.nlm.nih.gov/pubmed/27926723 https://www.nature.com/articles/nbt.3722 https://pubmed.ncbi.nlm.nih.gov/27926723/,http://dx.doi.org/10.1038/nbt.3722,27926723,10.1038/nbt.3722,2559756885,,0,001-512-102-458-203; 002-061-766-337-371; 004-392-115-121-939; 004-605-282-110-467; 006-106-512-950-952; 006-148-012-061-089; 006-587-739-859-985; 008-349-847-149-832; 008-643-941-274-151; 009-672-728-202-545; 011-577-941-199-345; 011-669-891-845-06X; 013-928-226-880-291; 014-299-776-857-392; 016-557-756-493-327; 021-502-196-049-932; 021-589-487-273-322; 022-291-842-077-196; 022-707-880-383-801; 029-255-413-165-637; 032-872-410-843-721; 035-373-601-218-462; 035-490-267-411-728; 037-873-503-395-813; 046-372-090-947-472; 048-587-540-256-46X; 051-425-281-185-199; 051-893-069-810-935; 059-149-073-001-124; 059-574-049-499-724; 059-726-012-898-553; 060-747-563-467-504; 060-986-766-053-991; 061-680-597-783-313; 067-733-149-165-823; 068-374-676-582-367; 077-216-334-371-79X; 077-309-424-387-034; 077-773-892-369-848; 081-833-691-200-401; 085-221-899-878-631; 088-748-343-249-637; 092-029-784-601-900; 095-563-603-963-814; 107-092-136-134-210; 113-304-341-467-769; 118-879-463-781-19X; 129-027-117-519-286; 129-042-276-277-873; 142-045-637-938-345; 142-147-291-448-45X; 150-731-142-022-239; 171-047-570-092-752; 194-603-816-704-843,166,false,,
042-872-416-149-696,Microcontroller based Navigation System for Visually Impaired People,2015-08-30,2015,journal article,IJARCCE,22781021; 23195940,Tejass Publishers,,Chaitali Kishor Lakde; Dr. Prakash S. Prasad,"Navigation assistance for visually impaired (NAVI) refers to systems that are capable to assist or guide people with vision loss, ranging from partially sighted to totally blind, by means of sound commands.Many researchers are working to assist visually impaired people in different ways like voice based assistance, ultrasonic based assistance, camera based assistance and in some advance way researchers are trying to give transplantation of real eyes with robotic eyes which can capable enough to plot the real image over patient retina using some biomedical technologies.In other way creating a fusion of sensing technology and voice based guidance system some of the products were developed which could give better result than individual technology by the use of microcontroller.There are some limitation in system like obstacle detection which could not see the object but detection the object and camera based system can""t work properly in different light level so the proposed system is a fusion of color sensing sensor and the obstacle sensor along with the voice based assistance system.The main idea of the proposed system to make person aware of path he is walking and also the obstacle in the path.",4,8,431,435,Visually impaired; Microcontroller; Computer science; Human–computer interaction; Computer vision; Artificial intelligence; Embedded system; Computer graphics (images),,,,,,http://dx.doi.org/10.17148/ijarcce.2015.4893,,10.17148/ijarcce.2015.4893,,,0,,5,true,,bronze
043-054-937-507-243,WorldCIST (2) - Assistive Platforms for the Visual Impaired: Bridging the Gap with the General Public,2017-03-29,2017,book chapter,Advances in Intelligent Systems and Computing,21945357; 21945365,Springer International Publishing,,Tania Rocha; Hugo Fernandes; Arsénio Reis; Hugo Paredes; João Barroso,"The visual impaired are a specific minority group that can benefit from specific assistive systems in order to mitigate their mobility and accessibility constrains. In the last decade, our research group has been integrating and developing assistive technologies, focused in human-computer interaction, artificial vision, assisted navigation, pervasive computing, among others. Several projects and prototypes have been developed with the main objective of improving the blind’s autonomy, mobility, and quality of life. Currently the technology has reached a maturation point that allows the development of systems based on video capturing, image recognition and location referencing, which are key for providing features of artificial vision, assisted navigation and spatial perception. The miniaturization of electronics can be used to create devices such as electronic canes that equipped with sensors can provide so much more contextual information to a blind user. The adoption of these systems is dependent of an information catalogue regarding points of interest and their physical location reference. In this paper we describe the current work on assistive systems for the blind and propose a new perspective on using the base information of those systems to provide new services to the general public. By bridging the gap between the two groups, we expect to further advance the development of the current systems and contribute to their economic sustainability.",,,602,608,Human–computer interaction; Electronics; Point of interest; Video capture; Ubiquitous computing; Bridging (programming); Minority group; Artificial vision; Computer science; Multimedia; Autonomy,,,,,https://repositorio.inesctec.pt/bitstream/123456789/7425/1/P-00M-JXK.pdf https://link.springer.com/chapter/10.1007/978-3-319-56538-5_61 https://dblp.uni-trier.de/db/conf/worldcist/worldcist2017-2.html#RochaFRPB17 https://doi.org/10.1007/978-3-319-56538-5_61 https://rd.springer.com/chapter/10.1007/978-3-319-56538-5_61,http://dx.doi.org/10.1007/978-3-319-56538-5_61,,10.1007/978-3-319-56538-5_61,2602859251,,0,005-708-970-476-858; 008-192-191-721-150; 023-809-915-772-22X; 042-222-969-039-177; 046-256-730-862-509; 083-853-269-246-35X; 130-552-169-471-000; 153-471-712-074-630; 160-105-761-673-49X,9,true,,green
043-062-981-799-589,A Mobile Lens: Voice-Assisted Smartphone Solutions for the Sightless to Assist Indoor Object Identification,2024-06-28,2024,journal article,EAI Endorsed Transactions on Internet of Things,24141399,European Alliance for Innovation n.o.,,Talal Saleem; V. Sivakumar,"<jats:p>Every aspect of life is organized around sight. For visually impaired individuals, accidents often occur while walking due to collisions with people or walls. To navigate and perform daily tasks, visually impaired people typically rely on white cane sticks, assistive trained guide dogs, or volunteer individuals. However, guide dogs are expensive, making them unaffordable for many, especially since 90% of fully blind individuals live in low-income countries. Vision is crucial for participating in school, reading, walking, and working. Without it, people struggle with independent mobility and quality of life. While numerous applications are developed for the general public, there is a significant gap in mobile on-device intelligent assistance for visually challenged people. Our custom mobile deep learning model shows object classification accuracy of 99.63%. This study explores voice-assisted smartphone solutions as a cost-effective and efficient approach to enhance the independent mobility, navigation, and overall quality of life for visually impaired or blind individuals.</jats:p>",10,,,,Identification (biology); Object (grammar); Computer science; Reading (process); Mobile device; Sight; Human–computer interaction; Quality of life (healthcare); Multimedia; Psychology; Artificial intelligence; World Wide Web; Botany; Physics; Astronomy; Political science; Law; Psychotherapist; Biology,,,,,,http://dx.doi.org/10.4108/eetiot.6450,,10.4108/eetiot.6450,,,0,018-775-168-927-916; 024-657-745-112-822; 025-102-115-504-204; 034-853-430-954-719; 043-472-522-243-436; 044-191-818-584-446; 061-629-846-800-142; 077-655-540-364-835; 092-452-392-581-46X; 144-920-621-726-001; 150-636-677-779-048; 152-127-960-081-400; 164-434-594-037-260,0,true,cc-by,gold
043-272-454-541-936,The uses of fuzzy logic in autonomous robot navigation,1997-12-16,1997,journal article,"Soft Computing - A Fusion of Foundations, Methodologies and Applications",14327643; 14337479,Springer Science and Business Media LLC,Germany,Alessandro Saffiotti,,1,4,180,197,Fuzzy electronics; Computational intelligence; Artificial intelligence; Social robot; Mobile robot navigation; Field (computer science); Robotics; Computer science; Fuzzy logic; Intelligent control,,,,,https://doi.org/10.1007/s005000050020 https://dblp.uni-trier.de/db/journals/soco/soco1.html#Saffiotti97 https://link.springer.com/article/10.1007%2Fs005000050020,http://dx.doi.org/10.1007/s005000050020,,10.1007/s005000050020,2005336137,,4,000-297-548-160-043; 000-965-352-753-390; 004-339-577-044-226; 004-550-828-567-78X; 005-606-978-612-220; 006-571-432-076-023; 008-900-169-370-213; 010-320-781-902-82X; 011-044-446-043-683; 011-642-220-143-518; 011-901-530-678-036; 012-202-011-712-099; 012-281-994-129-90X; 014-881-560-953-661; 016-637-565-008-907; 017-349-163-326-275; 017-974-689-597-794; 018-212-702-095-699; 021-591-352-804-774; 022-352-146-168-736; 027-414-277-115-434; 030-842-243-605-354; 033-000-441-382-866; 034-812-783-039-478; 036-321-480-982-385; 037-763-341-142-604; 038-822-929-041-816; 039-522-953-253-255; 044-131-120-344-825; 044-914-133-793-911; 046-620-973-675-175; 047-399-460-129-394; 048-026-522-565-277; 048-829-509-742-135; 048-862-205-112-489; 049-015-238-389-117; 050-103-018-911-554; 051-502-258-180-345; 052-008-028-859-240; 053-376-181-740-743; 054-317-229-137-262; 055-314-129-671-401; 057-176-221-770-752; 057-255-138-571-651; 060-012-333-896-533; 067-123-355-090-633; 067-590-213-958-751; 067-668-236-111-761; 072-279-031-814-65X; 073-513-002-351-309; 075-795-846-182-104; 076-276-079-508-741; 076-723-189-161-453; 079-170-737-096-376; 083-737-584-188-788; 088-966-080-975-877; 090-645-251-917-642; 091-940-459-009-174; 092-047-318-751-687; 096-964-493-117-847; 097-218-957-636-090; 097-635-220-441-704; 098-836-913-903-171; 104-945-056-237-511; 106-467-918-057-100; 111-142-222-832-020; 117-301-862-345-830; 118-102-291-718-756; 118-235-306-059-749; 123-345-885-500-217; 125-101-804-990-018; 126-717-220-534-846; 128-131-452-887-872; 129-354-707-032-054; 130-267-818-060-680; 130-318-366-510-122; 133-810-248-003-342; 135-468-973-840-110; 135-975-827-449-910; 137-681-539-641-19X; 144-551-134-931-031; 147-156-299-805-358; 147-785-696-623-234; 150-344-702-689-980; 164-296-770-566-012; 166-281-960-758-841; 169-293-082-650-569; 171-656-726-171-89X; 190-694-973-837-157; 191-133-171-389-176; 192-026-899-424-475; 193-034-911-871-750; 198-415-864-822-606,379,false,,
043-296-537-490-253,Touch and Go — Designing Haptic Feedback for a Hand-Held Mobile Device,,2004,journal article,BT Technology Journal,13583948; 15731995,Springer Science and Business Media LLC,Netherlands,Sile O'Modhrain,,22,4,139,145,Motion (physics); Haptic technology; Bridge (nautical); Mobile device; Key (cryptography); Social connectedness; Gesture; Computer science; Multimedia; Frame of reference,,,,,http://somodhrain.net/palpable/touchandgo.pdf http://www.somodhrain.com/palpable/touchandgo.pdf http://www.ttivanguard.com/genevareconn/TouchandGo.pdf https://media.mit.edu/publications/bttj/Paper15Pages139-145.pdf https://web.media.mit.edu/~walter/bttj/Paper15Pages139-145.pdf http://media.mit.edu/publications/bttj/Paper15Pages139-145.pdf https://link.springer.com/content/pdf/10.1023/B:BTTJ.0000047592.21315.ce.pdf https://link.springer.com/article/10.1023%2FB%3ABTTJ.0000047592.21315.ce http://web.media.mit.edu/~walter/bttj/Paper15Pages139-145.pdf,http://dx.doi.org/10.1023/b:bttj.0000047592.21315.ce,,10.1023/b:bttj.0000047592.21315.ce,2159223407,,4,007-014-054-767-445; 008-496-635-237-961; 008-759-320-691-440; 016-838-576-963-580; 032-904-492-107-421; 078-759-487-184-431; 085-459-294-950-119; 092-006-420-341-261; 096-803-870-115-728; 100-287-017-816-599; 103-076-816-507-177; 109-591-149-521-075; 109-812-016-341-739; 116-803-717-178-267; 127-221-599-296-033,10,false,,
043-616-807-560-697,Application of activity semantics and BPMN 2.0 in the generation and modeling of generic surgical process models,2017-05-19,2017,journal article,International journal of computer assisted radiology and surgery,18616429; 18616410,Springer Science and Business Media LLC,Germany,Juliane Neumann; Markus Wiemuth; Oliver Burgert; Thomas Neumuth,"In this paper a method for the generation of gSPM with ontology-based generalization was presented. The resulting gSPM was modeled with BPMN/BPMNsix in an efficient way and could be executed with BPMN workflow engines. In the next step the implementation of resource concepts, anatomical structures, and transition probabilities for workflow execution will be realized.",12,1,48,49,Software engineering; Ontology (information science); Generalization; Workflow; Business Process Model and Notation; Semantics (computer science); Workflow engine; Computer science; Resource (project management); Process modeling; Market segmentation; Artificial intelligence; Catalan; Natural language processing; Capsule endoscopy; Government (linguistics); Cluster analysis; Deep learning; Gesture recognition; Structure (mathematical logic); Gesture; Control (management); Computer vision; Exhibition; Human–computer interaction; Multimedia; Archaeology; History,,,,,https://publikationen.reutlingen-university.de/frontdoor/index/index/docId/1536,http://dx.doi.org/10.1007/s11548-017-1588-3,28527024,10.1007/s11548-017-1588-3,2555506591; 3122611314; 3198672233,,1,004-546-914-644-399; 006-331-384-545-426; 021-713-294-496-242; 025-590-318-434-897; 026-269-447-223-354; 030-184-553-489-350; 032-134-337-320-216; 034-014-499-765-927; 036-109-097-139-856; 041-380-567-854-256; 042-251-157-585-319; 055-197-669-822-901; 059-149-073-001-124; 063-664-723-719-236; 065-819-195-728-230; 069-210-008-643-887; 072-233-420-092-271; 087-600-784-649-866; 092-800-996-081-384; 107-687-174-825-840; 114-708-293-591-569; 123-514-995-401-054; 182-943-730-299-547,25,true,cc-by-nc-nd,green
043-806-251-616-153,Two-Stage vSLAM Loop Closure Detection Based on Sequence Node Matching and Semi-Semantic Autoencoder,2021-01-20,2021,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Zhonghua Wang; Zhen Peng; Yong Guan; Lifeng Wu,,101,2,1,21,Autoencoder; Sliding window protocol; Optimal matching; Artificial intelligence; Pattern recognition; Computational complexity theory; Matching (graph theory); Mobile robot; Computer science; Convolutional neural network; Node (circuits),,,,National Natural Science Foundation of China,https://link.springer.com/article/10.1007/s10846-020-01302-0,http://dx.doi.org/10.1007/s10846-020-01302-0,,10.1007/s10846-020-01302-0,3124555229,,0,002-345-873-500-677; 004-067-102-961-139; 005-313-669-057-631; 011-198-470-893-241; 019-423-688-316-008; 020-233-013-143-936; 021-824-057-414-364; 023-960-131-541-476; 025-432-367-636-862; 034-213-257-077-058; 035-159-685-124-983; 035-291-375-623-97X; 038-954-077-129-782; 041-333-506-109-883; 044-792-836-800-521; 045-023-830-446-955; 047-326-510-583-783; 050-136-713-033-491; 055-222-115-631-013; 057-559-658-232-111; 057-708-071-687-658; 066-564-286-090-054; 068-446-370-600-681; 071-532-949-260-881; 071-583-782-423-553; 073-080-634-221-501; 073-873-259-605-342; 078-004-997-349-261; 081-331-014-004-913; 084-039-963-843-971; 085-002-460-903-634; 086-290-111-320-583; 086-809-252-826-973; 087-069-977-936-200; 099-500-778-216-930; 102-786-306-974-437; 111-142-688-726-166; 111-801-107-673-657; 121-707-574-802-675; 122-290-861-271-905; 122-658-982-249-571; 140-257-849-363-637; 142-888-537-631-253; 145-722-308-591-082; 147-740-644-680-81X; 164-328-765-137-911; 165-346-671-460-887; 167-079-291-125-197; 181-797-068-816-79X; 199-814-118-067-236,4,false,,
043-852-528-123-480,IoT Based Smart Assistant for Blind Person and Smart Home Using the Bengali Language,2020-09-12,2020,journal article,SN Computer Science,2662995x; 26618907,Springer Science and Business Media LLC,,Wahidur Rahman; Rahabul Islam; Mahmodul Hasan; Shisir Mia; Mohammad Motiur Rahman,,1,5,1,13,System usability scale; Bengali; Spoken language; Visual impairment; Computer science; Multimedia; Home automation; Bluetooth; GSM; General Packet Radio Service,,,,,https://dblp.uni-trier.de/db/journals/sncs/sncs1.html#RahmanIHMR20 https://link.springer.com/article/10.1007/s42979-020-00317-6 https://doi.org/10.1007/s42979-020-00317-6 https://link.springer.com/content/pdf/10.1007/s42979-020-00317-6.pdf,http://dx.doi.org/10.1007/s42979-020-00317-6,,10.1007/s42979-020-00317-6,3084683018,,0,005-967-920-888-156; 007-550-271-206-838; 014-414-065-471-005; 014-494-578-721-199; 032-534-073-405-082; 037-968-556-719-019; 044-208-582-350-878; 047-328-841-532-680; 050-047-706-859-59X; 058-492-527-566-018; 073-190-376-677-414; 101-339-848-675-086; 101-637-146-335-197; 112-078-201-656-139; 120-329-011-392-738; 147-080-891-861-342,14,false,,
043-910-260-672-296,"The role of vision for navigation in the crown-of-thorns seastar, Acanthaster planci.",2016-08-01,2016,journal article,Scientific reports,20452322,Springer Science and Business Media LLC,United Kingdom,Robert Sigl; Sebastian Steibl; Christian Laforsch,"Coral reefs all over the Indo-Pacific suffer from substantial damage caused by the crown-of-thorns seastar Acanthaster planci, a voracious predator that moves on and between reefs to seek out its coral prey. Chemoreception is thought to guide A. planci. As vision was recently introduced as another sense involved in seastar navigation, we investigated the potential role of vision for navigation in A. planci. We estimated the spatial resolution and visual field of the compound eye using histological sections and morphometric measurements. Field experiments in a semi-controlled environment revealed that vision in A. planci aids in finding reef structures at a distance of at least 5 m, whereas chemoreception seems to be effective only at very short distances. Hence, vision outweighs chemoreception at intermediate distances. A. planci might use vision to navigate between reef structures and to locate coral prey, therefore improving foraging efficiency, especially when multidirectional currents and omnipresent chemical cues on the reef hamper chemoreception.",6,1,30834,30834,Coral; Foraging; Visual perception; Ecology; Crown of Thorns; Acanthaster; Compound eye; Fishery; Reef; Coral reef; Biology,,Animals; Chemoreceptor Cells/metabolism; Coral Reefs; Population Dynamics; Spatial Navigation/physiology; Starfish/physiology; Visual Perception/physiology,,,https://www.openchannels.org/literature/14146 https://ui.adsabs.harvard.edu/abs/2016NatSR...630834S/abstract https://www.openchannels.org/sites/default/files/literature/The%20role%20of%20vision%20for%20navigation%20in%20the%20crown-of-thorns%20seastar%2C%20Acanthaster%20planci.pdf https://pubmed.ncbi.nlm.nih.gov/27476750/ https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4967868 https://www.nature.com/articles/srep30834 https://epub.uni-bayreuth.de/3650/ https://eref.uni-bayreuth.de/41305/ https://www.nature.com/articles/srep30834.pdf https://core.ac.uk/display/157589356 http://europepmc.org/articles/PMC4967868 https://core.ac.uk/download/156890206.pdf,http://dx.doi.org/10.1038/srep30834,27476750,10.1038/srep30834,2460060343,PMC4967868,0,000-094-147-013-528; 000-336-368-828-340; 001-006-606-951-85X; 001-280-475-855-105; 001-932-420-266-750; 001-966-220-525-098; 002-257-674-184-157; 004-233-570-524-424; 005-721-819-005-902; 012-018-009-430-105; 012-241-421-341-78X; 013-107-977-509-882; 014-439-977-262-370; 017-399-140-783-938; 019-131-791-220-720; 020-402-156-981-747; 022-587-809-253-605; 024-921-951-966-802; 026-057-197-550-541; 027-922-094-318-192; 034-121-068-480-392; 039-701-064-871-487; 044-160-491-620-115; 053-825-457-503-337; 057-358-183-811-580; 062-527-587-042-007; 063-159-433-535-511; 063-470-178-545-510; 064-289-933-842-536; 066-898-784-770-650; 071-563-350-769-819; 073-865-623-324-417; 074-614-509-539-692; 083-783-303-108-540; 088-287-064-276-481; 090-090-403-839-895; 098-589-983-993-779; 130-465-927-289-772; 139-129-846-362-047; 149-692-542-021-839; 190-149-782-871-009,15,true,"CC BY, CC BY-NC-ND",gold
043-968-083-478-522,Design exploration of a tactile path defect detection vehicle,2023-08-01,2023,conference proceedings article,Third International Conference on Computer Vision and Pattern Analysis (ICCPA 2023),,SPIE,,Chenggang Huang; Jing Wang; Jianwei Ma,"In order to solve the problems of poor paving effect of the existing tactile path, late maintenance is not timely, resulting in inconvenient travel for the blind, the shortcomings of the existing tactile path management are analyzed, so as to guide the design. In the design process, a defect detection method based on the YOLO-V5 target detection algorithm was adopted to realize the processing of tactile path samples, training model and classification of tactile path defect types, and a complete set of tactile path defect detection equipment was designed. This equipment can detect the tactile path in real time, judge and classify the defects of the tactile path, find out the problematic tactile path and send the geographical location information back to the background staff to assist the municipal administration department and the tactile path rectification construction unit to grasp the overall construction situation of the tactile path. Compared with the existing artificial tactile path defect detection methods, the proposed tactile path defect detection method based on OLO-V5 has better recognition performance and higher detection efficiency, and can accurately detect and identify various tactile path defects. At the same time, this study focuses on the safety of travel for the visually impaired, which helps to improve the equal rights of the visually impaired to participate in social activities and enjoy public resources, and helps to optimize barrier-free facilities, which plays a crucial role in the harmonious and orderly development of the society.",,,,,Path (computing); Computer science; Tactile sensor; Computer vision; Artificial intelligence; Simulation; Real-time computing; Robot; Computer network,,,,,,http://dx.doi.org/10.1117/12.2684219,,10.1117/12.2684219,,,0,005-980-319-660-704; 006-910-304-617-124; 014-454-634-638-984; 015-968-400-395-152; 016-860-205-407-340; 018-797-747-752-732; 023-690-263-933-845; 061-669-945-412-088; 069-706-555-672-053; 092-456-552-472-800; 113-799-349-849-865; 127-789-607-200-006; 136-648-400-143-451; 151-506-104-184-929,0,false,,
044-369-786-971-382,Poster Session: 12th Annual Conference of Iscas,2008-05-22,2008,journal article,International Journal of Computer Assisted Radiology and Surgery,18616410; 18616429,Springer Science and Business Media LLC,Germany,,,3,S1,232,315,,,,,,,http://dx.doi.org/10.1007/s11548-008-0203-z,,10.1007/s11548-008-0203-z,,,0,,1,true,,green
045-174-042-097-086,Smart Stick For Visually Impaired On Streets Using Arduino UNO,,2023,journal article,international journal of engineering technology and management sciences,25814621,Mallikarjuna Infosys,,null Dr Sudha L K; null Ajay R; null Manu Gowda S H; null Poornima V B; null Vaibhav S,"<jats:p>A smart stick concept is devised to provide a smart electronic aid for blind people. Blind and visually impaired find difficulties in detecting obstacles during walking in the street. The system is intended to provide artificial vision and object detection, real time assistance via making use of Arduino UNO and is to provide a sound based assistance to blind people.The existing devices for the visually impaired only focus on travelling from one location to another. The device is aimed to help visually impaired with the same maneuver as that of sighted people involves providing a smart electronic aid for blind people to provide artificial vision and object detection, real assistance by; using Arduino Uno. An Arduino is used to advise the blind people about the barrier or obstacles and sends alarms using a buzzer. The aim of the overall system is to provide a low cost and efficient navigation, moisture detection and obstacle detection aid for blind which gives a sense of artificial sight by providing information about the environmental scenario of static and dynamic objectsround them, so that they can walk independently in the street</jats:p>",7,6,309,316,Arduino; Buzzer; Visually impaired; Obstacle; Sight; Computer science; Computer vision; Object (grammar); Focus (optics); Artificial intelligence; Human–computer interaction; Embedded system; Simulation; Engineering; Electrical engineering; Physics; Optics; Astronomy; Law; Political science; ALARM,,,,,,http://dx.doi.org/10.46647/ijetms.2023.v07i06.044,,10.46647/ijetms.2023.v07i06.044,,,0,073-352-846-305-279; 102-806-409-771-398; 139-605-693-422-559,0,true,,gold
045-205-438-482-211,ECCV Workshops (3) - Wearable RGBD Indoor Navigation System for the Blind,2015-03-20,2015,book chapter,Lecture Notes in Computer Science,03029743; 16113349,Springer International Publishing,Germany,Young Ho Lee; Gerard Medioni,"In this paper, we present a novel wearable RGBD camera based navigation system for the visually impaired. The system is composed of a smartphone user interface, a glass-mounted RGBD camera device, a real-time navigation algorithm, and haptic feedback system. A smartphone interface provides an effective way to communicate to the system using audio and haptic feedback. In order to extract orientational information of the blind users, the navigation algorithm performs real-time 6-DOF feature based visual odometry using a glass-mounted RGBD camera as an input device. The navigation algorithm also builds a 3D voxel map of the environment and analyzes 3D traversability. A path planner of the navigation algorithm integrates information from the egomotion estimation and mapping and generates a safe and an efficient path to a waypoint delivered to the haptic feedback system. The haptic feedback system consisting of four micro-vibration motors is designed to guide the visually impaired user along the computed path and to minimize cognitive loads. The proposed system achieves real-time performance at \(28.4\)Hz in average on a laptop, and helps the visually impaired extends the range of their activities and improve the mobility performance in a cluttered environment. The experiment results show that navigation in indoor environments with the proposed system avoids collisions successfully and improves mobility performance of the user compared to conventional and state-of-the-art mobility aid devices.",,,493,508,Interface (computing); Input device; Wearable computer; Haptic technology; Artificial intelligence; Navigation system; Waypoint; Computer vision; Computer science; Visual odometry; User interface,,,,,https://link.springer.com/content/pdf/10.1007%2F978-3-319-16199-0_35.pdf http://vigir.missouri.edu/~gdesouza/Research/Conference_CDs/ECCV_2014/workshops/w22/W22-62.pdf https://link.springer.com/chapter/10.1007/978-3-319-16199-0_35 https://dblp.uni-trier.de/db/conf/eccv/eccv2014w3.html#LeeM14 https://rd.springer.com/chapter/10.1007%2F978-3-319-16199-0_35,http://dx.doi.org/10.1007/978-3-319-16199-0_35,,10.1007/978-3-319-16199-0_35,1041443491,,1,004-756-256-314-376; 007-781-062-370-947; 010-292-874-026-413; 010-719-927-108-990; 014-478-034-924-123; 024-369-065-483-244; 025-278-629-527-832; 028-683-545-086-724; 033-119-829-457-834; 034-551-334-308-190; 044-239-148-413-612; 045-901-176-162-558; 048-603-106-043-151; 049-098-087-244-565; 051-766-223-654-722; 059-482-749-135-704; 060-029-581-945-26X; 065-992-777-809-97X; 068-409-552-839-615; 074-941-207-321-047; 079-132-025-467-063; 086-269-797-595-47X; 138-774-270-601-871; 158-602-232-996-432; 169-421-672-075-385; 170-387-846-090-147; 174-359-311-586-135; 176-619-480-167-936,50,false,,
045-309-399-228-849,An Evaluation of RetinaNet on Indoor Object Detection for Blind and Visually Impaired Persons Assistance Navigation,2020-01-23,2020,journal article,Neural Processing Letters,13704621; 1573773x,Springer Science and Business Media LLC,Netherlands,Mouna Afif; Riadh Ayachi; Yahia Said; Edwige Pissaloux; Mohamed Atri,,51,3,2265,2279,Computational intelligence; Artificial intelligence; Object detection; Task (project management); Visually Impaired Persons; Deep cnn; Object detector; Computer vision; Field (computer science); Computer science; Convolutional neural network,,,,,https://doi.org/10.1007/s11063-020-10197-9 https://link.springer.com/article/10.1007/s11063-020-10197-9 https://dblp.uni-trier.de/db/journals/npl/npl51.html#AfifASPA20,http://dx.doi.org/10.1007/s11063-020-10197-9,,10.1007/s11063-020-10197-9,3001112922,,0,000-416-032-829-85X; 007-316-191-175-159; 007-758-886-034-991; 008-783-981-093-093; 011-804-454-692-148; 014-166-672-119-214; 019-984-401-850-976; 020-233-013-143-936; 021-360-232-245-594; 026-363-993-267-742; 031-218-334-653-826; 033-992-591-000-055; 034-602-652-910-440; 036-284-108-144-414; 040-209-930-304-077; 040-772-880-193-972; 041-552-722-557-082; 043-022-952-629-736; 047-040-726-932-715; 055-222-115-631-013; 058-902-183-054-05X; 061-376-460-470-651; 061-579-383-511-500; 064-657-750-690-046; 075-533-194-094-730; 084-468-139-209-388; 085-686-689-031-272; 095-348-418-263-785; 097-642-574-506-283; 098-304-315-485-355; 108-758-506-856-814; 115-582-515-756-396; 123-314-473-302-722; 124-596-347-359-116; 137-478-852-138-685; 137-863-168-879-365; 139-552-118-652-140; 192-697-687-929-866,76,false,,
045-321-254-881-478,Auditory guidance with the Navbelt-a computerized travel aid for the blind,,1998,journal article,"IEEE Transactions on Systems, Man and Cybernetics, Part C (Applications and Reviews)",10946977,Institute of Electrical and Electronics Engineers (IEEE),United States,Shraga Shoval; Johann Borenstein; Yoram Koren,"A blind traveler walking through an unfamiliar environment and a mobile robot navigating through a cluttered environment have an important feature in common: both have the kinematic ability to perform the motion, but they are dependent on a sensory system to detect and avoid obstacles. The paper describes the use of a mobile robot obstacle avoidance system as a guidance device for blind and visually impaired people. Just as electronic signals are sent to a mobile robot's motion controllers, auditory signals can guide the blind traveler around obstacles, or alternatively, they can provide an ""acoustic image"" of the surroundings. The concept has been implemented and tested in a new travel aid for the blind, called the Navbelt. The Navbelt introduces two new concepts to electronic travel aids (ETA's) for the blind: it provides information not only about obstacles along the traveled path, but also assists the user in selecting the preferred travel path. In addition, the level of assistance can be automatically adjusted according to changes in the environment and the user's needs and capabilities. Experimental results conducted with the Navbelt simulator and a portable experimental prototype are presented.",28,3,459,467,Artificial intelligence; Mobile robot; PATH (variable); Computer vision; Kinematics; Computer science; Sonar; Obstacle avoidance; Feature (computer vision),,,,,https://ieeexplore.ieee.org/document/704589 http://ieeexplore.ieee.org/document/704589/ http://www-personal.umich.edu/~johannb/Papers/paper44.pdf http://dx.doi.org/10.1109/5326.704589 https://dx.doi.org/10.1109/5326.704589 https://dblp.uni-trier.de/db/journals/tsmc/tsmcc28.html#ShovalBK98 https://core.ac.uk/display/24497382,http://dx.doi.org/10.1109/5326.704589,,10.1109/5326.704589,2307588840,,3,010-791-941-379-195; 024-050-792-589-868; 024-369-065-483-244; 031-768-944-008-214; 039-453-372-697-011; 044-693-314-510-582; 050-683-368-696-413; 073-722-974-726-137; 086-470-638-340-132; 114-019-820-503-695; 135-646-014-794-306; 191-001-801-688-935,139,true,,green
045-372-840-476-205,ICARCV - SINVI: smart indoor navigation for the visually impaired,,,conference proceedings article,"ICARCV 2004 8th Control, Automation, Robotics and Vision Conference, 2004.",,IEEE,,Lee Wee Ching; Maylor K. H. Leung,"Vision is an important sensory functionality of humans. Without which, navigation would be difficult and hazardous. In this paper, we outline the use of a novel approach to developing an automated guiding system for the blind. The approach is to use visual clues interpreted from images captured from a lightweight camera, mounted on a user. Visual clues are deciphered from the surrounding environment of the user. With just this information, the system would be able to guide the user through an indoor environment via audio cues. Majority of the visual clues would be gathered from the ceiling region. This is due to its low probability of being obscured, thereby increasing the chances of successful navigation.",2,,1072,1077,Visual communication; Engineering; Artificial intelligence; Mobile robot; Mobile robot navigation; Ceiling (cloud); Visually impaired; Computer vision; Sensory system,,,,,http://ieeexplore.ieee.org/document/1468992/ http://dx.doi.org/10.1109/ICARCV.2004.1468992 https://dx.doi.org/10.1109/ICARCV.2004.1468992 https://ieeexplore.ieee.org/document/1468992/ https://dblp.uni-trier.de/db/conf/icarcv/icarcv2004.html#ChingL04,http://dx.doi.org/10.1109/icarcv.2004.1468992,,10.1109/icarcv.2004.1468992,2160879796,,0,006-385-443-352-776; 006-955-399-018-536; 013-847-555-426-159; 020-648-553-388-963; 034-196-646-262-503; 050-027-658-244-495; 057-217-642-333-612; 058-942-174-334-646; 064-309-126-323-022; 067-152-316-250-502; 075-380-736-372-703; 082-391-007-325-197; 128-288-848-632-871; 139-035-590-326-44X; 159-112-312-446-873,10,false,,
045-820-156-713-100,Microaneurysm detection in fundus images using a two-step convolutional neural network,2019-05-29,2019,journal article,Biomedical engineering online,1475925x,Springer Science and Business Media LLC,United Kingdom,Noushin Eftekhari; Hamid Reza Pourreza; Mojtaba Masoudi; Kamaledin Ghiasi-Shirazi; Ehsan Saeedi,"Diabetic retinopathy (DR) is the leading cause of blindness worldwide, and therefore its early detection is important in order to reduce disease-related eye injuries. DR is diagnosed by inspecting fundus images. Since microaneurysms (MA) are one of the main symptoms of the disease, distinguishing this complication within the fundus images facilitates early DR detection. In this paper, an automatic analysis of retinal images using convolutional neural network (CNN) is presented. Our method incorporates a novel technique utilizing a two-stage process with two online datasets which results in accurate detection while solving the imbalance data problem and decreasing training time in comparison with previous studies. We have implemented our proposed CNNs using the Keras library. In order to evaluate our proposed method, an experiment was conducted on two standard publicly available datasets, i.e., Retinopathy Online Challenge dataset and E-Ophtha-MA dataset. Our results demonstrated a promising sensitivity value of about 0.8 for an average of >6 false positives per image, which is competitive with state of the art approaches. Our method indicates significant improvement in MA-detection using retinal fundus images for monitoring diabetic retinopathy.",18,1,1,16,Deep learning; Artificial intelligence; Fundus (eye); Pattern recognition; Retinopathy; Diabetic retinopathy; Microaneurysm; Two step; Computer science; False positive paradox; Convolutional neural network,Convolutional neural network (CNN); Deep learning; Diabetic retinopathy (DR); Microaneurysm (MA),"Deep Learning; Fundus Oculi; Image Processing, Computer-Assisted/methods; Microaneurysm/diagnostic imaging; Tomography, X-Ray Computed",,,https://link.springer.com/article/10.1186/s12938-019-0675-9/figures/1 https://profdoc.um.ac.ir/paper-abstract-1075582.html https://europepmc.org/article/MED/31142335 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6542103 http://arxiv.org/pdf/1710.05191.pdf https://arxiv.org/abs/1710.05191 https://biomedical-engineering-online.biomedcentral.com/articles/10.1186/s12938-019-0675-9 https://dblp.uni-trier.de/db/journals/corr/corr1710.html#abs-1710-05191 https://link.springer.com/content/pdf/10.1186/s12938-019-0675-9.pdf https://link.springer.com/article/10.1186/s12938-019-0675-9,http://dx.doi.org/10.1186/s12938-019-0675-9,31142335,10.1186/s12938-019-0675-9,2963292306,PMC6542103,0,001-016-000-090-29X; 003-361-941-314-718; 003-874-216-793-094; 007-838-651-091-203; 011-621-446-270-859; 013-396-788-540-243; 017-896-466-264-668; 019-050-024-313-85X; 021-344-119-421-082; 022-715-747-022-214; 026-269-447-223-354; 027-929-540-533-274; 029-616-069-397-295; 029-746-847-757-679; 036-518-723-879-884; 052-049-918-603-942; 054-549-930-248-043; 059-149-073-001-124; 067-224-564-962-744; 067-609-715-460-264; 076-573-503-383-193; 082-018-704-878-453; 082-978-030-156-232; 084-607-435-272-557; 084-923-153-005-121; 093-122-728-348-827; 095-727-071-343-144; 100-399-182-103-513; 103-612-170-692-815; 106-455-297-428-504; 111-055-792-877-68X; 111-338-598-597-908; 119-584-313-016-925; 120-043-193-025-707; 122-140-933-176-496; 129-212-687-043-296; 129-518-469-583-421; 138-160-839-941-277; 176-296-349-393-467,117,true,"CC BY, CC0",gold
046-255-284-895-911,Design of Blind Guiding Robot Based on Speed Adaptation and Visual Recognition,,2023,journal article,IEEE Access,21693536,Institute of Electrical and Electronics Engineers (IEEE),United States,Liangdong Zhang; Keyan Jia; Junxiang Liu; Guijie Wang; Weimin Huang,"With the continuous increase in the number of visually impaired individuals, it has become urgent to address the traffic challenges faced by this group.A blind guiding robot based on speed adaptation and visual recognition was designed to address this problem.The speed adaptation of the robot and the blind person is achieved through feedback control of the distance and speed.Traffic signals are identified using optimized visual recognition method based on YOLOv5 transfer learning, and man-machine interaction is realized by applying multi-module units such as real-time image, speech, and positioning.The experimental results show that the deviation between the relative distance between the man and machine and the set distance was controlled within 13.1%, the relative velocity deviation was controlled within 0.3 m/s, the accuracy of identifying traffic signals reached 91.88%, and when the man-machine distance gap is large, the robot can timely control the man-machine distance to the set distance within 0.7 s, which effectively ensured the travel safety of blind people and provide the groundwork for the practical application of guiding blind robots.",11,,75971,75978,Robot; Computer science; Adaptation (eye); Set (abstract data type); Artificial intelligence; Computer vision; Physics; Optics; Programming language,,,,National Natural Science Foundation of China; Natural Science Foundation of Shandong Province; Natural Science Foundation of Shandong Province,https://ieeexplore.ieee.org/ielx7/6287639/6514899/10185057.pdf https://doi.org/10.1109/access.2023.3296066,http://dx.doi.org/10.1109/access.2023.3296066,,10.1109/access.2023.3296066,,,0,002-123-448-882-287; 009-748-285-068-532; 010-520-723-335-00X; 020-745-838-621-241; 021-041-672-063-77X; 023-758-717-466-427; 038-515-411-468-954; 039-203-899-703-113; 043-253-805-800-263; 047-356-385-281-782; 049-191-758-922-387; 049-317-239-158-314; 053-413-243-927-176; 053-567-501-093-090; 058-013-370-483-44X; 064-683-024-880-936; 067-868-917-544-489; 076-823-621-907-858; 083-744-521-989-268; 085-774-681-225-216; 113-809-734-902-69X; 118-779-181-566-864; 147-052-457-757-341; 147-898-042-304-243,2,true,"CC BY, CC BY-NC-ND",gold
046-357-697-554-258,"Analysis and Compensation of Geometric Distortions, Appearing when Observing Objects under Water",2018-09-19,2018,journal article,Pattern Recognition and Image Analysis,10546618; 15556212,Pleiades Publishing Ltd,United States,Ivan Konovalenko; D. Sidorchuk; G. M. Zenkin,,28,3,379,392,Camera resectioning; Inverse; Acoustics; Underwater vision; Observer (special relativity); Underwater light; Point light source; Computer science; Virtual image; Underwater,,,,,https://link.springer.com/article/10.1134%2FS1054661818030112,http://dx.doi.org/10.1134/s1054661818030112,,10.1134/s1054661818030112,2891111647,,0,001-709-085-677-517; 007-010-567-122-791; 015-386-226-058-82X; 017-389-354-393-581; 019-591-800-003-259; 021-581-730-299-420; 038-050-296-853-349; 050-111-290-293-046; 054-477-853-255-265; 073-571-242-662-830; 074-659-331-213-319; 075-468-280-660-817; 078-237-768-892-655; 135-801-439-226-002; 144-738-261-531-188; 154-404-609-157-194; 165-708-074-966-615,3,false,,
046-451-689-488-175,FATCHA: biometrics lends tools for CAPTCHAs,2016-04-15,2016,journal article,Multimedia Tools and Applications,13807501; 15737721,Springer Science and Business Media LLC,Netherlands,Maria De Marsico; Luca Marchionni; Andrea Novelli; Michael Oertel,,76,4,5117,5140,Password; Multimodal interaction; CAPTCHA; Spoofing attack; Authentication; Biometrics; Mobile device; Gesture; Facial recognition system; Computer security; Computer science,,,,,http://dblp.uni-trier.de/db/journals/mta/mta76.html#MarsicoMNO17 https://dblp.uni-trier.de/db/journals/mta/mta76.html#MarsicoMNO17 https://link.springer.com/article/10.1007/s11042-016-3518-8/fulltext.html https://link.springer.com/article/10.1007/s11042-016-3518-8,http://dx.doi.org/10.1007/s11042-016-3518-8,,10.1007/s11042-016-3518-8,2335857501,,0,001-120-527-487-664; 011-156-079-100-143; 012-016-836-079-639; 013-717-828-781-012; 028-281-891-386-162; 035-867-159-731-756; 037-054-305-166-826; 046-613-166-533-503; 047-601-899-123-243; 054-628-698-431-331; 055-241-124-371-632; 056-374-925-342-570; 056-621-206-285-122; 058-124-091-198-963; 060-746-344-882-260; 083-305-154-362-994; 097-679-624-277-028; 099-657-114-635-02X; 104-818-577-633-198; 105-497-664-735-530; 105-986-644-071-965; 115-345-987-070-692; 118-796-683-091-381; 148-638-261-971-068; 149-583-635-129-563; 156-233-890-792-534; 168-325-824-593-557; 197-758-264-429-171; 199-775-567-032-283,5,false,,
046-452-354-042-897,Rehabilitation Problems of Women Who Are Blind,,1997,journal article,Sexuality and Disability,01461044; 15736717,Springer Science and Business Media LLC,Netherlands,Ellen Rubin,,15,1,41,45,Ethnic group; Psychology; Educational equity; Rehabilitation; Sexual behavior; Equity (economics); Social psychology,,,,,https://link.springer.com/article/10.1023/A%3A1024767530957,http://dx.doi.org/10.1023/a:1024767530957,,10.1023/a:1024767530957,2260750267,,0,,5,false,,
046-567-569-529-360,Intelligent Glasses for Visually Impaired People,2022-12-04,2022,conference proceedings article,2022 14th International Conference on Computational Intelligence and Communication Networks (CICN),,IEEE,,Ali Mustafa; Ahmed Omer; Ogba Mohammed,"Vision impairment and blindness are a big concern in the research field of assistive technologies. An individual with sight impairment suffers from performing tasks that seem simple to a sighted person. However, these effortless tasks for sighted humans are very complex to be performed by machines. With fast processors and high-resolution cameras, researchers can develop accurate algorithms to assist blind and visually impaired (BVI) individuals in performing their daily activities. In this research, we designed an intelligent eyeglass that integrates various models to perform face recognition, object detection, and path navigation. When the system is powered, a welcoming message is played to the user asking him/her to choose one of the three modes. When the user pronounces the desired mode, the system uses Google speech recognition API to identify the request, tunes the required mode, and responds accordingly. Face recognition and object detection modes provide audio feedback of the names of the surrounding known people and objects, respectively. On the other hand, the navigation mode triggers a slight vibration whenever an obstacle is within a range of half a meter. Raspberry Pi 4 is used to control the system components because it can perform all the sophisticated computations needed in a short time, offering the user a realistic experience. Moreover, the low cost of Raspberry Pi, its small size, and its lightweight result in practical and simple-design eyeglasses.",,,,,Computer science; Obstacle; Sight; Human–computer interaction; Computer vision; Mode (computer interface); Object (grammar); Face (sociological concept); Artificial intelligence; Field (mathematics); Raspberry pi; Path (computing); Embedded system; Social science; Programming language; Physics; Mathematics; Astronomy; Sociology; Political science; Pure mathematics; Law; Internet of Things,,,,,,http://dx.doi.org/10.1109/cicn56167.2022.10008291,,10.1109/cicn56167.2022.10008291,,,0,012-016-836-079-639; 019-183-007-691-217; 048-254-076-918-785; 066-133-135-091-645; 092-927-006-718-541; 098-926-109-358-108; 102-952-660-914-374; 125-112-083-666-544; 141-649-000-612-322,3,false,,
046-778-659-207-605,Am I missing something,2019-03-13,2019,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Gill Whitney; Irena Kolar,"People with a visual impairment are more likely to experience social isolation as an effect of their vision loss. Social media can particularly benefit these users, but it is of concern if it cannot be fully and successfully used. This study was instigated at the request of an advisory group of visual impaired users and experts. The aim of the study was to investigate potential accessibility issues visually impaired users could encounter when using social media. A major concern was over missing content embedded in images on social media sites. A subsequent evaluation of Facebook posts carried out by a group of student participants demonstrated that nearly half of images considered contained embedded text, which would be inaccessible to visually impaired users. Despite efforts by social media companies to improve accessibility, any text present in images is not presented in an accessible way to visually impaired users. This research demonstrated the inequality that can arise from partial accessibility and the requirement to consider accessibility at all stages of design and development.",19,2,461,469,Internet privacy; Psychology; Social isolation; Visual impairment; Computer communication networks; Visually impaired; Social media,,,,,https://doi.org/10.1007/s10209-019-00648-z https://dblp.uni-trier.de/db/journals/uais/uais19.html#WhitneyK20 https://eprints.mdx.ac.uk/26246/ https://link.springer.com/article/10.1007/s10209-019-00648-z https://core.ac.uk/download/188183790.pdf,http://dx.doi.org/10.1007/s10209-019-00648-z,,10.1007/s10209-019-00648-z,2922215117,,0,004-017-242-451-24X; 019-502-517-183-269; 022-705-697-906-740; 026-657-238-964-513; 030-427-556-505-92X; 036-775-227-665-657; 043-212-316-018-384; 050-544-309-879-535; 051-898-216-518-508; 052-087-874-041-200; 061-729-705-527-929; 065-890-074-263-379; 069-866-772-896-46X; 071-376-652-376-072; 075-445-221-814-298; 076-446-263-957-802; 078-585-889-901-440; 081-720-886-688-215; 082-822-275-806-888; 084-334-291-393-470; 099-876-716-270-371; 106-689-500-679-774; 128-485-543-673-501; 128-772-800-889-015; 153-622-647-386-245,14,true,,green
047-182-660-757-048,"The Importance of Visual Experience, Gender, and Emotion in the Assessment of an Assistive Tactile Mouse",2015-04-27,2015,journal article,IEEE transactions on haptics,23294051; 19391412,Institute of Electrical and Electronics Engineers (IEEE),United States,Luca Brayda; Claudio Campus; Mariacarla Memeo; Laura Lucagrossi,"Tactile maps are efficient tools to improve spatial understanding and mobility skills of visually impaired people. Their limited adaptability can be compensated with haptic devices which display graphical information, but their assessment is frequently limited to performance-based metrics only which can hide potential spatial abilities in O&M protocols. We assess a low-tech tactile mouse able to deliver three-dimensional content considering how performance, mental workload, behavior, and anxiety status vary with task difficulty and gender in congenitally blind, late blind, and sighted subjects. Results show that task difficulty coherently modulates the efficiency and difficulty to build mental maps, regardless of visual experience. Although exhibiting attitudes that were similar and gender-independent, the females had lower performance and higher cognitive load, especially when congenitally blind. All groups showed a significant decrease in anxiety after using the device. Tactile graphics with our device seems therefore to be applicable with different visual experiences, with no negative emotional consequences of mentally demanding spatial tasks. Going beyond performance-based assessment, our methodology can help with better targeting technological solutions in orientation and mobility protocols.",8,3,279,286,Haptic technology; Psychology; Cognitive psychology; Orientation and Mobility; Workload; Task (project management); Visualization; Spatial memory; Multimedia; Anxiety; Cognitive load,,"Adult; Behavior/physiology; Blindness/psychology; Cognition/physiology; Computer Peripherals; Emotions; Female; Gender Identity; Humans; Male; Middle Aged; Self-Help Devices; Touch; Vision, Ocular/physiology; Visually Impaired Persons/psychology; Young Adult",,BLINDPAD,https://pubmed.ncbi.nlm.nih.gov/25935047/ https://www.computer.org/csdl/trans/th/2015/03/07095578.html https://dblp.uni-trier.de/db/journals/toh/toh8.html#BraydaCML15 https://ieeexplore.ieee.org/document/7095578/ http://ieeexplore.ieee.org/document/7095578/ https://www.ncbi.nlm.nih.gov/pubmed/25935047,http://dx.doi.org/10.1109/toh.2015.2426692,25935047,10.1109/toh.2015.2426692,2073074511,,1,001-962-293-213-943; 003-695-340-121-507; 004-128-555-882-691; 004-384-825-683-59X; 007-852-670-504-377; 008-497-336-393-379; 008-617-705-792-724; 009-961-243-759-696; 016-579-899-521-900; 021-623-548-398-863; 033-860-942-866-850; 035-302-218-303-500; 037-491-194-003-965; 038-046-586-077-577; 041-357-004-723-943; 041-893-960-738-589; 043-380-401-330-631; 055-539-164-934-497; 055-831-716-066-199; 058-800-703-440-142; 061-417-285-837-580; 067-774-850-735-696; 068-763-040-241-128; 074-958-473-179-443; 083-076-220-695-652; 088-773-975-481-885; 096-704-970-117-17X; 121-689-354-640-786; 129-246-509-089-57X; 132-418-470-493-669; 137-545-185-939-223; 145-340-102-128-937; 150-325-162-873-23X; 188-655-843-612-642; 188-949-497-835-125,20,true,implied-oa,hybrid
047-186-279-206-675,Visual Echolocation Concept for the Colorophone Sensory Substitution Device Using Virtual Reality.,2021-01-01,2021,journal article,"Sensors (Basel, Switzerland)",14248220; 14243210,Multidisciplinary Digital Publishing Institute (MDPI),Switzerland,Patrycja Bizoń-Angov; Dominik Osinski; Michał Wierzchoń; Jarosław Konieczny,"Detecting characteristics of 3D scenes is considered one of the biggest challenges for visually impaired people. This ability is nonetheless crucial for orientation and navigation in the natural environment. Although there are several Electronic Travel Aids aiming at enhancing orientation and mobility for the blind, only a few of them combine passing both 2D and 3D information, including colour. Moreover, existing devices either focus on a small part of an image or allow interpretation of a mere few points in the field of view. Here, we propose a concept of visual echolocation with integrated colour sonification as an extension of Colorophone—an assistive device for visually impaired people. The concept aims at mimicking the process of echolocation and thus provides 2D, 3D and additionally colour information of the whole scene. Even though the final implementation will be realised by a 3D camera, it is first simulated, as a proof of concept, by using VIRCO—a Virtual Reality training and evaluation system for Colorophone. The first experiments showed that it is possible to sonify colour and distance of the whole scene, which opens up a possibility to implement the developed algorithm on a hardware-based stereo camera platform. An introductory user evaluation of the system has been conducted in order to assess the effectiveness of the proposed solution for perceiving distance, position and colour of the objects placed in Virtual Reality.",21,1,237,,Human–computer interaction; Human echolocation; Stereo camera; Virtual reality; Computer science; Stereopsis; Sensory substitution; Orientation (computer vision); Sonification,3D camera; 3D scene sonification; auditory SSD; colour sonification; distance sonification; stereo vision; virtual reality,Animals; Blindness; Echolocation; Humans; Male; Self-Help Devices; Virtual Reality; Visually Impaired Persons,,Narodowe Centrum Nauki (2016/23/B/HS6/00275),https://www.mdpi.com/1424-8220/21/1/237/htm https://pubmed.ncbi.nlm.nih.gov/33401458/ https://doi.org/10.3390/s21010237 https://ui.adsabs.harvard.edu/abs/2021Senso..21..237B/abstract https://www.mdpi.com/1424-8220/21/1/237/pdf https://dblp.uni-trier.de/db/journals/sensors/sensors21.html#Bizon-AngovOWK21 https://www.scilit.net/article/c47128316a3e331289c73956a5b9dc62 https://www.ncbi.nlm.nih.gov/pubmed/33401458,http://dx.doi.org/10.3390/s21010237,33401458,10.3390/s21010237,3113586984,PMC7796075,0,000-099-414-952-654; 021-657-554-837-75X; 022-183-457-930-690; 022-858-944-348-940; 031-696-287-727-760; 034-953-150-195-330; 064-309-126-323-022; 076-911-364-196-257; 080-021-648-530-499; 080-411-167-059-855; 087-193-049-082-327; 089-671-332-491-261; 092-921-140-853-835; 094-875-434-948-731; 112-121-560-424-373; 112-709-811-917-373; 132-889-824-719-516; 155-330-779-796-500; 177-488-684-671-738,4,true,cc-by,gold
047-187-923-746-176,A Framework for Vision-Based Building Detection and Entering for Autonomous Delivery Drones,2023-03-20,2023,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Seyed Hojat Mirtajadini; Hamidreza Fahimi; Mohammad Shahbazi,,107,3,,,Facade; Window (computing); Artificial intelligence; Computer science; Computer vision; Global Positioning System; Drone; Robot; Real-time computing; Feature (linguistics); Set (abstract data type); Engineering; Telecommunications; Linguistics; Philosophy; Structural engineering; Biology; Genetics; Programming language; Operating system,,,,,,http://dx.doi.org/10.1007/s10846-023-01834-1,,10.1007/s10846-023-01834-1,,,0,000-629-238-284-348; 000-712-051-211-744; 001-739-549-546-724; 003-353-047-178-950; 007-287-690-296-692; 007-704-080-881-311; 012-485-054-985-647; 014-166-020-318-269; 015-198-142-832-699; 015-417-609-922-215; 017-408-005-576-110; 021-625-829-627-166; 030-912-324-534-229; 031-438-764-764-176; 031-884-412-359-011; 037-741-786-434-91X; 042-335-885-065-790; 042-521-739-064-57X; 042-581-249-792-384; 051-271-561-087-674; 053-605-967-389-525; 054-501-063-816-491; 057-766-579-133-133; 060-173-817-564-022; 073-008-345-973-970; 079-473-998-415-767; 093-308-884-273-105; 094-387-589-282-572; 097-974-376-300-074; 100-365-528-915-522; 105-458-977-539-117; 106-255-946-055-407; 110-079-356-673-437; 119-804-347-451-441; 120-746-470-199-408; 121-905-607-272-234; 124-743-972-655-030; 125-319-193-920-054; 139-711-753-375-82X,0,false,,
047-320-883-602-548,An improved object identification for NAVI,,2004,conference proceedings article,2004 IEEE Region 10 Conference TENCON 2004.,,IEEE,,R. Nagarajan; G. Sainarayanan; S. Yacoob; Rosalyn R. Porle,"Navigation assistance for visually impaired (NAVI) is a vision substitute system designed to assist blind people for autonomous navigation. NAVI working concept is based on 'image to sound' conversion whereby input image captured from the video sensor is processed and transformed into stereo sound pattern. The sound is conveyed to blind's auditory system via stereo headphones. Processing in NAVI can be divided into two sub modules, which are the vision module and sonification module. In the vision module, image processing is employed to identify objects from the image. This paper presents an improved object identification methodology for NAVI. Objects are identified by its closed boundary. Properties of each object are evaluated using AI technique as to assign vision preference, so that blind user can determine the location as well as other properties of that object. Experimentation and results are presented to evaluate the proposed methodology for blind navigation purposes.",,,455,458,Engineering; Stereophonic sound; Artificial intelligence; Headphones; Navigation assistance; Visually impaired; Computer vision; Object (computer science); Identification (information); Sonification; Image processing,,,,,http://ieeexplore.ieee.org/document/1414455/ https://eprints.ums.edu.my/id/eprint/902,http://dx.doi.org/10.1109/tencon.2004.1414455,,10.1109/tencon.2004.1414455,2159960829,,0,013-847-555-426-159; 022-423-247-312-069; 032-786-641-709-377; 039-002-090-199-249; 074-324-981-337-749; 090-204-559-385-052; 124-076-971-751-139,13,false,,
047-870-846-363-636,CYCLOPS: A mobile robotic platform for testing and validating image processing and autonomous navigation algorithms in support of artificial vision prostheses,2009-08-03,2009,journal article,Computer methods and programs in biomedicine,18727565; 01692607,Elsevier BV,Netherlands,Wolfgang Fink; M. A. Tarbell,,96,3,226,233,Algorithm; Artificial intelligence; Digital camera; Testbed; Robotics; Computer science; Internet access; Image processing; Joystick,,"Algorithms; Humans; Image Processing, Computer-Assisted/statistics & numerical data; Motion; Prostheses and Implants; Retina/surgery; Robotics/instrumentation; Telemedicine/instrumentation; Vision, Ocular",,,http://europepmc.org/abstract/MED/19651459 http://dblp.uni-trier.de/db/journals/cmpb/cmpb96.html#FinkT09 https://core.ac.uk/display/4882934 http://resolver.caltech.edu/CaltechAUTHORS:20091012-151923891 http://authors.library.caltech.edu/16282/ http://www.ncbi.nlm.nih.gov/pubmed/19651459 https://dblp.uni-trier.de/db/journals/cmpb/cmpb96.html#FinkT09 https://www.sciencedirect.com/science/article/abs/pii/S0169260709002053 https://arizona.pure.elsevier.com/en/publications/cyclops-a-mobile-robotic-platform-for-testing-and-validating-imag https://doi.org/10.1016/j.cmpb.2009.06.009,http://dx.doi.org/10.1016/j.cmpb.2009.06.009,19651459,10.1016/j.cmpb.2009.06.009,1994554446,,0,015-435-301-410-415; 037-066-371-479-896; 037-200-824-530-055; 047-957-267-743-900; 072-170-487-675-943; 077-158-188-477-63X; 082-647-571-198-391; 089-307-340-737-312; 095-528-627-886-792; 096-639-004-041-863; 102-154-052-436-27X; 123-504-917-763-341; 124-976-233-783-36X; 127-817-632-406-052; 135-474-401-907-923; 153-793-072-162-65X; 177-605-295-869-476; 183-704-336-392-573,13,false,,
048-247-212-625-245,Smart Navigation for Visually Impaired people using Artificial Intelligence,2022-05-05,2022,journal article,ITM Web of Conferences,22712097; 24317578,EDP Sciences,,Rajvardhan Shendge; Aditya Patil; Siddhi Kadu,"<jats:p>This research introduces a blind aid that uses a live object recognition system. People who are blind or partially sighted rely significantly on their other senses, such as touch and auditory cues, to comprehend their surroundings. There is a need to deploy a technology that assists visually impaired persons in their daily routines, as there is now very little aid. Existing solutions, such as Screen Reading software and Braille devices, assist visually impaired individuals in reading and gaining access to numerous gadgets. However, these technologies are rendered worthless When the blind need to perform basic activities like recognizing the situation before them, such as recognizing people or objects, technologies become ineffective. This method will benefit blind or visually challenged all across the world. The goal is to help a person with total or partial blindness obtain a second set of eyesight without the assistance of a guardian, allowing them to live a better and more independent life. This project outlines working to create a more welcoming and inclusive environment, focusing on assistive technology that provides services, resources, and information to those with visual disabilities.</jats:p>",44,,3053,03053,Visually impaired; Braille; Set (abstract data type); Human–computer interaction; Reading (process); Partially sighted; Assistive technology; Computer science; Blindness; Low vision; Screen reader; Visual impairment; Object (grammar); Disabled people; Multimedia; Psychology; Artificial intelligence; Applied psychology; Medicine; Life style; Psychiatry; Political science; Optometry; Law; Programming language; Operating system,,,,,https://www.itm-conferences.org/articles/itmconf/pdf/2022/04/itmconf_icacc2022_03053.pdf https://doi.org/10.1051/itmconf/20224403053 https://www.itm-conferences.org/10.1051/itmconf/20224403053/pdf,http://dx.doi.org/10.1051/itmconf/20224403053,,10.1051/itmconf/20224403053,,,0,000-637-094-068-880; 008-734-791-754-793; 021-643-768-856-975; 031-075-685-724-870; 034-446-688-560-241; 036-032-124-310-776; 043-196-700-803-427; 106-587-878-038-509; 124-275-263-960-476; 151-697-459-473-353,2,true,cc-by,gold
048-450-417-928-286,ICHMS - Human-Robot Interaction for Assisted Object Grasping by a Wearable Robotic Object Manipulation Aid for the Blind,,2020,book,2020 IEEE International Conference on Human-Machine Systems (ICHMS),,IEEE,,Lingqiu Jin; He Zhang; Yantao Shen; Cang Ye,"This paper presents a new hand-worn device, called wearable robotic object manipulation aid (W-ROMA), that can help a visually impaired individual locate a target object and guide the hand to take a hold of it. W-ROMA may assist the individual for navigational (e.g., grasping a chair and moving it to make path) or non-navigational purpose (e.g, grasping a mug). The device consists of a sensing unit and a guiding unit. The sensing unit uses a Structure Core sensor, comprising of an RGB-D camera and an Inertial Measurement Unit (IMU), to detect the target object and estimate the device pose. Based on the object and pose information, the guiding unit computes the Desired Hand Movement (DHM) and convey it to the user by an electro-tactile display to guide the hand to approach the object. A speech interface is developed and used as an additional way to convey the DHM and used for human-robot interaction. A new method, called Depth Enhanced Visual-Inertial Odometry (DVIO), is proposed for 6-DOF device pose estimation. It tightly couples the camera’s depth and visual data with the IMU data in a graph optimization process to produce more accurate pose estimation than the existing state-of-the-art approach. The estimated poses are used to “stitched” the imaging and point cloud data captured at different points to form a larger view of the scene for object detection. They can also be used to position the individual for wayfinding. Experimental results demonstrate that the DVIO method outperforms the state-of-the-art VIO approach in 6-DOF pose estimation.",,,1,6,Point cloud; Human–robot interaction; Wearable computer; Artificial intelligence; Object detection; Computer vision; Computer science; Odometry; Pose; Object (computer science); Inertial measurement unit,,,,,https://dblp.uni-trier.de/db/conf/ichms/ichms2020.html#JinZ0Y20,http://dx.doi.org/10.1109/ichms49158.2020.9209377,,10.1109/ichms49158.2020.9209377,3090755184,,0,004-642-445-148-308; 014-279-588-505-48X; 020-066-230-655-756; 020-537-032-050-893; 022-839-587-252-946; 036-407-649-013-756; 037-403-741-955-39X; 037-607-185-201-865; 038-858-720-835-740; 041-390-985-853-458; 069-480-623-909-181; 074-006-536-890-200; 075-519-293-886-898; 084-446-744-396-567; 087-953-806-585-538; 094-619-155-693-614; 098-440-015-522-27X; 099-837-467-202-965; 099-911-359-242-620; 108-908-819-844-620; 122-445-837-537-033; 128-428-343-385-846; 167-093-869-631-338,5,false,,
048-858-093-272-266,Work directions and new results in electronic travel aids for blind and visually impaired people,2010-07-22,2010,conference proceedings,,,,,Virgil Tiponut; Daniel Ianchis; Mihai Bash; Zoltan Haraszy,"Many efforts have been invested in the last years, based on sensor technology and signal processing, to develop electronic travel aids (ETA) capable to improve the mobility of blind users in unknown or dynamically changing environment. In spite of these efforts, the already proposed ETA do not meet the requirements of the blind community and the traditional tools (white cane and guiding dogs) are still the only used by visually impaired. In this paper, research efforts to improve the main two components of an ETA tool: the Obstacles Detection System (ODS) and the Man-machine Interface (MMI) are presented. Now, for the first time, the ODS under development is bioinspired from the visual system of insects, particularly from the locust and from the fly. Some original results of the author's team, related to the new concept of Acoustical Virtual Reality (AVR) used as a MMI is then discussed in more detail. Some conclusions and the further developments in this area are also presented.",,,347,352,Human–computer interaction; Interface (computing); Engineering; Human–machine interface; Work (electrical); Virtual reality; Visually Impaired Persons; White cane; Visually impaired; Telecommunications,,,,,http://dl.acm.org/citation.cfm?id=1984140.1984210,http://dl.acm.org/citation.cfm?id=1984140.1984210,,,2611061742,,0,011-458-682-081-403; 019-306-790-422-912; 031-523-666-942-844; 032-342-378-386-840; 041-484-446-914-300; 042-186-837-332-894; 045-531-424-234-149; 048-770-790-068-483; 063-957-925-086-43X; 066-114-366-503-885; 066-469-450-548-325; 071-405-244-808-059; 073-495-407-090-665; 074-671-802-599-660; 077-113-362-690-85X; 084-533-882-547-840; 094-108-145-447-415; 099-646-885-015-028; 111-536-450-707-100,0,false,,
049-093-744-243-624,Personalized emergency medical assistance for disabled people,2011-01-04,2011,journal article,User Modeling and User-Adapted Interaction,09241868; 15731391,Springer Science and Business Media LLC,Netherlands,Luca Chittaro; Elio Carchietti; Luca De Marco; Agostino Zampa,,21,4,407,440,Special case; International Classification of Functioning  Disability and Health; Knowledge-based systems; World Wide Web; Usability; Information system; Computer science; Knowledge base; Emergency medical services; Medical emergency; End user,,,,,https://link.springer.com/article/10.1007/s11257-010-9092-2 https://core.ac.uk/display/53343483 https://doi.org/10.1007/s11257-010-9092-2 https://dblp.uni-trier.de/db/journals/umuai/umuai21.html#ChittaroCMZ11 http://hcilab.uniud.it/images/stories/publications/2011-05/EmergencyAssitanceDisabled_UMUAI.pdf https://link.springer.com/content/pdf/10.1007%2Fs11257-010-9092-2.pdf https://rd.springer.com/content/pdf/10.1007%2Fs11257-010-9092-2.pdf,http://dx.doi.org/10.1007/s11257-010-9092-2,,10.1007/s11257-010-9092-2,2165295514,,0,001-276-234-757-991; 002-438-127-762-096; 009-176-637-243-621; 013-936-790-024-007; 016-781-178-495-190; 018-405-204-070-828; 019-216-864-814-419; 024-842-599-764-420; 025-678-016-143-310; 030-028-372-259-658; 031-488-648-419-362; 043-340-759-701-074; 060-921-211-734-884; 080-996-291-759-441; 090-025-121-958-529; 097-678-047-896-78X; 112-252-862-875-356; 115-371-026-168-091; 132-500-289-343-415; 139-952-235-209-28X; 145-319-654-073-35X,13,false,,
049-119-878-872-736,Color-to-speech sensory substitution device for the visually impaired,1997-09-18,1997,conference proceedings article,"Machine Vision Applications, Architectures, and Systems Integration VI",0277786x,SPIE,,Gabriel McMorrow; Xiaojun Wang; Paul F. Whelan,"A hardware device is presented that converts color to speech for use by the blind and visually impaired. The use of audio tones for transferring knowledge of colors identified to individuals was investigated but was discarded in favor of the use of direct speech. A unique color-clustering algorithm was implemented using a hardware description language (VHDL), which in-turn was used to program an Altera Corporation's programmable logic device (PLD). The PLD maps all possible incoming colors into one of 24 color names, and outputs an address to a speech device, which in-turn plays back one of 24 voice recorded color names. To the author's knowledge, there are only two such color to speech systems available on the market. However, both are designed to operate at a distance of less than an inch from the surface whose color is to be checked. The device presented here uses original front-end optics to increase the range of operation from less than an inch to sixteen feet and greater. Because of the increased range of operation, the device can not only be used for color identification, but also as a navigation aid.",3205,,272,281,Speech synthesis; Artificial intelligence; Programmable logic device; Speech recognition; Computer vision; VHDL; Computer science; Hardware description language; Direct speech; Color vision; Identification (information); Sensory substitution,,,,,http://doras.dcu.ie/4667/ http://www.vsg.dcu.ie/papers/col-speech.pdf https://www.spiedigitallibrary.org/conference-proceedings-of-spie/3205/1/Color-to-speech-sensory-substitution-device-for-the-visually-impaired/10.1117/12.285572.full https://ui.adsabs.harvard.edu/abs/1997SPIE.3205..272M/abstract http://doras.dcu.ie/4667/1/GmcM_spie_b_1997.pdf https://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=932045 https://core.ac.uk/download/11309053.pdf,http://dx.doi.org/10.1117/12.285572,,10.1117/12.285572,2115506183,,1,017-785-476-013-107; 034-280-032-096-231; 089-121-649-599-223; 145-309-397-529-387,0,false,,
049-186-847-201-569,Brain-machine interfaces: an overview,2014-01-01,2014,journal article,Translational Neuroscience,20816936; 20813856,Walter de Gruyter GmbH,Austria,Mikhail A. Lebedev,"Brain-machine interfaces (BMIs) hold promise to treat neurological disabilities by linking intact brain circuitry to assistive devices, such as limb prostheses, wheelchairs, artificial sensors, and computers. BMIs have experienced very rapid development in recent years, facilitated by advances in neural recordings, computer technologies and robots. BMIs are commonly classified into three types: sensory, motor and bidirectional, which subserve motor, sensory and sensorimotor functions, respectively. Additionally, cognitive BMIs have emerged in the domain of higher brain functions. BMIs are also classified as noninvasive or invasive according to the degree of their interference with the biological tissue. Although noninvasive BMIs are safe and easy to implement, their information bandwidth is limited. Invasive BMIs hold promise to improve the bandwidth by utilizing multichannel recordings from ensembles of brain neurons. BMIs have a broad range of clinical goals, as well as the goal to enhance normal brain functions.",5,1,99,110,Neuroscience; Cognition; Intact brain; Biological tissue; Computer science; Sensory system,,,,,https://www.degruyter.com/downloadpdf/j/tnsci.2014.5.issue-1/s13380-014-0212-z/s13380-014-0212-z.xml https://link.springer.com/10.2478/s13380-014-0212-z https://cyberleninka.org/article/n/149600.pdf https://www.degruyter.com/document/doi/10.2478/s13380-014-0212-z/html https://link.springer.com/article/10.2478/s13380-014-0212-z https://cyberleninka.org/article/n/149600,http://dx.doi.org/10.2478/s13380-014-0212-z,,10.2478/s13380-014-0212-z,2021526488,,0,000-052-379-550-319; 000-620-687-822-542; 001-018-334-343-831; 001-078-163-106-691; 001-406-348-488-905; 001-998-190-010-646; 002-161-252-922-133; 002-556-452-366-211; 003-899-976-598-508; 004-931-010-653-795; 005-390-027-602-721; 005-910-609-301-853; 007-255-141-423-787; 008-241-252-958-927; 008-425-429-320-66X; 008-882-304-396-584; 008-998-692-238-805; 009-239-934-516-556; 009-646-750-253-614; 009-880-844-567-521; 011-803-200-068-324; 013-070-273-963-159; 013-102-577-573-84X; 015-662-412-990-475; 017-772-995-205-300; 017-850-729-128-032; 018-541-973-747-30X; 018-685-518-313-318; 019-837-466-048-458; 020-123-250-751-743; 021-075-620-673-811; 024-677-955-950-339; 029-538-352-381-28X; 033-089-760-980-971; 033-680-152-491-873; 034-196-646-262-503; 034-535-304-023-821; 035-711-681-557-614; 036-274-480-131-346; 036-439-871-218-724; 036-934-321-656-963; 037-206-384-326-997; 037-830-454-424-250; 037-897-928-100-415; 038-781-802-926-598; 038-962-945-171-842; 039-374-940-359-841; 042-143-114-176-701; 044-407-800-218-739; 044-680-969-871-95X; 045-365-874-703-095; 045-544-826-325-134; 048-645-780-951-595; 049-018-224-908-039; 049-581-604-510-906; 053-754-100-706-742; 055-697-385-269-746; 056-786-156-818-458; 060-929-827-144-774; 062-437-472-062-13X; 062-444-657-598-008; 062-583-527-169-264; 062-730-865-863-803; 063-048-719-705-828; 064-814-684-744-427; 066-958-904-336-587; 070-724-314-469-841; 072-511-789-318-047; 072-642-222-407-946; 073-013-387-621-072; 073-223-074-126-56X; 080-211-245-249-691; 082-125-901-557-206; 082-550-850-658-126; 083-978-702-127-44X; 085-822-413-929-261; 088-586-813-027-328; 094-881-119-442-336; 097-112-350-957-271; 098-499-946-460-720; 101-360-246-799-774; 102-423-075-752-948; 108-916-843-470-446; 114-171-710-588-225; 126-161-361-182-070; 131-585-477-797-309; 156-467-530-125-609,76,true,cc-by-nc-nd,gold
049-716-464-360-111,Human-Robot Interaction Through Gesture-Free Spoken Dialogue,,2004,journal article,Autonomous Robots,09295593; 15737527,Springer Science and Business Media LLC,Netherlands,Vladimir Kulyukin,,16,3,239,257,Human–robot interaction; Artificial intelligence; Set (psychology); Cognitive robotics; Mobile robot; Natural language processing; Gesture; Operator (linguistics); Action (philosophy); Computer science; Robot,,,,,https://dblp.uni-trier.de/db/journals/arobots/arobots16.html#Kulyukin04 https://dialnet.unirioja.es/servlet/articulo?codigo=862022 https://link.springer.com/article/10.1023/B:AURO.0000025789.33843.6d https://link.springer.com/content/pdf/10.1023/B:AURO.0000025789.33843.6d.pdf,http://dx.doi.org/10.1023/b:auro.0000025789.33843.6d,,10.1023/b:auro.0000025789.33843.6d,1987618324,,5,003-627-187-789-10X; 008-086-871-753-528; 012-172-478-406-410; 015-422-247-751-514; 015-857-889-716-238; 024-147-097-959-880; 031-175-243-229-387; 033-061-308-561-40X; 034-988-617-059-157; 036-445-420-760-259; 037-967-323-429-243; 039-906-274-327-227; 045-568-616-540-617; 046-039-876-846-615; 046-435-971-843-185; 050-994-946-661-289; 062-948-955-235-394; 079-958-890-552-028; 091-051-144-835-121; 104-364-377-648-666; 112-360-849-172-62X; 120-045-399-028-566; 122-308-276-224-377; 131-645-695-093-934; 136-056-790-761-052; 139-785-742-026-853; 142-354-369-122-161; 153-057-997-342-382; 157-077-582-153-743; 174-943-204-236-60X; 178-411-457-816-221; 178-838-453-135-050; 189-042-622-891-152; 192-589-277-439-92X,44,false,,
050-005-989-936-35X,Accessibility evaluation of major assistive mobile applications available for the visually impaired,2023-12-04,2023,journal article,ITU Journal on Future and Evolving Technologies,26168375,International Telecommunication Union,,null Saidarshan Bhagat; null Padmaja Joshi; null Avinash Agarwal; null Shubhanshu Gupta,"<jats:p>People with visual impairments face numerous challenges in their daily lives, including mobility, access to information, independent living, and employment. Artificial Intelligence (AI) with Computer Vision (CV) has the potential to improve their daily lives, provide them with necessary independence, and it will also spawn new opportunities in education and employment. However, while many such AI/CV-based mobile applications are now available, these apps are still not the preferred choice amongst visually impaired persons and are generally limited to advanced users only, due to certain limitations. This study evaluates the challenges faced by visually impaired persons when using AI/CV-based mobile apps. Four popular AI/CV-based apps, namely Seeing AI, Supersense, Envision and Lookout, are assessed by blind and low-vision users. Hence these mobile applications are evaluated on a set of parameters, including generic parameters based on the Web Content Accessibility Guidelines (WCAG) and specific parameters related to mobile app testing. The evaluation not only focused on the guidelines but also on the feedback that was gathered from these users on parameters covering the apps' accuracy, response time, reliability, accessibility, privacy, energy efficiency and usability. The paper also identifies the areas of improvement in the development and innovation of these assistive apps. This work will help developers create better accessible AI-based apps for the visually impaired.</jats:p>",4,4,631,643,Usability; Visually impaired; Computer science; Web accessibility; Assistive technology; Mobile device; Screen reader; Multimedia; Mobile apps; Human–computer interaction; Internet privacy; World Wide Web; The Internet; Web standards,,,,,https://www.itu.int/dms_pub/itu-s/opb/jnl/S-JNL-VOL4.ISSUE4-2023-A45-PDF-E.pdf https://doi.org/10.52953/tnrv4696,http://dx.doi.org/10.52953/tnrv4696,,10.52953/tnrv4696,,,0,,0,true,cc-by-nc-nd,hybrid
050-007-989-745-990,A mobile phone system to find crosswalks for visually impaired pedestrians,2008-10-22,2008,journal article,Technology and disability,10554181; 1878643x,IOS Press,Netherlands,Huiying Shen; Kee-Yip Chan; James M. Coughlan; John A. Brabyn,"Urban intersections are the most dangerous parts of a blind or visually impaired pedestrian's travel. A prerequisite for safely crossing an intersection is entering the crosswalk in the right direction and avoiding the danger of straying outside the crosswalk. This paper presents a proof of concept system that seeks to provide such alignment information. The system consists of a standard mobile phone with built-in camera that uses computer vision algorithms to detect any crosswalk visible in the camera's field of view; audio feedback from the phone then helps the user align him/herself to it. Our prototype implementation on a Nokia mobile phone runs in about one second per image, and is intended for eventual use in a mobile phone system that will aid blind and visually impaired pedestrians in navigating traffic intersections.",20,3,217,224,Human–computer interaction; Schema crosswalk; Proof of concept; Engineering; Pedestrian; Mobile phone; Phone; Visually impaired; Audio feedback; Intersection (aeronautics),,,,NEI NIH HHS (R21 EY015187) United States; NEI NIH HHS (R21 EY015187-01A2) United States,https://pubmed.ncbi.nlm.nih.gov/20411035/ https://content.iospress.com/articles/technology-and-disability/tad00260 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2856957/ https://europepmc.org/articles/PMC2856957,http://dx.doi.org/10.3233/tad-2008-20304,20411035,10.3233/tad-2008-20304,1775112826,PMC2856957,0,020-397-774-916-201; 027-893-571-523-130; 029-066-983-204-332; 034-119-578-405-701; 046-262-856-533-736; 063-593-313-740-053; 068-858-525-190-391; 076-853-538-573-049; 090-150-759-047-102; 092-093-393-970-684; 099-150-824-995-115; 100-404-039-254-355; 109-617-690-629-802; 110-614-837-026-718; 121-565-008-467-672; 136-878-651-956-733; 140-813-351-823-696; 168-052-814-589-107,19,true,,unknown
050-130-070-252-430,Mobility and orientation aid for blind persons using artificial vision,2007-11-01,2007,journal article,Journal of Physics: Conference Series,17426596,IOP Publishing,,Gustavo Costa; Adrián Gusberti; Juan Pablo Graffigna; Martín Guzzo; Oscar H Nasisi,"Blind or vision-impaired persons are limited in their normal life activities. Mobility and orientation of blind persons is an ever-present research subject because no total solution has yet been reached for these activities that pose certain risks for the affected persons. The current work presents the design and development of a device conceived on capturing environment information through stereoscopic vision. The images captured by a couple of video cameras are transferred and processed by configurable and sequential FPGA and DSP devices that issue action signals to a tactile feedback system. Optimal processing algorithms are implemented to perform this feedback in real time. The components selected permit portability; that is, to readily get used to wearing the device.",90,1,012090,,Engineering; Artificial intelligence; Action (philosophy); Blind persons; Artificial vision; Computer vision; Field-programmable gate array; Software portability; Stereopsis; Digital signal processing; Orientation (computer vision),,,,,http://ui.adsabs.harvard.edu/abs/2007JPhCS..90a2090C/abstract https://iopscience.iop.org/article/10.1088/1742-6596/90/1/012090/meta,http://dx.doi.org/10.1088/1742-6596/90/1/012090,,10.1088/1742-6596/90/1/012090,2152505596,,0,000-069-249-354-951; 013-650-518-339-057; 036-742-969-097-379; 038-885-814-347-856; 045-673-862-191-277; 073-871-952-541-843; 091-850-552-790-138; 112-121-560-424-373; 152-597-804-167-479; 153-697-872-076-389; 193-123-800-849-774,8,true,,gold
050-194-524-526-639,An image enhancement tool: Pattern Recognition Image Augmented Resolution,2016-09-03,2016,journal article,Pattern Recognition and Image Analysis,10546618; 15556212,Pleiades Publishing Ltd,United States,Marco Righi; Mario D’Acunto; Ovidio Salvetti,,26,3,518,523,Modular design; Digital image processing; Image (mathematics); Image segmentation; Sub-pixel resolution; Pattern recognition (psychology); Artificial intelligence; Pattern recognition; Modularity (networks); Computer vision; Computer science; Image processing,,,,,https://link.springer.com/content/pdf/10.1134%2FS1054661816030160.pdf https://link.springer.com/article/10.1134%2FS1054661816030160,http://dx.doi.org/10.1134/s1054661816030160,,10.1134/s1054661816030160,2510833672,,0,009-732-731-940-25X; 023-263-133-774-798; 027-853-391-056-735; 029-212-418-304-077; 061-333-384-977-118; 085-627-333-737-784; 085-674-040-165-681; 090-123-264-083-482; 110-453-931-054-200; 125-850-943-924-350; 184-446-291-593-247,2,false,,
050-454-154-245-773,A smart obstacle avoiding technology based on depth camera for blind and visually impaired people,2023-08-11,2023,journal article,CCF Transactions on Pervasive Computing and Interaction,2524521x; 25245228,Springer Science and Business Media LLC,,Jian He; Xuena Song; Yuhan Su; Zhonghua Xiao,,5,4,382,395,Obstacle; Computer science; Computer vision; Artificial intelligence; Point cloud; Wearable computer; Enhanced Data Rates for GSM Evolution; Point (geometry); Process (computing); Set (abstract data type); Obstacle avoidance; Real-time computing; Embedded system; Mathematics; Geography; Archaeology; Geometry; Mobile robot; Robot; Programming language; Operating system,,,,Key Technologies Research and Development Program; the Natural Science Foundation of China,,http://dx.doi.org/10.1007/s42486-023-00136-7,,10.1007/s42486-023-00136-7,,,0,008-361-673-204-978; 012-185-761-308-501; 014-764-979-190-592; 025-716-725-120-01X; 031-218-334-653-826; 034-093-615-545-728; 039-274-384-165-289; 040-057-279-881-544; 049-575-510-227-033; 049-921-334-736-611; 069-706-555-672-053; 071-167-157-089-286; 073-848-308-083-663; 074-004-764-807-22X; 078-346-643-467-148; 094-907-387-316-750; 096-733-124-330-978; 141-989-402-375-017,0,false,,
050-647-003-434-367,Visual interpretability for deep learning: a survey,2018-01-28,2018,journal article,Frontiers of Information Technology & Electronic Engineering,20959184; 20959230,Zhejiang University Press,United States,Quanshi Zhang; Song-Chun Zhu,"This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, interpretability is always Achilles’ heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of a low interpretability of their black-box representations. We believe that high model interpretability may help people break several bottlenecks of deep learning, e.g., learning from a few annotations, learning via human–computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence.",19,1,27,39,Deep learning; Machine learning; Artificial intelligence; Debugging; Interpretability; Deep neural networks; Learning based; Visualization; Computer science; Artificial neural network; Convolutional neural network,,,,the ONR MURI project; the DARPA XAI Award; NSF IIS,https://paperity.org/p/85977947/visual-interpretability-for-deep-learning-a-survey https://rd.springer.com/article/10.1631/FITEE.1700808 https://www.arxiv-vanity.com/papers/1802.00614/ https://academic.hep.com.cn/fitee/EN/10.1631/FITEE.1700808 https://doi.org/10.1631/FITEE.1700808 https://dblp.uni-trier.de/db/journals/corr/corr1802.html#abs-1802-00614 https://journal.hep.com.cn/ckcest/fitee/CN/10.1631/FITEE.1700808 https://link.springer.com/content/pdf/10.1631/FITEE.1700808.pdf http://arxiv.org/abs/1802.00614,http://dx.doi.org/10.1631/fitee.1700808,,10.1631/fitee.1700808,2963374347,,4,003-364-728-341-470; 004-808-637-867-558; 009-279-217-391-224; 009-419-420-169-006; 012-373-676-998-42X; 012-491-580-321-77X; 015-979-687-053-443; 017-977-919-337-220; 018-035-504-888-781; 019-254-362-882-304; 020-233-013-143-936; 023-805-082-979-008; 029-596-977-717-535; 039-761-307-367-504; 042-748-773-416-723; 043-209-288-339-537; 044-615-027-205-434; 046-787-840-099-199; 047-709-303-999-565; 057-140-002-469-99X; 067-202-119-901-874; 068-876-930-301-20X; 078-141-527-597-27X; 080-012-198-088-60X; 084-676-944-594-449; 086-852-068-043-016; 089-307-523-904-814; 095-216-691-445-016; 107-536-140-863-111; 109-726-683-554-542; 111-113-492-067-397; 111-245-645-905-510; 118-320-045-963-399; 124-867-034-955-875; 130-536-511-180-701; 130-961-304-651-766; 139-552-118-652-140; 142-752-312-269-81X; 166-394-413-533-257; 170-037-052-993-58X; 175-200-994-318-341; 179-235-562-557-811; 185-246-612-835-842; 187-545-547-061-870; 191-540-874-787-128; 195-091-532-336-938,708,true,,green
050-688-824-154-611,Navigating Real-World Challenges: A Quadruped Robot Guiding System for Visually Impaired People in Diverse Environments,2024-05-11,2024,conference proceedings article,Proceedings of the CHI Conference on Human Factors in Computing Systems,,ACM,,Shaojun Cai; Ashwin Ram; Zhengtai Gou; Mohd Alqama Wasim Shaikh; Yu-An Chen; Yingjia Wan; Kotaro Hara; Shengdong Zhao; David Hsu,"Blind and Visually Impaired (BVI) people find challenges in navigating unfamiliar environments, even using assistive tools such as white canes or smart devices. Increasingly affordable quadruped robots offer us opportunities to design autonomous guides that could improve how BVI people find ways around unfamiliar environments and maneuver therein. In this work, we designed RDog, a quadruped robot guiding system that supports BVI individuals' navigation and obstacle avoidance in indoor and outdoor environments. RDog combines an advanced mapping and navigation system to guide users with force feedback and preemptive voice feedback. Using this robot as an evaluation apparatus, we conducted experiments to investigate the difference in BVI people's ambulatory behaviors using a white cane, a smart cane, and RDog. Results illustrated the benefits of RDog-based ambulation, including faster and smoother navigation with fewer collisions and limitations, and reduced cognitive load. We discuss the implications of our work for multi-terrain assistive guidance systems.",,,1,18,Robot; Visually impaired; Obstacle avoidance; Terrain; Human–computer interaction; Computer science; Obstacle; Navigation system; Work (physics); Doors; Global Positioning System; Simulation; Mobile robot; Artificial intelligence; Engineering; Mechanical engineering; Ecology; Telecommunications; Political science; Law; Biology; Operating system,,,,Ministry of Education - Singapore; National Research Foundation Singapore,https://dl.acm.org/doi/pdf/10.1145/3613904.3642227 https://doi.org/10.1145/3613904.3642227,http://dx.doi.org/10.1145/3613904.3642227,,10.1145/3613904.3642227,,,0,001-076-828-128-846; 004-302-589-701-458; 007-737-365-323-98X; 011-414-630-627-246; 011-797-116-120-363; 012-128-315-521-256; 013-056-435-015-274; 018-262-121-105-137; 019-828-744-033-99X; 019-925-883-027-48X; 020-373-201-301-684; 021-561-339-287-306; 023-989-567-107-266; 029-313-431-507-058; 030-127-369-356-741; 034-422-902-172-199; 037-370-232-558-03X; 037-550-015-414-716; 041-087-246-066-811; 049-467-020-706-025; 049-729-589-103-744; 059-815-766-380-149; 060-147-659-950-016; 065-078-747-021-140; 065-335-838-921-958; 066-246-145-459-483; 069-523-876-223-951; 073-928-284-197-323; 080-809-254-267-492; 081-146-706-373-635; 085-469-792-005-426; 087-272-456-866-28X; 094-871-870-293-980; 098-910-337-444-079; 102-551-569-903-749; 103-185-330-791-684; 105-790-942-783-392; 119-948-286-520-935; 121-340-418-949-802; 141-489-442-115-69X; 141-649-000-612-322; 158-368-277-657-984; 158-675-713-708-734; 164-061-088-525-756; 168-605-148-540-316; 172-095-244-078-296; 177-994-474-814-35X; 185-253-697-584-063; 196-866-598-969-196,0,true,cc-by,bronze
050-707-677-019-647,Human-Robot Interaction via a Joint-Initiative Supervised Autonomy (JISA) Framework,2022-03-11,2022,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Abbas Sidaoui; Naseem Daher; Daniel Asmar,,104,3,,,,,,,"University Research Board, American University of Beirut",,http://dx.doi.org/10.1007/s10846-022-01592-6,,10.1007/s10846-022-01592-6,,,0,001-298-245-371-258; 001-621-132-504-357; 002-365-249-083-978; 003-240-249-978-541; 005-515-885-541-95X; 006-084-755-651-952; 009-066-840-106-465; 010-199-501-078-783; 010-597-997-599-585; 010-720-630-004-187; 010-802-966-800-012; 011-852-152-785-687; 016-866-038-596-89X; 017-085-435-828-791; 018-533-200-386-215; 019-717-632-689-415; 019-993-131-033-216; 020-475-545-984-076; 021-418-821-591-988; 022-455-767-552-02X; 026-672-312-718-982; 027-389-449-971-279; 028-213-288-243-963; 031-040-055-709-385; 032-908-179-707-496; 038-278-031-924-675; 046-369-670-580-956; 055-665-587-845-75X; 057-605-415-718-904; 059-343-970-754-002; 061-052-294-604-165; 074-183-327-932-520; 082-450-464-579-94X; 082-816-213-096-832; 086-165-370-643-202; 087-706-488-321-116; 090-202-226-004-072; 091-452-455-693-367; 097-099-238-272-252; 097-608-593-048-59X; 098-969-709-087-25X; 099-550-490-819-885; 103-430-584-621-948; 105-896-052-357-821; 106-442-900-859-77X; 110-117-755-260-765; 115-595-296-036-454; 128-921-604-123-473; 131-649-927-455-438; 133-917-480-236-909; 138-692-328-236-207; 145-882-313-734-263; 154-932-089-099-427; 174-348-920-384-535; 186-518-268-920-02X; 187-550-180-759-653,2,false,,
051-209-066-342-087,On real time stereo image processing and sonification methodologies applied towards SVETA,,2006,dissertation,,,,,G. Balakrishnan,"The main objective of this thesis is to develop a Stereo Vision based Electronic Travel Aid (SVETA) for visually impaired people. A hardware system is developed, which includes stereo cameras and stereo earphones molded in a headgear and Compact Computing Device (CCD) duly placed in a designed pouch. The stereo cameras capture stereo images of the environment. The captured images are processed and mapped to stereo musical sound patterns to the earphones. Earlier efforts mentioned in the literature towards single; camera based vision aids, did not provide 3D information about the environment. Obstacle detection and its distance information are the significant features for comfortable blind navigation. In this thesis, to incorporate the distance information, stereo imaging techniques are; proposed. Stereo image processing requirement in this application is critical and therefore conventional stereo matching methodology cannot be applied directly in this problem. The stereo image processing developed in this thesis, is designed to highlight the object properties from background and compute its distance in real time.; Three methods are proposed for stereo image processing namely isolated object matching method, fuzzy relation method and improved area correlation method, whose merits and demerits are compared. The resultant image consists of 3D information of the objects with background suppressed. The resultant image is sonified to produce musical stereo acoustic patterns. Two methods are used for image; sonification. The frequency of the sound depends on the height or elevation of the image pixels in the image plane. The amplitude of the sound depends on the intensity value of the image pixels. The left half of the image is sonified to left earphone and the right half of the image is sonified to the right earphone. The pleasantness of the sound is improved using octave frequencies and certain voice; commands are also incorporated to alert the blind user about any impending obstacles. Blind and non blind volunteers were trained with the developed SVETA system and they were tested to identify the environment using SVETA. They were able to identify the objects based on its distance and other characteristics with the help of the musical sound. They were also able to navigate in indoor and restricted outdoor environments",,,,,Image plane; Computer graphics (images); Engineering; Artificial intelligence; Pixel; Stereo camera; Stereo imaging; Computer stereo vision; Computer vision; Stereo cameras; Stereopsis; ORTF stereo technique,,,,,http://eprints.ums.edu.my/10214/,http://eprints.ums.edu.my/10214/,,,65017900,,0,,0,false,,
051-514-402-816-140,Traffic light and moving object detection for a guide-dog robot,2020-07-09,2020,journal article,The Journal of Engineering,20513305,Institution of Engineering and Technology (IET),Egypt,Qiang Chen; Yinong Chen; Jinhui Zhu; Gennaro De Luca; Mei Zhang; Ying Guo,"Guide dogs are helpful for visually impaired people for navigating through the streets. However, it is expensive and time consuming to train a guide dog. In addition, a guide dog cannot decide when and where to cross a street safely, and it is up to the human to decide. Here, the authors propose a framework for creating a guide dog robot by using artificial intelligence and other technologies. The proposed framework is based on an Intel UP squared board, together with a Neural Compute Stick Movidius to process the images gathered from a GoPro camera. MobileNet single shot detector (SSD) is the main framework to detect the moving objects in the environment. The final decision is made after fusing the information gathered from all the sources. The authors also apply the Amazon Alexa device for the voice communication between the guide dog robot and the visually impaired person. A prototype of the proposed system is implemented and tested. Experimental results show that the proposed framework can process the information at a traffic intersection scene and can guide a blind person to cross the street safely.",2020,13,675,678,Artificial intelligence; Mobile robot; Object detection; Voice communication; Traffic signal; Single shot; Computer vision; Computer science; Robot; Detector; Process (computing),,,,Fundamental Research Funds for the Central Universities; China Scholarship Council,https://digital-library.theiet.org/content/journals/10.1049/joe.2019.1137 https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/joe.2019.1137,http://dx.doi.org/10.1049/joe.2019.1137,,10.1049/joe.2019.1137,3042120022,,0,058-013-370-483-44X; 150-419-741-029-022; 195-091-532-336-938,10,true,"CC BY, CC BY-ND, CC BY-NC, CC BY-NC-ND",gold
051-829-763-503-023,Human–Machine Requirements’ Convergence for the Design of Assistive Navigation Software: Τhe Case of Blind or Visually Impaired People,2021-12-01,2021,book chapter,Learning and Analytics in Intelligent Systems,26623447; 26623455,Springer International Publishing,,P. Theodorou; A. Meliones,"Autonomous navigation is a desirable capability for various types of “smart” devices or vehicles. The development of software designed for this purpose has become a central research field for major companies, as well as in academia. This trend is accompanied by a tendency to equip moving devices with artificial intelligence (AI) features. Interestingly, however, the most (and not artificially) intelligent unit which may require assistance from digital applications in order to achieve autonomous navigation is a blind or a visually impaired person (BVI). It is argued that as the capabilities of AI are being enhanced, convergence will occur among a significant subset of the requirements concerning assistive navigation software for the BVI and AI-equipped moving devices, respectively. The corresponding requirements which have been elicited through interviews with BVI people are presented. A subset of these requirements, which exhibit direct or prospective convergence with the corresponding requirements of AI devices is then outlined, with emphasis on possible opportunities for interaction between the two research topics.",,,263,283,Human–computer interaction; Human–machine system; Software; Convergence (relationship); Visually impaired; Field (computer science); Computer science; Requirements analysis,,,,,https://link.springer.com/chapter/10.1007/978-3-030-87132-1_12,http://dx.doi.org/10.1007/978-3-030-87132-1_12,,10.1007/978-3-030-87132-1_12,3215181828,,0,002-794-722-068-681; 007-489-086-930-63X; 015-714-235-272-975; 017-019-507-873-052; 018-444-619-697-026; 019-150-105-799-327; 021-183-124-908-966; 024-876-376-113-351; 027-748-465-609-095; 031-175-485-676-104; 031-972-023-601-484; 032-413-382-370-011; 040-660-173-610-646; 064-427-092-098-337; 071-473-658-566-47X; 073-338-244-636-803; 087-272-456-866-28X; 095-670-610-705-585; 098-910-337-444-079; 102-952-660-914-374; 110-690-766-883-070; 141-489-442-115-69X; 142-575-622-508-48X; 180-813-779-294-458; 190-714-153-564-662; 195-196-708-451-173,0,false,,
051-861-879-703-203,A Walking Assistant Robotic System for the Visually Impaired Based on Computer Vision and Tactile Perception,2015-07-30,2015,journal article,International Journal of Social Robotics,18754791; 18754805,Springer Science and Business Media LLC,Germany,Dejing Ni; Aiguo Song; Lei Tian; Xiaonong Xu; Chen Danfeng,,7,5,617,628,Image compression; Wearable computer; Artificial intelligence; Object detection; Task (project management); Mechatronics; Tactile perception; Computer vision; Robotics; Computer science; Feature extraction,,,,Natural Science Foundation of China,http://dx.doi.org/10.1007/s12369-015-0313-z https://link.springer.com/article/10.1007/s12369-015-0313-z https://dx.doi.org/10.1007/s12369-015-0313-z https://dblp.uni-trier.de/db/journals/ijsr/ijsr7.html#NiSTXC15 https://link.springer.com/10.1007/s12369-015-0313-z https://link.springer.com/article/10.1007/s12369-015-0313-z/fulltext.html,http://dx.doi.org/10.1007/s12369-015-0313-z,,10.1007/s12369-015-0313-z,2185273816,,1,001-076-828-128-846; 002-397-792-416-419; 004-766-408-593-108; 005-303-872-644-71X; 008-674-915-032-695; 010-004-081-294-240; 010-523-152-505-610; 010-937-704-727-204; 011-088-825-646-470; 013-056-435-015-274; 014-621-188-486-583; 019-550-995-456-408; 022-489-327-390-755; 025-295-668-075-968; 028-628-140-541-774; 030-127-369-356-741; 030-949-214-673-542; 046-155-924-701-401; 051-065-738-489-461; 059-407-478-677-340; 061-669-253-155-324; 068-708-501-300-740; 069-022-769-254-976; 069-116-650-623-84X; 070-148-918-957-43X; 076-121-436-407-96X; 091-906-536-550-206; 092-391-305-190-481; 092-575-825-025-769; 092-620-138-391-054; 099-724-356-303-076; 143-099-109-916-855; 162-016-782-983-620; 180-497-626-113-327; 183-935-933-940-273,23,false,,
052-142-044-790-359,Analysis and Validation of Cross-Modal Generative Adversarial Network for Sensory Substitution,2021-06-08,2021,journal article,International journal of environmental research and public health,16604601; 16617827,MDPI AG,Switzerland,Mooseop Kim; Yunkyung Park; Kyeong-Deok Moon; Chi Yoon Jeong,"Visual-auditory sensory substitution has demonstrated great potential to help visually impaired and blind groups to recognize objects and to perform basic navigational tasks. However, the high latency between visual information acquisition and auditory transduction may contribute to the lack of the successful adoption of such aid technologies in the blind community; thus far, substitution methods have remained only laboratory-scale research or pilot demonstrations. This high latency for data conversion leads to challenges in perceiving fast-moving objects or rapid environmental changes. To reduce this latency, prior analysis of auditory sensitivity is necessary. However, existing auditory sensitivity analyses are subjective because they were conducted using human behavioral analysis. Therefore, in this study, we propose a cross-modal generative adversarial network-based evaluation method to find an optimal auditory sensitivity to reduce transmission latency in visual-auditory sensory substitution, which is related to the perception of visual information. We further conducted a human-based assessment to evaluate the effectiveness of the proposed model-based analysis in human behavioral experiments. We conducted experiments with three participant groups, including sighted users (SU), congenitally blind (CB) and late-blind (LB) individuals. Experimental results from the proposed model showed that the temporal length of the auditory signal for sensory substitution could be reduced by 50%. This result indicates the possibility of improving the performance of the conventional vOICe method by up to two times. We confirmed that our experimental results are consistent with human assessment through behavioral experiments. Analyzing auditory sensitivity with deep learning models has the potential to improve the efficiency of sensory substitution.",18,12,6216,,Deep learning; Artificial intelligence; Visual perception; Transduction (psychology); Perception; Substitution (logic); Speech recognition; Computer science; Crossmodal; Sensory substitution; Latency (engineering),auditory sensitivity; cross-modal perception; generative adversarial network; sensory substitution; visual perception,"Acoustic Stimulation; Auditory Perception; Blindness; Humans; Vision, Ocular; Visually Impaired Persons",,Electronics and Telecommunications Research Institute,https://www.ncbi.nlm.nih.gov/pubmed/34201269 https://www.mdpi.com/1660-4601/18/12/6216 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8228544 https://www.mdpi.com/1660-4601/18/12/6216/pdf https://europepmc.org/article/PMC/PMC8228544 https://ksp.etri.re.kr/ksp/article/read?id=63680,http://dx.doi.org/10.3390/ijerph18126216,34201269,10.3390/ijerph18126216,3167811510,PMC8228544,0,002-391-643-288-41X; 004-772-554-982-380; 008-204-392-567-001; 009-322-831-272-066; 009-522-765-080-723; 013-847-555-426-159; 013-859-094-501-045; 014-645-445-445-966; 018-350-866-557-533; 018-483-878-434-767; 019-149-805-110-617; 021-513-991-593-463; 023-163-670-927-659; 026-483-053-931-62X; 027-033-143-017-928; 031-130-884-429-451; 031-258-275-252-008; 034-513-496-520-007; 040-030-169-396-345; 043-611-493-779-019; 044-202-560-313-315; 045-673-862-191-277; 046-627-582-261-33X; 046-991-476-786-467; 050-450-529-438-232; 054-879-961-055-393; 055-171-125-989-695; 058-010-143-705-790; 059-575-844-550-706; 066-098-247-447-035; 070-227-961-882-758; 071-251-401-761-444; 071-929-328-885-593; 073-449-475-149-780; 079-973-093-856-139; 080-021-648-530-499; 082-803-606-275-601; 084-225-140-986-487; 084-750-380-873-575; 084-897-651-524-93X; 086-023-599-511-650; 089-202-138-789-61X; 105-452-473-233-598; 112-005-770-268-104; 114-030-061-764-620; 115-027-939-580-424; 115-038-566-776-021; 117-483-878-304-451; 117-942-980-457-112; 121-230-728-705-902; 126-392-531-420-966; 137-983-820-062-88X; 151-155-218-729-614; 172-581-054-759-158; 177-488-684-671-738; 180-559-374-049-445; 184-271-710-759-415; 187-536-794-450-211; 193-393-387-753-911,5,true,cc-by,gold
052-250-527-663-352,Image guidance can support scaphoid K-wire insertion: an experimental study and initial clinical experience.,2012-11-30,2012,clinical trial,International journal of computer assisted radiology and surgery,18616429; 18616410,Springer Science and Business Media LLC,Germany,H. Schöll; Martin Mentzel; Almut Jones; J. Gülke; Florian Gebhard; Michael Kraus,,8,3,471,480,Radiology; Artificial intelligence; Scaphoid bone; Computer navigation; Fluoroscopic image; Intraoperative fluoroscopy; Image guidance; Computer vision; Computer science,,"Adult; Bone Wires; Cohort Studies; Feasibility Studies; Female; Fluoroscopy; Fracture Fixation, Internal; Fractures, Bone/surgery; Humans; Male; Middle Aged; Models, Biological; Radiographic Image Interpretation, Computer-Assisted; Scaphoid Bone/injuries; Surgery, Computer-Assisted; Young Adult",,,https://rd.springer.com/article/10.1007%2Fs11548-012-0799-x https://dblp.uni-trier.de/db/journals/cars/cars8.html#SchollMJGGK13 https://doi.org/10.1007/s11548-012-0799-x https://link.springer.com/article/10.1007/s11548-012-0799-x/fulltext.html https://link.springer.com/article/10.1007%2Fs11548-012-0799-x https://www.ncbi.nlm.nih.gov/pubmed/23196791 https://europepmc.org/article/MED/23196791,http://dx.doi.org/10.1007/s11548-012-0799-x,23196791,10.1007/s11548-012-0799-x,2044310924,,0,001-982-966-075-679; 003-692-253-933-044; 003-839-556-603-126; 005-507-812-994-499; 008-732-890-548-224; 009-606-694-358-561; 010-941-045-672-909; 018-715-993-776-287; 018-720-475-727-342; 019-592-776-729-942; 019-969-254-488-966; 020-021-658-179-659; 020-142-461-997-683; 021-937-972-240-012; 026-727-224-068-823; 027-935-095-216-713; 031-439-635-319-02X; 035-563-604-503-207; 039-116-572-215-235; 040-222-601-976-821; 040-737-451-275-247; 043-743-678-883-722; 044-914-619-814-049; 047-221-905-384-85X; 058-031-014-647-937; 058-449-856-185-263; 059-525-180-171-125; 062-728-932-128-786; 074-324-981-337-749; 086-027-051-536-99X; 091-538-077-999-98X; 091-737-709-823-993; 098-977-707-347-492; 107-167-560-615-682; 110-544-995-752-634; 110-899-180-807-270; 121-617-962-979-510; 135-625-639-937-597; 139-658-411-323-27X; 146-007-848-446-437; 149-352-036-526-091,3,false,,
052-532-903-881-042,NAYAN-DRISHTI: A Revolutionary Navigation/Visual Aid for the Visually Impaired,2022-01-07,2022,book chapter,ICT Analysis and Applications,23673370; 23673389,Springer Nature Singapore,,Salil Fernandes; Jordan D'souza; Anthony Kattikaren; Dipti Jadhav,"The project/proposed product hinges on three domains of computational technology, i.e., machine learning, convolutional neural networks, and Internet of Things. The aim of the project is to invent a product that is helpful to the disabled section of society as ideally as possible try to as well as to acquaint ourselves with the much talked about and ever-growing domains of computer technology. The main functions that our proposed product will offer are detection of the obstructing object and alerting via a speaker (along with classification and distance of the object from the user) and a navigation system (which obtains live data of the current location of the user with the help of the UBLOX GPS module). The proposed product is designed in such a way so as to provide an all in one multitasking and hassle-free solution to our user and to ease the burden that comes along with the disability of blindness. The proposed product is touted as a boon to our users since it not only will help them in identifying the obstructions ahead them but will also help them to navigate from their current location to their destination with freedom and no fear.",,,319,333,Product (mathematics); Computer science; Human–computer interaction; Object (grammar); Blindness; Convolutional neural network; Artificial intelligence; Multimedia; Medicine; Geometry; Mathematics; Optometry,,,,,,http://dx.doi.org/10.1007/978-981-16-5655-2_31,,10.1007/978-981-16-5655-2_31,,,0,011-876-327-472-778; 014-898-844-427-824; 026-643-112-784-819; 053-567-501-093-090; 090-579-353-754-095; 098-966-666-704-92X,0,false,,
052-995-732-160-953,Haptic access to conventional 2D maps for the visually impaired,,2007,journal article,Journal on Multimodal User Interfaces,17837677; 17838738,Springer Science and Business Media LLC,Germany,Konstantinos Kostopoulos; Konstantinos Moustakas; Dimitrios Tzovaras; G. Nikolakis; Céline Thillou; Bernard Gosselin,,1,2,13,19,Image (mathematics); Haptic technology; Artificial intelligence; Structure (mathematical logic); Street network; Braille; Computer vision; Computer science; Representation (mathematics); Motion planning; Segmentation,,,,,https://dblp.uni-trier.de/db/journals/jmui/jmui1.html#KostopoulosMTNT07 https://link.springer.com/article/10.1007/BF02910055 http://doi.org/10.1007/BF02910055 https://doi.org/10.1007/BF02910055,http://dx.doi.org/10.1007/bf02910055,,10.1007/bf02910055,1982970088,,2,012-750-295-449-313; 013-889-868-604-441; 014-466-173-833-872; 021-048-440-726-636; 036-149-176-762-271; 037-238-441-886-162; 043-814-570-325-361; 045-372-840-476-205; 052-673-321-270-910; 055-084-531-118-005; 068-126-505-766-37X; 085-008-427-439-371; 115-940-056-507-071; 136-706-260-262-151; 148-824-702-763-782,17,false,,
053-039-375-036-44X,Integrated Image Processing Device For Visually Impaired,2024-04-17,2024,conference proceedings article,"2024 International Conference on Communication, Computing and Internet of Things (IC3IoT)",,IEEE,,J. Raja; N Kaviya; B V Koushika; R Sushma,"In a world where technology is developing more quickly than ever before, many of these advances have been aimed at making people's lives easier. To aid with these efforts, we are creating an integrated picture processor for visually impaired people. The main forms of human communication nowadays are speech and text. A person must have the vision to view text-based information. Nonetheless, persons without the ability to see can still learn things by listening. The integrated image processor is an assistive text-reading tool that uses a camera to help the blind read text on labels, printed notes, and merchandise. It entails text extraction from an image using optical character recognition (OCR) and text-to-speech (TTS) conversion to turn it into speech. This method aids the blind in reading the text and serves as the foundation for the creation of a prototype that will enable the blind to identify objects in the real world. Using Jetson Nano, the text from product descriptions is retrieved and rendered as speech, with mobility as the primary consideration. It entails text extraction from an image using optical character recognition (OCR) and text-to-speech (TTS) conversion to turn it into speech. This method aids the blind in reading the text and serves as the foundation for the creation of a prototype that will enable the blind to identify objects in the real world. Using Raspberry Pi, the text from product descriptions is retrieved and rendered as speech, with mobility as the primary consideration. By including a battery backup, portability is made possible and could be used in future technology. The user can use the device at any time and any place because of its portability. The project also has a feature that uses OpenCV and TensorFlow to recognize cash notes. Additionally, this system incorporates deep learning-based object identification, which uses MobileNets and SSD methods for object detection, to recognize various items for recognizing barriers in front of the individual. The person can receive this information as helpful auditory feedback.",,,,,Visually impaired; Computer science; Computer vision; Image processing; Artificial intelligence; Computer graphics (images); Image (mathematics); Human–computer interaction,,,,,,http://dx.doi.org/10.1109/ic3iot60841.2024.10550301,,10.1109/ic3iot60841.2024.10550301,,,0,062-128-353-400-109; 153-016-564-585-716,0,false,,
053-085-272-113-18X,Mobile Tourism Recommendation System for Visually Disabled,2023-08-14,2023,book chapter,"Artificial Intelligence and Data Science in Recommendation System: Current Trends, Technologies and Applications",,BENTHAM SCIENCE PUBLISHERS,,Pooja Selvarajan; Poovizhi Selvan; Vidhushavarshini Sureshkumar; Sathiyabhama Balasubramaniam,"<jats:p>Mobile Tourism Recommendation System recommends to a tourist the best; attractions in a particular place according to his preferences, profile and interest. First,; a Recommender system offers a list of the city places likely to interest the user. This; list estimates the user demographic classification, likes in former trips, and preferences; for the current visit. Second, a planning module schedules the list of recommended; places according to their characteristics and user limitations. The planning system; decides how and when to perform the recommended activities. For implementing these; recommender methods, we have applied different machine learning algorithms, which; are the K-nearest neighbors (K-NN) for both Clean Boot (CB) and Consolidation; Function (CF) and the decision tree for all Data Framing (DF). Thus, executing a; recommendation system for tourists helps them with user-friendly planning. Blind; people can also use this. This application provides complete voice assistance for easy; navigation via a simple button click. Vibratory and voice feedback is provided for; accurate crash alerts for visually challenged people. The application extracts its; smartness by incorporating Android and Internet of Things (IoT) support. Since blind supported applications and devices are more expensive and many blinds can not afford; them, we aim to put forth a novel, low cost and reliable approach to help the blind; explore the possibilities and power of smartphone technology in navigation. We; additionally expect to find the static variables that should be tended to, food, tidiness,; and opening times, and valuable to suggest a tourist place depending on the travel; history of the client. In this investigation, we propose a cross-planning table; methodology depending on the area’s prevalence, appraisals, idle points, and; conclusion. A targeted work for proposal streamlining is defined as dependent on these; mappings. Our outcomes show that the consolidated highlights of Latent Dirichlet; Allocation (LDA), Support vector machines (SVM), appraisals, and cross mappings are; helpful for upgraded execution. The fundamental motivation of this study was to help; businesses related to tourism.</jats:p>",,,205,215,Android (operating system); Recommender system; TRIPS architecture; Computer science; Tourism; World Wide Web; Multimedia; Human–computer interaction; Geography; Archaeology; Parallel computing; Operating system,,,,,,http://dx.doi.org/10.2174/9789815136746123010013,,10.2174/9789815136746123010013,,,0,023-105-612-719-362; 023-168-069-103-520; 034-013-748-327-262; 055-792-857-609-67X; 063-950-955-576-77X; 072-332-352-543-491; 088-836-956-110-064; 094-312-082-398-844; 114-867-559-439-440; 119-750-835-330-867; 148-886-397-271-75X; 175-901-139-875-237; 197-937-994-857-809,0,false,,
053-230-171-891-810,IPTA - Smartphone based guidance system for visually impaired person,,2012,conference proceedings article,"2012 3rd International Conference on Image Processing Theory, Tools and Applications (IPTA)",,IEEE,,Muhammad Asad; Waseem Ikram,"In order to facilitate the visually impaired person in navigation, we have developed a prototype guidance system. The main assumption of this guidance system is that there are many straight paths in different real world scenarios. These straight paths have parallel edges, which when captured as an image seem to converge to a single point called the vanishing point. Proper feature extraction and mathematical modelling of the captured frame leads to the detection of these parallel edges. The vanishing point is then calculated and a decision system is formed which notifies the blind person about his/her deviation from a straight path. The scope of this system is limited to a straight path and has been tested in different lighting conditions and with different level of occlusion. A laptop mounted on a 2D robotic platform is used to develop and verify the robustness of the algorithm. Finally, a smartphone based real-time application has been implemented for this visual guidance system, in which the decision system returns an audio output to guide the visually impaired person. This application has an average execution rate of 20 frames per second, with each frame being of 320 by 240 pixel size. The system has an accuracy of 84.1% in a scenario with pedestrians and other objects, while without pedestrians it produces an accuracy of over 90%.",,,442,447,Artificial intelligence; Pixel; Edge detection; Guidance system; Computer vision; Frame rate; Computer science; Feature extraction; Computer graphics; Robustness (computer science); Vanishing point,,,,,https://www.researchgate.net/profile/Muhammad_Asad14/publication/261494029_Smartphone_based_guidance_system_for_visually_impaired_person/links/547f2e030cf2de80e7cc784b.pdf https://ieeexplore.ieee.org/document/6469553/ http://dblp.uni-trier.de/db/conf/ipta/ipta2012.html#AsadI12 https://dblp.uni-trier.de/db/conf/ipta/ipta2012.html#AsadI12,http://dx.doi.org/10.1109/ipta.2012.6469553,,10.1109/ipta.2012.6469553,1974736993,,0,001-977-631-509-918; 006-355-537-489-40X; 013-089-871-422-440; 016-596-154-667-651; 022-080-663-087-348; 027-893-571-523-130; 028-894-873-434-548; 046-262-856-533-736; 050-007-989-745-990; 061-665-974-023-921; 068-858-525-190-391; 071-155-883-706-818; 081-141-746-741-399; 092-642-014-398-50X; 093-219-699-117-536; 109-617-690-629-802; 128-154-672-313-987; 170-097-126-166-339,8,false,,
053-230-236-592-142,Mobile Robotics Technology in the Service of the Blind and Visually Impaired,,2001,journal article,IFAC Proceedings Volumes,14746670,Elsevier BV,,Shraga Shoval; Iwan Ulrich; Johann Borenstein,,34,4,39,44,Engineering; Artificial intelligence; Human–machine interface; Mobile robot; Obstacle; PATH (variable); Visually impaired; Computer vision; Robotics; Service (systems architecture); Obstacle avoidance,,,,,https://www.sciencedirect.com/science/article/pii/S1474667017342702,http://dx.doi.org/10.1016/s1474-6670(17)34270-2,,10.1016/s1474-6670(17)34270-2,2614720659,,0,058-443-047-291-990; 065-992-777-809-97X; 073-722-974-726-137; 078-736-735-476-395; 127-769-262-221-224; 135-646-014-794-306,1,false,,
053-240-315-716-245,A Camera-Based Mobility Aid for Visually Impaired People,2015-10-15,2015,journal article,KI - Künstliche Intelligenz,09331875; 16101987,Springer Science and Business Media LLC,,Tobias Schwarze; Martin Lauer; Manuel Schwaab; Michailas Romanovas; Sandra Böhm; Thomas Jürgensohn,"We present a wearable assistance system for visually impaired persons that perceives the environment with a stereo camera system and communicates obstacles and other objects to the user in form of intuitive acoustic feedback. The system is designed to complement traditional assistance aids. We describe the core techniques of scene understanding, head tracking, and sonification and show in an experimental study that it enables users to walk in unknown urban terrain and to avoid obstacles safely.",30,1,29,36,Wearable computer; Artificial intelligence; Stereo camera; Mobility aid; Visually Impaired Persons; Urban terrain; Visually impaired; Computer vision; Audio feedback; Computer science; Sonification,,,,Bundesministerium für Bildung und Forschung (DE),https://dblp.uni-trier.de/db/journals/ki/ki30.html#SchwarzeLSRBJ16 https://link.springer.com/content/pdf/10.1007%2Fs13218-015-0407-7.pdf https://elib.dlr.de/98831/ https://dx.doi.org/10.1007/s13218-015-0407-7 https://publikationen.bibliothek.kit.edu/1000135289 https://link.springer.com/article/10.1007%2Fs13218-015-0407-7 https://core.ac.uk/display/31019810 http://dx.doi.org/10.1007/s13218-015-0407-7 https://core.ac.uk/download/31019810.pdf,http://dx.doi.org/10.1007/s13218-015-0407-7,,10.1007/s13218-015-0407-7,2180774566,,4,004-198-036-488-477; 009-142-401-900-87X; 013-056-435-015-274; 043-471-326-261-026; 061-199-231-534-098; 066-278-641-931-35X; 073-928-284-197-323; 077-517-760-010-566; 080-192-732-768-316; 102-664-707-369-750; 115-249-087-039-008; 140-661-510-168-802; 170-387-846-090-147; 175-391-077-920-292,25,true,,green
053-543-102-387-140,ASSIST: Assistive Sensor Solutions for Independent and Safe Travel of Blind and Visually Impaired People,2021-12-01,2021,conference proceedings,,,,,Zhigang Zhu; Vishnu Nair; Greg Olmschenk; William Seiple,"This paper describes the interface and testing of an indoor navigation app - ASSIST - that guides blind & visually impaired (BVI) individuals through an indoor environment with high accuracy while augmenting their understanding of the surrounding environment. ASSIST features personalized inter-faces by considering the unique experiences that BVI individuals have in indoor wayfinding and offers multiple levels of multimodal feedback. After an overview of the technical approach and implementation of the first prototype of the ASSIST system, the results of two pilot studies performed with BVI individuals are presented. Our studies show that ASSIST is useful in providing users with navigational guidance, improving their efficiency and (more significantly) their safety and accuracy in wayfinding indoors.",,,,,Human–computer interaction; Interface (computing); Visually impaired; Computer science,,,,,https://par.nsf.gov/biblio/10286824-assist-assistive-sensor-solutions-independent-safe-travel-blind-visually-impaired-people,https://par.nsf.gov/biblio/10286824-assist-assistive-sensor-solutions-independent-safe-travel-blind-visually-impaired-people,,,3216639539,,0,,0,false,,
053-628-951-915-959,An inexpensive and portable talking-tactile terminal for the visually handicapped,,1986,journal article,Journal of medical systems,01485598; 1573689x,Springer Science and Business Media LLC,United States,A. I. Karshmer; Harley R. Myler; R. D. Davis,,10,3,229,244,Human–computer interaction; Interfacing; Terminal (electronics); Programmer; Task (project management); Communication Aids for Disabled; Self-Help Devices; Terminal device; Computer science; Reading (process),,Algorithms; Blindness/rehabilitation; Communication Aids for Disabled; Computers/economics; Costs and Cost Analysis; Humans; Microcomputers/economics; Reading; Self-Help Devices; Sensory Aids; Touch,,,https://europepmc.org/abstract/MED/2946791 https://pubmed.ncbi.nlm.nih.gov/2946791/ https://www.ncbi.nlm.nih.gov/pubmed/2946791 https://link.springer.com/article/10.1007%2FBF00992818,http://dx.doi.org/10.1007/bf00992818,2946791,10.1007/bf00992818,2057749578,,0,000-715-078-373-800; 013-572-780-984-43X; 016-916-819-374-037; 018-189-306-959-085; 064-309-640-955-153; 067-703-409-249-146; 070-081-071-494-56X; 086-749-754-777-428; 090-356-995-862-794; 103-753-857-716-667; 112-952-041-660-603; 138-297-834-060-718; 139-079-984-998-681; 142-422-346-370-703,7,false,,
053-898-899-315-453,AAAI Spring Symposium: Multidisciplinary Collaboration for Socially Assistive Robotics - Robots as Haptic and Locomotor Interfaces for the Blind.,,2007,conference proceedings,,,,,Vladimir Kulyukin,"To ensure independence, an assistive shopping device must provide the blind shopper with interfaces to the supermarket’s haptic and locomotor spaces. In the absence of vision, the egocentric and allocentric frames are known to align best in the haptic space. Robots can function as interfaces to the haptic and locomotor spaces to the extent that they can eliminate the need of frame alignment in the locomotor space and cue the shopper to the salient features of the environment sufficient for product retrieval. Research on spatial cognition and navigation of the visually impaired distinguishes two spatial categories: locomotor and haptic (Ungar 2000). The haptic space is defined as the immediate space around the individual that can be sensed by touch or limb motion without any bodily translation. The locomotor space is defined as a space whose exploration requires locomotion. In the absence of vision, the egocentric (self to object) and allocentric (object to object) frames of reference align best in the haptic space. As the haptic space translates with the body, lack of vision causes the frames to misalign in the locomotor space, which negatively affects action reliability. Giving the visually impaired equal access to everyday environments entails designing interfaces to the haptic and locomotor spaces of those environments that either eliminate the necessity of alignment or enable the visually impaired to align the frames when necessary. What environments should be targeted first? In cataloguing the most functionally difficult environments for the visually impaired, Passini and Proulx (Passini & Proulx 2000) top their list with shopping complexes. Grocery shopping is an activity that presents a barrier to independence for many visually impaired individuals who either do not go grocery shopping at all or depend on sighted guides, e.g., store staffers, spouses, and friends. Can robots function as effective interfaces to the haptic and locomotor spaces in the supermarket? We believe that they can. Neither guide dogs nor white canes can effectively interface to locomotor spaces, because they cannot help their users with macronavigation, which requires topological knowledge of the environment. Sighted guides do ensure the reliable maneuCopyright c © 2007, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. Figure 1: RoboCart. vering of the haptic space, but only at the expense of independence. Loss of independence translates into loss of privacy. Our experience with visually impaired individuals in our previous robot-assisted shopping experiments convinced us that many of them are not willing to use store staffers when shopping for personal hygiene items, medicine, and other products that require discretion (Kulyukin, Gharpure, & Nicholson 2005). A Trichotomous Spatial Ontology Spatial ontologies come about when we attempt to categorize space according to the ways we interact with it (Tversky et al. 1999). Freundschuh and Egenhofer (Freundschuh & Egenhofer 1997) give a comprehensive review of previous work on categorization of space and distinguish six categories based on manipulability, locomotion, and size and use their ontology to describe previous ontologies of space in the geography literature. We contribute to this line of research a trichotomous ontology of space in a supermarket. This trichotomy is an extension of the dichotomous ontology (haptic vs. locomotor) currently used by many researchers on spatial cognition of the visually impaired. Our trichotomy is certainly incomplete. We developed it solely for the purposes of describing the interactions between blind shoppers and RoboCart, our robotic shopping assistant shown in Figure 1. RoboCart assists the shopper in two stages. It first guides the shopper into the vicinity of the desired product and then instructs the shopper on how to maneuver the haptic space within that vicinity. In the first stage, RoboCart interfaces the shopper to the locomotor space, guiding the shopper to the required aisle section and interacting with other shoppers by asking them to yield the way when a passage is blocked. In the second stage, RoboCart cues the shopper to some salient features of the environment near the haptic interface through voice instructions grounded in the shopper’s egocentric frame of reference. The design of RoboCart reflects this dual interface functionality insomuch as the device consists of twomodules: locomotor and haptic. The locomotor module consists of a Pioneer 2DX mobile robotic base from ActivMedia, Inc. upon which a wayfinding toolkit is fitted in a PVC pipe structure. A shopping basket is mounted upon the PCV structure, as shown in Figure 1. The PVC structure includes a firm handle at the lower abdomen level. A 10-key numeric keypad is attached to the handle for giving instructions to RoboCart. Using the numeric keypad, the shopper can either browse through a list of products or enter a product number. Once the shopper confirms the selection, RoboCart guides the shopper to the vicinity of the product. The haptic module consists of a wireless omni-directional barcode reader. The reader is ergonomically modified to help the shopper align the reader with the shelf. After RoboCart brings the shopper in the vicinity of the product, RoboCart uses the shopper’s egocentric frame of reference to instruct the shopper through synthetic speech on how to find the product, e.g. Honey Nut Cheerios is on the top shelf to your right. The shopper finds the shelf and uses the barcode to scan the barcodes on that shelf. The product name of each scanned barcode is read to the shopper. As the robot moves, the rigid PVC handle gives the shopper haptic feedback about the robot’s maneuvers. To describe how the visually impaired shopper interacts with the supermarket space using RoboCart, we introduce the category of target space. The target space is the shoppercentric subspace of the locomotor space in which the shopper perceives the target product to be. The target space is always defined with respect to a specific target product. Haptic cues in the target space act as external reference points during the shopper’s maneuvering of the haptic space in the target space until the haptic space contains the product.",,,36,38,Human–computer interaction; Interface (computing); Haptic technology; Numeric keypad; Spatial cognition; Space (commercial competition); Salient; Computer science; Robot,,,,,http://www.aaai.org/Papers/Symposia/Spring/2007/SS-07-07/SS07-07-008.pdf https://www.aaai.org/Papers/Symposia/Spring/2007/SS-07-07/SS07-07-008.pdf https://dblp.uni-trier.de/db/conf/aaaiss/aaaiss2007-7.html#Kulyukin07a,http://www.aaai.org/Papers/Symposia/Spring/2007/SS-07-07/SS07-07-008.pdf,,,2401880901,,0,015-195-550-005-330; 022-335-702-913-705; 048-298-689-305-353; 094-763-081-817-982; 118-713-134-426-98X; 140-644-534-290-218,0,false,,
054-154-570-256-464,Wearable computers as a virtual environment interface for people with visual impairment,,1998,journal article,Virtual Reality,13594338; 14349957,Springer Science and Business Media LLC,United Kingdom,David Ross,"People who are totally blind or who have vevere visual impairments (e.g. less than 20/200 acuity, central macular scotomas, or advanced diabetic retinopathy) ‘see’ the environment in a fashion that may be completely foreign to those who operate in a very visual fashion. For this population, the visual complexity of the environment is not a concern. What is of concern are salient features found within the environment that relate to their ability to navigate successfully in, and/or interact with, the environment as needed. Toward the purpose of representing these salient features in comprehensive form, investigators at the Atlanta Veterans Affairs Research and Development Center are employing wearable computer technology to develop a virtual environment interface. The long-range goal is to create a simplistic virtual representation of the environment that includes only features related to the current navigational task and/or interactive needs of the person. In a completed study, the use of digital infrared transmitters as ‘beacons’ representing salient features of the environment was explored. The purpose of a current study now in progress is to evaluate and compare various user interface structures that were suggested by subjects during the preliminary study. The problem of interest in the current study is street-crossing; however, the results of this research should be applicable to many other problems, including identifying and locating building entrances, and identifying, locating and interacting with electronic devices such as information kiosks, automated teller machines, and self-serve point-of-sale terminals. The long-range result desired is a wearable computer with which one can easily identify and interact with a wide variety of devices in the environment via one familiar, easy-to-use interface.",3,3,212,221,Interface (computing); Wearable computer; Virtual reality; Virtual machine; Visual impairment; Population; Computer science; Multimedia; User interface; Virtual representation,,,,,https://dblp.uni-trier.de/db/journals/vr/vr3.html#Ross98 https://link.springer.com/article/10.1007%2FBF01408563,http://dx.doi.org/10.1007/bf01408563,,10.1007/bf01408563,2036742622,,0,030-344-810-220-767; 037-452-515-170-720; 047-926-647-780-322; 054-049-478-865-967; 077-352-552-120-40X; 133-238-762-604-971; 142-426-241-123-210; 160-955-031-583-039; 191-212-241-195-568,2,true,"CC BY, CC BY-NC-ND",gold
054-209-231-604-853,DETEKCIJA STEPENICA I PJEŠAČKIH PRIJELAZA MORFOLOŠKOM OBRADOM I ANALIZOM SLIKE U SVRHU USMJERAVANJA SLIJEPIH I SLABOVIDNIH OSOBA,2018-12-11,2018,dissertation,,,,,Kresimir Romic,"Most of the blind and visually impaired persons are still not using advanced navigation and orientation assistance systems. Though it is not yet time to fully expel standard methods such as a white cane, advances in technology now enable the development and gradual introduction of digital mobile systems for helping the blind and visually impaired people. This dissertation describes the issues that need to be solved by such a system, focusing on navigation methods using camera and digital image processing. This research is focused on specific situations when a person is in front of or on stairs and pedestrian crosswalks as potential critical points when walking. In addition to an overview of the existing methods, three newly developed methods are described in detail along with their evaluation. Developed methods include: method for stairs detection using vertical and horizontal analysis, multiresolution method for pedestrian crosswalks detection based on morphological analysis and line energy, method for sound guidance of the blind and visually impaired by determining space for safe movement. There is also an additionally developed framework for evaluating the methods for guidance of the blind and visually impaired on stairs and pedestrian crosswalks. Testing of the developed methods has shown some advantages over existing methods regarding the accuracy, the ability to use with wide-angle input images and the robustness in cases of concealed objects. By testing the processing speed for developed methods, possibility to perform in real-time is proven, which is extremely important for the assistance systems that should be used in the movement.",,,,,Digital image processing; Artificial intelligence; Energy (signal processing); Line (geometry); Pedestrian; Stairs; Visually Impaired Persons; Computer vision; Computer science; Orientation (computer vision); Robustness (computer science),,,,,https://dabar.srce.hr/en/islandora/object/etfos%3A2137 https://repozitorij.etfos.hr/islandora/object/etfos:2137/datastream/PDF/download https://dr.nsk.hr/islandora/object/etfos:2137/datastream/PDF/download https://dr.nsk.hr/islandora/object/etfos%3A2137 https://repozitorij.etfos.hr/islandora/object/etfos:2137,https://dabar.srce.hr/en/islandora/object/etfos%3A2137,,,2940204737,,0,,0,false,,
054-554-410-849-298,Symposia,2012-07-20,2012,journal article,Cognitive Processing,16124782; 16124790,Springer Science and Business Media LLC,Germany,,,13,S1,3,35,,,,,,,http://dx.doi.org/10.1007/s10339-012-0510-8,,10.1007/s10339-012-0510-8,,,0,,1,false,,
055-398-180-024-518,Multi-step anisotropic denoiser scheme applied for cardiac non-contrast CT images,2014-05-21,2014,journal article,International Journal of Computer Assisted Radiology and Surgery,18616410; 18616429,Springer Science and Business Media LLC,Germany,Jakub Zieliński; Krzysztof Nowiński,,9,S1,263,309,Algorithm; Scheme (programming language); Non contrast ct; Computer science; Anisotropy,,,,,https://pbn.nauka.gov.pl/sedno-webapp/works/461155,http://dx.doi.org/10.1007/s11548-014-1047-3,,10.1007/s11548-014-1047-3,2593236287,,0,,1,false,,
055-562-353-796-359,Assistive Helmet for Visually Impaired Human Beings,2024-05-16,2024,conference proceedings article,"2024 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)",,IEEE,,Ovidiu Ceoca; Eva-Henrietta Dulf,"The presence of visually impaired people around the globe has greatly increased due to many factors that affects the overall health of a person. In such way, this article reviews the current state of technology and aims a new way to help the people in question. It presents an assistive helmet that incorporates three essential features for the blind people to regain their selfconfidence through independent mobility and with all these, maintaining a low cost making it affordable for the majority. The features early mentioned are Obstacle Avoidance which will guide the person without being hurt by any sudden obstacles, Fall Detection which will produce a strident sound and light signals in order the person to be helped and, lastly, Track Monitoring which is responsible to record the path the user took.",,,,,Visually impaired; Computer science; Human–computer interaction; Computer vision; Artificial intelligence,,,,,,http://dx.doi.org/10.1109/aqtr61889.2024.10554135,,10.1109/aqtr61889.2024.10554135,,,0,001-903-133-002-733; 009-797-354-358-885; 014-370-728-043-246; 018-262-121-105-137; 021-808-392-522-733; 027-805-115-749-256; 055-495-282-581-758; 058-268-989-258-793; 061-205-550-296-347; 078-678-812-782-626; 084-779-991-197-800; 091-172-699-731-204,0,false,,
055-582-751-413-120,Turn off the graphics: designing non-visual interfaces for mobile phone games,,2009,journal article,Journal of the Brazilian Computer Society,01046500; 16784804,Springer Science and Business Media LLC,Brazil,Luis Valente; Clarisse Sieckenius de Souza; Bruno Feijó,"Mobile phones are a widespread platform for ICT applications because they are highly pervasive in contemporary society. Hence, we can think of mobile gaming as a serious candidate to being a prominent form of entertainment in the near future. However, most games (for computers, console and mobile devices) make extensive use of the visual medium, which tends to exclude visually-impaired users from the play. While mobile gaming could potentially reach many visually-impaired users, who are very familiar with this technology, currently there seems to be only very few alternatives for this community. In an attempt to explore new interactive possibilities for such users, this work presents an initial study on non-visual interfaces for mobile phone games. It is based on Semiotic Engineering principles, emphasizing communication through aural, tactile and gestural signs, and deliberately excluding visual information. Results include a number of issues that can be incorporated to a wider research agenda on mobile gaming accessibility, both for the visually-impaired and sighted.",15,1,45,58,Human–computer interaction; Mobile device; Mobile phone; Semiotic engineering; Mobile search; Computer science; Multimedia; Entertainment; Mobile Web; Mobile technology; Information and Communications Technology,,,,,https://doi.org/10.1007/BF03192576 http://www.scielo.br/scielo.php?script=sci_arttext&pid=S0104-65002009000100005 https://link.springer.com/article/10.1007/BF03192576 https://journal-bcs.springeropen.com/articles/10.1007/BF03192576 http://www.dbd.puc-rio.br/depto_informatica/09_15_valente.pdf http://www.journal-bcs.com/content/15/1/ https://paperity.org/p/8510000/turn-off-the-graphics-designing-non-visual-interfaces-for-mobile-phone-games http://www.scielo.br/scielo.php?pid=S0104-65002009000100005&script=sci_arttext https://link.springer.com/content/pdf/10.1007%2FBF03192576.pdf https://dblp.uni-trier.de/db/journals/jbcs/jbcs15.html#ValenteSF09 http://www.scielo.br/pdf/jbcos/v15n1/v15n1a05.pdf http://www.scielo.br/j/jbcos/a/zDZwQPjh7SR94VDzTCHRcFB/,http://dx.doi.org/10.1007/bf03192576,,10.1007/bf03192576,2165275817,,0,002-918-889-124-106; 005-218-968-413-485; 006-393-185-769-025; 008-944-876-801-558; 011-553-836-004-167; 011-890-988-178-737; 014-505-994-743-611; 021-950-956-619-686; 030-993-304-845-502; 036-018-967-090-600; 039-827-971-183-737; 039-844-983-507-166; 040-414-260-726-221; 056-939-308-434-90X; 058-523-110-758-515; 059-727-275-348-005; 061-082-618-649-392; 068-625-672-354-23X; 070-968-789-552-527; 081-060-784-264-242; 084-928-512-849-661; 109-685-548-640-693; 112-303-157-960-421; 113-542-102-739-599; 130-979-260-268-364; 138-148-139-576-473; 147-096-907-941-598; 151-498-742-564-697; 151-643-591-802-552; 155-272-952-800-428; 155-741-144-213-427; 156-367-587-430-470; 158-164-047-652-446; 162-108-330-859-256; 162-846-842-290-82X; 167-129-355-299-341; 167-246-465-250-509; 169-395-586-562-498; 172-664-183-392-739; 178-575-219-743-304; 189-899-072-216-55X; 192-792-071-687-163,16,true,cc-by-nc,gold
055-673-950-572-320,GeoCoach: A cross-device hypermedia system to assist visually impaired people to gain environmental accessibility,2017-09-25,2017,journal article,Informatik-Spektrum,01706012; 1432122x,Springer Science and Business Media LLC,Germany,Limin Zeng; Gerhard Weber,,40,6,527,539,Human–computer interaction; Engineering; Hypermedia; User studies; Cross device; Visually impaired; Geographic information system; Multimedia; Global Positioning System,,,,,https://dblp.uni-trier.de/db/journals/insk/insk40.html#ZengW17 https://link.springer.com/article/10.1007%2Fs00287-017-1071-0 https://rd.springer.com/article/10.1007/s00287-017-1071-0,http://dx.doi.org/10.1007/s00287-017-1071-0,,10.1007/s00287-017-1071-0,2757697509,,0,002-629-091-832-559; 003-204-682-225-868; 013-678-295-106-102; 014-898-844-427-824; 020-374-040-160-394; 020-593-126-563-557; 029-904-900-230-800; 030-638-752-805-647; 030-704-867-496-808; 030-771-869-007-334; 031-962-503-545-822; 049-293-834-043-374; 052-005-750-292-532; 053-794-028-526-128; 053-951-316-132-971; 057-545-918-303-995; 074-437-935-792-936; 076-112-198-316-215; 085-411-369-325-519; 087-510-774-676-36X; 099-873-453-549-026; 106-264-944-278-433; 108-020-023-456-065; 124-152-076-108-704; 132-997-113-420-890; 142-995-942-384-149; 183-935-933-940-273,2,false,,
055-885-636-582-859,A Review on Path Selection and Navigation Approaches Towards an Assisted Mobility of Visually Impaired People,2020-08-31,2020,journal article,Ksii Transactions on Internet and Information Systems,19767277; 22881468,Korea Society of Internet Information,South Korea,Waqas Nawaz; Kifayat Ullah Khan; Khalid Bashir,"Some things come easily to humans, one of them is the ability to navigate around. This capability of navigation suffers significantly in case of partial or complete blindness, restricting life activity. Advances in the technological landscape have given way to new solutions aiding navigation for the visually impaired. In this paper, we analyze the existing works and identify the challenges of path selection, context awareness, obstacle detection/identification and integration of visual and nonvisual information associated with real-time assisted mobility. In the process, we explore machine learning approaches for robotic path planning, multi constrained optimal path computation and sensor based wearable assistive devices for the visually impaired. It is observed that the solution to problem is complex and computationally intensive and significant effort is required towards the development of richer and comfortable paths for safe and smooth navigation of visually impaired people. We cannot overlook to explore more effective strategies of acquiring surrounding information towards autonomous mobility.",14,8,3270,3294,Human–computer interaction; Wearable computer; Information system; Obstacle; Context awareness; Computer science; Process (engineering); Identification (information); Motion planning; Path (graph theory),,,,,http://kiss.kstudy.com/thesis/thesis-view.asp?key=3819606 http://itiis.org/digital-library/23757,http://kiss.kstudy.com/thesis/thesis-view.asp?key=3819606,,,3106212645,,0,,0,false,,
055-918-155-280-33X,A Deep Learning Based Framework for Blind Road Occupancy Detection,2024-04-26,2024,journal article,"Highlights in Science, Engineering and Technology",27910210,Darcy & Roy Press Co. Ltd.,,Yiteng Liu,"<jats:p>This research paper addresses the pressing concern of road safety for the visually impaired in densely populated regions, especially in China. While tactile paving exists to guide blind individuals, unexpected obstacles pose serious hazards. The study proposes an Artificial Intelligence (AI) system utilizing advanced machine learning techniques to identify obstacles on blind roads, providing real-time feedback for navigation. To overcome data limitations, a virtual environment using Pybullet is created for data generation, combining synthetic and real-world images for training. This study introduces a Convolutional Neural Network (CNN)-based model and integrates a Vision Transformer (ViT) model, comparing their efficacies. The progressive training approach yields a highly effective CNN model, outperforming ViT models. The practical application of the CNN model in real-world scenarios has proven its efficacy in detecting obstacles, underscoring its reliability and significant contribution to improving road safety for visually impaired individuals. This research extends beyond providing a safety enhancement measure; it also sheds light on the wider applicability of such methods in addressing issues like the scarcity of real-world data.</jats:p>",94,,355,367,Computer science; Convolutional neural network; Artificial intelligence; Deep learning; Machine learning; Reliability (semiconductor); Power (physics); Physics; Quantum mechanics,,,,,https://drpress.org/ojs/index.php/HSET/article/download/20609/20748 https://doi.org/10.54097/x7zrtx05,http://dx.doi.org/10.54097/x7zrtx05,,10.54097/x7zrtx05,,,0,,0,true,cc-by-nc,hybrid
056-103-542-573-705,"Implementation of a Kalman filter in positioning for autonomous vehicles, and its sensitivity to the process parameters",,1997,journal article,The International Journal of Advanced Manufacturing Technology,02683768; 14333015,Springer Science and Business Media LLC,Germany,Shraga Shoval; I. Zeitoun; E. Lenz,,13,10,738,746,Engineering; Artificial intelligence; Kalman filter; Curvature; Mobile robot; Material handling; Computer vision; Sensor fusion; Robotics; Odometry; Simultaneous localization and mapping,,,,,https://link.springer.com/content/pdf/10.1007/BF01179074.pdf https://link.springer.com/article/10.1007/BF01179074,http://dx.doi.org/10.1007/bf01179074,,10.1007/bf01179074,2043548225,,0,016-592-712-463-058; 017-381-700-606-433; 018-800-736-799-177; 032-700-829-963-174; 052-699-426-812-689; 069-871-900-335-314; 070-656-931-492-770; 093-475-559-919-820; 096-007-528-950-499; 115-149-099-793-683; 123-627-191-882-375; 137-268-978-462-290; 155-412-009-023-594; 188-607-689-161-428,22,false,,
056-167-810-394-870,Blind Navigation Assistance for Visually Impaired based on Local Depth Hypothesis from a Single Image,,2013,journal article,Procedia Engineering,18777058,Elsevier BV,,R. Gnana Praveen; Roy Paily,"Abstract Assisting the visually impaired along their navigation path is a challenging task which drew the attention of several researchers. A lot of techniques based on RFID, GPS and computer vision modules are available for blind navigation assistance. In this paper, we proposed a depth estimation technique from a single image based on local depth hypothesis devoid of any user intervention and its application to assist the visually impaired people. The ambient space ahead of the user is captured by a camera and the captured image is resized for computational efficiency. The obstacles in the foreground of the image are segregated using edge detection followed by morphological operations. Then depth is estimated for each obstacle based on local depth hypothesis. The estimated depth map is then compared with the reference depth map of the corresponding depth hypothesis and the deviation of the estimated depth map from the reference depth map is used to retrieve the spatial information about the obstacles ahead of the user.",64,,351,360,Image (mathematics); Depth map; Canny edge detector; Artificial intelligence; Spatial analysis; Edge detection; Obstacle; Task (project management); Computer vision; Computer science; Global Positioning System,,,,,https://core.ac.uk/display/82816760 https://www.sciencedirect.com/science/article/pii/S1877705813016214 https://cyberleninka.org/article/n/258286,http://dx.doi.org/10.1016/j.proeng.2013.09.107,,10.1016/j.proeng.2013.09.107,2011437803,,1,008-214-681-885-646; 016-554-897-381-94X; 017-446-312-279-885; 057-890-353-263-937; 064-309-126-323-022; 089-138-919-058-162; 096-803-870-115-728; 114-117-278-888-562; 126-861-032-300-213; 135-442-888-799-760; 139-394-469-134-336,27,true,,gold
056-242-804-007-157,On Robustness of Robotic and Autonomous Systems Perception,2021-03-03,2021,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Cristiano Rafael Steffens; Lucas Ricardo Vieira Messias; Paulo Jorge Lilles Drews-Jr; Silvia Silva da Costa Botelho,,101,3,1,17,Impulse noise; Noise; Artificial intelligence; Lossy compression; Computer vision; Robotics; Computer science; Obstacle avoidance; Shot noise; Image sensor; Robustness (computer science),,,,Conselho Nacional de Desenvolvimento Científico e Tecnológico,https://dblp.uni-trier.de/db/journals/jirs/jirs101.html#SteffensMDB21 https://doi.org/10.1007/s10846-021-01334-0 https://link.springer.com/article/10.1007/s10846-021-01334-0,http://dx.doi.org/10.1007/s10846-021-01334-0,,10.1007/s10846-021-01334-0,3135545567,,0,006-078-205-324-892; 007-505-617-335-352; 008-250-486-561-134; 012-474-771-450-334; 013-156-255-523-712; 014-581-890-775-752; 016-857-503-734-983; 017-125-243-897-286; 017-159-309-030-630; 017-233-249-933-143; 019-294-276-991-105; 020-233-013-143-936; 021-360-232-245-594; 022-795-752-184-026; 023-691-744-455-105; 027-981-517-725-591; 030-872-599-386-807; 032-674-708-693-968; 033-294-099-934-704; 036-370-861-177-356; 037-245-843-100-586; 041-058-139-703-974; 041-940-505-475-261; 042-143-684-347-699; 042-240-098-526-453; 043-365-605-965-828; 048-350-888-401-79X; 049-162-983-805-846; 049-551-659-753-473; 049-921-334-736-611; 050-628-005-984-510; 051-034-543-544-035; 051-197-403-835-689; 051-425-281-185-199; 052-168-751-772-497; 053-126-860-665-202; 055-395-668-630-884; 057-308-477-941-81X; 057-567-246-168-045; 058-648-260-963-601; 063-625-320-581-998; 066-305-886-229-375; 066-327-011-101-288; 069-480-623-909-181; 072-042-319-365-480; 072-074-868-308-485; 073-060-212-503-957; 073-646-357-072-463; 075-144-242-965-66X; 079-892-003-732-729; 080-087-995-388-679; 080-878-396-430-995; 082-253-223-971-462; 085-507-633-373-250; 086-351-138-754-45X; 090-766-322-717-391; 097-830-655-456-386; 101-603-027-062-602; 104-208-613-793-423; 105-487-210-090-310; 106-880-020-771-593; 114-770-058-535-193; 116-030-405-883-678; 116-729-377-395-747; 129-566-833-077-178; 139-552-118-652-140; 145-923-888-427-983; 146-093-154-688-224; 167-054-302-146-073; 180-559-374-049-445; 192-697-687-929-866,5,false,,
056-327-215-784-903,System Configuration and Navigation of a Guide Dog Robot: Toward Animal Guide Dog-Level Guiding Work,2022-01-01,2022,preprint,arXiv (Cornell University),,,,Hochul Hwang; Tim Xia; Ibrahima Keita; Ken Suzuki; Joydeep Biswas; Sunghoon I. Lee; Donghyun Kim,"A robot guide dog has compelling advantages over animal guide dogs for its cost-effectiveness, potential for mass production, and low maintenance burden. However, despite the long history of guide dog robot research, previous studies were conducted with little or no consideration of how the guide dog handler and the guide dog work as a team for navigation. To develop a robotic guiding system that is genuinely beneficial to blind or visually impaired individuals, we performed qualitative research, including interviews with guide dog handlers and trainers and first-hand blindfold walking experiences with various guide dogs. Grounded on the facts learned from vivid experience and interviews, we build a collaborative indoor navigation scheme for a guide dog robot that includes preferred features such as speed and directional control. For collaborative navigation, we propose a semantic-aware local path planner that enables safe and efficient guiding work by utilizing semantic information about the environment and considering the handler's position and directional cues to determine the collision-free path. We evaluate our integrated robotic system by testing guide blindfold walking in indoor settings and demonstrate guide dog-like navigation behavior by avoiding obstacles at typical gait speed ($0.7 \mathrm{m/s}$).",,,,,Robot; Human–computer interaction; Planner; Computer science; Navigation system; Work (physics); Path (computing); Mobile robot navigation; Simulation; Artificial intelligence; Mobile robot; Engineering; Robot control; Mechanical engineering; Programming language,,,,,https://arxiv.org/abs/2210.13368,http://dx.doi.org/10.48550/arxiv.2210.13368,,10.48550/arxiv.2210.13368,,,0,,1,true,cc-by,green
056-386-175-882-508,Analysis and design framework for the development of indoor scene understanding assistive solutions for the person with visual impairment/blindness,2024-05-18,2024,journal article,Multimedia Systems,09424962; 14321882,Springer Science and Business Media LLC,Germany,Moeen Valipoor; Angélica de Antonio; Julián Cabrera,"<jats:title>Abstract</jats:title><jats:p>This paper discusses the challenges of the current state of computer vision-based indoor scene understanding assistive solutions for the person with visual impairment (P-VI)/blindness. It focuses on two main issues: the lack of user-centered approach in the development process and the lack of guidelines for the selection of appropriate technologies. First, it discusses the needs of users of an assistive solution through state-of-the-art analysis based on a previous systematic review of literature and commercial products and on semi-structured user interviews. Then it proposes an analysis and design framework to address these needs. Our paper presents a set of structured use cases that help to visualize and categorize the diverse real-world challenges faced by the P-VI/blindness in indoor settings, including scene description, object finding, color detection, obstacle avoidance and text reading across different contexts. Next, it details the functional and non-functional requirements to be fulfilled by indoor scene understanding assistive solutions and provides a reference architecture that helps to map the needs into solutions, identifying the components that are necessary to cover the different use cases and respond to the requirements. To further guide the development of the architecture components, the paper offers insights into various available technologies like depth cameras, object detection, segmentation algorithms and optical character recognition (OCR), to enable an informed selection of the most suitable technologies for the development of specific assistive solutions, based on aspects like effectiveness, price and computational cost. In conclusion, by systematically analyzing user needs and providing guidelines for technology selection, this research contributes to the development of more personalized and practical assistive solutions tailored to the unique challenges faced by the P-VI/blindness.</jats:p>",30,3,,,Visual impairment; Blindness; Computer science; Assistive technology; Visualization; Human–computer interaction; Optometry; Artificial intelligence; Psychology; Medicine; Neuroscience,,,,Universidad Politécnica de Madrid,https://link.springer.com/content/pdf/10.1007/s00530-024-01350-8.pdf https://doi.org/10.1007/s00530-024-01350-8,http://dx.doi.org/10.1007/s00530-024-01350-8,,10.1007/s00530-024-01350-8,,,0,000-225-212-488-19X; 001-517-000-295-477; 003-755-844-432-28X; 004-175-483-786-204; 004-216-069-855-831; 005-191-781-331-343; 005-571-068-932-180; 006-919-283-092-306; 007-530-851-453-706; 009-306-052-283-911; 011-414-630-627-246; 015-324-500-912-571; 015-586-423-151-288; 016-815-180-569-999; 017-700-464-985-739; 018-262-121-105-137; 020-233-013-143-936; 020-731-354-629-565; 020-962-225-757-788; 021-360-232-245-594; 023-735-000-187-111; 024-070-956-139-424; 025-522-385-395-133; 026-303-765-898-687; 026-704-862-400-606; 029-696-848-270-376; 030-440-500-148-114; 034-512-105-450-474; 035-291-375-623-97X; 036-077-342-885-518; 041-066-843-985-493; 042-567-876-492-099; 045-505-911-987-050; 046-821-320-195-132; 047-271-748-480-72X; 048-029-318-046-975; 049-102-821-933-373; 049-317-239-158-314; 053-826-604-836-17X; 055-849-300-249-018; 057-635-761-387-551; 059-644-292-671-783; 061-866-460-804-908; 064-100-103-232-577; 065-989-043-835-282; 067-536-405-193-186; 068-526-647-480-345; 069-810-669-971-158; 071-493-119-840-933; 072-832-627-910-440; 073-361-230-697-651; 077-507-367-411-516; 079-834-789-873-826; 080-589-710-669-822; 081-938-680-892-669; 085-250-662-197-46X; 085-692-245-452-982; 087-408-536-412-018; 090-972-393-068-517; 093-548-249-489-049; 094-936-883-199-250; 100-444-739-625-782; 100-817-282-240-389; 103-532-349-586-430; 105-790-942-783-392; 108-565-500-693-304; 108-623-076-527-130; 115-782-957-943-101; 116-482-311-077-453; 117-069-244-824-558; 119-791-451-189-762; 121-707-574-802-675; 133-317-652-903-976; 133-416-782-758-694; 134-754-739-510-667; 144-349-066-755-742; 162-153-535-514-940; 162-860-755-281-854; 163-506-338-712-945; 164-984-148-069-920; 173-164-658-058-36X; 174-191-736-093-583; 174-741-930-721-25X; 179-275-175-864-105; 180-559-374-049-445; 182-528-389-423-621; 184-959-454-182-551; 193-478-992-284-896,0,true,cc-by,hybrid
056-521-521-538-240,CamNav: a computer-vision indoor navigation system,2021-01-08,2021,journal article,The Journal of Supercomputing,09208542; 15730484,Springer Science and Business Media LLC,Netherlands,AbdelGhani Karkar; Somaya Al-Maadeed; Jayakanth Kunhoth; Ahmed Bouridane,,77,7,7737,7756,Orb (optics); Artificial intelligence; Navigation system; Computer vision; Class (biology); Computer science; Scale-invariant feature transform; Local binary patterns,,,,Qatar University,https://link.springer.com/article/10.1007/s11227-020-03568-5 https://dblp.uni-trier.de/db/journals/tjs/tjs77.html#KarkarAKB21 https://doi.org/10.1007/s11227-020-03568-5,http://dx.doi.org/10.1007/s11227-020-03568-5,,10.1007/s11227-020-03568-5,3119342790,,0,004-686-787-388-031; 011-522-753-452-712; 012-081-521-655-479; 012-387-483-703-880; 013-619-301-341-337; 016-160-842-644-202; 026-062-137-058-607; 027-599-471-667-527; 028-882-065-739-118; 029-360-136-216-567; 041-459-782-354-459; 042-251-157-585-319; 053-215-378-840-089; 055-376-024-532-10X; 059-149-073-001-124; 063-375-671-655-726; 064-144-289-519-65X; 071-806-151-845-245; 074-324-981-337-749; 078-986-222-652-017; 079-433-787-480-087; 079-535-935-176-19X; 085-242-767-142-895; 085-674-040-165-681; 090-170-384-821-699; 090-871-058-688-660; 091-061-227-748-735; 092-065-049-593-72X; 094-202-843-351-729; 101-269-289-434-356; 101-840-520-204-394; 105-716-263-924-709; 106-227-926-511-534; 110-409-001-674-560; 110-842-431-042-720; 112-548-473-692-356; 113-129-758-796-669; 117-179-977-971-165; 124-354-336-145-903; 129-784-081-593-326; 133-846-312-833-522; 145-898-655-711-084; 153-471-712-074-630; 168-330-006-032-201; 192-697-687-929-866; 192-751-999-674-131,7,false,,
056-547-713-371-217,Accessible smartphones for blind users: A case study for a wayfinding system,,2014,journal article,Expert Systems with Applications,09574174,Elsevier BV,United Kingdom,M. C. Rodriguez-Sanchez; M. A. Moreno-Alvarez; Estefanía Martín; Susana Borromeo; Juan Antonio Hernández-Tamames,,41,16,7210,7222,Human–computer interaction; Still face; Computer science; Multimedia; Android (operating system); User interface,,,,,https://dl.acm.org/doi/10.1016/j.eswa.2014.05.031 https://doi.org/10.1016/j.eswa.2014.05.031 https://www.sciencedirect.com/science/article/abs/pii/S095741741400311X https://dblp.uni-trier.de/db/journals/eswa/eswa41.html#Rodriguez-SanchezMMBT14 http://dblp.uni-trier.de/db/journals/eswa/eswa41.html#Rodriguez-SanchezMMBT14,http://dx.doi.org/10.1016/j.eswa.2014.05.031,,10.1016/j.eswa.2014.05.031,2002763313,,0,001-857-921-994-281; 002-408-516-508-721; 004-315-451-874-148; 007-704-187-139-969; 012-604-656-425-443; 016-846-403-481-823; 019-116-718-242-741; 023-868-294-867-153; 024-566-815-966-337; 026-180-397-595-991; 029-855-795-332-101; 032-517-859-134-478; 034-882-652-482-553; 034-891-557-796-532; 035-886-281-521-342; 036-990-754-479-620; 039-812-881-081-655; 039-901-769-379-747; 051-065-738-489-461; 054-690-312-759-359; 054-708-339-566-453; 056-937-991-322-072; 056-961-360-391-282; 057-831-473-241-96X; 057-900-602-112-766; 059-557-202-590-547; 064-336-967-054-246; 069-046-601-699-214; 070-010-014-511-265; 071-769-940-913-263; 072-187-862-049-284; 072-682-959-905-110; 074-988-362-684-540; 076-455-738-292-176; 078-592-303-685-388; 087-747-103-290-058; 093-925-865-515-918; 095-022-635-835-604; 101-064-588-300-762; 106-220-449-201-557; 110-490-778-005-092; 127-463-894-551-553; 131-100-501-537-286; 141-154-985-076-645; 142-575-622-508-48X; 150-207-052-587-884; 151-396-381-196-309; 167-042-927-870-231; 169-421-672-075-385,65,false,,
056-672-930-385-298,A computer-aid multi-task light-weight network for macroscopic feces diagnosis.,2022-02-28,2022,journal article,Multimedia tools and applications,13807501; 15737721,Springer Science and Business Media LLC,Netherlands,Ziyuan Yang; Lu Leng; Ming Li; Jun Chu,"The abnormal traits and colors of feces typically indicate that the patients are probably suffering from tumor or digestive-system diseases. Thus a fast, accurate and automatic health diagnosis system based on feces is urgently necessary for improving the examination speed and reducing the infection risk. The rarity of the pathological images would deteriorate the accuracy performance of the trained models. In order to alleviate this problem, we employ augmentation and over-sampling to expand the samples of the classes that have few samples in the training batch. In order to achieve an impressive recognition performance and leverage the latent correlation between the traits and colors of feces pathological samples, a multi-task network is developed to recognize colors and traits of the macroscopic feces images. The parameter number of a single multi-task network is generally much smaller than the total parameter number of multiple single-task networks, so the storage cost is reduced. The loss function of the multi-task network is the weighted sum of the losses of the two tasks. In this paper, the weights of the tasks are determined according to their difficulty levels that are measured by the fitted linear functions. The sufficient experiments confirm that the proposed method can yield higher accuracies, and the efficiency is also improved.",81,11,15671,15686,Leverage (statistics); Computer science; Task (project management); Feces; Artificial intelligence; Pattern recognition (psychology); Biology; Paleontology; Management; Economics,Color recognition; Data augmentation  over-sampling; Macroscopic feces image; Multi-task diagnosing; Trait recognition; Weighting selection,,,National Natural Science Foundation of China; National Natural Science Foundation of China; National Natural Science Foundation of China; Key Research and Development Program of Jiangxi Province,https://link.springer.com/content/pdf/10.1007/s11042-022-12565-0.pdf https://doi.org/10.1007/s11042-022-12565-0,http://dx.doi.org/10.1007/s11042-022-12565-0,35250359,10.1007/s11042-022-12565-0,,PMC8884099,0,004-241-274-385-378; 007-203-394-451-87X; 009-084-631-811-335; 009-223-915-790-65X; 012-278-331-092-707; 013-616-118-030-385; 014-213-312-884-165; 015-813-601-387-97X; 018-302-830-647-936; 020-233-013-143-936; 021-767-576-229-074; 022-578-416-124-824; 025-456-956-086-56X; 026-229-872-970-290; 030-569-949-314-212; 031-320-645-223-816; 033-369-969-078-192; 036-483-592-673-941; 037-774-762-376-218; 039-477-178-699-896; 041-505-760-023-207; 042-104-123-913-353; 044-626-322-021-818; 044-772-633-976-211; 046-338-108-175-310; 050-545-698-791-506; 054-871-100-158-001; 055-292-660-789-549; 055-603-286-600-827; 056-562-802-729-319; 057-786-610-308-017; 060-127-492-089-640; 060-189-554-259-670; 063-274-075-021-345; 066-838-284-400-261; 070-493-865-135-391; 070-690-420-536-320; 078-022-842-113-835; 082-108-935-461-726; 085-710-208-007-915; 090-103-797-189-601; 090-799-887-531-907; 091-536-494-715-66X; 091-889-072-687-797; 092-457-258-535-430; 102-355-976-811-114; 112-213-117-233-706; 126-323-791-447-744; 130-043-420-545-570; 132-810-780-767-932; 151-265-513-935-178; 171-476-685-991-016,3,true,,bronze
056-811-060-387-130,Road surface detection and differentiation considering surface damages,2021-01-11,2021,journal article,Autonomous Robots,09295593; 15737527,Springer Science and Business Media LLC,Netherlands,Thiago Rateke; Aldo von Wangenheim,"A challenge still to be overcome in the field of visual perception for vehicle and robotic navigation on heavily damaged and unpaved roads is the task of reliable path and obstacle detection. The vast majority of the researches have scenario roads in good condition, from developed countries. These works cope with few situations of variation on the road surface and even fewer situations presenting surface damages. In this paper we present an approach for road detection considering variation in surface types, identifying paved and unpaved surfaces and also detecting damage and other information on other road surfaces that may be relevant to driving safety. Our approach makes use of Convolutional Neural Networks (CNN) to perform semantic segmentation, we use the U-NET architecture with ResNet34, in addition we use the technique known as Transfer Learning, where we first train a CNN model without using weights in the classes as a basis for a second CNN model where we use weights for each class. We also present a new Ground Truth with image segmentation, used in our approach and that allowed us to evaluate our results. Our results show that it is possible to use passive vision for these purposes, even using images captured with low cost cameras.",45,2,299,312,Image segmentation; Ground truth; Transfer of learning; Artificial intelligence; Obstacle; Road surface; Computer vision; Field (computer science); Computer science; Convolutional neural network; Segmentation; Visual perception; Damages; Variation (game tree),,,,Coordenação de Aperfeiçoamento de Pessoal de Nível Superior,https://www.scilit.net/article/1834e05f6dd50ec2031c8be4d8cf1bdb?action=show-references https://dblp.uni-trier.de/db/journals/arobots/arobots45.html#RatekeW21 https://doi.org/10.1007/s10514-020-09964-3 https://link.springer.com/article/10.1007/s10514-020-09964-3 https://dblp.uni-trier.de/db/journals/corr/corr2006.html#abs-2006-13377,http://dx.doi.org/10.1007/s10514-020-09964-3,,10.1007/s10514-020-09964-3,3037773950; 3118616679,,0,000-575-132-189-034; 001-762-280-465-889; 002-309-302-189-960; 011-568-583-242-512; 013-115-057-730-410; 013-602-387-847-089; 016-354-417-956-737; 019-230-141-901-333; 019-544-229-700-829; 020-233-013-143-936; 022-443-190-106-579; 023-294-553-332-779; 023-499-588-430-044; 023-970-241-488-302; 025-670-117-400-666; 025-913-453-232-474; 028-322-846-712-553; 032-980-162-022-972; 035-326-207-950-362; 037-816-209-006-927; 038-265-836-546-365; 039-716-867-311-581; 041-857-388-753-092; 042-998-212-722-861; 047-326-510-583-783; 052-944-344-306-64X; 053-319-877-182-320; 055-968-253-505-075; 059-692-408-436-977; 060-578-350-723-40X; 065-613-717-246-797; 067-332-174-495-608; 068-431-371-370-919; 069-480-623-909-181; 072-806-828-237-521; 078-209-047-202-601; 079-227-260-551-072; 081-638-352-078-364; 084-737-868-362-721; 085-454-262-907-543; 085-862-069-984-46X; 094-745-569-510-408; 095-347-639-241-788; 095-732-028-947-256; 096-902-832-288-955; 097-179-179-603-700; 106-957-866-022-337; 109-176-046-622-406; 113-957-024-493-814; 140-410-763-710-034; 150-801-200-456-643; 159-372-051-442-161; 165-096-585-709-683; 167-131-435-704-717; 176-179-447-130-414; 177-247-606-063-967; 177-696-666-754-480; 179-490-099-676-904; 192-418-463-661-843,41,true,,green
056-916-160-072-321,Smart Walking Stick for Visually Impaired People,,2018,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Shravan Mohite; Abhishek M Patel; Milan Patel; Vaishali Gaikwad,,,,,,Human–computer interaction; Arduino; Microcontroller; Vibrator (mechanical); ALARM; Artificial vision; Walking stick; Visually impaired; Computer science; Process (computing),,,,,https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3274784 https://autopapers.ssrn.com/sol3/papers.cfm?abstract_id=3274784,http://dx.doi.org/10.2139/ssrn.3274784,,10.2139/ssrn.3274784,2946769083,,0,000-631-028-807-828; 002-449-629-591-50X; 007-468-367-707-685; 048-298-689-305-353; 051-766-223-654-722; 119-036-310-031-196,1,false,,
056-921-916-495-524,Hybrid deep neural network with adaptive galactic swarm optimization for text extraction from scene images,2020-08-09,2020,journal article,Soft Computing,14327643; 14337479,Springer Science and Business Media LLC,Germany,Digvijay Pandey; Binay Kumar Pandey; Subodh Wairya,,25,2,1563,1580,Deep learning; Computational intelligence; Artificial intelligence; Gabor transform; Pattern recognition; Computer science; Artificial neural network; Naive Bayes classifier; Composite image filter; Identification (information); Noise (video),,,,,https://doi.org/10.1007/s00500-020-05245-4 https://link.springer.com/article/10.1007/s00500-020-05245-4 https://dblp.uni-trier.de/db/journals/soco/soco25.html#PandeyPW21,http://dx.doi.org/10.1007/s00500-020-05245-4,,10.1007/s00500-020-05245-4,3048014709,,0,000-730-188-705-648; 003-062-736-861-78X; 003-634-972-326-157; 004-538-286-401-811; 004-797-524-101-176; 007-713-806-103-557; 010-996-917-013-74X; 012-205-185-085-455; 012-914-717-646-770; 014-485-403-709-425; 016-363-372-640-935; 017-314-090-466-057; 019-143-403-851-343; 021-693-474-257-323; 022-873-261-023-691; 026-270-111-115-884; 034-339-950-267-451; 035-200-844-156-889; 040-602-178-431-182; 041-438-314-834-395; 046-100-810-369-481; 046-246-671-985-250; 050-898-852-794-398; 061-297-480-752-183; 061-527-922-604-817; 063-721-444-770-635; 064-177-674-529-441; 064-657-401-679-973; 065-529-868-011-694; 066-972-921-391-650; 073-489-566-423-374; 073-951-247-604-642; 074-229-273-706-27X; 074-383-378-886-153; 076-786-873-576-426; 077-109-490-771-222; 083-268-168-773-380; 083-702-876-075-988; 084-386-802-204-315; 085-296-389-992-764; 086-539-734-790-11X; 088-702-507-859-201; 088-771-804-086-769; 097-232-832-716-063; 103-505-700-293-317; 104-571-048-811-284; 107-528-779-069-015; 109-911-921-483-167; 112-651-838-599-249; 114-888-284-970-47X; 120-894-043-523-01X; 125-466-234-816-916; 135-470-811-312-609; 137-827-347-993-376; 147-150-128-112-541; 157-778-773-552-781; 171-091-935-415-285; 176-274-394-625-59X,80,false,,
057-879-237-577-755,A man-machine vision interface for sensing the environment.,,1992,journal article,Journal of rehabilitation research and development,07487711; 19381352,Rehabilitation Research and Development Service,United States,Malek Adjouadi,"This study describes a computer vision approach for sensing the environment with the intent of helping people with a visual impairment. The principal goal in applying computer vision is to exploit, in an optimal fashion, the information acquired by the camera(s) to yield useful descriptions of the viewed environment. The objective is to seek efficient and reliable guidance cues in order to improve the mobility needs of individuals with a visual impairment. In this research direction, the following problems are identified and addressed: 1) the vision system design; 2) establishment of the mapping principles between the two-dimensional (2-D) camera images and the three-dimensional (3-D) real world; 3) development of appropriate imaging techniques for the interpretation of the 2-D images; and, 4) establishment of a communication link between the vision system and the user. The soundness of this research direction is assessed by means of a theoretical framework and experimental evaluations.",29,2,57,76,Human–computer interaction; Interface (computing); Developmental psychology; Principal (computer security); Psychology; Exploit; Visual impairment; Communication link; Man machine; Soundness; Machine vision,,"Algorithms; Blindness/rehabilitation; Depth Perception; Equipment Design; Humans; Image Processing, Computer-Assisted; Man-Machine Systems; Microcomputers; Rehabilitation, Vocational; Sensory Aids; Vision Disorders/rehabilitation; Vision, Ocular",,,https://europepmc.org/article/MED/1578393 http://www.rehab.research.va.gov/jour/92/29/2/pdf/adjouadi.pdf https://www.rehab.research.va.gov/jour/92/29/2/pdf/adjouadi.pdf https://www.ncbi.nlm.nih.gov/pubmed/1578393,http://dx.doi.org/10.1682/jrrd.1992.04.0057,1578393,10.1682/jrrd.1992.04.0057,2085358796,,2,016-743-056-039-923; 024-574-338-011-078; 027-694-663-064-726; 030-761-670-818-026; 079-296-222-150-146; 092-850-017-631-81X; 151-102-988-751-421,23,true,,bronze
057-887-571-627-279,Learning place-dependant features for long-term vision-based localisation,2015-07-23,2015,journal article,Autonomous Robots,09295593; 15737527,Springer Science and Business Media LLC,Netherlands,Colin McManus; Ben Upcroft; Paul Newman,,39,3,363,387,Asynchronous communication; Leverage (statistics); Artificial intelligence; Dependant; Landmark; Computer vision; Robotics; Computer science; Feature learning; Monocular; Outlier,,,,,https://dx.doi.org/10.1007/s10514-015-9463-y http://doi.org/10.1007/s10514-015-9463-y https://eprints.qut.edu.au/100804/ https://dl.acm.org/doi/10.1007/s10514-015-9463-y https://dblp.uni-trier.de/db/journals/arobots/arobots39.html#McManusUN15 https://ora.ox.ac.uk/objects/uuid:088715bb-cfba-408e-b1f6-b307ae43be97 https://doi.org/10.1007/s10514-015-9463-y https://link.springer.com/article/10.1007/s10514-015-9463-y http://dx.doi.org/10.1007/s10514-015-9463-y https://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.635217 https://link.springer.com/article/10.1007/s10514-015-9463-y/fulltext.html https://core.ac.uk/display/78104342,http://dx.doi.org/10.1007/s10514-015-9463-y,,10.1007/s10514-015-9463-y,1824640608,,0,000-539-939-459-594; 004-636-879-043-780; 009-517-591-482-661; 011-232-233-689-200; 012-642-196-605-159; 012-878-058-553-582; 019-041-554-117-456; 023-438-345-162-875; 023-455-400-637-234; 024-067-785-956-78X; 026-278-149-059-052; 033-222-166-814-807; 035-291-375-623-97X; 036-910-145-965-436; 040-027-977-587-337; 040-898-046-412-436; 043-723-268-693-792; 050-580-040-665-46X; 054-864-116-553-270; 068-446-370-600-681; 071-880-324-389-926; 073-168-356-769-011; 079-219-766-858-645; 079-499-077-131-386; 084-619-281-122-975; 086-616-766-470-037; 088-524-470-613-718; 095-169-554-048-613; 096-094-518-409-453; 098-440-015-522-27X; 100-987-779-099-597; 104-909-809-365-289; 110-300-542-766-522; 111-142-688-726-166; 127-937-376-823-08X; 133-358-101-299-371; 150-454-299-478-338; 153-160-493-195-249; 153-697-872-076-389; 154-289-332-516-99X; 180-524-612-469-147; 184-294-374-073-183; 193-123-800-849-774; 195-210-909-960-60X,29,false,,
057-890-353-263-937,Stereo vision in blind navigation assistance,2010-12-10,2010,conference proceedings,,,,,Hugo Fernandes; Paulo Costa; Vitor Filipe; Leontios J. Hadjileontiadis; João Barroso,"Visual impairment and blindness caused by infectious diseases has been greatly reduced, but increasing numbers of people are at risk of age-related visual impairment. Visual information is the basis for most navigational tasks, so visually impaired individuals are at disadvantage because appropriate information about the surrounding environment is not available. With the recent advances in inclusive technology it is possible to extend the support given to people with visual impairment during their mobility. In this context we propose a system, named SmartVision, whose global objective is to give blind users the ability to move around in unfamiliar environments, whether indoor or outdoor, through a user friendly interface. This paper is focused mainly in the development of the computer vision module of the SmartVision system.",,,1,6,Interface (computing); Visual perception; Disadvantage; Context (language use); Visual impairment; Computer science; Multimedia; User Friendly; Stereopsis; User interface,,,,,https://ieeexplore.ieee.org/abstract/document/5665579,https://ieeexplore.ieee.org/abstract/document/5665579,,,1530374040,,0,013-847-555-426-159; 045-418-646-209-673; 058-942-174-334-646; 070-665-310-804-510; 077-975-988-687-715; 101-333-962-375-602; 105-584-440-275-77X; 192-600-440-415-958,45,false,,
057-972-187-511-024,Characterizing and Predicting Engagement of Blind and Low-Vision People with an Audio-Based Navigation App,2022-04-27,2022,conference proceedings article,CHI Conference on Human Factors in Computing Systems Extended Abstracts,,ACM,,Tiffany Liu; Javier Hernandez; Mar Gonzalez-Franco; Antonella Maselli; Melanie Kneisel; Adam Glass; Jarnail Chudge; Amos Miller,"Audio-based navigation technologies can help people who are blind or have low vision (BLV) with more independent navigation, mobility, and orientation. We explore how such technologies are incorporated into their daily lives using machine learning models trained on the engagement patterns of over 4,700 BLV people. For mobile navigation apps, we identify user engagement features like the duration of first-time use and engagement with spatial audio callouts that are greatly relevant to predicting user retention. This work contributes a more holistic understanding of important features associated with user retention and strong app usage, as well as insight into the exploration of ambient surroundings as a compelling use case for assistive navigation apps. Finally, we provide design implications to improve the accessibility and usability of audio-based navigation technology.",,,,,Computer science; Usability; Human–computer interaction; Mobile apps; Orientation and Mobility; Audio feedback; Multimedia; Mobile device; User engagement; Visually impaired; World Wide Web; Engineering; Electrical engineering,,,,,,http://dx.doi.org/10.1145/3491101.3519862,,10.1145/3491101.3519862,,,0,000-631-028-807-828; 007-561-378-312-300; 007-905-684-555-577; 019-150-105-799-327; 024-779-069-584-802; 029-988-017-000-53X; 039-901-763-924-716; 062-266-890-933-328; 078-425-612-692-727; 090-884-510-770-832; 092-815-726-755-74X; 128-942-703-221-290; 142-014-274-474-024,3,false,,
058-013-370-483-44X,ICRA - Deep Trail-Following Robotic Guide Dog in Pedestrian Environments for People who are Blind and Visually Impaired - Learning from Virtual and Real Worlds,,2018,conference proceedings article,2018 IEEE International Conference on Robotics and Automation (ICRA),,IEEE,,Tzu-Kuan Chuang; Ni-Ching Lin; Jih-Shi Chen; Chen-Hao Hung; Yi-Wei Huang; Chunchih Tengl; Haikun Huang; Lap-Fai Yu; Laura Giarre; Hsueh-Cheng Wang,"Navigation in pedestrian environments is critical to enabling independent mobility for the blind and visually impaired (BVI) in their daily lives. White canes have been commonly used to obtain contact feedback for following walls, curbs, or man-made trails, whereas guide dogs can assist in avoiding physical contact with obstacles or other pedestrians. However, the infrastructures of tactile trails or guide dogs are expensive to maintain. Inspired by the autonomous lane following of self-driving cars, we wished to combine the capabilities of existing navigation solutions for BVI users. We proposed an autonomous, trail-following robotic guide dog that would be robust to variances of background textures, illuminations, and interclass trail variations. A deep convolutional neural network (CNN) is trained from both the virtual and realworld environments. Our work included major contributions: 1) conducting experiments to verify that the performance of our models trained in virtual worlds was comparable to that of models trained in the real world; 2) conducting user studies with 10 blind users to verify that the proposed robotic guide dog could effectively assist them in reliably following man-made trails.",,,5849,5855,Human–computer interaction; Mobile robot; Pedestrian; Visually impaired; Computer science; Metaverse; Convolutional neural network,,,,,https://dblp.uni-trier.de/db/conf/icra/icra2018.html#ChuangLCHHTHYGW18 https://ir.nctu.edu.tw:443/handle/11536/150770 https://ieeexplore.ieee.org/document/8460994/ https://iris.unimore.it/handle/11380/1167367 https://scholar.nycu.edu.tw/zh/publications/deep-trail-following-robotic-guide-dog-in-pedestrian-environments,http://dx.doi.org/10.1109/icra.2018.8460994,,10.1109/icra.2018.8460994,2889628315,,0,000-122-946-719-709; 008-402-661-890-702; 013-056-435-015-274; 013-510-761-818-117; 028-460-767-882-226; 030-127-369-356-741; 045-565-041-480-612; 045-599-200-378-144; 058-268-022-053-157; 063-334-343-483-565; 075-144-242-965-66X; 104-694-342-259-021; 134-910-012-092-697; 137-250-459-674-747; 147-187-948-964-386,47,false,,
058-028-260-452-046,Visual simultaneous localization and mapping: a survey,2012-11-13,2012,journal article,Artificial Intelligence Review,02692821; 15737462,Springer Science and Business Media LLC,Netherlands,Jorge Fuentes-Pacheco; José Ruiz-Ascencio; Juan Manuel Rendón-Mancha,,43,1,55,81,Artificial intelligence; Matching (graph theory); Salient; Computer vision; Field (computer science); Computer science; Scale (map); Representation (mathematics); Sonar; Simultaneous localization and mapping; Robot,,,,,https://dl.acm.org/doi/10.1007/s10462-012-9365-8 https://doi.org/10.1007/s10462-012-9365-8 https://link.springer.com/article/10.1007%2Fs10462-012-9365-8 https://dl.acm.org/citation.cfm?id=2717465 https://rd.springer.com/article/10.1007/s10462-012-9365-8 http://dx.doi.org/10.1007/s10462-012-9365-8 https://dblp.uni-trier.de/db/journals/air/air43.html#Fuentes-PachecoAR15 https://www.infona.pl/resource/bwmeta1.element.springer-df63384c-9f8e-31d1-bb75-a12be926afb8 https://dx.doi.org/10.1007/s10462-012-9365-8,http://dx.doi.org/10.1007/s10462-012-9365-8,,10.1007/s10462-012-9365-8,1979266466,,8,001-080-835-530-667; 001-489-360-478-212; 001-678-423-293-833; 001-851-448-796-820; 001-875-707-968-997; 002-345-873-500-677; 002-428-195-856-095; 002-580-724-068-489; 002-639-328-748-793; 002-724-634-490-58X; 003-375-945-279-099; 004-511-416-923-897; 004-556-058-759-212; 005-771-913-274-549; 006-271-548-869-792; 006-652-266-840-034; 007-391-459-034-77X; 007-637-722-872-410; 008-125-827-025-602; 009-517-560-819-831; 009-612-733-037-965; 011-969-064-214-381; 012-629-207-486-321; 015-266-678-032-675; 016-308-730-766-991; 016-807-775-776-516; 016-885-405-166-247; 017-019-672-760-676; 018-346-044-842-94X; 019-178-389-648-284; 019-526-745-480-990; 019-811-761-679-740; 020-414-977-931-294; 021-311-500-600-230; 021-678-078-234-298; 021-907-975-576-497; 022-366-833-865-178; 022-591-013-984-957; 023-147-671-941-065; 023-960-131-541-476; 024-051-089-074-699; 024-131-470-445-452; 026-258-380-163-081; 026-555-802-144-851; 027-885-063-486-687; 028-037-989-331-413; 028-053-633-538-317; 028-576-601-859-047; 029-061-861-419-041; 029-310-893-263-218; 030-559-295-446-128; 030-697-390-722-704; 031-171-849-060-274; 033-155-023-224-677; 033-317-313-009-085; 033-463-111-172-955; 033-968-430-627-500; 034-130-496-700-456; 034-986-160-519-213; 035-291-375-623-97X; 035-662-817-882-400; 035-875-490-531-527; 036-866-763-754-665; 037-143-885-059-991; 041-003-807-260-374; 041-333-506-109-883; 041-563-428-793-416; 043-966-961-651-562; 044-239-148-413-612; 044-898-668-023-385; 045-518-114-939-163; 050-999-208-714-142; 051-340-015-478-447; 052-585-862-761-026; 052-759-480-653-270; 052-946-485-810-857; 053-531-871-983-90X; 053-801-556-003-307; 053-968-239-103-181; 053-976-766-729-573; 054-712-759-408-904; 054-768-119-854-589; 054-789-581-133-75X; 055-851-181-041-97X; 056-213-005-468-886; 056-446-987-497-770; 057-042-196-938-512; 057-556-173-656-246; 057-793-015-492-337; 057-986-186-558-661; 059-405-286-411-282; 059-739-125-077-121; 063-098-347-208-382; 063-265-163-145-925; 065-726-925-775-248; 065-766-604-247-686; 065-976-488-557-411; 066-282-678-820-591; 066-703-579-836-443; 066-807-118-391-628; 066-874-502-692-388; 067-176-972-025-406; 068-446-370-600-681; 068-693-157-048-806; 069-923-812-907-281; 070-709-933-434-677; 073-048-589-974-722; 073-359-023-678-156; 074-010-788-046-390; 074-805-364-520-991; 077-676-441-183-329; 078-725-425-518-672; 079-096-045-462-678; 079-465-187-999-260; 080-034-162-941-610; 080-748-339-093-739; 082-176-447-985-875; 084-679-921-430-947; 084-696-045-240-370; 087-400-073-090-676; 088-038-079-589-221; 089-156-568-698-078; 089-749-683-560-887; 089-779-057-821-290; 092-514-942-244-064; 092-616-942-293-373; 093-194-541-364-440; 094-416-698-994-94X; 094-633-430-503-688; 097-415-368-040-355; 098-440-015-522-27X; 098-506-439-768-109; 098-903-394-130-10X; 099-370-727-349-060; 102-673-620-476-70X; 102-680-602-493-704; 102-786-306-974-437; 104-550-564-725-38X; 104-720-886-505-784; 107-362-112-311-662; 108-891-032-921-217; 110-300-542-766-522; 111-948-232-739-703; 113-297-357-779-414; 117-356-577-279-780; 118-275-662-533-570; 120-072-322-429-12X; 120-972-335-348-875; 121-074-907-594-310; 121-169-386-673-604; 122-202-952-837-203; 122-658-982-249-571; 122-855-605-453-430; 123-916-387-179-404; 124-016-376-186-855; 124-354-336-145-903; 126-467-346-430-613; 127-603-480-011-866; 130-306-791-948-944; 132-222-037-518-162; 132-467-148-084-411; 135-205-616-917-920; 136-274-363-848-642; 139-857-741-152-268; 144-216-247-471-846; 144-252-766-282-56X; 145-264-144-720-451; 146-768-315-872-827; 149-483-919-411-116; 152-254-175-996-172; 152-678-026-899-124; 160-850-885-831-269; 165-367-533-416-928; 166-910-784-564-122; 172-129-541-724-007; 179-830-926-335-31X; 182-195-453-502-816; 184-418-590-788-505; 192-135-765-584-206; 192-751-999-674-131; 193-123-800-849-774; 195-210-909-960-60X,772,false,,
058-056-495-422-448,Enhancement in web accessibility for visually impaired people using hybrid deep belief network –bald eagle search,2023-02-20,2023,journal article,Multimedia Tools and Applications,13807501; 15737721,Springer Science and Business Media LLC,Netherlands,Tejal Tiwary; Rajendra Prasad Mahapatra,,82,16,24347,24368,Computer science; Closed captioning; Globe; World Wide Web; Deep belief network; Selection (genetic algorithm); Task (project management); Object (grammar); Artificial intelligence; Social media; Eagle; Deep learning; Multimedia; Information retrieval; Image (mathematics); Medicine; Management; Economics; Ophthalmology; Paleontology; Biology,,,,,,http://dx.doi.org/10.1007/s11042-023-14494-y,,10.1007/s11042-023-14494-y,,,0,004-489-091-806-474; 008-200-339-668-585; 008-471-623-577-853; 011-290-575-138-391; 013-590-029-074-092; 014-309-616-307-734; 015-500-813-796-196; 015-791-830-463-441; 022-705-697-906-740; 026-054-800-284-898; 027-150-755-438-048; 035-454-468-938-069; 038-939-682-943-867; 041-660-737-654-748; 043-711-115-487-842; 044-252-755-794-29X; 050-973-281-246-597; 056-613-694-345-146; 058-097-006-093-702; 061-508-212-845-903; 066-382-023-401-087; 081-627-371-374-042; 083-783-057-855-718; 093-548-249-489-049; 101-428-075-302-45X; 107-528-295-509-766; 143-030-981-286-483; 145-902-098-209-92X; 151-502-069-512-203,4,false,,
058-121-238-874-142,Comprehensive Automation for Specialty Crops: Year 1 results and lessons learned,2010-08-11,2010,journal article,Intelligent Service Robotics,18612776; 18612784,Springer Science and Business Media LLC,Germany,Sanjiv Singh; Marcel Bergerman; J. Cannons; Benjamin Grocholsky; Bradley Hamner; German A. Holguin; Larry A. Hull; Vincent P. Jones; George Kantor; Harvey Koselka; Guiqin Li; James S. Owen; Johnny Park; Wenfan Shi; James Teza,,3,4,245,262,Engineering management; Automation; Agriculture; Artificial intelligence; Outreach; Production efficiency; Specialty crops; Plant science; Robotics; Computer science; Simulation,,,,,http://www.ri.cmu.edu/pub_files/2010/9/fulltext.pdf http://www.cpp.edu/~jlcannons/10-JISR.pdf https://link.springer.com/article/10.1007%2Fs11370-010-0074-3 https://link.springer.com/content/pdf/10.1007%2Fs11370-010-0074-3.pdf https://www.cpp.edu/~jlcannons/10-JISR.pdf,http://dx.doi.org/10.1007/s11370-010-0074-3,,10.1007/s11370-010-0074-3,2119502223,,1,000-407-576-076-038; 007-551-561-276-628; 018-179-884-364-148; 051-835-413-650-024; 052-317-575-761-604; 052-935-769-572-340; 056-168-526-519-917; 089-228-952-818-863; 095-563-603-963-814; 134-042-433-098-942; 134-772-785-246-552; 147-516-912-068-269; 151-192-406-782-829,33,false,,
058-170-853-128-388,An Advanced Telereflexive Tactical Response Robot,,2001,journal article,Autonomous Robots,09295593; 15737527,Springer Science and Business Media LLC,Netherlands,H. R. Everett; G. A. Gilbreath; D. A. Ciccimaro,,11,1,39,47,Interface (computing); Artificial intelligence; Teleoperation; Exploit; Systems engineering; Remote operation; Urban warfare; Robotics; Computer science; Simulation; Remote control; Robot,,,,,https://link.springer.com/article/10.1023/A:1011204111813 https://dblp.uni-trier.de/db/journals/arobots/arobots11.html#EverettGC01 https://dialnet.unirioja.es/servlet/articulo?codigo=732613,http://dx.doi.org/10.1023/a:1011204111813,,10.1023/a:1011204111813,1642590770,,0,004-084-623-178-080; 013-590-183-403-640; 025-278-629-527-832; 035-488-017-022-84X; 050-891-223-957-033; 068-743-749-610-498; 099-465-697-302-895; 141-995-630-420-618; 148-975-926-020-839,20,false,,
058-268-989-258-793,Electronic travel aid system for visually impaired people,,2017,conference proceedings article,2017 5th International Conference on Information and Communication Technology (ICoIC7),,IEEE,,Pasika Ranaweera; S. H. R. Madhuranga; H. F. A. S. Fonseka; D. M. L. D. Karunathilaka,"Visually impaired people face unique challenges in their day to day life while navigating in unfamiliar public locations. Using a walking stick relies on trial and error, particularly in unfamiliar locations. Also from a walking stick the user can only identify obstacles which are touching the stick and cannot identify the obstacles which are above his waist height. Electronic Travel Aids (ETAs) are devices that use sensor technology to assist and improve the blind users mobility in terms of safety. In this context, we implement a system whose objective is to give blind users the ability to move around in unfamiliar environment, whether indoor or outdoor, through an interface specifically designed to cater the visual imperfections. This ETA system grants the user the ability to travel independently with an automated alerting system designed for emergencies. The obstacles are detected using image processing and distance sensing through IR sensors. A navigation system was developed to assist the user via web access.",,,1,6,Human–computer interaction; Interface (computing); Artificial intelligence; Navigation system; Context (language use); Walking stick; Visually impaired; Computer vision; Trial and error; Computer science; Global Positioning System; Image processing,,,,,https://ieeexplore.ieee.org/document/8074700/,http://dx.doi.org/10.1109/icoict.2017.8074700,,10.1109/icoict.2017.8074700,2766786523,,0,013-056-435-015-274; 034-722-800-622-78X; 065-992-777-809-97X; 083-539-611-368-446; 101-333-962-375-602; 156-136-348-796-661,14,false,,
058-308-310-638-823,Snag Detection Robot for Visually Impaired Steering and Blind Individuals,,2018,conference proceedings article,2018 International Conference on Inventive Research in Computing Applications (ICIRCA),,IEEE,,S. Sowmiya; K. Valarmathi; S. Sathyavenkateshwaren; M. Gobinath; S. Thillaisivakavi,"The analysis field that created several of today's robots supported computing currently being known as psychological feature computing. It makes fast progress towards human brain. Everyday new reports and stories started regarding innovations in AI that have the potential to alter our lives. Our final goal is to create a robotic system to assist visually impaired folks navigate reception and improve the standard of their life. The main aim of the proposed system is to expand the electronic travel aid for the blind and visually impaired folks by emerging into the ultrasonic technology. Blind individuals tackle variety of visual challenges each day. This system represents an innovative project design and implementation of an Ultrasonic Navigation system in order to provide fully automatic obstacle detection with audible notification for blind folks. This blind guidance system is safe, resolute and profitable.",,,,,Human–computer interaction; Variety (cybernetics); Guidance system; Obstacle; Navigation system; Visually impaired; Field (computer science); Computer science; Feature (computer vision); Robot,,,,,http://xplorestaging.ieee.org/ielx7/8581980/8596764/08597178.pdf?arnumber=8597178 http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=8597178,http://dx.doi.org/10.1109/icirca.2018.8597178,,10.1109/icirca.2018.8597178,2909997668,,0,028-017-547-386-646; 069-385-969-956-190; 100-950-617-368-126; 120-720-028-159-467; 134-036-221-644-525; 148-465-603-692-446,3,false,,
058-692-860-484-32X,Enhanced 3D indoor position estimation from smartphone's inbuilt IMU and pressure sensor via hybrid approach,2024-05-13,2024,journal article,Cluster Computing,13867857; 15737543,Springer Science and Business Media LLC,Netherlands,Shiva Sharma; Manjit Kaur; Naresh Kumar,,,,,,Computer science; Inertial measurement unit; Position (finance); Estimation; Real-time computing; Artificial intelligence; Business; Management; Finance; Economics,,,,,,http://dx.doi.org/10.1007/s10586-024-04473-9,,10.1007/s10586-024-04473-9,,,0,001-814-426-044-107; 013-732-665-960-07X; 016-432-007-366-372; 018-212-990-220-502; 021-957-511-855-231; 036-093-022-249-416; 040-770-554-917-729; 045-615-198-897-056; 048-446-468-600-588; 062-169-808-318-744; 062-467-074-627-251; 063-859-725-294-825; 064-268-834-439-749; 067-501-756-705-542; 072-666-679-096-54X; 078-717-227-404-164; 092-002-727-524-363; 157-931-418-441-276; 168-005-331-189-166; 172-945-753-732-557; 174-859-339-950-008,0,false,,
058-838-134-955-27X,Mobile Robot Command by Man–Machine Co-Operation – Application to Disabled and Elderly People Assistance,,2002,journal article,Journal of Intelligent and Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Philippe Hoppenot; Etienne Colle,,34,3,235,252,Dead reckoning; Human–computer interaction; Engineering; Social robot; Personal robot; Robot learning; Mobile robot; Mobile robot navigation; Simulation; Robot control; Robot,,,,,https://link.springer.com/article/10.1023%2FA%3A1016303401978 https://dblp.uni-trier.de/db/journals/jirs/jirs34.html#HoppenotC02,http://dx.doi.org/10.1023/a:1016303401978,,10.1023/a:1016303401978,1559930524,,0,005-385-625-676-580; 007-873-376-290-040; 010-105-344-127-560; 016-087-219-050-156; 035-460-331-865-39X; 038-915-665-487-410; 069-015-904-599-892; 087-035-563-901-168; 089-730-101-921-831; 095-909-718-358-337; 097-535-198-922-385; 107-468-001-910-852; 158-189-108-732-660,19,false,,
058-862-840-054-831,Assistive System for the Visually Impaired using Multiple Cameras and Sensors,2022-10-08,2022,conference proceedings article,"2022 IEEE 4th International Conference on Cybernetics, Cognition and Machine Learning Applications (ICCCMLA)",,IEEE,,Harish Akula; G Durgaprasad Reddy; Morarjee Kolla,"On a global scale, at least 2.2 billion people are affected by a near or distance vision impairment. Vision is an essential aspect of human life, and vision loss is a problem many unfortunate people experience. Many blind people rely on white cane sticks and guide dogs for navigation, but they come with their own set of drawbacks. Assistive technologies are those utilities that aid visually impaired people in carrying out some specific tasks. This paper presents a novel assistive device that overcomes the issue of low visual coverage of the surroundings by using multiple cameras. The blind user can wear the cameras in the front, right, back, and left directions surrounding the neck, getting wider visual coverage. A raspberry pi microcomputer captures images from the cameras and uses them to find objects and faces without using the internet. The microcomputer provides voice feedback on the detections and the camera direction through earphones. An Arduino microcontroller connected to an ultrasonic sensor senses the obstacles and alerts the user with a vibration motor. A smartphone can connect to these Arduino and Raspberry pi modules via Bluetooth, and the user can send commands with the mobile application by touching or by voice. The system could get a broad picture of the surroundings and provide output in a reasonable amount of time.",,,,,Bluetooth; Computer science; Visually impaired; Microcomputer; Arduino; Computer vision; Raspberry pi; Microcontroller; Set (abstract data type); Proximity sensor; Artificial intelligence; Human–computer interaction; Embedded system; Wireless; Internet of Things; Telecommunications; Operating system; Programming language; Chip,,,,,,http://dx.doi.org/10.1109/icccmla56841.2022.9989288,,10.1109/icccmla56841.2022.9989288,,,0,017-571-811-406-431; 027-588-207-376-882; 034-259-321-454-457; 034-936-259-439-98X; 035-804-512-361-234; 069-480-623-909-181; 075-358-894-384-661; 084-621-956-547-983; 085-560-687-499-839; 097-312-123-618-145; 108-525-198-891-337; 113-483-703-037-077; 137-973-182-180-54X; 155-096-675-018-832; 160-672-274-228-816,0,false,,
059-072-992-267-393,Influence of estimation errors on wayfinding-decisions in unknownstreet networks – analyzing the least-angle strategy,,2000,journal article,Spatial Cognition and Computation,13875868,Springer Science and Business Media LLC,United States,Hartwig H. Hochmair; Andrew U. Frank,,2,4,283,313,Position (vector); Orientation (geometry); Artificial intelligence; Heuristic; Line (geometry); Intersection; Computer vision; Computer science; Software agent; Path integration; Process (computing),,,,,https://link.springer.com/article/10.1023%2FA%3A1015566423907 https://dx.doi.org/10.1023/A:1015566423907 http://dx.doi.org/10.1023/A:1015566423907 https://dblp.uni-trier.de/db/journals/scc/scc2.html#HochmairF00 https://dl.acm.org/doi/10.1023/A%3A1015566423907,http://dx.doi.org/10.1023/a:1015566423907,,10.1023/a:1015566423907,2127068769,,0,002-860-404-340-638; 002-931-231-292-337; 006-654-180-700-136; 016-795-216-845-76X; 017-563-233-459-696; 030-765-204-945-165; 031-178-019-121-578; 033-591-124-833-00X; 035-425-884-724-921; 036-616-049-977-19X; 037-703-101-538-576; 041-906-503-942-859; 042-273-090-822-689; 046-346-092-320-703; 048-823-227-031-514; 048-862-864-622-941; 050-523-233-649-246; 059-084-198-148-336; 059-193-913-867-341; 082-618-839-056-448; 084-996-469-414-746; 085-222-584-303-600; 097-843-979-935-212; 107-373-017-129-428; 111-915-513-334-111; 122-715-477-945-910; 126-039-525-595-674; 127-115-664-047-553; 147-734-220-180-60X; 162-527-547-301-284; 169-544-674-700-324; 189-921-739-365-675,108,false,,
059-264-107-268-243,Indoor Navigation Aid Systems for the Blind and Visually Impaired Based on Depth Sensors,,2021,book chapter,Examining Optoelectronics in Machine Vision and Applications in Industry 4.0,23270411; 2327042x,IGI Global,,Fernando Merchan; Martín Poveda; Danilo Cáceres-Hernández; Javier E. Sanchez-Galan,"<jats:p>This chapter focuses on the contributions made in the development of assistive technologies for the navigation of blind and visually impaired (BVI) individuals. A special interest is placed on vision-based systems that make use of image (RGB) and depth (D) information to assist their indoor navigation. Many commercial RGB-D cameras exist on the market, but for many years the Microsoft Kinect has been used as a tool for research in this field. Therefore, first-hand experience and advances on the use of Kinect for the development of an indoor navigation aid system for BVI individuals is presented. Limitations that can be encountered in building such a system are addressed at length. Finally, an overview of novel avenues of research in indoor navigation for BVI individuals such as integration of computer vision algorithms, deep learning for the classification of objects, and recent developments with stereo depth vision are discussed. </jats:p>",,,187,223,Artificial intelligence; Navigation aid; Visually impaired; Computer vision; Computer science,,,,,https://www.igi-global.com/chapter/indoor-navigation-aid-systems-for-the-blind-and-visually-impaired-based-on-depth-sensors/269676,http://dx.doi.org/10.4018/978-1-7998-6522-3.ch007,,10.4018/978-1-7998-6522-3.ch007,3117133423,,0,000-394-661-764-185; 001-045-356-116-924; 002-715-593-775-022; 004-078-355-287-887; 006-130-750-959-100; 007-223-103-783-187; 012-344-135-607-729; 013-227-217-874-672; 014-157-934-526-940; 015-016-116-643-819; 016-688-128-358-546; 017-318-000-242-753; 018-106-046-766-271; 018-557-513-171-588; 018-843-508-574-144; 019-428-416-746-939; 021-538-627-198-53X; 022-940-138-590-719; 023-856-694-133-345; 024-417-401-364-679; 025-973-439-187-19X; 027-763-143-738-105; 029-360-136-216-567; 030-127-369-356-741; 032-538-345-541-781; 032-814-735-466-13X; 035-115-110-416-139; 037-491-194-003-965; 038-261-996-116-331; 043-536-853-133-602; 045-038-242-962-218; 045-205-438-482-211; 045-802-145-074-623; 048-308-759-694-056; 050-017-473-526-702; 050-261-233-769-785; 050-670-466-441-411; 053-984-632-586-044; 055-495-282-581-758; 059-149-073-001-124; 060-578-904-142-843; 062-619-309-126-567; 063-117-355-616-790; 064-490-466-346-713; 066-037-919-140-345; 068-526-647-480-345; 070-740-216-883-969; 073-928-284-197-323; 075-519-293-886-898; 077-117-448-758-278; 083-345-653-191-276; 083-671-887-012-721; 084-750-792-083-414; 085-560-687-499-839; 089-681-761-307-978; 091-590-957-842-479; 091-906-536-550-206; 092-143-822-722-332; 096-846-442-673-950; 097-447-856-706-300; 098-465-120-719-120; 101-486-364-581-542; 111-302-231-162-268; 115-315-649-469-222; 119-075-209-073-06X; 123-863-288-894-65X; 131-501-871-312-356; 150-615-758-140-358; 151-039-447-100-109; 152-127-960-081-400; 152-145-055-495-637; 152-183-517-423-628; 163-109-750-928-888; 168-453-699-379-127; 168-511-383-336-908; 170-614-403-864-26X,2,false,,
059-290-388-191-324,Newvision: application for helping blind people using deep learning,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Kumar Srinivas Bobba; Kartheeban K; Vamsi Krishna Sai Boddu; Vijaya Mani Surendra Bolla; Dinesh Bugga,"As able-bodied people, we often take our vision for granted. For people who are visually impaired, however, their disability can have a significant impact on their daily lives. We are developing proprietary headgear that will help visually impaired people navigate their surroundings, identify objects and people, read text, and avoid obstacles. The headgear will use a combination of computer vision, distance estimation with ultrasonic sensors, voice recognition, and voice assistants to provide users with real-time information about their environment. Users will be able to interact with the headgear through voice commands, such as ''What is that?'' to identify an object or ''Navigate to the front door'' to find their way around. The headgear will then provide the user with a verbal description of the object or spoken navigation instructions. We believe that this headgear has the potential to make a significant difference in the lives of visually impaired people, allowing them to live more independently and participate more fully in society.",,,,,Visually impaired; Object (grammar); Computer science; Human–computer interaction; Multimedia; Artificial intelligence,,,,,https://arxiv.org/abs/2311.03395,http://dx.doi.org/10.48550/arxiv.2311.03395,,10.48550/arxiv.2311.03395,,,0,,0,true,cc-by,green
059-345-268-352-918,AUTOMATIC LOCALIZATION AND NAVIGATION ASSISTANCE FOR VISUALLY IMPAIRED PEOPLE USING ROBOT,,2014,,,,,,Nguyen Quoc Hung; Tran Thi Thanh Hai; Vu Duy Hai; Nguyen Quang Hoan,"This paper presents a new support system for the visually impaired navigation in small pervasive environments using mobile robots. The system uses computer vision techniques to build environment map, locate and detect obstacle. The proposed method enables the robot to quickly know its location on the map. The object detection and recognition algorithms are performed in real time in order to avoid static and dynamic obstacles in the environment. To help the visually impaired person navigating in the environment, the proposed method aims to find the optimal path that is the shortest and the most secure path for the moving of the human. In order to interact between human and robot, we design and develop an interface using touch screen and vibration patterns of a smart-phone device. The system is evaluated on a number of blind pupils. The experimental results confirmed that the proposed system is feasible to deploy in practical application. Bai bao nay trinh bay mot h» thng trae giup "" #",,,,,Interface (computing); Engineering; Artificial intelligence; Mobile robot; Mobile robot navigation; Object detection; Obstacle; Computer vision; Reflection mapping; Path (graph theory); Robot,,,,,http://fit.mta.edu.vn/chuyensan/Pdf_Files/102014/05_01.pdf,http://fit.mta.edu.vn/chuyensan/Pdf_Files/102014/05_01.pdf,,,2560326330,,0,000-257-042-575-00X; 002-345-873-500-677; 002-580-724-068-489; 004-193-609-931-550; 012-967-237-747-061; 013-056-435-015-274; 014-279-588-505-48X; 023-438-345-162-875; 023-960-131-541-476; 031-763-353-100-817; 042-465-797-170-692; 043-022-952-629-736; 045-321-254-881-478; 050-136-713-033-491; 051-766-223-654-722; 058-016-551-109-549; 061-190-872-413-287; 064-053-186-811-574; 065-664-501-877-944; 065-992-777-809-97X; 076-518-832-389-690; 102-786-306-974-437; 105-161-175-016-528; 109-930-243-130-269; 115-246-676-960-424; 123-863-288-894-65X; 140-394-829-785-809; 147-893-622-816-445; 160-234-441-041-937; 169-421-672-075-385; 170-387-846-090-147,0,false,,
059-370-601-416-730,Obstacle Detection and Terrain Classification for Autonomous Off-Road Navigation,,2005,journal article,Autonomous Robots,09295593; 15737527,Springer Science and Business Media LLC,Netherlands,Roberto Manduchi; Andres Castano; A. Talukder; Larry Matthies,"Autonomous navigation in cross-country environments presents many new challenges with respect to more traditional, urban environments. The lack of highly structured components in the scene complicates the design of even basic functionalities such as obstacle detection. In addition to the geometric description of the scene, terrain typing is also an important component of the perceptual system. Recognizing the different classes of terrain and obstacles enables the path planner to choose the most efficient route toward the desired goal.; ; This paper presents new sensor processing algorithms that are suitable for cross-country autonomous navigation. We consider two sensor systems that complement each other in an ideal sensor suite: a color stereo camera, and a single axis ladar. We propose an obstacle detection technique, based on stereo range measurements, that does not rely on typical structural assumption on the scene (such as the presence of a visible ground plane)s a color-based classification system to label the detected obstacles according to a set of terrain classess and an algorithm for the analysis of ladar data that allows one to discriminate between grass and obstacles (such as tree trunks or rocks), even when such obstacles are partially hidden in the grass. These algorithms have been developed and implemented by the Jet Propulsion Laboratory (JPL) as part of its involvement in a number of projects sponsored by the US Department of Defense, and have enabled safe autonomous navigation in high-vegetated, off-road terrain.",18,1,81,102,Tree (data structure); Artificial intelligence; Terrain; Set (abstract data type); Stereo camera; Obstacle; Perceptual system; Computer vision; Computer science; Component (UML); Path (graph theory),,,,,https://dialnet.unirioja.es/servlet/articulo?codigo=1040921 https://link.springer.com/article/10.1023%2FB%3AAURO.0000047286.62481.1d https://dx.doi.org/10.1023/B:AURO.0000047286.62481.1d https://dl.acm.org/citation.cfm?id=1030805 https://users.soe.ucsc.edu/~manduchi/papers/AutRobots.pdf http://users.soe.ucsc.edu/~manduchi/papers/AutRobots.pdf https://dblp.uni-trier.de/db/journals/arobots/arobots18.html#ManduchiCTM05 https://rd.springer.com/article/10.1023%2FB%3AAURO.0000047286.62481.1d https://dl.acm.org/doi/10.1023/B%3AAURO.0000047286.62481.1d,http://dx.doi.org/10.1023/b:auro.0000047286.62481.1d,,10.1023/b:auro.0000047286.62481.1d,2089652819,,15,000-799-610-858-54X; 001-848-982-236-105; 011-228-964-184-052; 012-525-454-950-076; 012-967-883-333-816; 014-526-825-853-468; 021-111-582-864-523; 027-185-002-217-726; 027-422-317-660-075; 033-000-788-019-046; 034-636-077-445-700; 040-207-628-320-413; 040-669-338-024-938; 044-140-831-751-480; 052-561-411-726-559; 065-536-791-582-266; 066-893-152-293-996; 069-104-701-202-415; 072-942-441-481-032; 077-430-332-361-106; 079-106-739-971-094; 079-996-084-302-54X; 081-561-685-507-937; 084-589-892-507-036; 086-411-248-618-557; 095-911-656-912-322; 103-978-905-569-962; 105-124-151-064-67X; 122-421-229-823-814; 129-529-394-856-619; 133-040-749-449-870; 133-864-872-584-834; 146-560-972-124-383; 155-143-042-341-196; 156-851-073-016-795; 165-429-461-053-363; 167-643-876-733-117; 181-476-601-862-982; 182-222-896-913-690; 184-248-399-858-764; 194-596-720-323-437,431,true,,green
060-067-757-771-070,Tracking moving objects during low altitude flight,,1996,journal article,Machine Vision and Applications,09328092; 14321769,Springer Science and Business Media LLC,Germany,Yuan-Liang Tang; Rangachar Kasturi,,9,1,20,31,Frame (networking); Artificial intelligence; Kalman filter; Control system; Object detection; Computer vision; Computer science; Tracking (particle physics); Machine vision; Object (computer science); Image processing,,,,,https://dblp.uni-trier.de/db/journals/mva/mva9.html#TangK96 https://link.springer.com/article/10.1007/BF01246636,http://dx.doi.org/10.1007/bf01246636,,10.1007/bf01246636,1971206676,,0,000-120-225-338-675; 004-038-953-344-400; 005-669-468-341-887; 019-387-589-134-668; 049-525-225-779-108; 057-630-010-468-956; 066-747-840-026-570; 070-382-899-160-809; 074-006-536-890-200; 081-217-783-316-39X; 082-998-883-953-493; 083-756-134-424-441; 094-952-156-655-707; 101-387-649-060-296; 101-531-159-781-49X; 110-259-678-174-081; 121-814-438-527-799; 170-328-277-604-194; 187-419-081-273-689,8,false,,
060-161-405-781-76X,Computer Vision-Based Assistive Technology for Blind and Visually Impaired People: A Deep Learning Approach,2023-03-21,2023,book chapter,Computer Assistive Technologies for Physically and Cognitively Challenged Users,,BENTHAM SCIENCE PUBLISHERS,,Roopa G.M.; Chetana Prakash; null Pradeep N.,"<jats:p>&lt;b&gt;&lt;/b&gt;&lt;b&gt;&lt;/b&gt;According to the World Health Organization (WHO), a minimum of 2.2 billion individuals worldwide have impaired vision or are blind. In contrast to hereditary blindness, gained visual impairment is frequently identified as a result of aging, lifestyle habits, or hereditary influences. Aging-related presbyopia has the largest influence on visual impairment and is the second most prevalent cause of blindness globally, and the rate of acquired blindness is predicted to rise dramatically as life expectancy rises. When performing most of the everyday tasks that non-disabled individuals do, visually and blind people face several problems. Thus, assistive gadgets have been utilized to help the blind and visually impaired overcome physical, social, infrastructural, and accessibility hurdles to independence, allowing them to live engaged, creative, and fruitful life as equal members of society. The usage of assistance equipment has increased, and numerous electronic help devices have been produced in recent years, which have been superseded by traditional aid gear, such as white canes. Currently, ATs are created by integrating various types of sensors, cameras, or feedback channels that combine with various implementation methodologies to increase movement for the visually handicapped. Assistive systems based on computer vision or machine learning approaches have emerged, and as technology has advanced, so has assistive technology. Assistive technology is a priority in the field of education and rehabilitation for individuals with blindness or low vision because it “equalizes the ability to access, store, and retrieve information between sighted people and those with visual impairments”. Nowadays, technological advances are making a difference in their ability to overcome difficulties to some extent. Every day, they encounter a slew of challenges, the most significant of which are establishing one's position, determining one's heading and movement directions, and comprehending the placements of things. The goal of assistive technology is to boost impaired people's faith, comfort, security, independence, and quality of life by enhancing their mobility and decreasing their impairment.&lt;b&gt;&lt;/b&gt;&lt;b&gt;&lt;/b&gt;&lt;br&gt;</jats:p>",,,48,72,Visual impairment; Life expectancy; Assistive technology; Visually impaired; Blindness; Psychology; Impaired Vision; Cognitive psychology; Human–computer interaction; Computer science; Medicine; Optometry; Population; Psychiatry; Environmental health,,,,,,http://dx.doi.org/10.2174/9789815079159123020005,,10.2174/9789815079159123020005,,,0,003-755-844-432-28X; 029-965-606-424-03X; 031-613-879-600-310; 033-599-848-965-32X; 041-393-801-742-23X; 044-975-609-671-047; 045-599-200-378-144; 050-669-412-625-372; 053-047-788-830-692; 061-946-877-986-006; 069-932-479-745-323; 078-463-839-388-030; 082-964-708-449-702; 090-601-565-815-572,0,false,,
060-229-919-441-197,Fuzzy Matching Scheme for Stereo Vision based Electronic Travel Aid,,2005,conference proceedings article,TENCON 2005 - 2005 IEEE Region 10 Conference,,IEEE,,G. Balakrishnan; G. Sainarayanan; R. Nagarajan; Sazali Yaacob,"Navigation is a significant barrier to individuals with visual impairments. In response to this barrier, a wearable stereo-vision system was investigated and a prototype system named stereo vision based electronic travel aid (SVETA) has been developed. Stereo cameras are employed in the SVETA in order to provide information about orientation, distance, shape and size of the obstacles or objects in front of the user. Stereo vision has long been one of the central problem in computer vision, and stereo matching is the most important and critical issue of stereo vision. In this paper, similarity measure for stereo matching based on fuzzy relations is used to establish the correspondence in the presence of intensity variations in stereo images. The strength of relationship of fuzzified data of the windows in left and the right images of stereo pairs is determined by considering the appropriate fuzzy aggregation operators. However, these measures fail to establish correspondence over the occluded pixels. Left/right consistence check is performed to overcome these problems. The final disparity map is then conveyed to the blind user through musical tone based image sonification. Experiments with standard and real time images have been conducted to demonstrate the effectiveness of the algorithms.",,,1,4,Artificial intelligence; Pixel; Stereo camera; Similarity measure; Computer stereo vision; Computer vision; Mathematics; Stereo cameras; Stereopsis; Pattern matching; Orientation (computer vision),,,,,http://ieeexplore.ieee.org/document/4085061/ http://dspace.unimap.edu.my:80/xmlui/handle/123456789/6895 http://eprints.ums.edu.my/1076/ https://ieeexplore.ieee.org/document/4085061/ http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.ieee-000004085061,http://dx.doi.org/10.1109/tencon.2005.301231,,10.1109/tencon.2005.301231,1977932785,,0,013-847-555-426-159; 024-202-993-739-041; 030-944-385-509-306; 033-277-528-396-939; 046-236-631-291-300; 058-942-174-334-646; 068-262-811-656-335; 077-975-988-687-715; 102-496-748-953-099; 105-584-440-275-77X; 123-273-858-461-486; 140-605-307-595-60X,5,false,,
060-326-760-135-933,Evaluation of semiautonomous navigation assistance system for power wheelchairs with blindfolded nondisabled individuals,,2010,journal article,Journal of rehabilitation research and development,19381352; 07487711,Rehabilitation Research and Development Service,United States,Vinod Sharma; Richard C. Simpson; Edmund F. LoPresti; Mark R. Schmeler,"INTRODUCTION Problem Statement Independent mobility is a key component in maintaining the physical and psychosocial health of an individual [1-3]. Further, for people with disabilities, independent mobility increases vocational and educational opportunities, reduces dependence on caregivers and family members, and promotes feelings of self-reliance [2]. Psychologically, a decrease in mobility can lead to feelings of emotional loss, anxiety, depression, reduced self-esteem, social isolation, stress, and fear of abandonment. Reduced functional mobility is linked with reduced participation and loss of social connections [1]. Power wheelchairs offer the benefits of independent mobility while allowing individuals to devote their energy to activities of daily living (ADL) [4-5]. Even though the benefits of powered mobility are well documented, the safety issues associated with operation of powered wheelchairs often prevent clinicians and rehabilitation practitioners from prescribing powered mobility [6-8]. One obstacle to safely operating a powered wheelchair is impaired vision. The American Federation for the Blind has estimated that 9.61 percent of all individuals who are legally blind also use a wheelchair or scooter, in addition to another 5.25 percent of individuals who have serious difficulties seeing but are not legally blind [9]. Further, 5.3 percent of all wheeled mobility equipment users are either legally blind or have serious difficulty in seeing [10]. Visual and physical impairments often accompany the natural aging process. Macular degeneration, cataracts, glaucoma, and diabetic retinopathy are the leading causes of visual impairments among older adults. According to the 2007 Disability Status Report [11], 40.3 percent of noninstitutionalized individuals aged 75 and older in the United States have conditions that substantially limit one or more basic physical activities, such as walking, climbing stairs, reaching, lifting, or carrying. Further, 23.6 percent of individuals in this population have sensory disabilities, which include blindness or severe visual impairment. The percentage of wheelchair users who are age 65 or older steadily increased from 2.7 percent in 1990 to 5.2 percent in 2005. Among the non-institutionalized population aged 85 and older, 12.28 percent use wheelchairs [10], most of which are manual wheelchairs pushed by a caregiver or a family member. Limited literature addresses the use of power wheelchairs by individuals with combined visual and mobility impairments. A case study of a visually impaired power wheelchair user who uses a white cane for navigation assistance was presented in Ganoza [12]. Another case study of a visually impaired person's use of a guide dog with a power wheelchair is presented in Greenbaum et al. [13]. Other authors have evaluated the merits and limitations of using a white cane with a manual and power wheelchair [14-15]. Some researchers also advocate the use of power-assisted manual wheelchairs for this population [9,16]. The process of training an individual with visual and mobility impairments to operate a wheelchair while using a cane or guide dog is very time, labor, and resource intensive. It requires the active involvement and participation of family members, caregivers, orientation and mobility experts, occupational therapists, rehabilitation engineers, and primary care providers [12-13]. Combined visual and mobility impairments will be encountered with increasing frequency because of the growing elderly population; therefore, having alternative assistive technology that offers independent mobility for such individuals is important. The use of smart wheelchairs has been researched since the early 1980s as a form of assistance for people who lack the visual, motor, or cognitive skills required to drive a power wheelchair [17]. A detailed discussion of the population of users who could benefit from smart wheelchairs is provided in Simpson et al. …",47,9,877,890,Engineering; Cognitive skill; Psychosocial; Social isolation; Orientation and Mobility; Rehabilitation; Wheelchair; Population; Activities of daily living; Physical medicine and rehabilitation,,Adult; Canes; Equipment Design/instrumentation; Female; Humans; Male; Man-Machine Systems; Middle Aged; Robotics/instrumentation; Task Performance and Analysis; User-Computer Interface; Wheelchairs,,NICHD NIH HHS (5R44HD040023-03) United States,https://www.ncbi.nlm.nih.gov/pubmed/21174252 http://www.rehab.research.va.gov/jour/10/479/pdf/sharma.pdf https://www.rehab.research.va.gov/jour/10/479/pdf/doaj/sharma.pdf,http://dx.doi.org/10.1682/jrrd.2010.02.0012,21174252,10.1682/jrrd.2010.02.0012,2135535597,,0,010-413-321-251-205; 011-797-116-120-363; 011-841-855-235-135; 018-461-076-405-123; 024-829-454-939-895; 029-229-824-816-132; 031-294-750-698-550; 031-551-875-561-966; 032-587-551-701-230; 034-178-622-168-88X; 046-115-171-395-772; 056-266-848-129-611; 062-008-798-641-080; 067-800-017-756-389; 087-596-278-182-678; 095-370-909-502-227; 096-277-047-284-35X; 104-159-800-858-933; 108-121-406-794-961; 110-483-413-485-008; 118-205-581-856-147; 121-644-216-727-227; 122-674-986-408-496; 150-618-661-926-219; 183-986-722-875-213; 199-550-003-363-864,17,true,,bronze
060-510-278-288-296,An embedded infrared night vision blind guide system based on machine vision,2024-01-19,2024,conference proceedings article,AOPC 2023: Infrared Devices and Infrared Technology; and Terahertz Technology and Applications,,SPIE,,Pengjie Zhao; Xiaoyue Yan; Quanfeng Guo; Chenxi Liu; Kun Gao,"An intelligent night vision system based on the NVIDIA Jetson Nano embedded AI edge computing platform has been developed and released. This system utilizes near-infrared active illumination imaging technology and applies a deep learning network based on Convolutional Neural Networks (CNN) to perform lane segmentation and obstacle recognition on the acquired night vision images. The processing results are interacted with the user through voice prompts. Firstly, lightweight ENet (Efficient Neural Network) is employed for lane segmentation. Secondly, a lightweight YoloV5 model is deployed on the Jetson Nano to recognize obstacles such as pedestrians on the road ahead. To ensure recognition accuracy and speed, the Tensor RT inference acceleration framework is utilized. Experimental results demonstrate that the system performs well in terms of road segmentation and pedestrian detection output frame rates, providing insight into achieving all-day navigation assistance. However, for better protection of the visually impaired population, the active illumination system of the device requires further improvement, and image processing needs further optimization.",,,,,Artificial intelligence; Computer vision; Computer science; Convolutional neural network; Segmentation; Machine vision; Image segmentation; Night vision; Obstacle; Image processing; Pedestrian; Image (mathematics); Engineering; Political science; Transport engineering; Law,,,,,,http://dx.doi.org/10.1117/12.3007795,,10.1117/12.3007795,,,0,106-587-878-038-509; 157-030-880-191-037,0,false,,
060-808-826-245-791,A residual dense network assisted sparse view reconstruction for breast computed tomography.,2020-12-03,2020,journal article,Scientific reports,20452322,Springer Science and Business Media LLC,United Kingdom,Zhiyang Fu; Hsin Wu Tseng; Srinivasan Vedantham; Andrew Karellas; Ali Bilgin,"To develop and investigate a deep learning approach that uses sparse-view acquisition in dedicated breast computed tomography for radiation dose reduction, we propose a framework that combines 3D sparse-view cone-beam acquisition with a multi-slice residual dense network (MS-RDN) reconstruction. Projection datasets (300 views, full-scan) from 34 women were reconstructed using the FDK algorithm and served as reference. Sparse-view (100 views, full-scan) projection data were reconstructed using the FDK algorithm. The proposed MS-RDN uses the sparse-view and reference FDK reconstructions as input and label, respectively. Our MS-RDN evaluated with respect to fully sampled FDK reference yields superior performance, quantitatively and visually, compared to conventional compressed sensing methods and state-of-the-art deep learning based methods. The proposed deep learning driven framework can potentially enable low dose breast CT imaging.",10,1,21111,,Compressed sensing; Artificial intelligence; Residual; Projection (set theory); Radiation dose; Computer vision; Computer science; Computed tomography; Reduction (complexity),,"Algorithms; Breast/diagnostic imaging; Female; Humans; Image Processing, Computer-Assisted; Linear Models; Tomography, X-Ray Computed",,NCI NIH HHS (R01 CA195512) United States; NCI NIH HHS (R01 CA199044) United States; NCI NIH HHS (R21 CA134128) United States,https://www.nature.com/articles/s41598-020-77923-0 https://www.nature.com/articles/s41598-020-77923-0.pdf https://www.ncbi.nlm.nih.gov/pubmed/33273541 https://arizona.pure.elsevier.com/en/publications/a-residual-dense-network-assisted-sparse-view-reconstruction-for-,http://dx.doi.org/10.1038/s41598-020-77923-0,33273541,10.1038/s41598-020-77923-0,3109797611,PMC7713379,0,000-524-338-757-738; 001-332-875-074-369; 002-127-828-273-326; 002-950-577-935-188; 006-343-432-958-435; 007-757-171-343-032; 010-037-102-410-599; 010-327-819-254-496; 015-862-943-767-631; 016-284-863-324-851; 017-527-124-031-959; 020-233-013-143-936; 020-601-558-518-378; 021-100-023-623-375; 023-138-384-002-351; 025-384-948-751-89X; 026-252-287-049-279; 028-477-830-954-490; 029-603-169-343-419; 031-405-643-804-759; 031-811-902-087-234; 032-151-768-018-38X; 034-299-290-298-629; 035-389-295-604-272; 035-420-634-910-593; 036-933-073-818-454; 037-537-425-911-284; 037-942-490-090-522; 038-535-517-038-459; 039-744-955-920-203; 040-118-053-753-933; 040-330-555-690-360; 041-130-608-342-993; 050-262-540-566-536; 050-754-953-319-809; 051-645-740-469-217; 051-890-927-883-855; 055-252-902-076-642; 059-015-191-751-571; 060-681-237-092-98X; 061-129-695-590-14X; 062-794-398-798-514; 063-270-961-170-873; 063-625-320-581-998; 064-330-203-246-662; 066-186-989-634-85X; 075-113-161-309-933; 075-351-652-322-984; 086-733-512-829-145; 089-588-138-667-676; 096-490-052-048-354; 096-844-889-160-271; 101-005-755-535-201; 101-035-356-988-593; 101-166-644-111-649; 102-355-976-811-114; 105-489-057-633-030; 111-571-277-936-72X; 116-030-405-883-678; 126-191-076-775-608; 134-172-691-466-66X; 134-856-994-580-547; 137-301-463-629-591; 139-552-118-652-140; 142-724-247-827-96X; 155-338-218-673-080; 184-228-764-690-384; 192-488-678-845-096,14,true,"CC BY, CC BY-NC-ND",gold
060-835-870-091-535,Ontology-assisted automatic precise information extractor for visually impaired inhabitants,2011-05-26,2011,journal article,Artificial Intelligence Review,02692821; 15737462,Springer Science and Business Media LLC,Netherlands,Ahmad C. Bukhari; Yong-Gi Kim,,38,1,9,24,Web Ontology Language; The Internet; World Wide Web; Ontology (information science); Information extraction; Web page; Ontology-based data integration; Protégé; Computer science; Suggested Upper Merged Ontology,,,,,https://dblp.uni-trier.de/db/journals/air/air38.html#BukhariK12 https://link.springer.com/content/pdf/10.1007%2Fs10462-011-9238-6.pdf https://link.springer.com/article/10.1007%2Fs10462-011-9238-6,http://dx.doi.org/10.1007/s10462-011-9238-6,,10.1007/s10462-011-9238-6,2144633466,,0,000-082-902-611-516; 000-424-243-390-605; 010-581-701-429-856; 015-614-874-241-915; 026-787-086-442-120; 030-630-986-568-027; 044-425-915-904-869; 055-723-917-422-69X; 056-839-169-895-213; 080-955-766-703-783; 085-880-744-774-642; 088-186-159-207-267; 098-929-995-264-870; 117-889-912-691-724; 122-177-487-631-362; 140-536-749-013-500; 143-441-767-976-48X; 147-250-053-013-378; 165-049-807-985-551,15,false,,
061-043-301-045-759,A Smart Glove for Visually Impaired People Who Attend to the Elections,2021-05-31,2021,journal article,SN Computer Science,2662995x; 26618907,Springer Science and Business Media LLC,,Pinar Oguz Ekim; Ecem Ture; Seray Karahan; Fulya Yenilmez,,2,4,1,12,Human–computer interaction; Support vector machine; Secrecy; Ballot; Phone; Computer science; Artificial neural network; Voting; Contextual image classification; Convolutional neural network,,,,,https://dblp.uni-trier.de/db/journals/sncs/sncs2.html#Oguz-EkimTKY21 https://link.springer.com/article/10.1007/s42979-021-00709-2 https://link.springer.com/content/pdf/10.1007/s42979-021-00709-2.pdf,http://dx.doi.org/10.1007/s42979-021-00709-2,,10.1007/s42979-021-00709-2,3172068456,,0,001-107-678-410-019; 001-811-998-049-508; 003-137-879-936-770; 021-360-232-245-594; 029-611-746-026-764; 029-750-588-132-771; 048-207-938-342-811; 048-843-443-397-228; 057-245-672-611-829; 060-106-533-593-630; 060-855-317-223-764; 075-902-258-323-098; 085-102-500-348-494; 098-926-109-358-108; 103-237-795-876-006; 109-806-014-448-128; 150-500-848-539-370; 176-435-553-082-962; 181-099-141-403-601,1,false,,
061-205-550-296-347,When Ultrasonic Sensors and Computer Vision Join Forces for Efficient Obstacle Detection and Recognition,2016-10-28,2016,journal article,"Sensors (Basel, Switzerland)",14248220; 14243210,Multidisciplinary Digital Publishing Institute (MDPI),Switzerland,Bogdan Mocanu; Ruxandra Tapu; Titus Zaharia,"In the most recent report published by the World Health Organization concerning people with visual disabilities it is highlighted that by the year 2020, worldwide, the number of completely blind people will reach 75 million, while the number of visually impaired (VI) people will rise to 250 million. Within this context, the development of dedicated electronic travel aid (ETA) systems, able to increase the safe displacement of VI people in indoor/outdoor spaces, while providing additional cognition of the environment becomes of outmost importance. This paper introduces a novel wearable assistive device designed to facilitate the autonomous navigation of blind and VI people in highly dynamic urban scenes. The system exploits two independent sources of information: ultrasonic sensors and the video camera embedded in a regular smartphone. The underlying methodology exploits computer vision and machine learning techniques and makes it possible to identify accurately both static and highly dynamic objects existent in a scene, regardless on their location, size or shape. In addition, the proposed system is able to acquire information about the environment, semantically interpret it and alert users about possible dangerous situations through acoustic feedback. To determine the performance of the proposed methodology we have performed an extensive objective and subjective experimental evaluation with the help of 21 VI subjects from two blind associations. The users pointed out that our prototype is highly helpful in increasing the mobility, while being friendly and easy to learn.",16,11,1807,,Engineering; Wearable computer; Artificial intelligence; Exploit; Obstacle; Video camera; Context (language use); Computer vision; Audio feedback; Cognitive neuroscience of visual object recognition; Ultrasonic sensor,acoustic feedback; computer vision techniques  machine learning algorithms; object recognition; obstacle detection; ultrasonic network; wearable assistive device,"Acoustics; Adult; Aged; Cell Phone/instrumentation; Humans; Image Processing, Computer-Assisted/instrumentation; Machine Learning; Middle Aged; Self-Help Devices; Visually Impaired Persons",,"Romanian National Authority for Scientific Research and Innovation, CNCS - UEFISCDI",https://www.mendeley.com/catalogue/d39273f3-1ded-343c-8f33-efc17b5dc233/ https://dblp.uni-trier.de/db/journals/sensors/sensors16.html#MocanuTZ16 https://www.mdpi.com/1424-8220/16/11/1807/pdf http://dx.doi.org/10.3390/s16111807 https://hal.archives-ouvertes.fr/hal-01451489 https://www.ncbi.nlm.nih.gov/pubmed/27801834 http://europepmc.org/articles/PMC5134466 https://ui.adsabs.harvard.edu/abs/2016Senso..16.1807M/abstract https://www.mdpi.com/1424-8220/16/11/1807 https://doaj.org/article/600967adf7c84238b4d8fdf0357f01d7 https://dx.doi.org/10.3390/s16111807,http://dx.doi.org/10.3390/s16111807,27801834,10.3390/s16111807,2541074189,PMC5134466,1,000-631-028-807-828; 010-719-927-108-990; 012-642-196-605-159; 017-739-144-960-987; 023-868-294-867-153; 026-703-881-731-30X; 030-127-369-356-741; 030-500-337-935-178; 039-274-384-165-289; 048-446-984-045-429; 057-203-943-288-264; 057-793-015-492-337; 059-647-236-855-012; 060-884-484-928-318; 064-043-611-038-916; 068-708-501-300-740; 085-507-633-373-250; 098-440-015-522-27X; 105-656-733-955-402; 127-070-672-662-621; 127-483-973-270-088; 151-967-570-127-958; 170-387-846-090-147,74,true,cc-by,gold
061-423-504-377-213,Navigation Aid for the Blind People in their Native Language,,2016,journal article,Asian Journal of Research in Social Sciences and Humanities,22497315,Diva Enterprises Private Limited,,R Faustina Jeya Rose; G. Bhuvaneswari,"Visually impaired people are at a hindrance, as they lack information on the obstacles and hazards in their paths. The essential information on their travel such as speed, direction of the objects ahead which is to be known is less. The advanced existing methods use robotic assistance, RFID's which are more complex and costly. Hence this paper proposes a cost effective system where a mobile application is created which connects with a sensing device. Ultrasonic is the main concept used for sensing the objects or obstacles at their path. The direction in which the user has to move once the object at the path sensed is provided as a voice instruction in their native language by the mobile application.",6,7,854,859,Human–computer interaction; Engineering; First language; Navigation aid; Visually impaired; Simulation; Object (computer science); Path (graph theory),,,,,http://www.indianjournals.com/ijor.aspx?target=ijor:ajrssh&volume=6&issue=7&article=066&type=pdf,http://dx.doi.org/10.5958/2249-7315.2016.00470.6,,10.5958/2249-7315.2016.00470.6,2467416269,,0,,0,false,,
061-483-553-522-346,ECCV Workshops (3) - Mobile Panoramic Vision for Assisting the Blind via Indexing and Localization,2015-03-20,2015,book chapter,Computer Vision - ECCV 2014 Workshops,03029743; 16113349,Springer International Publishing,Germany,Feng Hu; Zhigang Zhu; Jianting Zhang,"In this paper, we propose a first-person localization and navigation system for helping blind and visually-impaired people navigate in indoor environments. The system consists of a mobile vision front end with a portable panoramic lens mounted on a smart phone, and a remote GPU-enabled server. Compact and effective omnidirectional video features are extracted and represented in the smart phone front end, and then transmitted to the server, where the features of an input image or a short video clip are used to search a database of an indoor environment via image-based indexing to find both the location and the orientation of the current view. To deal with the high computational cost in searching a large database for a realistic navigation application, data parallelism and task parallelism properties are identified in database indexing, and computation is accelerated by using multi-core CPUs and GPUs. Experiments on synthetic data and real data are carried out to demonstrate the capacity of the proposed system, with respect to real-time response and robustness.",,,600,614,Mobile computing; Artificial intelligence; Navigation system; Computer vision; Computer science; Task parallelism; Database index; Data parallelism; Search engine indexing; Robustness (computer science),,,,,https://rd.springer.com/chapter/10.1007/978-3-319-16199-0_42 https://dblp.uni-trier.de/db/conf/eccv/eccv2014w3.html#HuZZ14 http://vigir.missouri.edu/~gdesouza/Research/Conference_CDs/ECCV_2014/workshops/w22/W22-29.pdf https://link.springer.com/chapter/10.1007%2F978-3-319-16199-0_42 https://link.springer.com/content/pdf/10.1007%2F978-3-319-16199-0_42.pdf,http://dx.doi.org/10.1007/978-3-319-16199-0_42,,10.1007/978-3-319-16199-0_42,2281304535,,0,000-650-161-968-026; 006-156-994-118-493; 013-967-555-226-129; 015-040-438-613-436; 016-209-364-096-807; 027-278-877-933-090; 028-418-701-106-129; 035-291-375-623-97X; 051-050-388-716-887; 053-416-471-857-361; 058-372-973-589-766; 061-190-872-413-287; 073-873-259-605-342; 156-482-225-015-089; 162-368-734-244-304; 178-540-021-649-208,16,false,,
061-611-750-218-70X,Right mix of speech and non-speech: hybrid auditory feedback in mobility assistance of the visually impaired,2014-03-26,2014,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Ibrar Hussain; Ling Chen; Hamid Turab Mirza; Gencai Chen; Saeed-Ul Hassan,,14,4,527,536,Auditory display; User experience design; Repetition (rhetorical device); Auditory feedback; Task (project management); Speech recognition; Computer communication networks; Still face; Visually impaired; Computer science,,,,,https://link.springer.com/article/10.1007/s10209-014-0350-7 https://doi.org/10.1007/s10209-014-0350-7 https://dblp.uni-trier.de/db/journals/uais/uais14.html#0001CMCH15 https://link.springer.com/content/pdf/10.1007%2Fs10209-014-0350-7.pdf,http://dx.doi.org/10.1007/s10209-014-0350-7,,10.1007/s10209-014-0350-7,2024068873,,0,002-391-643-288-41X; 002-646-377-924-410; 003-179-052-061-924; 009-069-676-420-498; 015-978-126-687-61X; 016-068-376-695-077; 016-913-703-544-037; 020-024-664-152-552; 024-030-742-896-837; 024-093-010-731-859; 026-180-397-595-991; 040-541-454-896-255; 046-983-906-128-801; 051-065-738-489-461; 051-766-223-654-722; 053-583-192-599-759; 054-708-339-566-453; 055-323-413-595-244; 055-888-665-838-79X; 056-092-375-535-400; 062-344-155-654-323; 063-593-313-740-053; 073-407-269-569-770; 086-346-817-733-742; 100-082-897-349-538; 101-737-585-999-487; 107-839-387-674-978; 141-489-442-115-69X; 158-956-895-620-282,8,false,,
061-615-443-004-347,Fuzzy-rule-based object identification methodology for NAVI system,2005-08-25,2005,journal article,EURASIP Journal on Advances in Signal Processing,16876180; 16876172,Springer Science and Business Media LLC,Germany,R. Nagarajan; G. Sainarayanan; Sazali Yaacob; Rosalyn R. Porle,"We present an object identification methodology applied in a navigation assistance for visually impaired (NAVI) system. The NAVI has a single board processing system (SBPS), a digital video camera mounted headgear, and a pair of stereo earphones. The captured image from the camera is processed by the SBPS to generate a specially structured stereo sound suitable for vision impaired people in understanding the presence of objects/obstacles in front of them. The image processing stage is designed to identify the objects in the captured image. Edge detection and edge-linking procedures are applied in the processing of image. A concept of object preference is included in the image processing scheme and this concept is realized using a fuzzy-rule base. The blind users are trained with the stereo sound produced by NAVI for achieving a collision-free autonomous navigation.",2005,14,2260,2267,Stereophonic sound; Artificial intelligence; Noise reduction; Edge detection; Fuzzy rule; Computer vision; Computer science; Object (computer science); Cognitive neuroscience of visual object recognition; Identification (information); Image processing,,,,,http://dx.doi.org/10.1155/ASP.2005.2260 https://paperity.org/p/75447310/fuzzy-rule-based-object-identification-methodology-for-navi-system https://link.springer.com/content/pdf/10.1155%2FASP.2005.2260.pdf https://asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.2260 https://eprints.ums.edu.my/id/eprint/1863 https://dx.doi.org/10.1155/ASP.2005.2260 https://dblp.uni-trier.de/db/journals/ejasp/ejasp2005.html#NagarajanSYP05 https://ui.adsabs.harvard.edu/abs/2005EJASP2005...83N/abstract https://link.springer.com/article/10.1155/ASP.2005.2260,http://dx.doi.org/10.1155/asp.2005.2260,,10.1155/asp.2005.2260,2162208571,,2,013-847-555-426-159; 036-075-005-858-923; 038-801-642-442-767; 045-673-862-191-277; 059-328-254-271-872; 064-237-901-943-725; 074-324-981-337-749; 093-849-454-595-958; 124-976-233-783-36X; 175-992-099-433-069,5,true,cc-by,gold
061-777-854-733-319,Visual display for surgical targeting: concepts and usability study.,2021-04-08,2021,journal article,International journal of computer assisted radiology and surgery,18616429; 18616410,Springer Science and Business Media LLC,Germany,Milovan Regodic; Zoltán R. Bárdosi; Georgi Diakov; Malik Galijašević; Christian F. Freyschlag; Wolfgang Freysinger,"Interactive image-guided surgery technologies enable accurate target localization while preserving critical nearby structures in many surgical interventions. Current state-of-the-art interfaces largely employ traditional anatomical cross-sectional views or augmented reality environments to present the actual spatial location of the surgical instrument in preoperatively acquired images. This work proposes an alternative, simple, minimalistic visual interface intended to assist during real-time surgical target localization. The estimated 3D pose of the interventional instruments and their positional uncertainty are intuitively presented in a visual interface with respect to the target point. A usability study with multidisciplinary participants evaluates the proposed interface projected in surgical microscope oculars against cross-sectional views. The latter was presented on a screen both stand-alone and combined with the proposed interface. The instruments were electromagnetically navigated in phantoms. The usability study demonstrated that the participants were able to detect invisible targets marked in phantom imagery with significant enhancements for localization accuracy and duration time. Clinically experienced users reached the targets with shorter trajectories. The stand-alone and multi-modal versions of the proposed interface outperformed cross-sectional views-only navigation in both quantitative and qualitative evaluations. The results and participants’ feedback indicate potential to accurately navigate users toward the target with less distraction and workload. An ongoing study evaluates the proposed system in a preclinical setting for auditory brainstem implantation.",16,9,1565,1576,Human–computer interaction; Interface (computing); Augmented reality; Usability; Point (typography); Distraction; Image-guided surgery; Surgical instrument; Workload; Computer science,Electromagnetic tracking; Image-guided surgery; Surgical targeting; Usability study; Visual guidance,"Augmented Reality; Cross-Sectional Studies; Humans; Imaging, Three-Dimensional; Phantoms, Imaging; Surgery, Computer-Assisted; User-Computer Interface",,Österreichische Forschungsförderungsgesellschaft (855783),https://www.ncbi.nlm.nih.gov/pubmed/33830426 https://link.springer.com/article/10.1007/s11548-021-02355-8 https://link.springer.com/content/pdf/10.1007/s11548-021-02355-8.pdf https://dblp.uni-trier.de/db/journals/cars/cars16.html#RegodicBDGFF21 https://europepmc.org/article/MED/33830426,http://dx.doi.org/10.1007/s11548-021-02355-8,33830426,10.1007/s11548-021-02355-8,3145966429,PMC8355000,0,001-705-338-724-935; 002-620-657-668-321; 003-918-052-051-538; 004-626-175-833-750; 013-240-089-823-171; 025-557-510-481-795; 027-487-963-787-764; 028-694-153-573-412; 030-439-851-106-84X; 033-119-829-457-834; 033-657-233-763-695; 034-082-793-352-087; 036-153-484-741-706; 065-749-753-226-163; 094-554-733-431-292; 129-660-828-851-545; 135-305-283-998-598; 151-065-308-253-657,4,true,cc-by,hybrid
061-807-574-797-865,Universal access in e-voting for the blind,2010-02-12,2010,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Juan E. Gilbert; Yolanda McMillian; Ken Rouse; Philicity Williams; Gregory Rogers; Jerome McClendon; Winfred Mitchell; Priyanka Gupta; Idong Mkpong-Ruffin; E. Vincent Cross,,9,4,357,365,Internet privacy; Trustworthiness; Equity (finance); Electronic voting; Population; Computer communication networks; Visually impaired; Computer security; Computer science; Voting; Universal design,,,,,https://doi.org/10.1007/s10209-009-0181-0 https://link.springer.com/article/10.1007/s10209-009-0181-0 https://dblp.uni-trier.de/db/journals/uais/uais9.html#GilbertMRWRMMGMC10 https://dl.acm.org/doi/10.1007/s10209-009-0181-0 https://link.springer.com/content/pdf/10.1007%2Fs10209-009-0181-0.pdf http://dblp.uni-trier.de/db/journals/uais/uais9.html#GilbertMRWRMMGMC10 https://link.springer.com/article/10.1007/s10209-009-0181-0/fulltext.html https://jglobal.jst.go.jp/en/detail?JGLOBAL_ID=201102283554729497,http://dx.doi.org/10.1007/s10209-009-0181-0,,10.1007/s10209-009-0181-0,1976615574,,0,022-215-359-483-157; 059-011-392-370-486; 072-870-430-766-972,23,false,,
061-817-185-971-97X,An efficient object detection system for indoor assistance navigation using deep learning techniques,2022-03-03,2022,journal article,Multimedia Tools and Applications,13807501; 15737721,Springer Science and Business Media LLC,Netherlands,Mouna Afif; Riadh Ayachi; Yahia Said; Edwige Pissaloux; Mohamed Atri,,81,12,16601,16618,Computer science; Convolutional neural network; Artificial intelligence; Landmark; Object detection; Deep learning; Task (project management); Computer vision; Object (grammar); Real-time computing; Pattern recognition (psychology); Management; Economics,,,,,,http://dx.doi.org/10.1007/s11042-022-12577-w,,10.1007/s11042-022-12577-w,,,0,006-637-087-577-95X; 012-057-068-104-205; 016-209-364-096-807; 016-586-064-465-125; 020-233-013-143-936; 027-826-057-962-368; 033-245-290-160-646; 035-277-967-833-996; 042-096-130-327-793; 044-396-615-675-996; 044-472-558-807-885; 045-309-399-228-849; 047-040-726-932-715; 054-778-792-715-676; 061-579-383-511-500; 073-573-125-648-967; 075-533-194-094-730; 085-686-689-031-272; 088-977-342-793-007; 097-642-574-506-283; 109-485-258-269-762; 124-596-347-359-116; 134-017-779-445-935; 134-730-745-769-935; 137-478-852-138-685; 140-713-787-227-600; 144-694-731-681-315; 146-885-084-039-864; 160-695-376-585-699,9,false,,
061-818-930-384-199,Surgical Robotics and Instrumentation,2010-05-11,2010,journal article,International Journal of Computer Assisted Radiology and Surgery,18616410; 18616429,Springer Science and Business Media LLC,Germany,,,5,S1,143,152,Robotics; Instrumentation (computer programming); Artificial intelligence; Computer science; Medical physics; Health informatics; Medical robotics; Medicine; Computer vision; Robot; Data science; General surgery; Pathology; Public health; Programming language,,,,,,http://dx.doi.org/10.1007/s11548-010-0453-4,,10.1007/s11548-010-0453-4,,,0,,0,false,,
061-933-888-557-079,3-D Object Recognition of a Robotic Navigation Aid for the Visually Impaired,2017-09-01,2017,journal article,IEEE transactions on neural systems and rehabilitation engineering : a publication of the IEEE Engineering in Medicine and Biology Society,15580210; 15344320,Institute of Electrical and Electronics Engineers (IEEE),United States,Cang Ye; Xiangfei Qian,"This paper presents a 3-D object recognition method and its implementation on a robotic navigation aid to allow real-time detection of indoor structural objects for the navigation of a blind person. The method segments a point cloud into numerous planar patches and extracts their inter-plane relationships (IPRs).Based on the existing IPRs of the object models, the method defines six high level features (HLFs) and determines the HLFs for each patch. A Gaussian-mixture-model-based plane classifier is then devised to classify each planar patch into one belonging to a particular object model. Finally, a recursive plane clustering procedure is used to cluster the classified planes into the model objects. As the proposed method uses geometric context to detect an object, it is robust to the object’s visual appearance change. As a result, it is ideal for detecting structural objects (e.g., stairways, doorways, and so on). In addition, it has high scalability and parallelism. The method is also capable of detecting some indoor non-structural objects. Experimental results demonstrate that the proposed method has a high success rate in object recognition.",26,2,441,450,Point cloud; 3D single-object recognition; Artificial intelligence; Visual appearance; Object model; Computer vision; Computer science; Feature extraction; Object-oriented design; Cognitive neuroscience of visual object recognition; Cluster analysis,,"Algorithms; Canes; Cluster Analysis; Humans; Models, Statistical; Normal Distribution; Pattern Recognition, Automated; Robotics; Self-Help Devices; Vision Disorders/rehabilitation",,NIBIB NIH HHS (R01 EB018117) United States; NEI NIH HHS (R01 EY026275) United States,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5843551 https://pubmed.ncbi.nlm.nih.gov/28880185/ https://ieeexplore.ieee.org/document/8024025/,http://dx.doi.org/10.1109/tnsre.2017.2748419,28880185,10.1109/tnsre.2017.2748419,2753027569,PMC5843551,0,002-978-775-225-739; 009-001-441-004-253; 010-306-378-600-804; 013-551-333-215-556; 013-672-230-759-113; 015-585-976-020-814; 024-964-397-377-997; 029-188-673-280-386; 030-127-369-356-741; 036-734-605-969-025; 041-142-363-026-244; 044-537-674-820-336; 045-412-348-145-977; 052-283-451-281-593; 064-521-070-547-235; 065-719-616-117-529; 066-175-334-451-24X; 066-806-296-593-666; 067-163-091-503-84X; 071-806-151-845-245; 078-736-735-476-395; 082-432-999-638-636; 089-738-656-495-377; 104-694-342-259-021; 105-724-695-899-818; 106-778-908-131-221; 109-525-288-959-67X; 121-913-978-357-552; 144-567-139-801-910; 174-687-363-117-552; 178-127-291-657-997; 181-797-068-816-79X,52,true,"CC BY, CC BY-NC-ND",gold
061-955-503-155-050,"Artificial intelligence and disability: too much promise, yet too little substance?",2020-10-06,2020,journal article,AI and Ethics,27305953; 27305961,Springer Science and Business Media LLC,,Peter Smith; Laura Smith,"Much has been written about the potential of artificial intelligence (AI) to support, and even transform, the lives of disabled people. It is true that many advances have been made, ranging from robotic arms and other prosthetic limbs supported by AI, decision support tools to aid clinicians and the disabled themselves, and route planning software for those with visual impairment. Many individuals are benefiting from the use of such tools, improving our accessibility and changing lives. But what are the true limits of such tools? What are the ethics of allowing AI tools to suggest different courses of action, or aid in decision-making? And does AI offer too much promise for individuals? I have recently undergone a life changing accident which has left me severely disabled, and together with my daughter who is blind, we shall explore the day-to-day realities of how AI can support, and frustrate, disabled people. From this, we will draw some conclusions as to how AI software and technology might best be developed in the future.",1,1,81,86,Artificial intelligence; Psychology; Software; Action (philosophy); Visual impairment; Prosthetic limb; Decision support tools; Disabled people; Route planning software,,,,,https://link.springer.com/content/pdf/10.1007/s43681-020-00004-5.pdf https://link.springer.com/article/10.1007/s43681-020-00004-5 https://derby.openrepository.com/handle/10545/625640 https://derby.aws.openrepository.com/handle/10545/625640 https://doi.org/10.1007/s43681-020-00004-5 https://dblp.uni-trier.de/db/journals/aiethics/aiethics1.html#SmithS21,http://dx.doi.org/10.1007/s43681-020-00004-5,,10.1007/s43681-020-00004-5,3091948313,,0,000-817-486-678-475; 006-815-245-827-600; 007-155-913-653-024; 014-978-849-075-805; 015-245-163-317-687; 024-084-908-638-269; 050-183-762-063-648; 056-478-695-511-987; 056-893-737-083-078; 069-335-256-971-379; 091-430-349-260-560; 093-534-809-670-325; 110-074-946-591-316; 120-266-639-237-666; 122-901-730-854-158; 132-388-567-719-244; 172-634-815-634-533; 173-302-479-464-63X; 178-592-640-995-980; 182-762-433-904-963,26,true,,bronze
061-991-932-623-906,Robotic manipulation for the shoe-packaging process,2017-03-07,2017,journal article,The International Journal of Advanced Manufacturing Technology,02683768; 14333015,Springer Science and Business Media LLC,Germany,Luis Gracia; Carlos Perez-Vidal; Dennis Mronga; Jose-Manuel de Paco; Jose-Maria Azorin; Jose de Gea,"This paper presents the integration of a robotic system in a human-centered environment, as it can be found in the shoe manufacturing industry. Fashion footwear is nowadays mainly handcrafted due to the big amount of small production tasks. Therefore, the introduction of intelligent robotic systems in this industry may contribute to automate and improve the manual production steps, such us polishing, cleaning, packaging, and visual inspection. Due to the high complexity of the manual tasks in shoe production, cooperative robotic systems (which can work in collaboration with humans) are required. Thus, the focus of the robot lays on grasping, collision detection, and avoidance, as well as on considering the human intervention to supervise the work being performed. For this research, the robot has been equipped with a Kinect camera and a wrist force/ torque sensor so that it is able to detect human interaction and the dynamic environment in order to modify the robot’s behavior. To illustrate the applicability of the proposed approach, this work presents the experimental results obtained for two actual platforms, which are located at different research laboratories, that share similarities in their morphology, sensor equipment and actuation system.",92,1,1053,1067,Torque sensor; Human–computer interaction; Engineering; Visual inspection; Manufacturing; Work (physics); Collision detection; Focus (computing); Process (engineering); Simulation; Robot,,,,"Secretaría de Estado de Investigacion, Desarrollo e Innovacion; Secretaría de Estado de Investigación, Desarrollo e Innovación; Deutscher Akademischer Austauschdienst",https://core.ac.uk/display/158628453 https://riunet.upv.es/handle/10251/151046 https://riunet.upv.es/bitstream/10251/151046/3/Gracia%3bPerez-Vidal%3bMronga%20-%20Robotic%20manipulation%20for%20the%20shoe-packaging%20process.pdf https://link.springer.com/article/10.1007/s00170-017-0212-6,http://dx.doi.org/10.1007/s00170-017-0212-6,,10.1007/s00170-017-0212-6,2594438192,,0,005-346-226-813-62X; 009-174-508-376-269; 012-016-836-079-639; 012-044-107-840-692; 012-169-287-590-238; 016-950-359-745-528; 017-179-380-976-910; 039-236-181-761-374; 042-878-847-356-424; 044-910-439-584-09X; 063-655-869-624-040; 065-858-728-464-912; 066-479-374-130-216; 066-519-927-796-442; 066-749-653-813-64X; 075-689-921-761-386; 084-574-605-471-092; 087-820-564-872-729; 090-878-226-160-884; 113-160-067-826-801; 113-973-789-339-35X; 133-369-277-309-40X; 133-863-313-225-777; 139-283-000-184-094; 141-059-863-986-063; 141-307-011-595-847; 142-228-997-967-141; 154-936-657-730-044; 180-742-516-456-659; 181-162-771-384-328; 190-787-546-814-80X; 192-987-902-466-24X,14,true,cc-by-nc-nd,green
062-128-353-400-109,Smart Cap - Wearable Visual Guidance System for Blind,,2018,conference proceedings article,2018 International Conference on Inventive Research in Computing Applications (ICIRCA),,IEEE,,A. Nishajith; J. Nivedha; Shilpa. S. Nair; J. Mohammed Shaffi,"Science and technology always try to make human life easier. The people who are having complete blindness or low vision faces many difficulties during their navigation. Blindness can occur due to many reasons including disease, injury or other conditions that limit vision. The main purpose of this paper is to develop a navigation aid for the blind and the visually impaired people. In this paper, we design and implement a smart cap which helps the blind and the visually impaired people to navigate freely by experiencing their surroundings. The scene around the person will be captured by using a NoIR camera and the objects in the scene will be detected. The earphones will give a voice output describing the detected objects. The architecture of the system includes the processor Raspberry Pi 3, NoIR camera, earphones and a power source. The processor collects the frames of the surroundings and convert it to voice output. The device uses TensorFlow API, open-source machine learning library developed by the Google Brain Team for the object detection and classification. TensorFlow helps in creating machine learning models capable of identifying and classifying multiple objects in a single image. Thus, details corresponding to various objects present within a single frame are obtained using TensorFlow API. A Text to Speech Synthesiser (TTS) software called eSpeak is used for converting the details of the detected object (in text format) to speech output. So the video captured by using the NoIR camera is finally converted to speech signals and thus narration of the scene describing various objects is done. Objects which come under 90 different classes like cell phone, vase, person, couch etc are detected.",,,275,278,Architecture; Speech synthesis; Wearable computer; Artificial intelligence; Object detection; Software; Phone; Computer vision; Computer science; Object (computer science); Formatted text,,,,,https://ieeexplore.ieee.org/document/8597327,http://dx.doi.org/10.1109/icirca.2018.8597327,,10.1109/icirca.2018.8597327,2910634227,,0,007-814-036-819-438; 031-218-334-653-826; 036-461-515-059-334; 042-568-425-640-966; 123-881-382-785-846; 137-032-010-877-936; 149-961-061-562-856,29,false,,
062-174-191-589-857,Interactive hierarchy-based auditory displays for accessing and manipulating relational diagrams,2011-10-14,2011,journal article,Journal on Multimodal User Interfaces,17837677; 17838738,Springer Science and Business Media LLC,Germany,Oussama Metatla; Nick Bryan-Kinns; Tony Stockman,,5,3,111,122,Human–computer interaction; Artificial intelligence; Modality (human–computer interaction); User studies; Multiple perspective; Hierarchy; Computer science,,,,,http://isam.eecs.qmul.ac.uk/projects/ccmi/JMUIpaper.pdf https://link.springer.com/article/10.1007/s12193-011-0067-3 http://www.eecs.qmul.ac.uk/~nickbk/papers/JMUIpaper.pdf https://dblp.uni-trier.de/db/journals/jmui/jmui5.html#MetatlaBS12 https://link.springer.com/content/pdf/10.1007%2Fs12193-011-0067-3.pdf,http://dx.doi.org/10.1007/s12193-011-0067-3,,10.1007/s12193-011-0067-3,1977621004,,0,004-727-932-168-958; 010-831-496-054-483; 011-679-964-026-452; 024-093-010-731-859; 024-848-286-367-819; 029-246-210-560-499; 030-026-719-884-083; 031-831-285-802-72X; 032-020-305-685-792; 034-048-164-196-633; 035-140-507-958-256; 039-735-730-669-594; 043-135-954-364-664; 044-208-645-237-793; 056-017-240-141-05X; 058-960-390-771-612; 060-976-334-680-335; 061-773-125-131-133; 067-217-663-836-852; 069-504-137-008-648; 072-609-480-254-95X; 074-653-725-917-231; 080-877-996-625-003; 089-629-145-629-991; 092-412-865-427-491; 095-390-499-430-942; 106-505-804-414-215; 114-803-614-686-637; 129-241-231-885-05X; 131-512-616-525-307; 139-079-984-998-681; 144-837-804-461-424; 146-346-864-680-166; 156-737-647-714-471; 162-359-344-062-471; 165-431-480-577-376; 176-459-214-555-135; 186-082-467-559-447; 186-524-945-238-812; 187-885-656-558-940; 192-588-056-613-897,11,false,,
062-182-097-635-036,Adaptations for Young Children with Visual Impairments in Regular Settings,,1996,journal article,Early Childhood Education Journal,10823301; 15731707,Springer Science and Business Media LLC,United States,Phyllis K. Mayfield; Katherine McCormick; Martha J. Cook,,23,4,231,233,Developmental psychology; Sociology of Education; Mainstreaming; Inclusion (education); Psychology; Cognitive psychology; Legislation; Visual learning; Visual observation; Learning potential; Community setting,,,,,https://eric.ed.gov/?id=EJ528150 https://link.springer.com/content/pdf/10.1007/BF02353343.pdf https://link.springer.com/article/10.1007%2FBF02353343,http://dx.doi.org/10.1007/bf02353343,,10.1007/bf02353343,2015827508,,0,003-105-560-587-111; 004-618-893-383-651; 050-331-121-922-31X; 098-137-517-751-222,5,false,,
062-231-151-516-539,The measurement-aided welding cell—giving sight to the blind,2015-12-11,2015,journal article,The International Journal of Advanced Manufacturing Technology,02683768; 14333015,Springer Science and Business Media LLC,Germany,Valtteri Tuominen,"This article introduces the concept of a measurement-aided welding cell (MAWC). It then focuses on developing the MAWC for body and chassis components in the automotive industry. Industry 4.0 requires flexibility and reconfigurability from manufacturing systems, which has been addressed by cellular manufacturing systems (CMS). Traditional automotive welding technology is not flexible enough to be used as a CMS. So, the core of automotive production does not meet the needs of next-generation manufacturing and Industry 4.0. For the first time, this demand has been answered—by the MAWC. The MAWC is based on two handling robots, a welding robot and an optical measurement system, all integrated into one welding cell. The measurement system controls guides and gives feedback to the handling and welding robots. This way, accurate welding is based on actual part-to-part adjustment, rather than hard mechanical tooling. The lack of hard mechanical tooling allows flexibility. Measurement-assisted assembly and machining is not new. However, it has not previously been applied to complex welding processes, because traditional measurement technologies have not been satisfactory. A newly developed system using multi-camera measurement technology meets the flexibility requirements. The technology required for a MAWC is reviewed and found to be in common use in the automotive industry. Three development projects are presented in which the main functions of a MAWC are successfully demonstrated. Two of these projects were done with BMW. A MAWC allows the manufacturing process to immediately swap to a different product on the same line, increasing the production line utilisation rate and making it possible to produce several low-volume products in one manufacturing system. So, the MAWC will not only yield benefits from its scalable capacity and global decentralization, but it will also open new business opportunities for manufacturing low-volume luxury car parts or post-production spare parts.",86,1,371,386,Manufacturing engineering; Robot welding; Engineering; Spare part; Welding; Cellular manufacturing; Reconfigurability; Flexibility (engineering); Automotive industry; Production line,,,,,https://paperity.org/p/74787475/the-measurement-aided-welding-cell-giving-sight-to-the-blind https://link.springer.com/article/10.1007/s00170-015-8193-9 https://core.ac.uk/display/81752317 https://link.springer.com/content/pdf/10.1007%2Fs00170-015-8193-9.pdf https://aaltodoc.aalto.fi:443/handle/123456789/29969,http://dx.doi.org/10.1007/s00170-015-8193-9,,10.1007/s00170-015-8193-9,2286131889,,0,005-870-915-242-974; 010-882-999-924-68X; 016-097-163-251-686; 016-316-082-761-251; 016-493-233-036-135; 019-909-346-771-350; 023-672-765-523-847; 025-737-776-479-738; 026-285-197-442-203; 027-688-760-193-406; 041-883-456-189-42X; 043-262-669-627-80X; 044-699-184-017-294; 047-732-537-198-520; 049-825-363-806-260; 054-096-139-168-325; 056-601-684-934-133; 067-827-716-189-926; 068-250-983-865-514; 070-804-756-591-596; 093-241-801-045-390; 093-391-381-641-543; 093-890-963-057-351; 098-600-291-343-796; 101-385-222-679-349; 105-351-603-562-304; 112-261-284-421-663; 114-722-089-468-724; 125-752-599-440-533; 127-976-520-335-841; 129-692-849-290-288; 152-403-059-442-793; 156-096-181-671-109; 157-923-453-554-663; 161-809-579-071-355; 193-542-153-663-127; 198-528-121-812-707,22,true,cc-by,hybrid
062-271-018-949-569,Knowledge-based image analysis for geophysical interpretation,,1993,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Ioannis Pitas; Anastasios N. Venetsanopoulos,,7,2,115,137,Data mining; Engineering; Synthetic seismogram; Pattern recognition (psychology); Knowledge representation and reasoning; Fuzzy set; Seismic to simulation; Geophysical imaging; Geophysics; Image processing; Horizon (geology),,,,,https://link.springer.com/article/10.1007/BF01257815 https://dblp.uni-trier.de/db/journals/jirs/jirs7.html#PitasV93,http://dx.doi.org/10.1007/bf01257815,,10.1007/bf01257815,2017063121,,0,000-489-422-231-105; 005-352-695-871-073; 008-623-077-313-702; 011-883-359-022-523; 012-914-721-542-173; 013-494-129-268-660; 019-227-266-204-892; 022-566-265-325-355; 029-806-210-702-218; 030-048-974-076-056; 030-856-447-407-285; 034-790-763-715-681; 038-624-154-280-020; 040-535-455-202-015; 041-197-070-368-29X; 042-024-827-925-504; 050-296-618-980-194; 050-320-077-462-032; 054-856-521-763-116; 058-830-961-215-842; 065-260-447-841-666; 067-212-412-965-972; 068-148-700-619-276; 074-521-625-905-853; 081-089-992-741-639; 088-578-987-957-306; 092-029-784-601-900; 095-087-061-350-778; 102-891-106-782-953; 109-073-315-780-620; 116-462-258-330-887; 119-443-751-371-725; 130-227-896-111-870; 144-032-903-629-554; 146-398-360-667-499; 149-022-484-988-87X; 156-016-514-189-650; 162-510-456-164-420; 163-395-938-186-29X; 166-021-900-566-142; 192-210-091-817-153; 194-381-476-297-100,5,false,,
062-630-991-312-529,ICCHP (1) - Determining a Blind Pedestrian's Location and Orientation at Traffic Intersections.,,2014,journal article,"Computers helping people with special needs : ... International Conference, ICCHP ... : proceedings. International Conference on Computers Helping People with Special Needs",,,Germany,Giovanni Fusco; Huiying Shen; Vidya Nariyambut murali; James M. Coughlan,"This paper describes recent progress on Crosswatch, a smartphone-based computer vision system developed by the authors for providing guidance to blind and visually impaired pedestrians at traffic intersections. One of Crosswatch’s key capabilities is determining the user’s location (with precision much better than what is obtainable by GPS) and orientation relative to the crosswalk markings in the intersection that he/she is currently standing at; this capability will be used to help him/her find important features in the intersection, such as walk lights, pushbuttons and crosswalks, and achieve proper alignment to these features. We report on two new contributions to Crosswatch: (a) experiments with a modified user interface, tested by blind volunteer participants, that makes it easier to acquire intersection images than with previous versions of Crosswatch; and (b) a demonstration of the system’s ability to localize the user with precision better than what is obtainable by GPS, as well as an example of its ability to estimate the user’s orientation.",8547,,427,432,Schema crosswalk; Artificial intelligence; Intersection; Pedestrian; Visual impairment; Poison control; Computer vision; Computer science; Simulation; Global Positioning System; User interface; Orientation (computer vision),assistive technology; blindness; smartphone; traffic intersection; visual impairment,,,NEI NIH HHS (R01 EY018345) United States,https://www.safetylit.org/citations/index.php?fuseaction=citations.viewdetails&citationIds[]=citjournalarticle_473957_19 https://link.springer.com/content/pdf/10.1007%2F978-3-319-08596-8_65.pdf https://europepmc.org/article/PMC/PMC4293520 https://rd.springer.com/chapter/10.1007/978-3-319-08596-8_65 https://dblp.uni-trier.de/db/conf/icchp/icchp2014-1.html#FuscoSMC14 https://link.springer.com/chapter/10.1007/978-3-319-08596-8_65 http://www.ski.org/sites/default/files/publications/fuscoshenmuralicoughlan_icchp2014.pdf https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4293520 https://www.ski.org/sites/default/files/publications/fuscoshenmuralicoughlan_icchp2014.pdf,http://dx.doi.org/10.1007/978-3-319-08596-8_65,25599097,10.1007/978-3-319-08596-8_65,2117922182,PMC4293520,0,004-559-796-546-992; 027-780-924-643-322; 029-662-702-439-081; 040-445-706-184-474; 047-676-233-635-740; 047-838-261-582-286; 051-612-966-213-045; 075-402-037-032-739; 075-642-567-625-059,7,true,,green
062-831-399-930-337,Representing a stable environment by egocentric updating and invariant representations,,1999,journal article,Spatial Cognition and Computation,13875868,Springer Science and Business Media LLC,United States,Ranxiao Frances Wang,,1,4,431,445,Artificial intelligence; Reference frame; Spatial representation; Computer vision; Computer science; Invariant (mathematics); Robustness (computer science),,,,,https://link.springer.com/article/10.1023/A:1010043814328 https://dblp.uni-trier.de/db/journals/scc/scc1.html#Wang99,http://dx.doi.org/10.1023/a:1010043814328,,10.1023/a:1010043814328,1546841329,,0,002-145-079-144-71X; 002-685-000-074-462; 003-372-408-101-339; 005-274-871-858-441; 006-993-240-462-691; 009-324-907-764-702; 010-500-392-766-156; 011-975-729-964-664; 017-230-621-398-703; 019-970-232-566-327; 020-545-493-038-440; 027-151-978-594-704; 031-922-513-835-616; 033-591-124-833-00X; 038-156-842-271-481; 039-049-118-197-703; 044-184-894-898-249; 045-853-865-147-931; 051-274-185-844-531; 061-485-545-490-013; 070-686-599-782-510; 072-853-641-144-060; 074-792-812-248-435; 079-531-893-222-451; 083-520-777-795-27X; 091-866-131-460-313; 099-027-071-247-96X; 102-555-702-781-751; 110-657-030-293-903; 113-452-377-881-827; 163-380-100-757-501; 178-615-072-000-111,41,false,,
063-117-355-616-790,Indoor positioning and wayfinding systems: a survey,2020-05-02,2020,journal article,Human-centric Computing and Information Sciences,21921962,Springer Science and Business Media LLC,United States,Jayakanth Kunhoth; AbdelGhani Karkar; Somaya Al-Maadeed; Abdulla Al-Ali,"Navigation systems help users access unfamiliar environments. Current technological advancements enable users to encapsulate these systems in handheld devices, which effectively increases the popularity of navigation systems and the number of users. In indoor environments, lack of Global Positioning System (GPS) signals and line of sight with orbiting satellites makes navigation more challenging compared to outdoor environments. Radio frequency (RF) signals, computer vision, and sensor-based solutions are more suitable for tracking the users in indoor environments. This article provides a comprehensive summary of evolution in indoor navigation and indoor positioning technologies. In particular, the paper reviews different computer vision-based indoor navigation and positioning systems along with indoor scene recognition methods that can aid the indoor navigation. Navigation and positioning systems that utilize pedestrian dead reckoning (PDR) methods and various communication technologies, such as Wi-Fi, Radio Frequency Identification (RFID) visible light, Bluetooth and ultra-wide band (UWB), are detailed as well. Moreover, this article investigates and contrasts the different navigation systems in each category. Various evaluation criteria for indoor navigation systems are proposed in this work. The article concludes with a brief insight into future directions in indoor positioning and navigation systems.",10,1,1,41,Dead reckoning; Human–computer interaction; Mobile device; Radio-frequency identification; Pedestrian; Computer science; Bluetooth; Global Positioning System; Radio frequency,,,,Qatar University,https://doi.org/10.1186/s13673-020-00222-0 https://link.springer.com/content/pdf/10.1186/s13673-020-00222-0.pdf https://link.springer.com/article/10.1186/s13673-020-00222-0/figures/1 https://dblp.uni-trier.de/db/journals/hcis/hcis10.html#KunhothKAA20 https://link.springer.com/article/10.1186/s13673-020-00222-0 https://hcis-journal.springeropen.com/articles/10.1186/s13673-020-00222-0,http://dx.doi.org/10.1186/s13673-020-00222-0,,10.1186/s13673-020-00222-0,3030402745,,0,000-276-479-976-352; 001-122-595-546-506; 004-473-482-368-858; 004-770-396-724-546; 005-526-067-112-575; 007-223-103-783-187; 007-809-072-085-869; 008-064-291-231-427; 008-139-948-663-237; 010-878-503-089-86X; 010-882-152-302-842; 010-955-615-069-982; 011-369-981-296-055; 011-552-293-977-610; 012-557-160-321-73X; 012-642-196-605-159; 013-292-794-721-204; 014-478-034-924-123; 015-008-890-486-381; 017-992-423-449-064; 018-277-384-793-543; 020-260-934-897-437; 020-546-063-291-643; 020-932-340-092-358; 021-089-043-235-519; 026-684-493-399-893; 027-298-301-487-030; 027-834-236-352-397; 028-094-088-190-140; 028-753-316-269-878; 028-882-065-739-118; 029-360-136-216-567; 029-563-815-617-338; 029-603-934-974-438; 030-001-663-838-537; 030-785-858-832-416; 030-877-543-890-441; 030-960-709-221-107; 031-250-472-337-581; 031-755-053-318-46X; 031-953-039-238-823; 032-284-166-524-962; 033-324-893-365-304; 035-159-685-124-983; 036-017-937-159-609; 036-280-949-293-766; 037-053-436-974-589; 038-549-964-822-226; 040-965-133-572-320; 043-401-043-427-432; 043-794-164-324-472; 044-285-243-620-529; 045-205-438-482-211; 045-309-399-228-849; 046-408-269-250-810; 046-647-501-503-754; 050-397-619-218-217; 050-524-939-731-65X; 053-215-378-840-089; 053-375-717-823-025; 053-701-934-194-211; 055-376-024-532-10X; 055-660-300-282-556; 056-348-572-454-956; 056-755-512-382-960; 057-847-555-860-541; 058-324-415-139-780; 059-149-073-001-124; 059-430-192-138-478; 059-651-664-982-044; 059-993-364-370-118; 060-578-904-142-843; 060-579-242-385-348; 063-200-348-257-985; 063-417-520-342-514; 064-053-186-811-574; 064-309-293-653-029; 065-032-907-680-425; 065-120-797-032-75X; 065-650-515-032-187; 066-019-349-945-23X; 066-246-145-459-483; 067-894-235-988-508; 068-330-978-587-951; 069-380-914-823-507; 070-116-981-693-266; 070-569-904-920-551; 071-426-678-672-876; 071-806-151-845-245; 072-481-987-766-954; 074-089-299-077-609; 075-395-669-514-231; 076-111-507-166-077; 077-042-912-905-881; 077-101-929-493-550; 079-310-940-223-758; 079-433-787-480-087; 079-535-935-176-19X; 080-395-954-753-523; 081-772-018-442-834; 081-871-676-661-821; 083-175-435-645-365; 084-993-834-667-298; 085-242-767-142-895; 085-720-318-908-772; 085-815-105-604-187; 085-856-866-387-915; 087-281-121-863-639; 088-298-160-207-690; 089-082-056-973-911; 089-257-666-428-875; 089-386-876-941-547; 090-068-320-321-803; 090-871-058-688-660; 091-061-227-748-735; 092-009-081-390-854; 093-040-839-743-206; 093-800-368-909-589; 094-202-843-351-729; 097-187-749-266-209; 099-565-998-859-687; 101-269-289-434-356; 101-790-519-213-047; 106-227-926-511-534; 110-409-001-674-560; 110-417-258-089-156; 110-912-584-227-014; 111-057-241-780-637; 112-154-051-246-370; 115-277-930-324-266; 117-174-570-523-405; 127-070-672-662-621; 131-901-446-653-961; 133-087-411-839-941; 133-846-312-833-522; 134-766-397-887-111; 136-274-363-848-642; 137-797-970-196-585; 145-898-655-711-084; 146-486-002-138-853; 148-679-698-323-590; 148-904-368-584-526; 153-471-712-074-630; 159-668-380-289-754; 163-091-395-908-467; 168-330-006-032-201; 171-091-935-415-285; 184-841-947-612-501; 191-211-270-309-451; 192-697-687-929-866; 192-751-999-674-131,157,true,cc-by,hybrid
063-393-774-201-03X,Updating our understanding of situation awareness in relation to remote operators of autonomous vehicles.,2021-02-19,2021,journal article,Cognitive research: principles and implications,23657464,Springer Science and Business Media LLC,England,Clare Mutzenich; Szonya Durant; Shaun Helman; Polly Dalton,"The introduction of autonomous vehicles (AVs) could prevent many accidents attributable to human driver error. However, even entirely driverless vehicles will sometimes require remote human intervention. Current taxonomies of automated driving do not acknowledge the possibility of remote control of AVs or the challenges that are unique to such a driver in charge of a vehicle that they are not physically occupying. Yet there are significant differences between situation awareness (SA) in normal driving contexts and SA in these remote driving operations. We argue that the established understanding of automated driving requires updating to include the context of remote operation that is likely to come in to play at higher levels of automation. It is imperative to integrate the role of the remote operator within industry standard taxonomies, so that regulatory frameworks can be established with regards to the training required for remote operation, the necessary equipment and technology, and a comprehensive inventory of the use cases under which we could expect remote operation to be carried out. We emphasise the importance of designing control interfaces in a way that will maximise remote operator (RO) SA and we identify some principles for designing systems aimed at increasing an RO’s sense of embodiment in the AV that requires temporary control.",6,1,1,17,Risk analysis (engineering); Automation; Situation awareness; Relation (database); Remote operation; Context (language use); Poison control; Use case; Computer science; Remote control,,Automation; Automobile Driving; Awareness; Humans,,Economic and Social Research Council,https://europepmc.org/article/PMC/PMC7892648 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7892648 https://eric.ed.gov/?q=source%3A%22Cognitive+Research%3A+Principles+and+Implications%22&ff1=souCognitive+Research%3A+Principles+and+Implications&id=EJ1287720 https://link.springer.com/article/10.1186/s41235-021-00271-8 https://cognitiveresearchjournal.springeropen.com/articles/10.1186/s41235-021-00271-8 https://link.springer.com/content/pdf/10.1186/s41235-021-00271-8.pdf,http://dx.doi.org/10.1186/s41235-021-00271-8,33604779,10.1186/s41235-021-00271-8,3133431827,PMC7892648,3,001-223-009-827-114; 001-844-253-416-757; 004-153-420-657-045; 005-046-101-341-476; 005-279-965-190-107; 005-861-492-610-995; 007-130-504-131-056; 009-962-238-587-011; 010-378-217-410-586; 013-245-243-058-328; 014-198-323-594-216; 015-136-856-270-399; 015-889-466-097-232; 016-631-562-957-261; 017-085-435-828-791; 019-574-515-141-842; 020-569-774-253-856; 021-356-954-503-717; 023-438-829-005-171; 026-829-733-190-285; 031-294-750-698-550; 031-407-431-274-070; 033-028-162-470-93X; 034-582-235-854-147; 035-346-866-273-21X; 036-057-901-553-766; 040-717-083-513-441; 042-576-125-131-422; 042-966-366-162-40X; 044-451-762-072-473; 045-360-418-900-017; 046-483-032-206-125; 051-284-978-850-113; 053-857-999-157-199; 059-504-815-840-60X; 060-279-302-894-538; 063-250-510-542-975; 064-827-132-147-862; 067-132-833-219-580; 067-377-159-956-671; 069-505-153-093-39X; 074-601-015-090-044; 077-772-994-056-725; 078-223-313-472-948; 082-149-971-147-241; 083-361-697-471-149; 086-045-354-043-307; 095-861-522-777-724; 100-766-006-668-840; 107-413-577-698-980; 116-353-357-581-750; 118-639-436-835-460; 120-369-210-501-630; 126-287-122-176-232; 138-036-683-675-479; 138-350-699-188-726; 162-561-528-923-319,32,true,cc-by,gold
063-469-303-614-434,Intelligent Robot Vision Sensors in VLSI,,1999,journal article,Autonomous Robots,09295593; 15737527,Springer Science and Business Media LLC,Netherlands,Ralph Etienne-Cummings,,7,3,225,237,Very-large-scale integration; Artificial intelligence; Microcontroller; Edge detection; Mobile robot; Image stabilization; Target acquisition; Motion detection; Computer vision; Computer science; Workstation; Real-time computing,,,,,https://link.springer.com/article/10.1023/A:1008968319725 https://dblp.uni-trier.de/db/journals/arobots/arobots7.html#Etienne-Cummings99 https://dl.acm.org/doi/10.1023/A%3A1008968319725,http://dx.doi.org/10.1023/a:1008968319725,,10.1023/a:1008968319725,1604597194,,0,002-944-769-161-016; 016-942-406-190-335; 019-847-739-077-609; 023-352-339-874-06X; 027-743-933-568-43X; 028-893-767-652-651; 029-642-470-198-830; 043-085-105-732-116; 054-097-779-941-069; 056-869-292-902-527; 058-071-175-900-493; 074-256-161-268-31X; 081-884-386-113-897; 119-321-490-917-375; 150-618-424-415-925; 158-528-949-919-343; 167-643-876-733-117; 169-572-435-926-906; 196-053-762-741-17X,10,false,,
063-522-169-334-940,Automatic parking of vehicles: A review of literatures,2014-10-02,2014,journal article,International Journal of Automotive Technology,12299138; 19763832,Springer Science and Business Media LLC,South Korea,Wenbo Wang; Y Song; Juan Zhang; H Deng,,15,6,967,978,Engineering; Industrial engineering; Parking guidance and information; Point (typography); Transport engineering; Reversing; Radar; Fuzzy logic; Motion planning; Digital signal processing; Image processing,,,,,https://rd.springer.com/article/10.1007/s12239-014-0102-y https://trid.trb.org/view/1329418 https://link.springer.com/article/10.1007/s12239-014-0102-y,http://dx.doi.org/10.1007/s12239-014-0102-y,,10.1007/s12239-014-0102-y,2045731909,,0,000-159-284-270-453; 000-906-111-411-074; 001-238-107-054-487; 003-282-435-841-51X; 006-517-653-311-528; 007-899-485-053-672; 011-237-929-104-277; 011-283-091-717-033; 014-365-020-735-638; 015-112-447-567-371; 015-171-806-119-563; 015-456-682-082-074; 016-387-532-905-012; 016-623-442-260-873; 017-994-979-939-025; 018-398-051-840-525; 019-982-529-504-90X; 024-176-528-768-166; 024-490-697-374-26X; 024-923-378-528-189; 025-540-314-909-327; 026-980-421-817-327; 032-251-838-798-042; 037-423-792-391-123; 042-163-973-079-422; 042-423-989-836-924; 044-462-499-916-264; 049-701-171-605-304; 051-062-741-958-800; 051-821-124-250-22X; 059-169-481-563-474; 059-231-026-314-542; 067-123-355-090-633; 067-390-145-887-755; 067-728-299-967-462; 071-654-979-555-336; 073-722-974-726-137; 080-573-417-979-93X; 080-759-442-370-996; 081-301-807-455-194; 086-687-775-886-382; 091-157-941-547-849; 093-472-711-189-196; 097-520-442-045-264; 099-406-593-221-820; 099-421-002-068-357; 100-400-464-398-415; 101-581-069-973-194; 102-639-134-809-995; 104-500-928-675-331; 111-159-403-699-389; 113-184-800-063-509; 118-242-877-825-457; 128-302-362-993-245; 132-920-769-380-363; 134-379-543-417-194; 136-572-575-813-770; 138-726-478-812-102; 152-242-482-282-69X; 154-680-862-794-69X; 158-821-964-552-28X; 169-284-727-009-310; 170-906-162-626-010; 174-636-180-038-739; 194-781-671-744-001; 196-479-546-748-287,73,false,,
063-583-084-824-325,18th annual conference of the international society for computer aided surgery,2014-05-22,2014,journal article,International Journal of Computer Assisted Radiology and Surgery,18616410; 18616429,Springer Science and Business Media LLC,Germany,,,9,S1,311,355,Computer science; Medicine; General surgery; Medical physics,,,,,,http://dx.doi.org/10.1007/s11548-014-1049-1,,10.1007/s11548-014-1049-1,,,0,,1,false,,
063-848-176-030-726,Intelligent wheeled mobile robots for blind navigation application,2017-04-18,2017,journal article,Engineering Computations,02644401,Emerald,United Kingdom,Ter-Feng Wu; Pu-Sheng Tsai; Nien-Tsu Hu; Jen-Yang Chen,"Purpose; ; ; ; ; Visually impaired people have long been living in the dark. They cannot realize the colorful world with their vision, so they rely on hearing, touch and smell to feel the space they live in. Lacking image information, they face challenges in the external environment and barrier spaces. They face danger that is hundreds of times higher than that faced by normal people. Especially during outdoor activities, they can only explore the surrounding environment aided by their hearing and crutches and then based on a vague impression speculate where they are located. To let the blind confidently take each step, this paper proposes sticking the electronic tag of the radio-frequency identification (RFID) system on the back of guide bricks.; ; ; ; ; Design/methodology/approach; ; ; ; ; Thus, the RFID reader, ultrasonic sensor and voice chip on a wheeled mobile robot link the front end to the crutch. Once the blind person nears a guide brick, the RFID will read the message on the tag through the voice broadcast system, and a voice will inform the visually impaired person of the direction to walk and information of the surrounding environment. In addition, the CMOS image sensor set up in the wheeled mobile robot is used to detect the black marking on the guide brick and to guide the blind to walk forward or turn around between the two markings. Finally, the lithium battery charging control unit was installed on the wheeled mobile robot. The ATtiny25 microcontroller conducts the battery charge and discharge control and monitoring of the current battery capacity.; ; ; ; ; Findings; ; ; ; ; The development of this system will let visually impaired people acquire environmental information, road guidance function and nearby traffic information.; ; ; ; ; Originality/value; ; ; ; ; Through rich spatial environment messages, the blind can have the confidence and courage to go outside.",34,2,214,238,Human–computer interaction; Engineering; Embedded system; Set (psychology); Microcontroller; Mobile robot; Control (management); Front and back ends; Image sensor; Identification (information); Control unit,,,,,https://www.emerald.com/insight/content/doi/10.1108/EC-08-2015-0256/full/html https://www.emeraldinsight.com/doi/abs/10.1108/EC-08-2015-0256,http://dx.doi.org/10.1108/ec-08-2015-0256,,10.1108/ec-08-2015-0256,2606155080,,0,010-306-378-600-804; 015-632-759-289-699; 030-127-369-356-741; 049-935-156-019-544; 053-930-824-035-833; 073-928-284-197-323; 086-239-326-011-813; 093-798-340-196-599; 096-803-870-115-728,5,false,,
063-883-773-689-012,Indoor Scene Recognition: An Attention-Based Approach Using Feature Selection-Based Transfer Learning and Deep Liquid State Machine,2023-09-08,2023,journal article,Algorithms,19994893,MDPI AG,Switzerland,Ranjini Surendran; Ines Chihi; J. Anitha; D. Jude Hemanth,"<jats:p>Scene understanding is one of the most challenging areas of research in the fields of robotics and computer vision. Recognising indoor scenes is one of the research applications in the category of scene understanding that has gained attention in recent years. Recent developments in deep learning and transfer learning approaches have attracted huge attention in addressing this challenging area. In our work, we have proposed a fine-tuned deep transfer learning approach using DenseNet201 for feature extraction and a deep Liquid State Machine model as the classifier in order to develop a model for recognising and understanding indoor scenes. We have included fuzzy colour stacking techniques, colour-based segmentation, and an adaptive World Cup optimisation algorithm to improve the performance of our deep model. Our proposed model would dedicatedly assist the visually impaired and blind to navigate in the indoor environment and completely integrate into their day-to-day activities. Our proposed work was implemented on the NYU depth dataset and attained an accuracy of 96% for classifying the indoor scenes.</jats:p>",16,9,430,430,Artificial intelligence; Computer science; Deep learning; Transfer of learning; Machine learning; Classifier (UML); Robotics; Feature extraction; Segmentation; Feature selection; Fuzzy logic; Computer vision; Pattern recognition (psychology); Robot,,,,,https://www.mdpi.com/1999-4893/16/9/430/pdf?version=1694152805 https://doi.org/10.3390/a16090430,http://dx.doi.org/10.3390/a16090430,,10.3390/a16090430,,,0,000-159-208-707-617; 000-300-569-781-265; 004-269-574-716-057; 008-783-981-093-093; 012-642-196-605-159; 013-264-685-692-354; 016-027-780-619-630; 020-233-013-143-936; 024-073-198-603-796; 026-195-773-001-993; 028-800-975-139-638; 033-239-723-330-575; 035-165-023-635-742; 037-182-418-585-695; 041-056-605-156-906; 041-730-155-578-539; 042-193-432-877-20X; 042-957-162-269-462; 043-583-449-880-595; 048-624-844-128-096; 048-964-061-984-989; 049-828-204-888-098; 051-915-945-716-485; 056-470-723-813-91X; 057-600-100-739-337; 058-640-347-116-468; 059-149-073-001-124; 063-407-360-189-514; 064-191-774-369-984; 065-885-904-575-800; 076-518-832-389-690; 079-310-940-223-758; 090-494-261-137-062; 101-509-403-820-075; 102-393-888-629-600; 115-725-112-822-171; 122-584-894-500-851; 139-552-118-652-140; 140-394-829-785-809; 152-384-415-872-028; 159-431-590-070-878; 192-208-388-646-735; 199-989-212-795-63X,1,true,cc-by,gold
063-920-538-482-230,Terrestrial health applications of visual assessment technology and machine learning in spaceflight associated neuro-ocular syndrome.,2022-08-25,2022,journal article,NPJ microgravity,23738065,Springer Science and Business Media LLC,United States,Joshua Ong; Alireza Tavakkoli; Nasif Zaman; Sharif Amit Kamran; Ethan Waisberg; Nikhil Gautam; Andrew G Lee,"The neuro-ocular effects of long-duration spaceflight have been termed Spaceflight Associated Neuro-Ocular Syndrome (SANS) and are a potential challenge for future, human space exploration. The underlying pathogenesis of SANS remains ill-defined, but several emerging translational applications of terrestrial head-mounted, visual assessment technology and machine learning frameworks are being studied for potential use in SANS. To develop such technology requires close consideration of the spaceflight environment which is limited in medical resources and imaging modalities. This austere environment necessitates the utilization of low mass, low footprint technology to build a visual assessment system that is comprehensive, accessible, and efficient. In this paper, we discuss the unique considerations for developing this technology for SANS and translational applications on Earth. Several key limitations observed in the austere spaceflight environment share similarities to barriers to care for underserved areas on Earth. We discuss common terrestrial ophthalmic diseases and how machine learning and visual assessment technology for SANS can help increase screening for early intervention. The foundational developments with this novel system may help protect the visual health of both astronauts and individuals on Earth.",8,1,37,,Spaceflight; Modalities; Human spaceflight; Computer science; Artificial intelligence; Medicine; Space exploration; Engineering; Aerospace engineering; Social science; Sociology,,,,National Aeronautics and Space Administration (NASA) (80NSSC20K1831),https://www.nature.com/articles/s41526-022-00222-7.pdf https://doi.org/10.1038/s41526-022-00222-7,http://dx.doi.org/10.1038/s41526-022-00222-7,36008494,10.1038/s41526-022-00222-7,,PMC9411571,0,000-924-648-927-520; 001-154-583-062-578; 003-728-332-900-63X; 005-003-264-004-220; 006-361-697-702-119; 006-366-337-188-489; 007-074-026-760-953; 008-041-855-664-307; 008-392-891-317-999; 009-777-591-096-753; 010-017-577-709-131; 010-516-604-568-87X; 010-633-110-120-322; 011-653-914-636-214; 012-416-322-612-611; 015-587-219-113-182; 017-247-165-092-110; 018-720-411-406-70X; 019-297-097-948-375; 019-535-177-423-529; 019-794-231-219-42X; 019-924-426-080-352; 020-233-013-143-936; 020-832-201-117-174; 021-554-984-846-03X; 023-289-307-205-356; 023-404-297-711-271; 026-940-765-282-26X; 027-632-020-787-492; 030-980-902-467-529; 031-091-875-583-669; 031-473-854-789-995; 034-712-744-754-331; 035-167-131-703-860; 035-887-954-738-760; 036-013-296-987-800; 036-316-118-231-38X; 038-424-502-306-586; 039-070-053-630-735; 040-011-425-844-511; 040-882-089-134-257; 043-137-264-735-834; 045-454-469-294-712; 046-784-970-117-881; 051-080-748-567-529; 051-262-965-281-479; 052-110-593-621-834; 052-168-751-772-497; 054-885-999-279-107; 056-246-165-770-719; 059-754-390-458-924; 062-685-872-234-036; 064-982-636-925-197; 065-258-776-374-242; 070-903-529-333-793; 072-268-333-836-957; 073-127-824-008-678; 073-761-132-636-946; 075-287-349-971-558; 076-419-056-748-326; 082-152-225-527-864; 083-871-787-233-064; 085-596-482-863-371; 088-209-578-388-33X; 091-315-490-760-595; 093-214-952-569-454; 094-099-158-096-35X; 095-539-510-178-868; 102-182-464-463-90X; 112-878-168-952-539; 118-627-610-290-681; 122-095-502-739-824; 135-487-218-119-489; 140-398-739-072-109; 147-874-047-069-098; 152-760-990-058-563; 157-017-410-052-992,50,true,cc-by,gold
064-462-487-779-261,Autonomous surgery in the era of robotic urology: friend or foe of the future surgeon?,2020-09-23,2020,journal article,Nature reviews. Urology,17594820; 17594812,Springer Science and Business Media LLC,United Kingdom,Martin J. Connor; Prokar Dasgupta; Hashim U. Ahmed; Asif Raza,,17,11,643,649,Urology; Surgery; Urogenital diseases; Robotic systems; Urological surgery; Clinical evidence; Medicine,,Brachytherapy; Humans; Male; Prostatic Hyperplasia/surgery; Prostatic Neoplasms/radiotherapy; Reference Standards; Robotic Surgical Procedures/methods; Robotics/methods; Urologic Surgical Procedures/methods; Urology/methods,,,https://pubmed.ncbi.nlm.nih.gov/32968240/ http://www-nature-com-443.webvpn.bjmu.tsg211.com/articles/s41585-020-0375-z https://www.nature.com/articles/s41585-020-0375-z https://kclpure.kcl.ac.uk/portal/en/publications/autonomous-surgery-in-the-era-of-robotic-urology(0b9c2a65-ad59-451f-839e-76dff2b9ce77)/export.html https://www.nature.com/articles/s41585-020-0375-z.pdf https://www.ncbi.nlm.nih.gov/pubmed/32968240,http://dx.doi.org/10.1038/s41585-020-0375-z,32968240,10.1038/s41585-020-0375-z,3088723426,,0,001-332-350-361-488; 001-984-134-313-434; 003-042-066-739-954; 003-797-314-424-835; 004-039-827-306-950; 006-059-471-208-719; 006-496-025-917-190; 010-881-171-884-716; 011-775-124-325-038; 014-823-641-298-659; 018-682-668-518-913; 019-539-122-049-793; 021-321-945-448-154; 023-428-892-513-137; 023-449-779-446-166; 024-101-887-775-193; 030-313-615-485-645; 030-772-830-206-509; 031-786-731-016-439; 034-218-766-597-492; 034-906-589-980-069; 040-420-207-438-889; 042-852-706-013-860; 046-753-052-569-086; 049-732-432-033-44X; 052-786-351-717-614; 054-207-548-913-735; 054-667-985-548-323; 056-733-873-417-72X; 058-338-841-664-104; 064-981-272-404-061; 071-631-697-968-448; 089-147-248-837-492; 090-434-333-166-221; 100-863-074-061-797; 114-187-503-981-434; 128-804-100-149-941; 130-322-587-025-595; 145-342-546-051-749,17,false,,
064-490-543-411-153,A Review on Teleoperation of Mobile Ground Robots: Architecture and Situation Awareness,2020-10-21,2020,journal article,"International Journal of Control, Automation and Systems",15986446; 20054092,Springer Science and Business Media LLC,South Korea,Samwel Opiyo; Jun Zhou; Emmy Mwangi; Wang Kai; Idris Sunusi,,19,3,1384,1407,Human–computer interaction; Human–robot interaction; Situation awareness; Artificial intelligence; Teleoperation; Mobile robot; Search and rescue; Mechatronics; Robotics; Computer science; Robot,,,,,https://link.springer.com/article/10.1007/s12555-019-0999-z https://link.springer.com/10.1007/s12555-019-0999-z https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002684961,http://dx.doi.org/10.1007/s12555-019-0999-z,,10.1007/s12555-019-0999-z,3094167359,,0,000-257-779-983-163; 000-463-839-680-517; 000-555-092-088-66X; 001-154-350-168-291; 003-326-760-646-154; 003-870-449-146-13X; 005-210-952-880-680; 005-272-709-179-214; 006-068-066-410-467; 008-098-500-163-80X; 008-130-682-952-486; 008-195-451-864-371; 008-967-288-713-772; 009-224-277-032-604; 011-156-932-190-669; 011-750-796-808-517; 012-212-780-868-88X; 012-707-477-400-63X; 014-222-169-355-344; 016-302-448-343-859; 016-369-780-224-225; 016-478-578-722-549; 016-615-137-794-422; 016-739-649-471-71X; 017-240-157-302-358; 018-096-549-922-428; 018-575-596-954-260; 019-538-129-430-955; 019-582-767-571-398; 019-847-083-794-735; 021-032-243-446-137; 021-620-896-944-88X; 022-809-716-106-295; 023-400-214-147-612; 023-491-367-915-527; 023-684-515-065-538; 026-192-589-736-574; 026-607-720-013-622; 026-801-408-916-033; 027-063-629-000-701; 027-631-001-273-942; 027-864-807-314-130; 027-883-531-913-223; 028-376-084-628-171; 029-844-051-924-24X; 029-959-001-347-536; 030-016-724-175-987; 030-226-192-774-449; 031-165-985-704-07X; 031-576-624-785-237; 032-133-450-262-391; 032-399-544-692-796; 036-158-534-555-331; 036-920-160-805-819; 040-079-838-137-859; 040-871-598-669-301; 041-265-411-596-314; 042-904-048-791-864; 042-977-633-620-123; 043-136-280-992-30X; 044-005-429-492-204; 044-705-859-681-58X; 046-039-876-846-615; 046-145-946-407-765; 046-328-510-998-666; 047-487-495-568-039; 047-673-932-543-278; 049-557-793-709-710; 049-703-332-125-010; 049-958-322-737-528; 051-415-567-951-443; 052-317-575-761-604; 052-799-792-848-882; 053-119-443-805-950; 053-353-320-145-855; 054-944-835-446-092; 057-541-566-070-885; 058-762-600-456-200; 059-090-790-315-683; 060-307-539-873-44X; 060-565-923-740-053; 060-646-200-996-762; 061-592-090-109-24X; 062-525-044-276-780; 063-069-723-215-20X; 065-189-704-356-022; 065-258-407-790-208; 067-398-692-799-881; 067-638-922-109-648; 070-075-898-281-676; 071-668-702-912-73X; 072-509-500-832-537; 072-644-776-101-614; 074-016-294-328-475; 074-048-146-272-162; 074-519-856-966-431; 074-601-015-090-044; 075-245-514-750-113; 075-882-294-406-725; 076-028-292-027-880; 076-252-753-840-839; 076-774-748-979-236; 077-109-003-960-502; 078-278-427-385-437; 078-396-495-129-416; 078-731-270-424-217; 085-385-953-121-631; 085-838-807-288-344; 087-353-549-379-821; 089-536-548-034-105; 090-012-572-144-206; 092-374-513-574-231; 093-657-486-981-478; 094-067-587-361-725; 094-533-954-364-30X; 095-793-916-494-100; 096-416-229-044-59X; 096-937-216-536-177; 099-445-171-142-091; 100-487-250-448-119; 100-891-688-805-138; 103-666-498-933-882; 104-457-485-476-921; 104-855-861-105-432; 105-277-732-719-926; 109-247-691-852-496; 109-295-461-733-973; 109-761-887-339-346; 111-189-562-532-671; 114-323-956-789-174; 114-935-132-713-430; 119-728-463-681-410; 120-601-200-244-777; 122-318-029-627-741; 124-051-929-501-266; 126-989-354-890-693; 127-467-839-599-173; 131-016-886-187-878; 132-954-767-785-151; 135-921-617-108-591; 136-039-344-027-857; 137-034-284-496-320; 140-216-117-464-909; 149-167-477-960-323; 150-148-650-092-841; 151-098-208-177-379; 152-349-164-823-032; 152-625-125-371-414; 154-285-467-265-317; 155-382-334-031-710; 156-265-794-873-939; 157-065-570-698-158; 158-100-471-520-124; 159-854-551-068-219; 162-284-944-420-830; 163-628-805-243-46X; 164-106-664-716-208; 166-444-372-975-544; 170-865-065-405-963; 174-990-909-535-134; 175-061-861-547-880; 175-360-163-334-327; 176-178-122-775-578; 176-319-087-261-788; 178-892-002-887-744; 182-930-102-607-286; 191-531-333-747-524; 193-050-217-395-926; 194-838-277-872-667; 199-351-169-288-903,52,false,,
064-715-423-322-767,Guided Blind Guidance APP Based on Path Planning and Obstacle Detection,2023-11-24,2023,conference proceedings article,"Proceedings of the 2023 5th International Conference on Internet of Things, Automation and Artificial Intelligence",,ACM,,Xiaoya Fang; Weisen He; Han Yu; Shuolei Wu; Ruxing Zhang; Jiajia Wu,"This article proposes a new guiding software based on advanced route planning and object detection technology that aims to increase the mobility and convenience of visually impaired users dramatically. This APP use deep learning algorithms to efficiently recognize and locate surrounding impediments, allowing users to walk more confidently and safely. In addition to its strong obstacle recognition function, the guidance APP includes a huge data analysis remote navigation function. This program can not only connect to existing navigation systems, but also design the safest and most efficient trip paths for users by analyzing a significant quantity of multidimensional travel and map data. Furthermore, users can save frequently visited locations for future travel, improving navigation stability and accuracy. This innovation not only helps visually challenged individuals more independent, but it also boosts their self-esteem. This navigation APP is a great marriage of technology and human care, delivering a more intelligent, convenient, and dependable navigation solution for visually impaired people, allowing them to better integrate into society and live more independent and independent lives.",,,,,Obstacle; Computer science; Motion planning; Path (computing); Artificial intelligence; Computer vision; Computer network; Robot; Geography; Archaeology,,,,,,http://dx.doi.org/10.1145/3653081.3653218,,10.1145/3653081.3653218,,,0,011-002-762-023-793; 045-373-121-943-369; 046-247-152-234-951; 069-787-265-449-109; 086-888-002-713-472; 087-272-456-866-28X; 093-557-122-200-652; 136-297-784-099-186; 161-337-715-569-623,0,false,,
064-964-065-389-714,Double-Diamond Model-Based Orientation Guidance in Wearable Human-Machine Navigation Systems for Blind and Visually Impaired People.,2019-10-28,2019,journal article,"Sensors (Basel, Switzerland)",14248220; 14243210,Multidisciplinary Digital Publishing Institute (MDPI),Switzerland,Xiaochen Zhang; Hui Zhang; Linyue Zhang; Yi Zhu; Fei Hu,"This paper presents the analysis and design of a new, wearable orientation guidance device in modern travel aid systems for blind and visually impaired people. The four-stage double-diamond design model was applied in the design process to achieve human-centric innovation and to ensure technical feasibility and economic viability. Consequently, a sliding tactile feedback wristband was designed and prototyped. Furthermore, a Bezier curve-based adaptive path planner is proposed to guarantee collision-free planned motion. Proof-of-concept experiments on both virtual and real-world scenarios are conducted. The evaluation results confirmed the efficiency and feasibility of the design and imply the design's remarkable potential in spatial perception rehabilitation.",19,21,4670,,Motion (physics); Human–computer interaction; Human–machine system; Wearable computer; Diamond model; Computer science; Orientation (computer vision),blind and visually impaired people; double diamond; navigation aids; tactile feedback; user-centric design,"Blindness/rehabilitation; Humans; Man-Machine Systems; Models, Theoretical; Sensory Aids; Visually Impaired Persons/rehabilitation; Wearable Electronic Devices",,"Ministry of Education of the People's Republic of China (18YJCZH249); Ministry of Education of the People's Republic of China (17YJCZH275); Guangzhou Science, Technology and Innovation Commission (201904010241)",https://pesquisa.bvsalud.org/portal/resource/pt/mdl-31661798 https://europepmc.org/article/MED/31661798 https://www.mdpi.com/1424-8220/19/21/4670/pdf http://doi.org/10.3390/s19214670 http://dblp.uni-trier.de/db/journals/sensors/sensors19.html#ZhangZZZH19 https://www.mdpi.com/1424-8220/19/21/4670 https://www.ncbi.nlm.nih.gov/pubmed/31661798 https://doaj.org/article/80827a846d57484a9b7d793d6ab788d8 https://pubmed.ncbi.nlm.nih.gov/31661798/ https://dblp.uni-trier.de/db/journals/sensors/sensors19.html#ZhangZZZH19 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6864851,http://dx.doi.org/10.3390/s19214670,31661798,10.3390/s19214670,2981582759,PMC6864851,0,001-632-846-182-445; 002-841-567-358-302; 002-862-810-731-561; 003-485-284-705-210; 005-497-078-814-142; 007-223-103-783-187; 007-305-095-446-752; 007-530-851-453-706; 007-662-127-098-066; 008-169-450-643-21X; 009-160-989-834-373; 009-896-970-982-095; 010-227-096-101-948; 015-016-116-643-819; 016-233-703-646-667; 016-478-974-811-762; 016-988-681-008-984; 017-310-654-680-14X; 018-843-508-574-144; 019-625-752-326-979; 021-176-270-241-156; 021-657-554-837-75X; 021-716-877-928-450; 022-557-628-555-690; 023-387-820-196-963; 029-170-236-791-182; 030-884-979-386-25X; 031-175-485-676-104; 032-284-166-524-962; 032-661-597-680-28X; 033-442-984-511-466; 036-907-907-418-504; 039-300-695-617-867; 049-485-542-422-096; 053-567-501-093-090; 057-847-555-860-541; 060-041-403-626-524; 060-491-668-234-911; 063-283-720-654-386; 065-078-747-021-140; 067-481-889-463-852; 069-317-441-795-929; 070-068-111-472-212; 070-875-178-006-938; 075-519-293-886-898; 083-299-642-195-322; 091-327-917-116-626; 091-764-827-661-79X; 105-278-566-106-933; 114-740-836-402-286; 118-628-165-016-127; 132-318-683-854-36X; 147-913-128-146-234; 152-183-517-423-628; 155-330-779-796-500; 165-646-371-179-035; 177-522-114-459-74X; 186-810-372-950-411; 191-329-883-171-76X,18,true,cc-by,gold
065-028-506-334-941,Automating Content Extraction of HTML Documents,,2005,journal article,World Wide Web,1386145x; 15731413,Springer Science and Business Media LLC,Netherlands,Suhit Gupta; Gail E. Kaiser; Peter Grimm; Michael F. Chiang; Justin Starren,"Web pages often contain clutter (such as unnecessary images and extraneous links) around the body of an article that distracts a user from actual content. Extraction of ""useful and relevant"" content from web pages has many applications, including cell phone and PDA browsing, speech rendering for the visually impaired, and text summarization. Most approaches to making content more readable involve changing font size or removing HTML and data components such as images, which takes away from a webpage's inherent look and feel. Unlike ""Content Reformatting,"" which aims to reproduce the entire webpage in a more convenient form, our solution directly addresses ""Content Extraction."" We have developed a framework that employs an easily extensible set of techniques. It incorporates advantages of previous work on content extraction. Our key insight is to work with DOM trees, a W3C specified interface that allows programs to dynamically access document structure, rather than with raw HTML markup. We have implemented our approach in a publicly available Web proxy to extract content from HTML web pages. This proxy can be used both centrally, administered for groups of users, as well as by individuals for personal browsers. We have also, after receiving feedback from users about the proxy, created a revised version with improved performance and accessibility in mind.",8,2,179,224,World Wide Web; HTML; Automatic summarization; Web page; Information retrieval; Look and feel; Computer science; Markup language; Document Structure Description,,,,,https://academiccommons.columbia.edu/doi/10.7916/D8K64RX3 https://link.springer.com/content/pdf/10.1007/s11280-004-4873-3.pdf http://www.psl.cs.columbia.edu/wp-content/uploads/2011/03/3463-WWWJ.pdf https://core.ac.uk/display/24616801 https://academiccommons.columbia.edu/doi/10.7916/D81Z4C6R/download https://ohsu.pure.elsevier.com/en/publications/automating-content-extraction-of-html-documents-2 https://dl.acm.org/doi/10.1007/s11280-004-4873-3 https://www.scholars.northwestern.edu/en/publications/automating-content-extraction-of-html-documents http://doi.org/10.1007/s11280-004-4873-3 https://link.springer.com/article/10.1007/s11280-004-4873-3 https://dblp.uni-trier.de/db/journals/www/www8.html#GuptaKGCS05 https://academiccommons.columbia.edu/catalog/ac:109730 https://doi.org/10.1007/s11280-004-4873-3,http://dx.doi.org/10.1007/s11280-004-4873-3,,10.1007/s11280-004-4873-3,2087406717,,6,002-201-032-015-180; 010-136-645-510-231; 010-307-621-994-212; 013-472-494-980-376; 020-964-304-526-100; 021-785-782-634-029; 024-992-876-101-937; 026-924-960-431-363; 032-512-830-457-078; 040-220-214-951-238; 041-958-524-894-184; 046-252-325-361-999; 048-480-380-555-89X; 055-315-784-548-439; 060-305-618-872-681; 070-783-984-336-888; 070-875-178-006-938; 071-596-723-849-186; 074-414-253-763-220; 086-380-454-718-736; 092-645-402-737-79X; 104-235-673-324-008; 109-027-998-557-702; 117-477-098-791-278; 118-039-961-825-798; 132-467-351-908-914; 137-370-039-406-654; 167-377-823-610-990; 173-029-152-796-048; 184-724-922-353-395,89,true,,green
065-046-356-081-836,How path integration abilities of blind people change in different exploration conditions.,2024-05-17,2024,journal article,Frontiers in neuroscience,16624548; 1662453x,Frontiers Media SA,Switzerland,Shehzaib Shafique; Walter Setti; Claudio Campus; Silvia Zanchi; Alessio Del Bue; Monica Gori,"For animals to locate resources and stay safe, navigation is an essential cognitive skill. Blind people use different navigational strategies to encode the environment. Path integration significantly influences spatial navigation, which is the ongoing update of position and orientation during self-motion. This study examines two separate things: (i) how guided and non-guided strategies affect blind individuals in encoding and mentally representing a trajectory and (ii) the sensory preferences for potential navigational aids through questionnaire-based research. This study first highlights the significant role that the absence of vision plays in understanding body centered and proprioceptive cues. Furthermore, it also underscores the urgent need to develop navigation-assistive technologies customized to meet the specific needs of users.",18,,1375225,,Path integration; Affect (linguistics); Human–computer interaction; Orientation (vector space); Computer science; Encoding (memory); Cognition; ENCODE; Motion (physics); Sensory system; Path (computing); Trajectory; Sensory cue; Cognitive psychology; Psychology; Computer vision; Communication; Neuroscience; Biochemistry; Chemistry; Physics; Geometry; Mathematics; Astronomy; Gene; Programming language,blind navigation; environment encoding; feedback preference; guided condition; path integration; shape completion; spatial navigation; triangle completion task,,,,https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2024.1375225/pdf https://doi.org/10.3389/fnins.2024.1375225,http://dx.doi.org/10.3389/fnins.2024.1375225,38826777,10.3389/fnins.2024.1375225,,PMC11140012,0,000-298-942-413-437; 002-083-475-485-89X; 002-461-480-084-324; 002-748-287-818-793; 004-080-846-002-033; 005-433-812-226-570; 005-829-367-618-103; 006-451-895-748-694; 007-714-784-220-529; 012-575-758-438-018; 014-236-257-169-581; 017-674-358-742-868; 022-945-380-076-286; 024-404-021-037-251; 025-152-363-196-592; 025-501-666-878-788; 025-771-457-853-883; 026-828-179-571-122; 030-746-375-376-362; 031-975-734-302-804; 035-645-391-953-19X; 037-545-584-168-942; 038-350-525-349-15X; 049-409-375-999-386; 052-731-143-530-822; 053-240-315-716-245; 053-811-801-430-011; 055-749-303-547-020; 059-373-172-591-584; 061-089-901-099-148; 063-388-850-988-043; 067-894-235-988-508; 085-486-703-275-980; 113-071-971-798-760; 113-091-480-741-549; 117-580-364-280-370; 126-453-746-656-171; 133-794-283-831-736; 141-649-000-612-322; 148-150-208-507-670; 180-113-220-263-442; 192-919-913-278-338; 199-013-221-727-415,0,true,cc-by,gold
065-078-747-021-140,A review of assistive spatial orientation and navigation technologies for the visually impaired,2017-08-28,2017,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Hugo Fernandes; Paulo Costa; Vitor Filipe; Hugo Paredes; João Barroso,"The overall objective of this work is to review the assistive technologies that have been proposed by researchers in recent years to address the limitations in user mobility posed by visual impairment. This work presents an ""umbrella review."" Visually impaired people often want more than just information about their location and often need to relate their current location to the features existing in the surrounding environment. Extensive research has been dedicated into building assistive systems. Assistive systems for human navigation, in general, aim to allow their users to safely and efficiently navigate in unfamiliar environments by dynamically planning the path based on the user's location, respecting the constraints posed by their special needs. Modern mobile assistive technologies are becoming more discrete and include a wide range of mobile computerized devices, including ubiquitous technologies such as mobile phones. Technology can be used to determine the user's location, his relation to the surroundings (context), generate navigation instructions and deliver all this information to the blind user.",18,1,155,168,Human–computer interaction; Relation (database); Special needs; Context (language use); PATH (variable); Visual impairment; Assistive technology; Visually impaired; Computer science; Multimedia; Orientation (computer vision),,,,Fundação para a Ciência e a Tecnologia,https://link.springer.com/article/10.1007/s10209-017-0570-8 https://dl.acm.org/citation.cfm?id=3332656 https://dl.acm.org/doi/10.1007/s10209-017-0570-8 https://dblp.uni-trier.de/db/journals/uais/uais18.html#FernandesCFPB19 https://rd.springer.com/article/10.1007/s10209-017-0570-8 https://doi.org/10.1007/s10209-017-0570-8,http://dx.doi.org/10.1007/s10209-017-0570-8,,10.1007/s10209-017-0570-8,2749554940,,0,000-783-876-986-948; 001-076-828-128-846; 001-996-430-142-340; 002-352-012-178-407; 002-397-792-416-419; 004-330-362-156-385; 005-991-142-819-380; 006-654-180-700-136; 007-744-926-415-40X; 008-166-975-180-648; 009-063-048-778-483; 009-817-479-222-434; 013-056-435-015-274; 015-060-540-025-348; 016-554-897-381-94X; 019-550-995-456-408; 022-489-327-390-755; 027-893-571-523-130; 029-243-296-948-051; 030-341-933-364-791; 031-155-299-027-09X; 036-407-649-013-756; 036-990-754-479-620; 038-010-954-704-091; 038-713-853-097-000; 039-002-090-199-249; 039-079-110-624-640; 039-550-419-384-018; 040-660-173-610-646; 041-401-584-883-008; 042-556-116-976-361; 042-798-007-506-557; 043-342-420-332-512; 043-536-853-133-602; 045-459-312-704-237; 045-525-438-291-231; 046-262-856-533-736; 046-369-152-086-632; 046-983-906-128-801; 047-310-867-082-408; 048-446-984-045-429; 049-330-201-064-482; 049-592-217-207-783; 051-766-223-654-722; 051-970-317-843-151; 053-951-251-264-665; 054-946-271-406-472; 056-167-810-394-870; 057-042-196-938-512; 058-155-799-281-81X; 059-757-274-327-752; 062-315-074-488-914; 063-593-313-740-053; 063-954-288-428-519; 065-032-907-680-425; 068-708-501-300-740; 069-116-650-623-84X; 070-665-310-804-510; 071-582-252-819-556; 071-806-151-845-245; 073-266-716-456-692; 073-562-690-045-065; 076-108-165-443-224; 076-113-485-862-133; 079-919-054-001-210; 083-146-360-435-079; 083-776-247-916-926; 084-111-378-111-079; 088-470-436-866-921; 090-884-510-770-832; 092-207-666-926-877; 092-391-305-190-481; 094-039-812-213-553; 094-159-343-067-325; 100-632-361-070-479; 102-206-740-107-287; 103-490-387-033-105; 104-013-878-921-179; 104-694-342-259-021; 105-584-440-275-77X; 107-526-230-054-474; 110-392-465-517-397; 112-566-440-552-073; 112-849-097-976-249; 119-516-706-529-460; 125-761-483-523-729; 129-279-870-405-749; 131-228-773-487-14X; 131-257-871-619-401; 132-318-683-854-36X; 134-448-474-253-195; 136-854-439-827-372; 137-719-919-028-988; 141-489-442-115-69X; 144-045-024-507-975; 148-061-387-688-459; 151-015-279-636-660; 152-346-171-074-996; 159-957-101-471-061; 161-901-608-230-16X; 169-004-645-115-567; 174-227-261-327-754; 176-518-155-704-547; 183-996-310-924-532; 185-855-888-820-573; 188-576-600-176-975; 196-198-371-207-25X,62,true,,green
065-324-467-859-927,AAATE Conf. - Real-time pedestrian crossing recognition for assistive outdoor navigation,,2015,journal article,Studies in health technology and informatics,18798365; 09269630,IOS Press,Netherlands,S. Fontanesi; Alessandro Frigerio; Luca Fanucci; William Li,"Navigation in urban environments can be difficult for people who are blind or visually impaired. In this project, we present a system and algorithms for recognizing pedestrian crossings in outdoor environments. Our goal is to provide navigation cues for crossing the street and reaching an island or sidewalk safely. Using a state-of-the-art Multisense S7S sensor, we collected 3D pointcloud data for real-time detection of pedestrian crossing and generation of directional guidance. We demonstrate improvements to a baseline, monocular-camera-based system by integrating 3D spatial prior information extracted from the pointcloud. Our system's parameters can be set to the actual dimensions of real-world settings, which enables robustness of occlusion and perspective transformation. The system works especially well in non-occlusion situations, and is reasonably accurate under different kind of conditions. As well, our large dataset of pedestrian crossings, organized by different types and situations of pedestrian crossings in order to reflect real-word environments, is publicly available in a commonly used format (ROS bagfiles) for further research.",217,,963,968,Artificial intelligence; Pedestrian; Pedestrian crossing; Computer vision; Robotics; Computer science; Simulation; Wearable technology; Robustness (computer science),,"Algorithms; Artificial Intelligence; Blindness/rehabilitation; Cloud Computing; Computer Systems; Cues; Datasets as Topic; Humans; Imaging, Three-Dimensional; Pedestrians; Robotics; Software Design; Urban Population; Vision Disorders/rehabilitation; Wearable Electronic Devices",,,https://www.safetylit.org/citations/index.php?fuseaction=citations.viewdetails&citationIds[]=citjournalarticle_493481_6 http://europepmc.org/abstract/MED/26294593 https://doi.org/10.3233/978-1-61499-566-1-963 https://arpi.unipi.it/handle/11568/832770 https://dblp.uni-trier.de/db/conf/aaate/aaate2015.html#FontanesiFFL15,https://www.ncbi.nlm.nih.gov/pubmed/26294593,26294593,,2281483847,,0,,2,false,,
065-688-097-333-422,Intelligent non-visual navigation of complex HTML structures,,2002,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Enrico Pontelli; Douglas J. Gillan; Gopal Gupta; Arthur I. Karshmer; Emad Saad; W. Xiong,,2,1,56,69,Semantics; Spatial analysis; Semantic Web Stack; Traverse; Web page; Information retrieval; Representation (systemics); Visual navigation; Focus (computing); Computer science,,,,,https://dblp.uni-trier.de/db/journals/uais/uais2.html#PontelliGGKSX02 https://dx.doi.org/10.1007/s10209-002-0036-4 https://link.springer.com/article/10.1007/s10209-002-0036-4 http://dx.doi.org/10.1007/s10209-002-0036-4,http://dx.doi.org/10.1007/s10209-002-0036-4,,10.1007/s10209-002-0036-4,1535239931,,0,013-350-108-535-78X; 036-149-176-762-271; 049-202-015-624-216; 057-934-038-962-57X; 098-572-364-265-333; 118-373-324-615-222; 161-428-668-072-043; 172-747-034-324-138,17,false,,
065-798-012-417-22X,M-CASEngine: A collaborative environment for computer-assisted surgery,2006-05-19,2006,journal article,International Journal of Computer Assisted Radiology and Surgery,18616410; 18616429,Springer Science and Business Media LLC,Germany,Hanping Lufei; Weisong Shi; Vipin Chaudhary,,1,S1,447,449,Dynamic data; Computer-assisted surgery; Data preparation; Gateway (computer program); Overall performance; Computer science; Multimedia; Access control; Plan (drawing); Segmentation; Operation room; Patient information; Computer network; Interface (computing); Automaton; Laparoscopic surgery; Motion recognition; Simulation; Medicine; Machine learning; Artificial intelligence; Hidden Markov model; Surgical training; Surgical simulator; Joint (building); Computer graphics (images); Medical physics; Engineering; Architectural engineering,,,,,https://researchconnect.wayne.edu/en/publications/m-casengine-a-collaborative-environment-for-computer-assisted-sur,http://dx.doi.org/10.1007/s11548-006-0032-x,,10.1007/s11548-006-0032-x,2619174060; 3022079194; 2589286636; 2546706569,,0,029-807-713-326-917; 035-596-177-483-109,19,false,,
065-831-055-423-605,Development of a New Robotic System for Assisting Visually Impaired People,2011-09-14,2011,journal article,International Journal of Social Robotics,18754791; 18754805,Springer Science and Business Media LLC,Germany,Genci Capi; Hideki Toda,,4,1,33,38,Artificial intelligence; Natural language; Stairs; Mechatronics; Robotic systems; Laser data; Visually impaired; Computer vision; Robotics; Computer science; Cluster analysis,,,,,https://link.springer.com/article/10.1007/s12369-011-0103-1 https://link.springer.com/content/pdf/10.1007%2Fs12369-011-0103-1.pdf,http://dx.doi.org/10.1007/s12369-011-0103-1,,10.1007/s12369-011-0103-1,1984557712,,0,034-443-036-134-243; 051-766-223-654-722; 056-785-601-003-691; 061-190-872-413-287; 069-419-248-108-995; 081-295-126-738-529; 102-551-479-494-136; 127-006-931-445-95X; 137-936-116-445-551; 155-161-478-415-546; 169-421-672-075-385,12,false,,
065-992-777-809-97X,ICRA - The GuideCane-a computerized travel aid for the active guidance of blind pedestrians,,,conference proceedings article,Proceedings of International Conference on Robotics and Automation,,IEEE,,Johann Borenstein; Iwan Ulrich,"This paper introduces the GuideCane, a novel device designed to help blind or visually impaired travellers to navigate safely and quickly among obstacles and other hazards faced by blind pedestrians. The GuideCane, currently under development at the University of Michigan's Mobile Robotics Lab, comprises of a long handle and a ""sensor head"" unit that is attached at the distal end of the handle. The sensor head is mounted on a steerable but unpowered two-wheeled steering axle. During operation, the user pushes the lightweight GuideCane ahead of him/her. Ultrasonic sensors mounted on the sensor head detect obstacles and steer the device around it. The user feels the steering command as a very noticeable physical force through the handle and is able to follow the GuideCane's path easily and without any conscious effort.",2,,1283,1288,Engineering; Axle; Artificial intelligence; Head (watercraft); Object detection; PATH (variable); Optical receivers; Visually impaired; Computer vision; Robotics; Sonar; Real-time computing,,,,,https://ieeexplore.ieee.org/document/614314 http://ieeexplore.ieee.org/document/614314 https://dblp.uni-trier.de/db/conf/icra/icra1997.html#BorensteinU97 https://doi.org/10.1109/ROBOT.1997.614314 http://www-personal.umich.edu/~johannb/Papers/paper65.pdf,http://dx.doi.org/10.1109/robot.1997.614314,,10.1109/robot.1997.614314,2109990343,,25,018-640-372-788-337; 031-612-744-612-651; 046-649-192-050-710; 055-224-290-217-847; 058-443-047-291-990; 067-728-299-967-462; 072-148-091-087-749; 073-722-974-726-137; 078-736-735-476-395; 082-338-324-037-276; 086-597-810-545-403; 087-035-563-901-168; 087-838-253-289-179; 087-887-890-493-528; 090-738-057-382-94X; 094-003-193-394-783; 098-811-589-762-176; 101-709-814-035-182; 116-005-314-582-189; 132-110-390-316-485; 135-646-014-794-306; 141-995-630-420-618; 197-962-968-344-879,246,true,,green
066-037-919-140-345,A Navigation Aid for Blind People.,2011-05-10,2011,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Mounir Bousbia-Salah; Maamar Bettayeb; Allal Larbi,,64,3,387,400,Engineering; Artificial intelligence; Microcontroller; Obstacle; PATH (variable); Speech output; Navigation aid; Computer vision; Simulation; Sonar; Ultrasonic sensor,,,,,https://dblp.uni-trier.de/db/journals/jirs/jirs64.html#Bousbia-SalahBL11 https://dialnet.unirioja.es/servlet/articulo?codigo=3788741 https://doi.org/10.1007/s10846-011-9555-7 https://link.springer.com/article/10.1007/s10846-011-9555-7,http://dx.doi.org/10.1007/s10846-011-9555-7,,10.1007/s10846-011-9555-7,2082477213,,1,001-689-696-637-789; 003-535-200-282-867; 006-917-922-806-186; 013-847-555-426-159; 014-621-188-486-583; 016-554-897-381-94X; 019-550-995-456-408; 022-697-627-694-941; 030-127-369-356-741; 049-152-372-243-400; 056-773-999-189-634; 057-257-708-970-021; 058-443-047-291-990; 064-427-092-098-337; 066-297-751-248-221; 068-581-156-196-413; 078-736-735-476-395; 089-247-928-551-533; 090-758-269-328-935; 096-803-870-115-728; 104-228-581-432-583; 120-413-407-974-701; 133-206-670-623-263; 135-646-014-794-306; 143-977-254-342-225; 163-198-197-767-915; 191-179-283-564-825,89,false,,
066-114-366-503-885,Generation of the head related transfer functions using artificial neural networks,2009-07-22,2009,conference proceedings,,,,,Zoltan Haraszy; Daniel Ianchis; Virgil Tiponut,"The new Acoustic Virtual Reality (AVR) concept is often used as a man-machine interface in electronic travel aid (ETA), that help blind and visually impaired individuals to navigate in real outdoor environments. According to this concept, the presence of obstacles in the surrounding environment and the path to the desired target will be signalized to the blind subject by burst of sounds, whose virtual source position suggests the position of the real obstacles and the direction of movement, respectively. The practical implementation of the AVR concept requires the so-called Head Related Transfer Functions (HRTFs) values to be known in every point of the 3D space and for each subject. These values can be determined by using a quite complex procedure, which requires many measurements for each individual. In the present paper, an artificial neural network (ANN) is proposed in order to generate the values of the HRTFs. The proposed method, valid for only on subject, speeds up the implementation of the AVR concept after the ANN training has been completed. Finally, some experimental results, conclusions and further developments are also presented.",,,114,118,Interface (computing); Engineering; Position (vector); Artificial intelligence; Virtual reality; Head (linguistics); Point (geometry); Computer vision; Artificial neural network; Transfer function; Path (graph theory),,,,,http://www.wseas.us/e-library/conferences/2009/rodos/CIRCUITS/CIRCUITS15.pdf,http://www.wseas.us/e-library/conferences/2009/rodos/CIRCUITS/CIRCUITS15.pdf,,,2470724304,,0,019-306-790-422-912; 019-550-995-456-408; 030-127-369-356-741; 044-192-308-604-739; 050-966-266-525-728; 051-766-223-654-722; 061-190-872-413-287; 063-957-925-086-43X; 067-332-342-991-804; 071-405-244-808-059; 073-928-284-197-323; 077-113-362-690-85X; 084-533-882-547-840; 092-862-654-304-908; 099-646-885-015-028; 100-773-828-432-613; 190-621-332-208-185,9,false,,
066-448-148-032-894,"Data Acquisition, Processing, and Reduction for Home-Use Trial of a Wearable Video Camera-Based Mobility Aid",2020-06-11,2020,journal article,Translational vision science & technology,21642591,Association for Research in Vision and Ophthalmology (ARVO),United States,Shrinivas Pundlik; Vilte Baliutaviciute; Mojtaba Moharrer; Alex R. Bowers; Gang Luo,"Purpose Evaluating mobility aids in naturalistic conditions across many days is challenging owing to the sheer amount of data and hard-to-control environments. For a wearable video camera-based collision warning device, we present the methodology for acquisition, reduction, review, and coding of video data for quantitative analyses of mobility outcomes in blind and visually impaired participants. Methods Scene videos along with collision detection information were obtained from a chest-mounted collision warning device during daily use of the device. The recorded data were analyzed after use. Collision risk events flagged by the device were manually reviewed and coded using a detailed annotation protocol by two independent masked reviewers. Data reduction was achieved by predicting agreements between reviewers based on a machine learning algorithm. Thus, only those events for which disagreements were predicted would be reviewed by the second reviewer. Finally, the ultimate disagreements were resolved via consensus, and mobility-related outcome measures such as percentage of body contacts were obtained. Results There were 38 hours of device use from 10 participants that were reviewed by both reviewers, with an agreement level of 0.66 for body contacts. The machine learning algorithm trained on 2714 events correctly predicted 90.5% of disagreements. For another 1943 events, the trained model successfully predicted 82% of disagreements, resulting in 81% data reduction. Conclusions The feasibility of mobility aid evaluation based on a large volume of naturalistic data is demonstrated. Machine learning-based disagreement prediction can lead to data reduction. Translational relevance These methods provide a template for determining the real-world benefit of a mobility aid.",9,7,14,14,Machine learning; Collision; Wearable computer; Data reduction; Artificial intelligence; Data acquisition; Coding (social sciences); Collision detection; Video camera; Mobility aid; Computer science,mobility aid; naturalistic mobility; wearable video camera,Algorithms; Blindness; Humans; Machine Learning; Visually Impaired Persons; Wearable Electronic Devices,,,https://tvst.arvojournals.org/article.aspx?articleid=2766331 http://www.ncbi.nlm.nih.gov/pubmed/32832221 https://arvojournals.org/article.aspx?articleid=2766331&resultClick=1 https://www.ncbi.nlm.nih.gov/pubmed/32832221 https://pubmed.ncbi.nlm.nih.gov/32832221/ http://tvst.arvojournals.org/article.aspx?articleid=2766331,http://dx.doi.org/10.1167/tvst.9.7.14,32832221,10.1167/tvst.9.7.14,3035600512,PMC7414611,0,004-756-256-314-376; 013-016-358-471-463; 016-327-721-221-926; 026-642-262-675-816; 027-234-292-085-407; 027-418-298-137-518; 028-284-623-246-941; 030-617-400-771-313; 035-852-009-468-188; 043-772-750-575-189; 044-093-096-610-606; 053-773-685-502-393; 058-775-753-244-594; 069-656-898-335-443; 069-730-043-740-809; 070-196-192-642-645; 074-361-205-504-420; 079-574-573-024-51X; 085-872-401-520-843; 097-755-664-289-72X; 097-944-201-880-049; 116-000-304-517-428; 121-862-300-690-389,2,true,"CC BY, CC BY-NC-ND",gold
066-469-450-548-325,Multi-subject head related transfer function generation using artificial neural networks,2010-07-22,2010,conference proceedings,,,,,Zoltan Haraszy; Sebastian Micut; Virgil Tiponut; Titus Slavici,"The new Acoustic Virtual Reality (AVR) concept is often used as a man-machine interface in electronic travel aid (ETA), that help blind and visually impaired individuals to navigate in real outdoor environments. According to this concept, the presence of obstacles in the surrounding environment and the path to the desired target will be signalized to the blind subject by burst of sounds, whose virtual source position suggests the position of the real obstacles and the direction of movement, respectively. The practical implementation of the AVR concept requires the so-called Head Related Transfer Functions (HRTFs) to be known in every point of the 3D space and for each subject. These functions can be determined by using a quite complex procedure, which requires many measurements for each individual. In the present paper, an updated version of the previously proposed artificial neural network (ANN) is presented and used, in order to generate the HRTFs. The proposed method, valid for any subject, speeds up the implementation of the AVR concept after the ANN training has been completed. Finally, some experimental results, conclusions and further developments are also presented.",,,399,404,Interface (computing); Engineering; Position (vector); Head-related transfer function; Artificial intelligence; Virtual reality; Point (geometry); Computer vision; Artificial neural network; Transfer function; Path (graph theory),,,,,http://dl.acm.org/citation.cfm?id=1981410,http://dl.acm.org/citation.cfm?id=1981410,,,202294182,,0,019-306-790-422-912; 019-431-359-682-592; 019-550-995-456-408; 030-127-369-356-741; 044-192-308-604-739; 050-966-266-525-728; 051-766-223-654-722; 059-448-026-654-528; 063-957-925-086-43X; 066-114-366-503-885; 067-332-342-991-804; 071-405-244-808-059; 073-928-284-197-323; 077-113-362-690-85X; 084-533-882-547-840; 092-862-654-304-908; 094-108-145-447-415; 099-646-885-015-028; 100-773-828-432-613; 190-621-332-208-185,5,false,,
066-526-177-885-125,Fuzzy image processing scheme for autonomous navigation of human blind,,2007,journal article,Applied Soft Computing,15684946,Elsevier BV,Netherlands,G. Sainarayanan; R. Nagarajan; Sazali Yaacob,,7,1,257,264,Digital image processing; Stereophonic sound; Pattern recognition (psychology); Artificial intelligence; Fuzzy clustering; Obstacle; Computer vision; Computer science; Identification (information); Sonification; Image processing,,,,,https://dl.acm.org/doi/10.1016/j.asoc.2005.06.005 https://dblp.uni-trier.de/db/journals/asc/asc7.html#SainarayananNY07 https://www.sciencedirect.com/science/article/abs/pii/S1568494605000670 http://www.sciencedirect.com/science/article/pii/S1568494605000670 http://eprints.ums.edu.my/1170/,http://dx.doi.org/10.1016/j.asoc.2005.06.005,,10.1016/j.asoc.2005.06.005,2026161286,,0,010-025-512-242-323; 013-847-555-426-159; 033-536-677-191-914; 044-080-619-911-182; 045-673-862-191-277; 118-643-017-360-933; 129-400-209-376-57X,98,false,,
066-613-611-576-698,A Survey on Rain Removal from Video and Single Image.,2021-12-27,2021,journal article,Science China Information Sciences,1674733x; 18691919; 18622836; 10092757,Springer Science and Business Media LLC,China,Hong Wang; Minghan Li; Yichen Wu; Qian Zhao; Deyu Meng,"Rain streaks might severely degenerate the performance of video/image processing tasks. The investigations on rain removal from video or a single image has thus been attracting much research attention in the field of computer vision and pattern recognition, and various methods have been proposed against this task in the recent years. However, there is still not a comprehensive survey paper to summarize current rain removal methods and fairly compare their generalization performance, and especially, still not a off-the-shelf toolkit to accumulate recent representative methods for easy performance comparison and capability evaluation. Aiming at this meaningful task, in this study we present a comprehensive review for current rain removal methods for video and a single image. Specifically, these methods are categorized into model-driven and data-driven approaches, and more elaborate branches of each approach are further introduced. Intrinsic capabilities, especially generalization, of representative state-of-the-art methods of each approach have been evaluated and analyzed by experiments implemented on synthetic and real data both visually and quantitatively. Furthermore, we release a comprehensive repository, including direct links to 74 rain removal papers, source codes of 9 methods for video rain removal and 20 ones for single image rain removal, 19 related project pages, 6 synthetic datasets and 4 real ones, and 4 commonly used image quality metrics, to facilitate reproduction and performance comparison of current existing methods for general users. Some limitations and research issues worthy to be further investigated have also been discussed for future research of this direction.",65,1,,,Data mining; Pattern recognition (psychology); Generalization; Task (project management); Rain removal; Field (computer science); Computer science; Image quality; Image processing,,,,,https://arxiv.org/pdf/1909.08326.pdf https://arxiv.org/abs/1909.08326 http://dblp.uni-trier.de/db/journals/corr/corr1909.html#abs-1909-08326 http://ui.adsabs.harvard.edu/abs/2019arXiv190908326W/abstract https://dblp.uni-trier.de/db/journals/corr/corr1909.html#abs-1909-08326,http://dx.doi.org/10.1007/s11432-020-3225-9,,10.1007/s11432-020-3225-9,2974467376,,0,000-322-667-376-308; 001-243-747-198-071; 002-058-360-207-449; 002-329-539-139-572; 002-605-448-180-957; 003-815-373-689-135; 007-194-610-049-876; 010-037-102-410-599; 010-134-001-579-285; 011-007-487-146-58X; 011-535-480-458-942; 013-310-718-609-25X; 013-605-198-581-772; 013-741-367-746-117; 014-038-050-518-96X; 016-374-586-573-970; 018-610-893-432-099; 019-809-574-066-452; 020-059-762-471-637; 020-233-013-143-936; 020-615-876-550-311; 020-790-029-321-276; 020-908-905-592-086; 020-973-364-491-848; 021-555-920-908-750; 022-636-810-990-782; 024-869-206-633-746; 029-997-933-246-409; 031-079-969-433-555; 031-404-616-689-519; 032-314-497-417-349; 033-834-329-911-749; 033-857-848-652-736; 034-012-532-888-63X; 037-373-975-121-609; 039-135-942-929-563; 039-307-802-842-608; 039-696-695-482-310; 040-708-198-084-830; 041-311-843-713-531; 041-978-177-992-924; 042-098-408-371-211; 042-420-551-533-773; 042-712-160-327-416; 044-095-303-626-459; 044-445-912-450-150; 044-497-699-907-063; 044-524-493-321-164; 045-073-277-453-687; 045-785-623-069-051; 046-002-368-199-248; 046-424-608-174-622; 047-780-991-127-34X; 047-943-024-460-415; 049-007-727-262-993; 049-642-309-986-828; 050-143-655-903-932; 050-156-351-021-685; 050-595-345-573-55X; 050-606-944-534-705; 050-731-185-813-481; 051-238-589-435-242; 051-608-263-246-682; 052-988-800-376-161; 054-111-415-918-579; 054-826-790-789-480; 060-607-155-931-318; 061-945-630-022-083; 067-442-175-163-709; 069-442-024-857-79X; 071-170-315-512-914; 076-104-983-582-359; 076-117-572-534-865; 076-845-396-922-88X; 078-227-734-027-357; 078-475-804-628-856; 080-079-346-437-661; 085-655-932-183-312; 092-195-816-260-113; 093-432-185-837-133; 093-874-580-665-886; 096-358-878-261-436; 096-717-380-007-985; 099-476-698-538-596; 101-949-441-612-719; 101-964-523-689-109; 101-973-352-424-601; 105-236-065-638-143; 105-245-617-533-513; 105-448-889-069-435; 106-029-653-216-70X; 106-957-433-344-656; 107-680-187-761-813; 109-314-470-277-026; 111-689-938-411-562; 112-221-362-848-843; 114-404-809-277-149; 115-389-057-546-708; 118-347-997-840-820; 121-576-456-375-193; 121-605-473-009-574; 125-394-728-461-916; 126-004-828-109-450; 127-404-825-968-216; 130-557-372-579-191; 137-301-463-629-591; 138-291-500-463-085; 138-614-135-428-487; 139-885-550-955-779; 140-895-288-214-960; 145-340-425-576-259; 146-245-704-718-070; 151-252-403-920-917; 154-791-767-205-30X; 155-301-496-045-739; 159-627-002-503-425; 167-539-303-966-52X; 172-756-967-527-054; 175-188-519-975-810; 180-692-263-289-983; 196-987-866-076-134,45,true,,green
066-741-866-630-306,Musical Vision: an interactive bio-inspired sonification tool to convert images into music,2018-11-08,2018,journal article,Journal on Multimodal User Interfaces,17837677; 17838738,Springer Science and Business Media LLC,Germany,Antonio Polo; Xavier Sevillano,,13,3,231,243,Human–computer interaction; Polyphony; Human visual system model; Rendering (computer graphics); Computer science; Melody; Musical; MIDI; Interactive design; Sonification,,,,,https://dblp.uni-trier.de/db/journals/jmui/jmui13.html#PoloS19 https://link.springer.com/article/10.1007/s12193-018-0280-4,http://dx.doi.org/10.1007/s12193-018-0280-4,,10.1007/s12193-018-0280-4,2899919357,,0,003-995-776-215-584; 012-379-756-809-77X; 013-847-555-426-159; 021-856-294-020-779; 023-703-322-837-643; 026-504-813-974-498; 026-835-137-329-413; 033-762-294-423-867; 040-084-155-994-268; 040-806-721-645-997; 045-673-862-191-277; 062-539-554-849-822; 062-947-267-748-757; 064-121-100-029-787; 066-360-557-774-972; 066-741-063-205-488; 072-499-712-430-541; 072-621-528-123-732; 077-133-431-811-095; 080-021-648-530-499; 080-368-322-525-051; 090-902-135-478-536; 097-930-055-713-236; 108-396-749-438-718; 122-822-693-327-547; 122-997-724-493-359; 148-214-767-901-332,13,false,,
067-061-150-700-038,Visual Assistant for Blind People using Raspberry Pi,2021-06-12,2021,journal article,"International Journal of Scientific Research in Computer Science, Engineering and Information Technology",24563307,Technoscience Academy,,Tejal Adep; Rutuja Nikam; Sayali Wanewe; Ketaki B. Naik,"<jats:p>Blind people face the problem in daily life. They can't even walk without any aid. Many times they rely on others for help. Several technologies for the assistance of visually impaired people have been developed. Among the various technologies being utilized to assist the blind, Computer Vision-based solutions are emerging as one of the most promising options due to their affordability and accessibility. This paper proposes a system for visually impaired people. The proposed system aims to create a wearable visual aid for visually impaired people in which speech commands are accepted by the user. Its functionality addresses the identification of objects and signboards. This will help the visually impaired person to manage day-to-day activities and navigate through his/her surroundings. Raspberry Pi is used to implement artificial vision using python language on the Open CV platform.</jats:p>",7,3,671,675,Computer graphics (images); Video capture; Raspberry pi; Computer science; Python (programming language),,,,,https://ijsrcseit.com/paper/CSEIT2173142.pdf https://ijsrcseit.com/CSEIT2173142,http://dx.doi.org/10.32628/cseit2173142,,10.32628/cseit2173142,3174877664,,0,013-461-740-909-249; 113-642-994-383-257; 174-700-094-903-486,5,true,,gold
067-650-435-280-941,Indoor Wearable Navigation System Using 2D SLAM Based on RGB-D Camera for Visually Impaired People,2021-05-05,2021,book chapter,Advances in Intelligent Systems and Computing,21945357; 21945365,Springer Singapore,,Heba Hakim; Ali Fadhil,"To guide the visually impaired and blind person to his/her final goal safely and efficiently in unknown indoor environment, a wearable navigation system is presented in this paper. The essential components of the proposed navigation system are mapping, path planning, path following and obstacle avoiding. The system uses a wearable RGB-D camera mounted on a head to construct a 2D map for the surrounding environment which is implemented by a well-known simultaneously and location mapping (SLAM) in mobile robotics. Once a 2D map is creating, the optimal path is generated by path planning method to guide the visually impaired person from start location to his/her destination. Also, an ultrasonic sensor is used in order to detect the temporary obstacles during the following of the optimal path. All data collection and processing is done on Raspberry Pi 3 B+ board due to its low cost, small size and lower power consumption. The visually impaired person receives the continuous guiding commands as voice commands through earphone. The performance of the proposed system has been successfully evaluated in various real-time scenarios.",,,661,672,Wearable computer; Artificial intelligence; Voice command device; Navigation system; PATH (variable); Computer vision; Robotics; Computer science; Obstacle avoidance; Motion planning; RGB color model,,,,,https://link.springer.com/chapter/10.1007/978-981-33-4389-4_60,http://dx.doi.org/10.1007/978-981-33-4389-4_60,,10.1007/978-981-33-4389-4_60,3158119680,,0,047-271-748-480-72X; 053-567-501-093-090; 054-367-982-946-305; 060-135-893-855-211; 061-101-033-659-713; 064-053-186-811-574; 067-000-945-686-55X; 068-276-667-569-970; 070-606-003-984-347; 079-349-286-645-136; 089-738-656-495-377; 117-320-452-579-582; 136-648-400-143-451; 161-901-608-230-16X; 175-661-713-504-140; 184-446-291-593-247,4,false,,
067-765-459-491-663,Blind in a virtual world — using distance information to accomplish virtual tasks,,2013,journal article,Multisensory Research,22134794; 22134808,Brill,Netherlands,Amir Amedi; Shelly Levy-Tzedek; Shachar Maidenbaum; Daniel-Robert Chebat,"Distance information is critical to our understanding of our surrounding environment, especially in virtual reality settings. Unfortunately, as we gage distance mainly visually, the blind are prevented from properly utilizing this parameter to formulate 3D cognitive maps and cognitive imagery of their surroundings. Our purpose is to increase the accessibility of virtual environments to the blind using distance information which they will receive as auditory information. We aim to create a setup which will enable the blind and visually impaired to experience novel environments virtually before travelling to them in the real world, and additionally will enable the blind better accessibility to virtual environments for purposes such as entertainment and education. Here, blind and sighted-blindfolded subjects performed navigation and shape-discrimination tasks in virtual environments, using a simple transformation between virtual distance and sound, based on the concept of a virtual guide cane (paralleling in a virtual environment the ‘EyeCane’, developed in our lab). We show qualitatively that with minimal training it is possible for blind and blindfolded subjects to easily learn this transformation, enabling the discrimination of virtual 3D orientation and shapes and navigation in basic virtual environments using a standard mouse and audio-system.",26,1-2,171,171,Human–computer interaction; Transformation (function); Psychology; Cognition; Cognitive map; Virtual reality; Virtual machine; SIMPLE (military communications protocol); Sensory substitution; Orientation (computer vision),,,,,https://brill.com/view/journals/msr/26/0/article-p171_128.xml?language=en https://booksandjournals.brillonline.com/content/journals/10.1163/22134808-000s0128,http://dx.doi.org/10.1163/22134808-000s0128,,10.1163/22134808-000s0128,1971258898,,0,,1,false,,
067-774-850-735-696,Properties of spatial representations: Data from sighted and blind subjects,,1993,journal article,Perception & psychophysics,00315117; 15325962,Springer Science and Business Media LLC,United States,Ralph Norman Haber; Lyn Haber; Charles A. Levin; Rebecca L. Hollyfield,"Five questions concerning the properties of spatial representations are explored. (1) How accurately does a spatial representation correspond to the true scene? (2) If inaccurate, how does it differ? (3) Are representations of a familiar scene more accurate than those of an unfamiliar one? (4) Do representations of a scene currently in view differ from those retained in memory? (5) Do the representations of the blind have properties comparable to those-of the sighted? Seven sighted and 7 highly mobile blind subjects, all familiar with a room, and 6 sighted subjects unfamiliar with it, were asked to estimate the absolute distances between 10 salient objects in the room. The 14 familiar subjects made their estimates twice: while they were in the room, and while they were remote from it. Regression analyses showed that the estimates of all subjects had strong metric properties, being linearly related to true distance, with a true zero point; and multidimensional scaling showed that all subjects produced distance estimates that could be scaled in two dimensions to closely match the actual locations of the objects. Familiarity had no effect. The effect of location of testing was the same for both the sighted and the blind: all subjects displayed better spatial knowledge when tested in the room; and all subjects underestimated true distance substantially when tested out of the room. The results showed no qualitative differences as a function of blindness, at least for these highly skilled blind travelers.",54,1,1,13,Psychophysics; Psychology; Cognition; Cognitive psychology; Metric (mathematics); Memoria; Blindness; Highly skilled; Social psychology; Multidimensional scaling; Mental representation,,Adult; Blindness/psychology; Female; Humans; Imagination; Male; Middle Aged; Orientation; Perceptual Distortion; Social Environment; Space Perception; Visual Perception,,NEI NIH HHS (EY07801) United States,https://paperity.org/p/21185527/properties-of-spatial-representations-data-from-sighted-and-blind-subjects https://www.ncbi.nlm.nih.gov/pubmed/8351180 https://link.springer.com/article/10.3758/BF03206932 https://link.springer.com/content/pdf/10.3758/BF03206932.pdf,http://dx.doi.org/10.3758/bf03206932,8351180,10.3758/bf03206932,1985756234,,0,001-754-649-932-230; 001-847-768-315-748; 003-750-902-459-323; 011-362-233-838-179; 015-351-611-682-557; 016-795-216-845-76X; 018-654-826-336-966; 018-945-737-932-328; 020-324-872-352-537; 025-047-497-128-427; 025-771-457-853-883; 030-580-016-618-422; 036-622-530-432-269; 053-990-977-881-541; 054-731-257-249-943; 054-999-963-477-65X; 063-091-679-802-140; 070-935-505-662-732; 074-203-293-291-774; 082-618-839-056-448; 083-631-216-544-333; 088-312-225-086-016; 089-138-402-043-355; 106-217-288-690-603; 110-629-835-823-07X; 125-024-006-324-786; 126-497-340-481-10X; 165-329-143-165-257; 169-924-490-744-300,45,true,,bronze
067-903-778-889-313,Improving search engine interfaces for blind users: a case study,2006-05-12,2006,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Patrizia Andronico; Marina Buzzi; Carlos Castillo; Barbara Leporini,"This article describes a research project aimed at improving search engine usability for sightless persons who use assistive technology to navigate the web. At the beginning of this research, a preliminary study was performed concerning accessibility and usability of search tools, and eight guidelines were formulated for designing search engine user interfaces. Then, the derived guidelines were applied in modifying the source code of Google’s interface, while maintaining the same look and feel, in order to demonstrate that with very little effort it is possible to make interaction easier, more efficient, and less frustrating for sightless individuals. After providing a general overview of the project, the paper focuses on interface design and implementation.",5,1,23,40,Usability engineering; Human–computer interaction; World Wide Web; Usability; Usability inspection; Interface (Java); Look and feel; Computer science; Source code; User interface; Search engine,,,,,http://chato.cl/papers/andronico_05_improving_search_engine_interfaces_blind_users.pdf https://link.springer.com/article/10.1007%2Fs10209-006-0022-3/fulltext.html https://dblp.uni-trier.de/db/journals/uais/uais5.html#AndronicoBCL06 https://link.springer.com/article/10.1007/s10209-006-0022-3 https://rd.springer.com/article/10.1007%2Fs10209-006-0022-3 http://giove.isti.cnr.it/attachments/publications/2006-A0-14.pdf http://hiis.isti.cnr.it/attachments/publications/2006-A0-14.pdf https://www.researchgate.net/profile/Carlos_Castillo/publication/200110569_Improving_search_engine_interfaces_for_blind_users_a_case_study/links/0912f505a10d57e2a7000000.pdf https://core.ac.uk/display/22736566 https://chato.cl/papers/andronico_05_improving_search_engine_interfaces_blind_users.pdf https://link.springer.com/content/pdf/10.1007%2Fs10209-006-0022-3.pdf,http://dx.doi.org/10.1007/s10209-006-0022-3,,10.1007/s10209-006-0022-3,1996982690,,0,006-788-385-194-234; 006-812-887-054-730; 009-613-507-591-161; 029-383-581-442-716; 067-300-104-229-623; 072-870-430-766-972; 075-494-451-849-374; 081-395-136-247-131; 105-772-651-290-898; 107-960-131-714-547; 120-159-854-240-979; 136-510-447-458-985; 144-186-263-340-913; 153-479-410-690-098; 166-492-338-087-160; 169-479-871-713-230; 183-617-660-742-453; 183-950-570-779-227,42,true,,green
068-122-210-845-54X,The influence of spatial structure and configuration on behaviour: case study of a complex school environment,2009-08-20,2009,journal article,Cognitive Processing,16124782; 16124790,Springer Science and Business Media LLC,Germany,Erica Calogero; Ruth Dalton,,10,2,131,147,Engineering; Crowding; Cognitive psychology; Space syntax; Peer learning; Space (commercial competition); Movement (music); Probabilistic logic; Contrast (statistics); Social psychology; Social learning,,,,,http://nrl.northumbria.ac.uk/id/eprint/21995 https://researchportal.northumbria.ac.uk/en/publications/the-influence-of-spatial-structure-and-configuration-on-behaviour https://eprints.lancs.ac.uk/id/eprint/138682/,http://dx.doi.org/10.1007/s10339-009-0330-7,,10.1007/s10339-009-0330-7,257743443,,0,,0,false,,
068-196-395-552-832,Linking the design of university learning spaces to students' experience,2021-09-06,2021,journal article,Cognitive Processing,16124782; 16124790,Springer Science and Business Media LLC,Germany,Beatrix Emo; Gregoire Farquet; Momoyo Kaijima; Christoph Hoelscher,"Fundamental limits on the human ability to process all available; information in real-world situations necessarily require that some; information is filtered out by attentional processes, and so that; information is not used to build ‘situation awareness’ (SA). The; implication is that two (or more) people in the same situation may not; perceive it in the same way, nor make the same decisions as to how to; operate. Such differences in perception can become very important in; safety critical situations such as the one that we report here—firefighter attendance at a road-traffic collision (RTC). Using a desktop; virtual reality simulation of a RTC, we examined firefighters’; awareness of their situation (SA)—and also their understanding of; what aspects of that situation are relevant to the successful completion; of their task (situation understanding, SU). Our data, collected from; 685 firefighters, suggest that the firefighters showed pronounced; individual differences in the amount of information they accepted; when building SA, but that they were almost universally predisposed; to believe that information was relevant to their task (even though it; may not have been). Non-firefighters, when compared with firefighters were generally more likely to accept a wider range of information; spatially distributed across the RTC situation, but less likely to; believe it to be relevant. The implication of our data is that individuals; in the same situation may perceive that situation in different ways due; to the way they handle the available information—and that training; and experience affect that perception.",22,S1,39,39,Mathematics education; Psychology; User experience design; Spatial cognition; Informal learning; Architectural design; Collision; Situation awareness; Cognitive psychology; Perception; Attendance; Task (project management); Successful completion; Affect (psychology); Process (engineering); Human–computer interaction; Affordance; Measure (physics); Computer science; Cognition; Action (physics); Cognitive science; Behavioural sciences; Neuroscience; Psychotherapist; Physics; Quantum mechanics,,,,,https://digitalcollection.zhaw.ch/handle/11475/23144,http://dx.doi.org/10.1007/s10339-021-01058-x,,10.1007/s10339-021-01058-x,3204609301; 3214077637; 3204605417,,0,,5,true,,green
068-224-060-029-770,Clearway Companion - an Ai Powered Aid for Visually Impaired,2024-01-01,2024,preprint,,,Elsevier BV,,Akshay  Vijay Panchal; Ms. Chinmayi Naik; Mr. Devang Mahimkar; Mr. Ajay Chougule; Dr. Megha Gupta,,,,,,Visually impaired; Computer science; Psychology; Human–computer interaction,,,,,,http://dx.doi.org/10.2139/ssrn.4809356,,10.2139/ssrn.4809356,,,0,,0,false,,
068-243-601-251-133,A Navigation System for Assistant Robots Using Visually Augmented POMDPs,,2005,journal article,Autonomous Robots,09295593; 15737527,Springer Science and Business Media LLC,Netherlands,María Elena López; Luis M. Bergasa; Rafael Barea; María Soledad Escudero,,19,1,67,87,Markov decision process; Architecture; Artificial intelligence; Partially observable Markov decision process; Navigation system; Computer science; Sonar; Feature (computer vision); Robot; Robustness (computer science),,,,,https://doi.org/10.1007/s10514-005-0607-3 https://link.springer.com/article/10.1007/s10514-005-0607-3 http://dx.doi.org/10.1007/s10514-005-0607-3 http://www.robesafe.com/personal/bergasa/papers/A%20navigation%20system%20for_AR_2005.pdf https://dblp.uni-trier.de/db/journals/arobots/arobots19.html#LopezBBE05 http://www.robesafe.es/personal/bergasa/papers/A%20navigation%20system%20for_AR_2005.pdf https://dialnet.unirioja.es/servlet/articulo?codigo=1187670 https://dx.doi.org/10.1007/s10514-005-0607-3,http://dx.doi.org/10.1007/s10514-005-0607-3,,10.1007/s10514-005-0607-3,1976395353,,0,006-369-146-204-101; 007-160-511-582-805; 010-261-472-646-546; 010-541-669-187-810; 016-060-859-870-806; 016-541-375-569-15X; 017-474-481-735-990; 021-560-605-759-392; 026-548-761-974-000; 028-279-265-302-782; 035-349-566-367-971; 044-939-849-574-117; 048-822-004-166-396; 052-533-677-114-068; 060-024-659-176-809; 069-359-458-478-984; 071-844-103-548-729; 072-875-149-435-568; 076-916-650-748-465; 079-588-821-231-401; 090-172-719-638-159; 102-080-847-778-792; 122-096-833-005-555; 125-389-216-238-220; 126-972-629-267-702; 134-259-148-088-080; 158-713-310-595-151; 171-916-888-913-890; 172-751-141-074-770; 178-231-930-636-624; 182-789-304-642-274,33,false,,
068-818-234-534-565,Training a Vision Guided Mobile Robot,,1998,journal article,Machine Learning,08856125; 15730565,Springer Science and Business Media LLC,Netherlands,Gordon Wyeth,"This paper presents the design, implementation and evaluation of a trainable vision guided mobile robot. The robot, CORGI, has a CCD camera as its only sensor which it is trained to use for a variety of tasks. The techniques used for train ing and the choice of natural light vision as the primary sensor makes the methodology immediately applicable to tasks such as trash collection or fruit picking. For example, the robot is readily trained to perform a ball finding task which involves avoiding obstacles and aligning with tennis balls. The robot is able to move at speeds up to 0.8 ms^-1 while performing this task, and has never had a collision in the trained environment. It can process video and update the actuators at 11 Hz using a single d20 microprocessor to perform all computation. Further results are shown to evaluate the system for generalization across unseen domains, fault tolerance and dynamic environments.",31,1,201,222,Artificial intelligence; Social robot; Personal robot; Robot learning; Mobile robot; Mobile robot navigation; Computer vision; Computer science; Machine vision; Robot control; Robot,,,,,https://link.springer.com/content/pdf/10.1023%2FA%3A1007405111315.pdf https://dblp.uni-trier.de/db/journals/arobots/arobots5.html#Wyeth98 https://link.springer.com/content/pdf/10.1023/A:1008870625003.pdf https://espace.library.uq.edu.au/view/UQ:401001 https://paperity.org/p/7449846/training-a-vision-guided-mobile-robot https://doi.org/10.1023/A:1008870625003 https://link.springer.com/article/10.1023/A:1007405111315,http://dx.doi.org/10.1023/a:1007405111315,,10.1023/a:1007405111315,1501801909,,0,008-452-315-283-913; 021-229-759-657-858; 023-795-887-537-788; 038-152-270-163-339; 038-793-280-596-306; 047-853-807-776-871; 070-156-682-223-295; 075-447-186-131-377; 079-341-335-963-186; 082-832-327-829-790; 083-218-000-317-642; 085-674-040-165-681; 091-267-470-450-756; 091-837-054-729-24X; 104-838-855-754-466; 106-217-848-854-558; 109-329-096-793-355; 180-171-457-447-122,5,true,,bronze
068-871-963-007-928,Learning and navigating digitally rendered haptic spatial layouts.,2023-12-16,2023,journal article,NPJ science of learning,20567936,Springer Science and Business Media LLC,England,Ruxandra I Tivadar; Benedetta Franceschiello; Astrid Minier; Micah M Murray,"Learning spatial layouts and navigating through them rely not simply on sight but rather on multisensory processes, including touch. Digital haptics based on ultrasounds are effective for creating and manipulating mental images of individual objects in sighted and visually impaired participants. Here, we tested if this extends to scenes and navigation within them. Using only tactile stimuli conveyed via ultrasonic feedback on a digital touchscreen (i.e., a digital interactive map), 25 sighted, blindfolded participants first learned the basic layout of an apartment based on digital haptics only and then one of two trajectories through it. While still blindfolded, participants successfully reconstructed the haptically learned 2D spaces and navigated these spaces. Digital haptics were thus an effective means to learn and translate, on the one hand, 2D images into 3D reconstructions of layouts and, on the other hand, navigate actions within real spaces. Digital haptics based on ultrasounds represent an alternative learning tool for complex scenes as well as for successful navigation in previously unfamiliar layouts, which can likely be further applied in the rehabilitation of spatial functions and mitigation of visual impairments.",8,1,61,,Haptic technology; Computer graphics (images); Computer science; Human–computer interaction; Engineering drawing; Artificial intelligence; Engineering,,,,Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung (Swiss National Science Foundation) (169206),https://www.nature.com/articles/s41539-023-00208-4.pdf https://doi.org/10.1038/s41539-023-00208-4,http://dx.doi.org/10.1038/s41539-023-00208-4,38102127,10.1038/s41539-023-00208-4,,PMC10724186,0,000-143-901-299-264; 000-185-521-073-031; 000-659-254-096-600; 002-151-993-281-08X; 002-343-442-143-488; 004-068-595-943-303; 004-485-127-865-575; 005-075-966-426-21X; 005-536-425-724-459; 005-697-811-239-782; 006-204-178-632-439; 006-321-115-294-56X; 006-813-235-552-771; 007-262-852-938-221; 007-797-333-936-671; 008-058-765-263-033; 008-816-010-353-364; 009-696-271-600-856; 011-854-944-174-242; 014-185-662-685-224; 014-642-265-251-693; 015-480-673-045-351; 015-729-471-527-230; 015-734-328-892-821; 018-031-085-269-362; 018-340-724-883-318; 021-183-124-908-966; 024-817-878-423-085; 025-705-910-124-270; 028-500-694-389-505; 028-842-346-535-801; 029-382-620-854-808; 029-981-112-895-94X; 029-984-553-188-873; 031-056-855-501-837; 031-160-271-128-821; 031-487-210-084-452; 032-015-410-743-396; 033-515-233-830-591; 035-210-645-727-414; 035-249-797-221-031; 037-389-881-152-996; 040-030-169-396-345; 040-643-058-009-586; 041-093-889-515-749; 043-493-772-446-001; 046-152-348-100-670; 046-614-159-346-51X; 046-879-300-958-704; 046-965-483-493-241; 047-194-700-536-708; 047-735-315-156-249; 048-568-672-676-825; 051-603-549-352-058; 051-610-315-419-913; 053-722-346-596-938; 053-794-028-526-128; 054-082-106-354-351; 054-795-154-566-738; 054-879-961-055-393; 060-225-652-302-332; 060-614-968-414-313; 061-959-171-322-454; 064-741-618-597-559; 066-608-905-207-154; 066-862-969-373-235; 066-869-212-502-877; 067-866-980-368-474; 068-829-447-415-205; 069-154-949-542-582; 069-371-750-398-350; 070-009-985-171-541; 071-438-835-926-699; 071-547-586-198-741; 072-033-785-736-168; 073-493-197-253-091; 078-739-612-232-431; 079-973-093-856-139; 083-924-881-693-987; 084-934-084-521-74X; 084-968-023-782-538; 085-028-773-225-657; 085-536-849-866-778; 088-942-335-888-358; 089-730-190-470-496; 089-964-526-168-20X; 092-923-598-568-103; 095-641-000-898-057; 098-410-822-239-350; 100-385-550-383-508; 103-174-507-130-889; 105-235-190-868-891; 109-818-310-344-735; 112-199-581-742-67X; 122-604-991-619-438; 123-890-691-474-106; 127-175-614-832-865; 133-904-468-605-633; 143-492-061-970-967; 149-402-143-153-318; 150-437-897-599-194; 152-000-882-073-641; 155-503-280-718-987; 156-484-796-355-211,1,true,cc-by,gold
068-888-754-026-832,A Wearable RFID-Based Navigation System for the Visually Impaired,2022-11-15,2022,conference proceedings article,2022 10th RSI International Conference on Robotics and Mechatronics (ICRoM),,IEEE,,Fateme Zare; Paniz Sedighi; Mehdi Delrobaei,"Recent studies have focused on developing advanced assistive devices to help blind or visually impaired people. Navigation is challenging for this community; however, developing a simple yet reliable navigation system is still an unmet need. This study targets the navigation problem and proposes a wearable assistive system. We developed a smart glove and shoe set based on radio-frequency identification technology to assist visually impaired people with navigation and orientation in indoor environments. The system enables the user to find the directions through audio feedback. To evaluate the device's performance, we designed a simple experimental setup. The proposed system has a simple structure and can be personalized according to the user's requirements. The results identified that the platform is reliable, power efficient, and accurate enough for indoor navigation.",,,,,Visually impaired; Computer science; Wearable computer; Human–computer interaction; Embedded system; Computer vision,,,,,https://arxiv.org/pdf/2303.14792 https://arxiv.org/abs/2303.14792 https://figshare.com/articles/preprint/A_Wearable_RFID-Based_Navigation_System_for_the_Visually_Impaired/22337803/1/files/40050889.pdf https://figshare.com/articles/preprint/A_Wearable_RFID-Based_Navigation_System_for_the_Visually_Impaired/22337803,http://dx.doi.org/10.1109/icrom57054.2022.10025351,,10.1109/icrom57054.2022.10025351,,,0,001-461-750-787-745; 003-787-428-155-830; 008-361-673-204-978; 008-502-238-157-587; 008-558-848-807-295; 011-267-682-195-476; 016-988-681-008-984; 017-499-516-520-553; 025-764-110-104-300; 032-661-597-680-28X; 037-199-326-802-024; 038-326-446-864-583; 040-057-279-881-544; 080-148-076-219-168; 089-082-056-973-911; 090-420-681-934-605; 093-548-249-489-049; 094-910-272-492-209; 099-072-913-456-358; 104-685-875-026-63X; 109-525-288-959-67X,3,true,,green
068-904-413-024-844,Eye for Blind: A Deep Learning-Based Sensory Navigation System for the Blind,2023-12-22,2023,book chapter,ICT Analysis and Applications,23673370; 23673389,Springer Nature Singapore,,Mrunal Shidore; Sahil Kakurle; Manish Shetkar; Malhar Kapshe; Abhijeet Kamble,"The research proposes a new system to assist visually impaired individuals in navigating their surroundings by introducing an “Eye for Blind” system that employs Convolutional Neural Networks (CNN). Our approach utilizes a CNN model to recognize images and generates audio feedback with descriptive labels. This innovative system has the potential to help visually impaired individuals detect and recognize photos, enhancing their independence and quality of life. We evaluated our proposed system using publicly available datasets, and the outcomes suggest that our approach has a high level of accuracy in recognizing social media posts, demonstrating its potential as a solution for the visually impaired community. By using the Eye for Blind system, visually impaired individuals will be able to navigate their surroundings more effectively and efficiently, eliminating their dependence on assistance from others. This technology has enormous potential to improve the quality of life for visually impaired individuals and can contribute to the development of other similar systems that can enhance their independence and mobility. In conclusion, our proposed system has significant potential to make a positive impact on the lives of visually impaired individuals, and we hope that it can be further developed and implemented in the future.",,,229,240,Visually impaired; Computer science; Convolutional neural network; Independence (probability theory); Artificial intelligence; Quality (philosophy); Human–computer interaction; Computer vision; Philosophy; Statistics; Mathematics; Epistemology,,,,,,http://dx.doi.org/10.1007/978-981-99-6568-7_21,,10.1007/978-981-99-6568-7_21,,,0,006-637-346-868-71X; 006-757-908-435-833; 009-171-960-350-539; 010-215-660-205-695; 015-968-400-395-152; 019-535-516-864-012; 028-711-501-082-866; 029-011-000-053-812; 059-485-619-431-787; 156-630-717-674-920,0,false,,
069-022-769-254-976,"Enhancing the mental representations of space used by blind pedestrians, based on an image schemata model",2012-08-26,2012,journal article,Cognitive processing,16124790; 16124782,Springer Science and Business Media LLC,Germany,Reda Yaagoubi; Geoffrey Edwards; Thierry Badard; Mir Abolfazl Mostafavi,,13,4,333,347,Human–computer interaction; Semantics; Artificial intelligence; Spatial analysis; Structure (mathematical logic); Schematic; Space (commercial competition); Poison control; Computer vision; Computer science; Orientation (computer vision); Mental representation,,"Blindness/psychology; Female; Humans; Imagination; Male; Models, Theoretical; Orientation; Space Perception; Spatial Behavior; Walking",,,https://dx.doi.org/10.1007/s10339-012-0523-3 https://www.ncbi.nlm.nih.gov/pubmed/22923043 https://link.springer.com/article/10.1007/s10339-012-0523-3 https://dblp.uni-trier.de/db/journals/cp/cp13.html#YaagoubiEBM12 https://www.safetylit.org/citations/index.php?fuseaction=citations.viewdetails&citationIds[]=citjournalarticle_375101_9 https://link.springer.com/content/pdf/10.1007%2Fs10339-012-0523-3.pdf http://europepmc.org/abstract/MED/22923043 http://dx.doi.org/10.1007/s10339-012-0523-3,http://dx.doi.org/10.1007/s10339-012-0523-3,22923043,10.1007/s10339-012-0523-3,2076781796,,0,000-659-254-096-600; 001-201-299-722-443; 004-331-296-096-867; 010-482-804-959-514; 014-207-841-921-194; 028-834-688-057-07X; 035-238-697-396-754; 036-616-049-977-19X; 038-101-885-863-614; 040-597-465-811-176; 042-323-835-076-274; 044-438-004-418-782; 044-737-699-270-744; 048-327-566-365-049; 063-541-173-415-317; 084-580-435-051-802; 129-052-893-331-416; 135-605-772-438-086; 135-793-420-224-624; 195-772-152-326-950,6,false,,
069-226-894-925-753,Teleguidance-based remote navigation assistance for visually impaired and blind people—usability and user experience,2021-05-24,2021,journal article,Virtual reality,13594338; 14349957,Springer Science and Business Media LLC,United Kingdom,Babar Chaudary; Sami Pohjolainen; Saima Aziz; Leena Arhippainen; Petri Pulli,"This paper reports the development of a specialized teleguidance-based navigation assistance system for the blind and the visually impaired. We present findings from a usability and user experience study conducted with 11 blind and visually impaired participants and a sighted caretaker. Participants sent live video feed of their field of view to the remote caretaker’s terminal from a smartphone camera attached to their chest. The caretaker used this video feed to guide them through indoor and outdoor navigation scenarios using a combination of haptic and voice-based communication. Haptic feedback was provided through vibrating actuators installed in the grip of a Smart Cane. Two haptic methods for directional guidance were tested: (1) two vibrating actuators to guide left and right movement and (2) a single vibrating actuator with differentiating vibration patterns for the same purpose. Users feedback was collected using a meCUE 2.0 standardized questionnaire, interviews, and group discussions. Participants’ perceptions toward the proposed navigation assistance system were positive. Blind participants preferred vibrational guidance with two actuators, while partially blind participants preferred the single actuator method. Familiarity with cane use and age were important factors in the choice of haptic methods by both blind and partially blind users. It was found that smartphone camera provided sufficient field of view for remote assistance; position and angle are nonetheless important considerations. Ultimately, more research is needed to confirm our preliminary findings. We also present an expanded evaluation model developed to carry out further research on assistive systems.",27,1,1,18,Human–computer interaction; Haptic technology; Usability; Virtual reality; User experience design; Perception; Visual impairment; Navigation assistance; Computer science; Computer graphics,Haptic; Navigation assistance; Remote assistance; Teleguidance; Visual impairment; Voice-based communication,,,University of Oulu including Oulu University Hospital,https://www.ncbi.nlm.nih.gov/pubmed/34054327 https://europepmc.org/article/MED/34054327 https://link.springer.com/article/10.1007/s10055-021-00536-z https://www.scilit.net/article/db5c518d0cc82dc8901e0c1d20cffc87?action=show-references https://link.springer.com/content/pdf/10.1007/s10055-021-00536-z.pdf https://paperity.org/p/261938290/teleguidance-based-remote-navigation-assistance-for-visually-impaired-and-blind-people https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8142295,http://dx.doi.org/10.1007/s10055-021-00536-z,34054327,10.1007/s10055-021-00536-z,3163959941,PMC8142295,0,002-391-643-288-41X; 003-634-369-751-95X; 004-179-296-353-268; 007-519-654-024-606; 007-527-622-633-327; 007-530-851-453-706; 012-381-618-181-665; 013-417-291-981-465; 016-242-921-285-593; 018-031-085-269-362; 018-294-918-801-734; 018-582-576-906-523; 018-843-508-574-144; 024-497-613-082-035; 028-453-069-035-93X; 033-687-327-893-105; 037-056-866-495-944; 038-103-027-460-199; 038-958-501-071-435; 040-170-335-949-748; 044-097-632-032-714; 044-277-668-258-449; 046-824-464-627-655; 047-798-470-037-651; 049-106-304-885-412; 051-766-223-654-722; 054-266-700-668-448; 057-886-251-982-373; 060-763-483-393-015; 075-519-293-886-898; 075-541-853-431-383; 075-842-653-595-091; 076-854-462-895-141; 082-172-375-360-711; 084-067-462-286-079; 084-912-789-426-243; 090-593-846-802-324; 107-841-159-955-885; 109-167-882-345-577; 109-525-288-959-67X; 109-594-059-497-650; 110-320-624-506-95X; 110-510-842-185-282; 125-351-519-179-754; 127-524-326-170-02X; 133-300-802-059-627; 141-601-200-804-924; 143-047-514-445-300; 145-427-773-820-117; 145-940-216-825-852; 158-675-713-708-734; 173-137-869-065-590,22,true,"CC BY, CC BY-NC-ND",gold
069-419-248-108-995,Robotic travel aid for the blind: HARUNOBU-6,,1998,,,,,,Hideo Mori; Shinji Kotani,"We have been developing Robotic Travel Aid (RoTA) “HARUNOBU” to guide the visually impaired in the sidewalk or campus. RoTA is a motor wheel chair equipped with vision system, sonar, differential GPS system, dead reckoning system and a portable GIS. We estimate the performance of RoTA in two viewpoints, the viewpoint of guidance and the viewpoint of safety. RoTA is superior to the guide dog in the navigation function, and is inferior to the guide dog in the mobility. It can show the route from the current location to the destination but cannot walk up and down stairs. RoTA is superior to the portable navigation system in the orientation, obstacle avoidance and physical support to keep balance of walking, but is inferior in portability.",,,,,Dead reckoning; Artificial intelligence; Geography; Stairs; Navigation system; Differential GPS; Navigation function; Computer vision; Machine vision; Software portability; Obstacle avoidance,,,,,http://www.icdvrat.org/1998/papers/1998_24.pdf https://www.icdvrat.org/1998/papers/1998_24.pdf http://playpen.icomtek.csir.co.za/~acdc/assistive%20devices/Artabilitation2008/archive/1998/papers/1998_24.pdf,http://www.icdvrat.org/1998/papers/1998_24.pdf,,,2150794650,,0,072-400-524-676-429; 073-211-104-818-914; 088-470-436-866-921; 091-963-226-594-351; 139-791-546-659-910,39,false,,
069-523-876-223-951,Navigating blind people with walking impairments using a smart walker,2016-08-11,2016,journal article,Autonomous Robots,09295593; 15737527,Springer Science and Business Media LLC,Netherlands,Andreas Wachaja; Pratik Agarwal; Mathias Zink; Miguel Reyes Adame; Knut Möller; Wolfram Burgard,,41,3,555,573,Human–computer interaction; Controller (computing); Artificial intelligence; Limited vision; Human motion; Smart walker; Field (computer science); Robotics; Computer science; User interface,,,,,https://dl.acm.org/doi/10.1007/s10514-016-9595-8 https://link.springer.com/article/10.1007/s10514-016-9595-8 https://dblp.uni-trier.de/db/journals/arobots/arobots41.html#WachajaAZAMB17,http://dx.doi.org/10.1007/s10514-016-9595-8,,10.1007/s10514-016-9595-8,2509894563,,0,005-894-340-066-16X; 006-500-640-442-683; 007-502-378-744-488; 010-224-051-212-691; 011-527-612-257-65X; 011-572-366-239-378; 012-675-047-185-120; 013-056-435-015-274; 015-490-373-388-693; 019-819-253-195-076; 026-672-312-718-982; 029-270-236-495-701; 030-127-369-356-741; 030-440-500-148-114; 034-842-200-498-902; 039-274-384-165-289; 049-861-706-148-342; 051-679-394-284-935; 052-231-134-911-518; 065-761-044-156-942; 074-450-912-998-903; 075-547-657-514-672; 081-745-807-748-779; 091-535-297-106-369; 104-409-851-282-486; 104-694-342-259-021; 120-093-276-975-914; 126-305-859-279-726; 135-646-014-794-306; 139-233-304-780-009; 141-077-134-230-230; 141-489-442-115-69X; 149-539-384-318-344; 150-544-536-666-156,65,false,,
069-524-555-242-566,Robust Gesture Recognition and Classification for Visually Impaired Persons Using Growth Optimizer with Deep Stacked Autoencoder,2023-08-26,2023,journal article,Journal of Disability Research,16589912,King Salman Center for Disability Research,,Mashael Maashi; Mohammed Abdullah Al-Hagery; Mohammed Rizwanullah; Azza Elneil Osman,"<jats:p>Visual impairment affects the major population of the world, and impaired vision people need assistance for their day-to-day activities. With the enormous growth and usage of new technologies, various devices were developed to help them with object identification in addition to navigation in the indoor and outdoor surroundings. Gesture detection and classification for blind people aims to develop technologies to assist those people to navigate their surroundings more easily. To achieve this goal, using machine learning and computer vision techniques is a better solution to classify and detect hand gestures. Such methods are utilized for finding the shape, position, and movement of the hands in real-time. With this motivation, this article presents a robust gesture recognition and classification using growth optimizer with deep stacked autoencoder (RGRC-GODSAE) model for visually impaired persons. The goal of the RGRC-GODSAE technique lies in the accurate recognition and classification of gestures to assist visually impaired persons. The RGRC-GODSAE technique follows the Gabor filter approach at the initial stage to remove noise. In addition, the RGRC-GODSAE technique uses the ShuffleNet model as a feature extractor and the GO algorithm as a hyperparameter optimizer. Finally, the deep stacked autoencoder model is exploited for the automated recognition and classification of gestures. The experimental validation of the RGRC-GODSAE technique is carried out on the benchmark dataset. The extensive comparison study showed better gesture recognition performance of the RGRC-GODSAE technique over other deep learning models.</jats:p>",2,2,,,,,,,,,http://dx.doi.org/10.57197/jdr-2023-0029,,10.57197/jdr-2023-0029,,,0,008-571-155-499-142; 009-808-784-036-14X; 053-719-284-260-904; 053-826-604-836-17X; 064-985-026-046-586; 072-903-413-459-370; 076-572-590-565-23X; 095-970-960-047-29X; 121-073-944-064-576; 167-054-908-572-00X; 167-223-498-322-466; 173-213-167-492-717; 180-399-985-698-209; 185-985-617-558-994,1,false,,
069-608-818-240-967,Motion based smart assistant for visually impaired people,2020-04-30,2020,journal article,Indian Journal of Science and Technology,09746846; 09745645,Indian Society for Education and Environment,India,Muhammad Abbas Khan lowast; Muhammad Zubair Khan; Sohaib Ehsan Bazai; Syed Owais Ahmed; Hayat Khan; Nasir Ejaz; Naqeeb Ullah,"Objectives/Aim: The present paper deals with building a smart assistant with the aim of assisting the visually impaired people in mobility with confidence by realizing the nearby obstacles and also implement image processing techniques to recognize people. Methods: We use Raspberry Pi 4 which has increased computing performance and is interfaced to the picamera, GPS and GSM modules. Arduino pro mini with buzzer, ultrasonic sensors and vibration motors are used for obstacle detection. Certain libraries important for image processing are used such as OpenCV, Dlib, face detection (Haar cascades, HOG + Linear SVM, or CNNs), espeak for converting text to speech. Programming was implemented through Python and Arduino compiler. Findings: Analysis was carried out using the proposed system for the blind and visually impaired people who could move around comfortably with confidence and were also able to detect objects and recognize people. Hence the proposed system removes the use for the cumbersome white cane in exchange for small and handy modules which can be used in the form of wearables mounted on shoulders, hands and legs to detect the obstacles from multiple directions and provide for a comfortable wear. It was determined that the Raspberry pi 4 was incapable of running CNN detection for which ideally computer GPU was required, so HOG detection method was used instead. Application/Improvements: This project can be implemented mainly in the commercial field of helping visually impaired people with poor eyesight or being completely blind. Industrial applications can be devised and enhanced like robots and machineries along with Security, Identifying and Tracking.; Keywords: Face Recognition; Obstacle Detection; Open Source Computer Vision (OpenCV); Deep Learning; Histogram of Oriented Gradients (HOG); Support Vector Machine (SVM); Convolutional Neural Network (CNN)",13,16,1612,1618,Deep learning; Arduino; Wearable computer; Artificial intelligence; Histogram of oriented gradients; Facial recognition system; Computer vision; Computer science; Face detection; Convolutional neural network; Image processing,,,,,https://indjst.org/articles/motion-based-smart-assistant-for-visually-impaired-people,http://dx.doi.org/10.17485/ijst/v13i16.18,,10.17485/ijst/v13i16.18,3034887008,,0,,3,true,cc-by,gold
069-867-996-479-694,ICARCV - Classification criteria for local navigation digital assistance techniques for the visually impaired,,2014,conference proceedings article,2014 13th International Conference on Control Automation Robotics & Vision (ICARCV),,IEEE,,Navya Amin; Markus Borschbach,"This paper presents the criteria for classification of the existing techniques for the development of navigation aid for the visually impaired people. Digital assistance for the visually impaired people to navigate easily even during the presence of static and moving obstacles has been a food for research over the years. The motivation for the current work is the development of a smart hearing aid for people with hearing loss. The filtering of essential information to enable a better sensing of the surrounding environment for people with diminished vision is the main aim of developing a navigation system for people with such impairment. The current study gives an overview of the techniques used for obstacle detection, detection of path for navigation and system for alerting the users about existing obstacles in their path. It also presents the selected algorithms used for detecting the obstacles in the path and alarming the user about its presence which offers as a criteria for the comparison of the techniques for the development of a navigation system for the blind.",,,1724,1728,Engineering; Mobile robot navigation; Obstacle; Navigation system; PATH (variable); Hearing aid; Navigation aid; Visually impaired; Visualization; Multimedia,,,,,https://ieeexplore.ieee.org/document/7064576/ https://dblp.uni-trier.de/db/conf/icarcv/icarcv2014.html#AminB14 https://www.researchgate.net/profile/Markus_Borschbach/publication/269746432_Classification_Criteria_for_Local_Navigation_Digital_Assistance_Techniques_for_the_Visually_Impaired/links/549539750cf29b9448211870.pdf,http://dx.doi.org/10.1109/icarcv.2014.7064576,,10.1109/icarcv.2014.7064576,2034590959,,1,000-631-028-807-828; 021-588-459-531-940; 034-373-226-233-399; 045-874-715-328-967; 051-766-223-654-722; 056-773-999-189-634; 064-043-611-038-916; 077-703-665-866-285; 085-094-607-137-825; 097-390-623-922-438; 120-413-407-974-701; 158-102-696-392-53X; 166-719-051-375-011; 168-445-619-792-756; 189-752-077-104-243,3,false,,
070-465-103-080-912,A dataset for the recognition of obstacles on blind sidewalk,2021-08-16,2021,journal article,Universal Access in The Information Society,16155289; 16155297,Springer Verlag,Germany,Wu Tang; De-er Liu; Xiaoli Zhao; Zenghui Chen; Chen Zhao,"Recently, the technology of assisting the navigation of visually impaired persons with computer vision has been greatly developed. A number of scholars have conducted related research, including indoor and outdoor object detection for blind people. However, there are still problems with some existing methods or datasets. Our work mainly proposes a dataset (OD) for assisting the detection and recognition of outdoor obstacles for blind people on blind sidewalk. We classify some common obstacles, train the dataset with state-of-the-art detectors to obtain detection models, and then analyze and compare these models in detail. The results show that our proposed dataset is very challenging. The OD and the detection model can be obtained at the following URL: https://github.com/TW0521/Obstacle-Dataset.git;  .",,,1,14,Artificial intelligence; Object detection; Visually Impaired Persons; Computer communication networks; Related research; Computer vision; Computer science,,,,,https://link.springer.com/article/10.1007%2Fs10209-021-00837-9,https://link.springer.com/article/10.1007%2Fs10209-021-00837-9,,,3194740919,,0,000-051-707-082-100; 002-242-013-537-46X; 004-019-652-142-123; 004-193-609-931-550; 004-242-945-383-184; 005-225-964-313-490; 006-565-780-206-428; 010-684-117-336-855; 014-036-969-711-168; 015-967-911-639-302; 016-027-592-222-572; 016-658-296-028-629; 017-159-670-613-140; 017-289-190-110-902; 018-152-176-285-561; 025-176-734-034-020; 026-303-765-898-687; 031-218-334-653-826; 032-052-662-821-330; 041-584-324-491-598; 042-197-355-305-758; 045-309-399-228-849; 048-395-635-842-481; 049-317-239-158-314; 050-113-304-231-41X; 063-656-074-507-347; 064-043-611-038-916; 069-480-623-909-181; 071-773-804-778-45X; 075-262-128-514-571; 075-465-285-579-483; 085-686-689-031-272; 102-122-613-011-188; 102-787-341-517-228; 113-452-723-216-812; 118-372-317-168-327; 123-748-458-842-892; 125-112-083-666-544; 127-671-470-361-947; 130-394-016-563-923; 133-499-790-890-245; 136-190-477-330-077; 149-162-653-144-382; 150-550-978-046-262; 173-955-288-654-439; 178-570-402-586-391; 186-002-073-406-20X; 186-175-701-515-323; 187-505-099-083-817,0,false,,
070-477-188-646-572,Sensory Augmentation for Navigation in Difficult Urban Environments by People With Visual Impairment,,,dissertation,,,,,Anthony Denis Johnston,"Independent mobility in completing such tasks as walking through a town centre is taken for granted by well-bodied individuals. However, for those with a disability such as impairment of vision, mobility and navigation can become challenging tasks not easily undertaken. The barriers to access for blind and partially sighted individuals are increased when familiar navigational cues are removed in difficult urban environments such as Shared Space. The research consisted of investigating methods of navigation employed by people with visual impairment and designing a device to restore confidence to this group so as to lower the barriers of access to such environments. ; ; Investigation was carried out through the deployment of a questionnaire; discussions with groups representing blind and partially sighted people; and a site visit to Shared Space environments. Statistical analysis was carried out on the results of the questionnaire to ascertain the navigational habits of blind and partially sighted individuals in different environments. From the analysis and the results of the discussions and site visit it was established that it would be socially acceptable to design a secondary aid to navigation that would complement the primary aids of long cane or guide dog. A concept experiment was carried out to test the idea that knowledge about changes in surface colour could help with navigation. ; ; A prototype device that could be used by individuals with visual impairment to increase their confidence when navigating a difficult environment was designed, built and tested. Different programming methods were researched and trialled to effectively use machine vision to provide a solution to analyse video feed from a passive camera and return useful information to a blind or partially sighted user. ; ; The device was tested indoors and outdoors and found to be effective at detecting changes in surface colour. Further work is needed to run the software on a more compact platform such as a mobile phone, but initial results show that the concept is viable and that the barriers that present to blind and partially sighted people navigating difficult urban environments can be much reduced through the use of this technology.",,,,,Software deployment; Human–computer interaction; Shared space; Mobile phone; Test (assessment); Town centre; Visual impairment; Site Visit; Computer science; Machine vision,,,,,https://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.780266 http://oro.open.ac.uk/61730/ https://core.ac.uk/download/210588934.pdf,http://dx.doi.org/10.21954/ou.ro.0000f122,,10.21954/ou.ro.0000f122,2965940429,,0,,0,false,,
071-024-361-518-334,Character Recognition in Scene Images Using MSER and CNN,,2022,book chapter,Communications in Computer and Information Science,18650929; 18650937,Springer Nature Switzerland,Germany,R. P. Rajeswari; B. Aradhana,"Text detection in Scene images has procured significance in recent decade. Due to its diversified applications in blind navigation assistance for Visually impaired, traffic monitoring, Automatic driving assistance systems etc., Text detection has stimulated new research avenues in area of computer vision Text detection is a trivial task because of varying color, font face and size, orientation of text against complex background. A diversity of deep learning techniques are introduced by researchers for graphical text detection in images. The article proposed method consisting of 3 stages. First, we use Otsu’s method for text separation from background. Secondly Text ROI’s are extracted using Maximally stable Ensemble method (MSER). Finally, each extracted text ROI is classified using ConvNets. CNN classifier have been trained to recognize Scene Text Characters.",,,99,107,Artificial intelligence; Computer science; Text detection; Computer vision; Classifier (UML); Pattern recognition (psychology); Character (mathematics); Image (mathematics); Mathematics; Geometry,,,,,,http://dx.doi.org/10.1007/978-3-031-22405-8_8,,10.1007/978-3-031-22405-8_8,,,0,019-143-403-851-343; 023-391-866-456-785; 027-645-007-092-55X; 027-818-841-898-128; 040-003-975-869-318; 040-658-235-156-600; 045-608-632-117-863; 052-268-794-424-765; 068-427-566-495-365; 069-115-852-860-937; 074-677-119-931-515; 076-786-873-576-426; 077-776-073-936-54X; 077-870-374-337-373; 087-619-845-459-207; 088-176-274-390-803; 088-637-088-495-31X; 139-953-993-991-762; 146-721-815-792-26X; 164-938-706-945-925; 171-129-200-331-129,1,false,,
071-069-374-139-842,"Spatial knowledge acquisition: using technology, training, and techniques to enhance spatial learning for two special populations",2006-08-09,2006,journal article,Cognitive Processing,16124782; 16124790,Springer Science and Business Media LLC,Germany,James R. Marston,,7,1,170,170,Sensory cue; Signage; Psychology; Spatial analysis; Set (psychology); Cognition; Cognitive map; Cognitive psychology; Remote infrared audible signage; Spatial contextual awareness,,,,,https://link.springer.com/article/10.1007/s10339-006-0129-8 https://link.springer.com/content/pdf/10.1007%2Fs10339-006-0129-8.pdf,http://dx.doi.org/10.1007/s10339-006-0129-8,,10.1007/s10339-006-0129-8,1983625020,,0,,0,false,,
071-472-632-444-357,COMSNETS - An electronic travel aid for navigation of visually impaired persons,,2011,conference proceedings article,2011 Third International Conference on Communication Systems and Networks (COMSNETS 2011),,IEEE,,Amit Kumar; Rusha Patra; M. Manjunatha; Jayanta Mukhopadhyay; Arun Majumdar,"This paper presents an electronic travel aid for blind people to navigate safely and quickly, an obstacle detection system using ultrasonic sensors and USB camera based visual navigation has been considered. The proposed system detects the obstacles up to 300 cm via sonar and sends feedback (beep sound) to inform the person about its location. In addition to this, an USB webcam is connected with eBox 2300™ Embedded System for capturing the field of view of the user, which is used for finding the properties of the obstacle in particular, in the context of this work, locating a human being. Identification of human presence is based on face detection and cloth texture analysis. The major constraints for these algorithms to run on Embedded System are small image frame (160×120) having reduced faces, limited memory and very less processing time available to achieve real time image processing requirements. The algorithms are implemented in C++ using Visual Studio 5.0 IDE, which runs on Windows CE™ environment.",,,1,5,Artificial intelligence; Obstacle; Context (language use); Computer vision; Computer science; Face detection; USB; Microsoft Visual Studio; Sonar; Image sensor; Image processing,,,,,https://dblp.uni-trier.de/db/conf/comsnets/comsnets2011.html#KumarPMMM11 http://ieeexplore.ieee.org/document/5716517/ https://www.researchgate.net/profile/Rusha_Patra/publication/224219996_An_electronic_travel_aid_for_navigation_of_visually_impaired_persons/links/5500828a0cf2de950a6de7f3.pdf http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.ieee-000005716517 https://ieeexplore.ieee.org/document/5716517/,http://dx.doi.org/10.1109/comsnets.2011.5716517,,10.1109/comsnets.2011.5716517,2109072487,,24,006-956-729-410-739; 013-847-555-426-159; 030-127-369-356-741; 056-773-999-189-634; 063-122-584-706-147; 068-581-156-196-413; 096-803-870-115-728; 101-333-962-375-602; 130-097-955-008-505; 132-287-528-433-371; 133-206-670-623-263; 150-210-084-124-830; 192-600-440-415-958,58,false,,
071-579-320-493-82X,Autonomous Vision-Based Helicopter Flights Through Obstacle Gates,2009-08-21,2009,journal article,Journal of Intelligent and Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Franz Andert; Florian-M. Adolf; Lukas Goormann; Jörg Steffen Dittrich,,57,1,259,280,Engineering; Mission control center; Obstacle; Course (navigation); Ground control station; Sensor fusion; Simulation; Correctness; Obstacle avoidance; Image processing,,,,,https://core.ac.uk/display/30987583 https://link.springer.com/content/pdf/10.1007%2F978-90-481-8764-5_13.pdf https://dialnet.unirioja.es/servlet/articulo?codigo=3115907 https://link.springer.com/chapter/10.1007%2F978-90-481-8764-5_13 https://rd.springer.com/chapter/10.1007/978-90-481-8764-5_13 https://dblp.uni-trier.de/db/journals/jirs/jirs57.html#AndertAGD10 https://dl.acm.org/doi/10.1007/s10846-009-9357-3 https://elib.dlr.de/59770/ https://doi.org/10.1007/s10846-009-9357-3,http://dx.doi.org/10.1007/s10846-009-9357-3,,10.1007/s10846-009-9357-3,1975856230,,1,006-422-963-628-65X; 015-195-808-864-846; 015-498-335-293-091; 018-278-007-671-916; 020-435-815-801-561; 026-568-489-682-671; 028-303-905-295-179; 036-219-411-315-772; 046-197-139-458-851; 050-612-875-928-719; 050-836-329-574-602; 050-837-432-931-270; 058-584-994-083-857; 062-550-542-840-027; 066-891-750-135-972; 068-779-590-174-798; 074-006-536-890-200; 076-049-032-713-000; 084-307-984-123-875; 089-156-568-698-078; 091-893-786-592-003; 094-076-667-917-273; 098-440-015-522-27X; 104-215-281-040-69X; 120-039-472-487-240; 151-687-458-559-324,23,false,,
071-806-151-845-245,Toward a Computer Vision-based Wayfinding Aid for Blind Persons to Access Unfamiliar Indoor Environments.,2012-05-24,2012,journal article,Machine vision and applications,09328092; 14321769,Springer Science and Business Media LLC,Germany,Yingli Tian; Xiaodong Yang; Chucai Yi; Aries Arditi,"Independent travel is a well known challenge for blind and visually impaired persons. In this paper, we propose a proof-of-concept computer vision-based wayfinding aid for blind people to independently access unfamiliar indoor environments. In order to find different rooms (e.g. an office, a lab, or a bathroom) and other building amenities (e.g. an exit or an elevator), we incorporate object detection with text recognition. First we develop a robust and efficient algorithm to detect doors, elevators, and cabinets based on their general geometric shape, by combining edges and corners. The algorithm is general enough to handle large intra-class variations of objects with different appearances among different indoor environments, as well as small inter-class differences between different objects such as doors and door-like cabinets. Next, in order to distinguish intra-class objects (e.g. an office door from a bathroom door), we extract and recognize text information associated with the detected objects. For text recognition, we first extract text regions from signs with multiple colors and possibly complex backgrounds, and then apply character localization and topological analysis to filter out background interference. The extracted text is recognized using off-the-shelf optical character recognition (OCR) software products. The object type, orientation, location, and text information are presented to the blind traveler as speech.",24,3,521,535,Doors; Pattern recognition (psychology); Artificial intelligence; Object detection; Software; Computer vision; Computer science; Optical character recognition; Object type; Geometric shape; Orientation (computer vision),Indoor wayfinding; blind/visually impaired persons; computer vision; object detection; optical character recognition (OCR); text extraction,,,NEI NIH HHS (R21 EY020990) United States; NEI NIH HHS (R21 EY020990-01) United States,https://dx.doi.org/10.1007/s00138-012-0431-7 http://dx.doi.org/10.1007/s00138-012-0431-7 https://europepmc.org/articles/PMC3636776 https://dblp.uni-trier.de/db/journals/mva/mva24.html#TianYYA13 https://link.springer.com/article/10.1007%2Fs00138-012-0431-7 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3636776/ https://rd.springer.com/article/10.1007/s00138-012-0431-7,http://dx.doi.org/10.1007/s00138-012-0431-7,23630409,10.1007/s00138-012-0431-7,2106917970,PMC3636776,8,003-050-926-366-503; 005-003-427-120-577; 007-042-921-016-163; 008-766-288-584-076; 013-056-435-015-274; 014-950-710-125-076; 016-408-213-077-520; 021-272-916-231-150; 021-652-866-302-313; 023-441-936-758-424; 025-723-965-930-316; 034-119-578-405-701; 034-989-947-148-907; 039-300-525-606-76X; 040-032-511-261-446; 046-262-856-533-736; 054-374-371-905-358; 071-186-574-734-606; 071-473-658-566-47X; 071-520-103-682-994; 072-995-846-715-537; 074-324-981-337-749; 077-859-197-993-689; 083-294-350-082-331; 089-733-804-554-103; 091-785-995-981-234; 099-820-000-852-453; 100-260-517-243-811; 100-469-573-054-81X; 106-677-330-222-49X; 114-064-723-555-816; 122-729-034-369-253; 123-333-497-819-500; 123-491-827-241-873; 126-327-812-713-131; 132-445-993-748-812; 133-002-154-376-046; 136-263-796-565-556; 137-767-217-880-697; 145-898-655-711-084; 154-664-177-679-925; 189-691-486-505-033,94,true,,green
071-895-492-147-282,ICCV Workshops - Seeing Without Sight — An Automatic Cognition System Dedicated to Blind and Visually Impaired People,,2017,conference proceedings article,2017 IEEE International Conference on Computer Vision Workshops (ICCVW),,IEEE,,Bogdan Mocanu; Ruxandra Tapu; Titus Zaharia,"In this paper we present an automatic cognition system, based on computer vision algorithms and deep convolutional neural networks, designed to assist the visually impaired (VI) users during navigation in highly dynamic urban scenes. A first feature concerns the realtime detection of various types of objects existent in the outdoor environment relevant from the perspective of a VI person. The objects are followed between successive frames using a novel tracker, which exploits an offline trained neural-network and is able to track generic objects using motion patterns and visual attention models. The system is able to handle occlusions, sudden camera/object movements, rotation or various complex changes. Finally, an object classification module is proposed that exploits the YOLO algorithm and extends it with new categories specific to assistive devices applications. The feedback to VI users is transmitted as a set of acoustic warning messages through bone conducting headphones. The experimental evaluation, performed on the VOT 2016 dataset and on a set of videos acquired with the help of VI users, demonstrates the effectiveness and efficiency of the proposed method.",,,1452,1459,Algorithm design; Perspective (graphical); Artificial intelligence; Set (psychology); Computer vision; Visualization; Computer science; Object (computer science); Feature (computer vision); Convolutional neural network,,,,,https://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w22/Tapu_Seeing_Without_Sight_ICCV_2017_paper.pdf https://dblp.uni-trier.de/db/conf/iccvw/iccvw2017.html#MocanuTZ17,http://dx.doi.org/10.1109/iccvw.2017.172,,10.1109/iccvw.2017.172,2772550156,,0,010-719-927-108-990; 012-225-431-707-546; 015-198-142-832-699; 018-137-915-481-430; 023-868-294-867-153; 025-112-431-982-303; 039-273-747-593-385; 039-274-384-165-289; 040-137-212-207-555; 041-583-554-186-850; 048-446-984-045-429; 049-317-239-158-314; 053-574-289-747-231; 058-268-022-053-157; 061-205-550-296-347; 061-427-166-076-036; 064-043-611-038-916; 068-708-501-300-740; 075-903-419-661-27X; 084-914-723-450-530; 130-215-455-068-370; 139-040-911-677-344; 151-967-570-127-958; 168-445-619-792-756; 170-387-846-090-147; 178-606-451-314-116,14,false,,
072-240-298-491-062,RGB-D Sensor-Based Computer Vision Assistive Technology for Visually Impaired Persons,2014-07-15,2014,book chapter,Computer Vision and Machine Learning with RGB-D Sensors,21916586; 21916594,Springer International Publishing,United States,Yingli Tian,"A computer vision-based wayfinding and navigation aid can improve the mobility of blind and visually impaired people to travel independently. In this chapter, we focus on RGB-D sensor-based computer vision technologies in application to assist blind and visually impaired persons. We first briefly review the existing computer vision based assistive technology for the visually impaired. Then we provide a detailed description of the recent RGB-D sensor based assistive technology to help blind or visually impaired people. Next, we present the prototype system to detect and recognize stairs and pedestrian crosswalks based on RGB-D images. Since both stairs and pedestrian crosswalks are featured by a group of parallel lines, Hough transform is applied to extract the concurrent parallel lines based on the RGB (Red, Green, and Blue) channels. Then, the Depth channel is employed to recognize pedestrian crosswalks and stairs. The detected stairs are further identified as stairs going up (upstairs) and stairs going down (downstairs). The distance between the camera and stairs is also estimated for blind users. The detection and recognition results on our collected datasets demonstrate the effectiveness and efficiency of our developed prototype. We conclude the chapter by the discussion of the future directions.",,,173,194,Artificial intelligence; Hough transform; Pedestrian; Stairs; Visually Impaired Persons; Assistive technology; Computer vision; Focus (computing); Computer science; Cognitive neuroscience of visual object recognition; RGB color model,,,,,https://link.springer.com/chapter/10.1007/978-3-319-08651-4_9/fulltext.html https://link.springer.com/chapter/10.1007/978-3-319-08651-4_9 https://rd.springer.com/chapter/10.1007%2F978-3-319-08651-4_9,http://dx.doi.org/10.1007/978-3-319-08651-4_9,,10.1007/978-3-319-08651-4_9,28174781,,0,003-634-369-751-95X; 004-766-408-593-108; 006-955-399-018-536; 007-499-222-301-387; 007-975-066-276-195; 013-056-435-015-274; 017-612-265-041-399; 017-994-776-387-929; 018-659-243-361-88X; 022-489-327-390-755; 023-928-444-110-407; 027-893-571-523-130; 030-718-617-225-586; 031-368-644-910-588; 032-987-496-927-36X; 035-925-286-660-21X; 037-491-194-003-965; 041-380-323-842-523; 043-056-567-014-861; 044-659-512-375-773; 045-583-440-225-239; 046-262-856-533-736; 047-126-935-003-311; 050-264-496-787-575; 051-612-966-213-045; 051-886-701-586-661; 051-970-317-843-151; 056-451-994-325-439; 057-731-809-827-780; 059-647-236-855-012; 059-930-393-216-111; 060-041-403-626-524; 062-957-059-042-10X; 071-473-658-566-47X; 071-806-151-845-245; 075-903-419-661-27X; 076-133-810-427-53X; 085-102-500-348-494; 092-391-305-190-481; 093-256-914-961-610; 096-014-228-878-398; 099-150-824-995-115; 101-767-617-199-036; 105-969-976-015-690; 106-976-850-477-023; 109-617-690-629-802; 112-365-549-669-557; 116-976-189-445-151; 120-499-391-106-298; 121-674-583-467-559; 135-646-014-794-306; 148-405-057-283-922; 150-597-344-978-541; 163-109-750-928-888; 166-719-051-375-011; 168-052-814-589-107; 195-842-744-142-232,22,false,,
072-556-735-586-395,ICRA - A Visual Positioning System for Indoor Blind Navigation,,2020,conference proceedings article,2020 IEEE International Conference on Robotics and Automation (ICRA),,IEEE,,He Zhang; Cang Ye,"This paper presents a visual positioning system (VPS) for real-time pose estimation of a robotic navigation aid (RNA) for assistive navigation. The core of the VPS is a new method called depth-enhanced visual-inertial odometry (DVIO) that uses an RGB-D camera and an inertial measurement unit (IMU) to estimate the RNA’s pose. The DVIO method extracts the geometric feature (the floor plane) from the camera’s depth data and integrates its measurement residuals with that of the visual features and the inertial data in a graph optimization framework for pose estimation. A new measure based on the Sampson error is introduced to describe the measurement residuals of the near-range visual features with a known depth and that of the far-range visual features whose depths are unknown. The measure allows for the incorporation of both types of visual features into graph optimization. The use of the geometric feature and the Sampson error improves pose estimation accuracy and precision. The DVIO method is paired with a particle filter localization (PFL) method to locate the RNA in a 2D floor plan and the information is used to guide a visually impaired person. The PFL reduces the RNA’s position and heading error by aligning the camera’s depth data with the floor plan map. Together, the DVIO and the PFL allow for accurate pose estimation for wayfinding and 3D mapping for obstacle avoidance. Experimental results demonstrate the usefulness of the RNA in assistive navigation in indoor spaces.",,,9079,9085,Monte Carlo localization; Artificial intelligence; Inertial frame of reference; Computer vision; Computer science; Odometry; Pose; Feature extraction; Obstacle avoidance; Feature (computer vision); Inertial measurement unit,,,,,https://dblp.uni-trier.de/db/conf/icra/icra2020.html#ZhangY20,http://dx.doi.org/10.1109/icra40945.2020.9196782,,10.1109/icra40945.2020.9196782,3089613853,,0,000-175-937-006-386; 001-360-928-241-406; 003-086-871-090-793; 007-223-103-783-187; 007-831-437-996-843; 009-491-671-396-314; 012-081-521-655-479; 023-271-995-456-910; 024-964-397-377-997; 026-672-312-718-982; 037-403-741-955-39X; 038-597-169-941-177; 044-949-263-620-170; 048-510-019-754-753; 050-095-187-695-437; 059-732-088-154-391; 064-521-070-547-235; 065-276-856-923-220; 068-708-501-300-740; 072-240-115-741-103; 074-006-536-890-200; 074-820-380-371-928; 075-519-293-886-898; 075-644-152-884-63X; 087-583-672-152-087; 099-911-359-242-620; 105-822-868-525-267; 146-380-366-847-727; 166-405-745-827-631; 169-675-860-937-325; 170-387-846-090-147; 193-123-800-849-774,14,false,,
072-928-351-147-021,Towards efficient mobile image-guided navigation through removal of outliers,2016-12-07,2016,journal article,EURASIP Journal on Image and Video Processing,16875281; 16875176,Springer Science and Business Media LLC,Egypt,Ekaterina Sirazitdinova; Stephan M. Jonas; Jan Lensen; Deyvid Kochanov; Richard Houben; Hans Slijp; Thomas M. Deserno,"A novel approach for positioning using smartphones and image processing techniques is developed. Using structure from motion, 3D reconstructions of given tracks are created and stored as sparse point clouds. Query images are matched later to these 3D models. High computational costs of image matching and limited storage require compressing point clouds without loss of positioning performance. In this work, localization is improved and memory and storage requirements are minimized. We assumed that the computational speed and, at the same time, storage requirements benefit from reducing the number of points with appropriate outlier detection. In particular, our hypothesis was that positioning accuracy is maintained while reducing outliers in a reconstructed model. To evaluate the hypothesis, three methods were compared: (i) density-based (Sotoodeh, International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences XXXVI-5, 2006), (ii) connectivity-based (Wang et al. Comput Graph Forum 32(5):207–10, 2013), and (iii) our distance-based approach. In tenfold cross-validation, applied to a pre-reconstructed reference 3D model, localization accuracy was measured. In each new model, the positions of test images were identified and compared to the according positions in the reference model. We observed that outlier removal has a positive impact on matching run-time and storage requirements, while there are no significant differences in the localization error within the methods. That confirmed our initial hypothesis and allows mobile application of image-based positioning.",2016,1,43,,Photogrammetry; Point cloud; Graph (abstract data type); Structure from motion; Reference model; Artificial intelligence; Pattern recognition; Computer vision; Computer science; Anomaly detection; Outlier; Image processing,,,,German Federal Ministry of Education and Research; European Commission's Ambient Assisted Living (AAL) Joint Program ICT for Ageing Well,https://link.springer.com/10.1186/s13640-016-0146-1 https://link.springer.com/content/pdf/10.1186%2Fs13640-016-0146-1.pdf http://jivp-eurasipjournals.springeropen.com/articles/10.1186/s13640-016-0146-1 https://link.springer.com/article/10.1186/s13640-016-0146-1 https://paperity.org/p/78404798/towards-efficient-mobile-image-guided-navigation-through-removal-of-outliers https://jivp-eurasipjournals.springeropen.com/articles/10.1186/s13640-016-0146-1 https://publications.rwth-aachen.de/record/682245 https://dblp.uni-trier.de/db/journals/ejivp/ejivp2016.html#SirazitdinovaJL16 https://core.ac.uk/display/81549303,http://dx.doi.org/10.1186/s13640-016-0146-1,,10.1186/s13640-016-0146-1,2560307387,,0,003-204-682-225-868; 003-788-045-078-262; 004-796-220-360-369; 010-780-598-905-528; 012-642-196-605-159; 014-700-540-000-93X; 017-647-894-467-755; 019-628-108-233-150; 019-957-068-329-154; 021-768-822-457-596; 021-855-372-553-446; 026-345-407-862-479; 028-010-237-748-128; 028-303-905-295-179; 030-530-604-211-746; 033-055-318-123-963; 035-291-375-623-97X; 035-364-893-794-909; 040-514-213-030-932; 052-628-486-362-853; 054-777-831-929-040; 055-515-472-737-042; 055-676-274-953-454; 056-579-047-092-860; 077-123-406-112-277; 081-632-902-796-372; 084-194-833-199-807; 097-488-074-824-506; 099-643-392-623-259; 103-458-488-093-30X; 126-396-237-183-697; 129-996-200-177-603; 145-289-402-804-641; 152-801-539-447-124; 154-277-278-325-691; 172-431-933-985-473; 179-133-561-273-318; 181-797-068-816-79X; 182-365-275-328-331; 192-751-999-674-131; 195-559-240-248-101; 197-045-902-530-697,1,true,cc-by,gold
072-939-392-238-724,The development of an augmented reality audio application for visually impaired persons,2022-11-09,2022,journal article,Multimedia Tools and Applications,13807501; 15737721,Springer Science and Business Media LLC,Netherlands,Alper Tunga Akın; Çetin Cömert,,82,11,17493,17512,Computer science; Obstacle; Augmented reality; Visually impaired; Artificial intelligence; Deep learning; Object (grammar); Human–computer interaction; Monocular; Object detection; Computer vision; Audio feedback; Multimedia; Pattern recognition (psychology); Physics; Political science; Acoustics; Law,,,,Karadeniz Teknik Üniversitesi,,http://dx.doi.org/10.1007/s11042-022-14134-x,,10.1007/s11042-022-14134-x,,,0,004-216-069-855-831; 005-427-234-377-412; 006-926-779-027-538; 016-446-586-220-032; 020-962-225-757-788; 025-522-385-395-133; 028-143-078-747-070; 029-301-017-431-280; 031-218-334-653-826; 031-797-761-047-862; 046-821-320-195-132; 047-326-510-583-783; 049-317-239-158-314; 061-669-945-412-088; 061-946-877-986-006; 068-526-647-480-345; 069-480-623-909-181; 079-310-940-223-758; 093-548-249-489-049; 105-169-300-517-193; 106-264-944-278-433; 121-350-695-153-190; 162-646-338-130-900; 180-515-374-647-90X,3,false,,
073-180-689-038-913,RiTA - Intelligent Smart Glass for Visually Impaired Using Deep Learning Machine Vision Techniques and Robot Operating System (ROS),2018-05-31,2018,journal article,Robot Intelligence Technology and Applications 5,21945357; 21945365,Springer International Publishing,,Aswath Suresh; Chetan Arora; Debrup Laha; Dhruv Gaba; Siddhant Bhambri,"The Smart Glass represents potential aid for people who are visually impaired that might lead to improvements in the quality of life. The smart glass is for the people who need to navigate independently and feel socially convenient and secure while they do so. It is based on the simple idea that blind people do not want to stand out while using tools for help. This paper focuses on the significant work done in the field of wearable electronics and the features which comes as add-ons. The Smart glass consists of ultrasonic sensors to detect the object ahead in real-time and feeds the Raspberry for analysis of the object whether it is an obstacle or a person. It can also assist the person on whether the object is closing in very fast and if so, provides a warning through vibrations in the recognized direction. It has an added feature of GSM, which can assist the person to make a call during an emergency situation. The software framework management of the whole system is controlled using Robot Operating System (ROS). It is developed using ROS catkin workspace with necessary packages and nodes. The ROS was loaded on to Raspberry Pi with Ubuntu Mate.",,,99,112,Human–computer interaction; Smart glass; Field (computer science); Computer science; Machine vision; Wearable technology; Workspace; GSM; Object (computer science); Software framework,,,,,https://dblp.uni-trier.de/db/conf/rita/rita2017.html#SureshALGB17 https://rd.springer.com/chapter/10.1007/978-3-319-78452-6_10 https://link.springer.com/chapter/10.1007/978-3-319-78452-6_10,http://dx.doi.org/10.1007/978-3-319-78452-6_10,,10.1007/978-3-319-78452-6_10,2806619669,,0,013-056-435-015-274; 031-218-334-653-826; 049-317-239-158-314; 052-220-278-389-016; 092-178-637-451-358; 146-093-154-688-224; 171-091-935-415-285,15,false,,
073-218-969-526-162,Assistive Framework for Automatic Detection of All the Zones in Retinopathy of Prematurity Using Deep Learning,2021-07-08,2021,journal article,Journal of digital imaging,1618727x; 08971889,Springer Science and Business Media LLC,Germany,Ranjana Agrawal; Sucheta Kulkarni; Rahee Walambe; Ketan Kotecha,"Retinopathy of prematurity (ROP) is a potentially blinding disorder seen in low birth weight preterm infants. In India, the burden of ROP is high, with nearly 200,000 premature infants at risk. Early detection through screening and treatment can prevent this blindness. The automatic screening systems developed so far can detect “severe ROP” or “plus disease,” but this information does not help schedule follow-up. Identifying vascularized retinal zones and detecting the ROP stage is essential for follow-up or discharge from screening. There is no automatic system to assist these crucial decisions to the best of the authors’ knowledge. The low contrast of images, incompletely developed vessels, macular structure, and lack of public data sets are a few challenges in creating such a system. In this paper, a novel method using an ensemble of “U-Network” and “Circle Hough Transform” is developed to detect zones I, II, and III from retinal images in which macula is not developed. The model developed is generic and trained on mixed images of different sizes. It detects zones in images of variable sizes captured by two different imaging systems with an accuracy of 98%. All images of the test set (including the low-quality images) are considered. The time taken for training was only 14 min, and a single image was tested in 30 ms. The present study can help medical experts interpret retinal vascular status correctly and reduce subjective variation in diagnosis.",34,4,932,947,Deep learning; Artificial intelligence; Test set; Pattern recognition; Hough transform; Blinding; Retinopathy of prematurity; Early detection; Single image; Computer science; Segmentation,Artificial Intelligence; Automatic zone detection; Machine learning; Retinopathy of prematurity(ROP); Segmentation; U-Net,"Deep Learning; Humans; Infant, Low Birth Weight; Infant, Newborn; Infant, Premature; Retina/diagnostic imaging; Retinopathy of Prematurity/diagnostic imaging",,,https://europepmc.org/article/MED/34240273 https://pubmed.ncbi.nlm.nih.gov/34240273/ https://www.ncbi.nlm.nih.gov/pubmed/34240273 https://link.springer.com/article/10.1007/s10278-021-00477-8,http://dx.doi.org/10.1007/s10278-021-00477-8,34240273,10.1007/s10278-021-00477-8,3182572893,PMC8455784,0,001-228-456-541-188; 002-696-701-605-135; 009-133-803-409-050; 011-584-997-657-021; 011-728-884-565-534; 018-306-710-478-734; 023-024-890-535-93X; 025-103-085-377-957; 025-240-259-513-519; 025-326-135-984-439; 028-178-630-811-911; 028-696-653-010-949; 029-025-087-472-004; 031-134-154-951-131; 033-756-653-910-456; 034-425-937-361-479; 035-321-502-617-973; 036-395-183-059-57X; 036-488-695-953-789; 037-342-746-529-457; 038-640-281-813-248; 040-062-586-626-451; 043-137-264-735-834; 044-420-164-121-894; 045-690-709-641-985; 048-385-522-660-949; 057-182-246-762-119; 061-521-264-710-206; 062-038-075-869-011; 062-193-768-537-159; 065-138-746-914-213; 069-795-341-071-276; 071-325-691-928-028; 086-305-021-524-864; 098-322-020-642-391; 106-675-509-959-644; 123-948-819-759-119; 126-192-139-137-668; 134-264-524-062-586; 149-399-271-158-987; 157-425-818-727-845; 167-131-435-704-717; 195-091-532-336-938,21,true,,unknown
073-393-946-682-259,Indoor Localization for the Blind Based on the Fusion of a Metaheuristic Algorithm with a Neural Network Using Energy-Efficient WSN,2022-09-08,2022,journal article,Arabian Journal for Science and Engineering,2193567x; 21914281; 13198025,Springer Science and Business Media LLC,Saudi Arabia,Sadik Kamel Gharghan; Rasha Diaa Al-Kafaji; Siraj Qays Mahdi; Salah L. Zubaidi; Hussein Mohammed Ridha,,48,5,6025,6052,Particle swarm optimization; Artificial neural network; Algorithm; Computer science; Bat algorithm; Artificial intelligence; Wireless sensor network; Energy (signal processing); Search algorithm; Metaheuristic; Backtracking; Mathematics; Statistics; Computer network,,,,,,http://dx.doi.org/10.1007/s13369-022-07188-4,,10.1007/s13369-022-07188-4,,,0,002-290-678-987-947; 002-526-278-924-179; 003-757-720-352-973; 003-787-428-155-830; 005-602-900-806-406; 006-675-659-208-672; 007-026-602-547-75X; 008-139-948-663-237; 008-246-865-454-88X; 009-693-721-124-364; 010-653-842-426-00X; 012-657-555-589-130; 013-838-638-578-065; 014-600-336-375-250; 015-312-407-884-811; 015-842-322-340-610; 015-907-391-152-335; 016-038-460-651-045; 016-196-933-972-126; 018-994-167-217-701; 019-772-295-164-934; 020-260-934-897-437; 020-611-452-933-716; 023-686-282-607-007; 025-008-107-631-157; 028-164-592-895-124; 028-744-192-669-778; 029-927-540-699-18X; 031-570-291-086-683; 032-102-165-870-302; 033-556-646-765-506; 033-827-148-322-587; 033-878-874-590-631; 036-783-192-936-84X; 038-654-361-156-61X; 040-378-968-470-270; 040-940-319-245-132; 041-519-553-611-468; 046-313-586-891-45X; 046-625-623-299-790; 053-174-742-652-87X; 053-570-829-728-958; 054-388-859-104-594; 056-365-639-842-406; 056-468-168-036-548; 057-847-555-860-541; 058-205-243-901-801; 060-350-025-722-226; 060-593-237-056-786; 061-099-258-274-056; 061-895-056-540-317; 061-925-252-984-446; 062-778-105-366-495; 064-656-359-548-835; 065-424-696-572-354; 069-689-572-820-466; 070-932-524-574-929; 071-399-837-875-750; 071-874-314-568-962; 073-875-311-990-209; 074-158-550-986-957; 075-154-479-485-276; 084-835-313-002-959; 085-286-815-313-383; 086-042-914-721-505; 086-862-517-589-321; 088-414-484-880-916; 088-421-683-612-944; 088-824-929-291-009; 092-207-250-516-874; 092-741-758-425-615; 096-939-794-456-629; 098-910-337-444-079; 102-193-701-204-965; 104-263-237-043-793; 106-127-602-090-43X; 108-481-773-656-48X; 110-831-712-365-194; 113-668-691-391-052; 117-654-190-112-471; 120-823-281-233-415; 122-172-181-925-576; 126-294-465-741-446; 126-930-972-570-981; 127-070-672-662-621; 128-106-785-921-069; 144-675-777-893-088; 161-183-693-463-019; 161-431-870-692-921; 161-893-739-842-759; 167-116-989-805-75X; 173-312-792-945-957; 191-491-710-824-043,7,false,,
073-830-720-957-961,Computer Vision and IoT-Based Smart System for Visually Impaired People,2023-04-21,2023,conference proceedings article,2023 International Conference on Artificial Intelligence and Applications (ICAIA) Alliance Technology Conference (ATCON-1),,IEEE,,Rajlakshmi Ghatkamble; K Ratish Kumar; S John Hrithik; J Harshith Kumar; P S Sujan,"Moving from one location to another is one of the biggest issues that visually impaired individuals have. For these folks, walking canes that are readily accessible solely act as obstacle sensors. Long overdue is the requirement for an affordable guiding and navigation system for the blind. This paper’s major goal is to use ultrasonic technology to broaden the electronic mobility aid for blind and visually impaired walkers. The research described in this article involves designing and implementing an ultrasonic navigation system to offer blind pedestrians completely autonomous obstacle avoidance as well as auditory and tactile feedback. A camera will be used to detect objects higher than knee height. This blind steering method is risk-free, accurate, and efficient.",,,,,Obstacle; Visually impaired; Computer science; Computer vision; Obstacle avoidance; Navigation system; Artificial intelligence; Human–computer interaction; Ultrasonic sensor; Simulation; Physics; Political science; Acoustics; Robot; Law; Mobile robot,,,,,,http://dx.doi.org/10.1109/icaia57370.2023.10169589,,10.1109/icaia57370.2023.10169589,,,0,007-621-951-852-169; 014-004-574-193-910; 014-879-710-106-693; 022-874-978-020-281; 034-936-259-439-98X; 046-079-221-945-969; 057-625-956-966-343; 069-663-105-715-784; 086-663-811-050-106; 106-587-878-038-509; 118-227-577-262-001; 134-544-206-108-733; 137-032-010-877-936; 147-832-250-508-396; 162-959-573-965-614,0,false,,
073-848-308-083-663,Seeing Eye Phone: a smart phone-based indoor localization and guidance system for the visually impaired,2013-11-12,2013,journal article,Machine Vision and Applications,09328092; 14321769,Springer Science and Business Media LLC,Germany,Dong Zhang; Dah-Jye Lee; Brandon Taylor,,25,3,811,822,Pattern recognition (psychology); Artificial intelligence; Direct linear transformation; Guidance system; Task (computing); Phone; Computer vision; Computer science; Pose; Machine vision; Orientation (computer vision),,,,,https://link.springer.com/article/10.1007/s00138-013-0575-0/fulltext.html https://link.springer.com/article/10.1007/s00138-013-0575-0 https://dblp.uni-trier.de/db/journals/mva/mva25.html#ZhangLT14,http://dx.doi.org/10.1007/s00138-013-0575-0,,10.1007/s00138-013-0575-0,1975715077,,0,012-642-196-605-159; 013-921-202-169-313; 019-925-883-027-48X; 027-105-986-240-132; 028-709-784-462-857; 035-291-375-623-97X; 041-902-102-240-495; 061-190-872-413-287; 068-858-525-190-391; 073-928-284-197-323; 078-877-431-259-702; 102-551-479-494-136; 104-419-250-459-046; 104-694-342-259-021; 110-650-958-929-874; 121-674-470-605-281; 128-777-751-513-793; 166-703-010-015-697; 181-797-068-816-79X; 186-895-914-002-666; 193-123-800-849-774; 193-325-954-560-370; 198-082-019-164-025,15,false,,
073-941-752-526-439,Empowering the Blind: An AI Driven Indoor Assistance for Visually Impaired,2024-04-30,2024,journal article,International Journal for Research in Applied Science and Engineering Technology,23219653,International Journal for Research in Applied Science and Engineering Technology (IJRASET),,Mrs. Shraddha. P. Mankar; Riya Gawande; Dipanshu Sankhala; Siddhi Vispute; Nikhita Watpal,"<jats:p>Abstract: This review paper presents the design and development of a comprehensive mobile application aimed at enhancing the daily lives of visually impaired individuals. The proposed mobile app offers real-time assistance for various visual recognition tasks, including object detection, distance estimation, currency recognition, barcode detection, color recognition, and emotion analysis. The primary objective of this application is to provide visually impaired users with a powerful tool to navigate their surroundings, identify objects and currency, scan barcodes, discern colors, and even gauge the emotions of those they interact with. The app integrates pre-trained deep learning models for object detection and facial emotion analysis, computer vision techniques for distance estimation, and optical character recognition for currency recognition. User feedback and engagement are central to the ongoing improvement of the application, ensuring that it remains a valuable resource for the community it serves. Ethical considerations and privacy concerns are also addressed, with a focus on data security and user privacy. By presenting the development and functionalities of this app, this review paper not only contributes to the field of assistive technology but also underscores the importance of harnessing cutting-edge technology to improve the quality of life for visually impaired individuals.</jats:p>",12,4,5702,5711,Visually impaired; Computer science; Psychology; Human–computer interaction,,,,,,http://dx.doi.org/10.22214/ijraset.2024.61273,,10.22214/ijraset.2024.61273,,,0,,0,true,,gold
074-004-764-807-22X,Obstacle detection and avoidance module for the blind,,2016,conference proceedings article,2016 World Automation Congress (WAC),,IEEE,,Paulo Costa; Hugo Fernandes; João Barroso; Hugo Paredes; Leontios J. Hadjileontiadis,"Assistive technology enables people to achieve independence when performing daily tasks and it enhances their overall quality of life. Visual information is the basis for most navigational tasks, so visually impaired individuals are at disadvantage due to the lack of sufficient information about their surrounding environment. With recent advances in inclusive technology it is possible to extend the support given to people with visual disabilities in terms of their mobility. In this context we present and describe a wearable system (Blavigator project), whose global objective is to assist visually impaired people in their navigation on indoor and outdoor environments. This paper is focused mainly on the Computer Vision module of the Blavigator prototype. We propose an object collision detection algorithm based on stereo vision. The proposed algorithm uses Peano-Hilbert Ensemble Empirical Mode Decomposition (PH-EEMD) for disparity image processing and a two layer disparity image segmentation to detect nearby objects. Using the adaptive ensemble empirical mode decomposition (EEMD) image analysis real time is not achieved, with PH-EEMD results on a fast implementation suitable for real time applications.",,,1,6,Image segmentation; Wearable computer; Artificial intelligence; Collision detection; Context (language use); Computer vision; Visualization; Computer science; Global Positioning System; Stereopsis; Image processing,,,,,https://ieeexplore.ieee.org/document/7582990/ http://ieeexplore.ieee.org/abstract/document/7582990,http://dx.doi.org/10.1109/wac.2016.7582990,,10.1109/wac.2016.7582990,2527888204,,0,001-702-427-653-717; 013-490-800-078-84X; 031-671-446-094-307; 033-326-857-003-882; 046-656-110-039-960; 047-320-883-602-548; 051-065-738-489-461; 058-468-795-176-403; 059-225-311-317-231; 063-500-826-600-173; 083-853-269-246-35X; 086-490-490-750-001; 105-582-110-126-078; 141-489-442-115-69X; 153-471-712-074-630; 171-967-210-532-103,10,false,,
074-087-317-513-170,Spatial understanding and temporalcorrelation for a mobile robot,,2000,journal article,Spatial Cognition and Computation,13875868,Springer Science and Business Media LLC,United States,K. Madhava Krishna; Prem Kalra,,2,3,219,259,Network model; Artificial intelligence; Network architecture; Mobile robot; Mobile robot navigation; Fuzzy rule; Landmark; Computer science; Robot; Classifier (UML),,,,,https://link.springer.com/content/pdf/10.1023%2FA%3A1013129628305.pdf https://doi.org/10.1023/A:1013129628305 https://link.springer.com/article/10.1023/A%3A1013129628305 https://dblp.uni-trier.de/db/journals/scc/scc2.html#KrishnaK00,http://dx.doi.org/10.1023/a:1013129628305,,10.1023/a:1013129628305,129910331,,0,008-900-169-370-213; 014-646-146-843-184; 021-246-388-906-820; 024-108-784-234-126; 026-223-883-616-935; 026-419-660-230-458; 028-873-996-024-116; 030-525-521-264-301; 034-160-610-653-890; 035-656-623-096-70X; 037-260-303-484-422; 038-169-771-049-949; 043-852-075-942-394; 045-210-201-864-98X; 048-138-440-934-741; 059-280-816-994-635; 060-075-467-134-874; 070-098-593-959-484; 073-513-002-351-309; 073-722-974-726-137; 089-880-896-151-343; 090-880-548-643-611; 091-267-470-450-756; 095-257-941-816-926; 098-957-633-128-861; 107-405-434-755-87X; 109-769-422-320-49X; 112-745-935-179-737; 114-234-505-786-113; 115-385-447-327-06X; 123-273-858-461-486; 139-693-065-598-118; 148-160-373-867-871; 150-991-201-839-320; 157-535-800-629-996; 159-469-160-319-040; 178-242-627-182-109,3,false,,
074-132-202-006-187,ICCHP - ODILIA - A Mobility Concept for the Visually Impaired,,,book chapter,Lecture Notes in Computer Science,03029743; 16113349,Springer Berlin Heidelberg,Germany,Bernhard Mayerhofer; Bettina Pressl; Manfred Wieser,"A navigation system for visually impaired people has to take into account the special requirements of these users. Within this group, there is also need for a customizable man-machine interface tailored to the individual. It has to be suitable for people depending on orientation by the sense of hearing or on tactile orientation, always avoiding disturbance of the user's remaining senses. On the other side, the hardware for data input and on-trip control should not exceed a certain size and weight. To be accepted for daily use, the overall system must not be stigmatizing the user. Stigmatizing means, that visually impaired users often do not want to be apparently distinguishable from the average pedestrian by wearing noticeable equipment. Another point is reliability and accuracy of the system which are essential features, because a blind person can be reliant on the system when entering an unknown area. The navigation system developed in ODILIA should provide accuracy, reliability of routing and guidance and the possibility to give the user an impression of the surrounding area.",,,1109,1116,Dead reckoning; Human–computer interaction; Interface (computing); Artificial intelligence; Point (typography); Control (management); Pedestrian; Navigation system; Orientation (mental); Computer vision; Computer science; Reliability (statistics),,,,,https://rd.springer.com/chapter/10.1007/978-3-540-70540-6_166 https://dblp.uni-trier.de/db/conf/icchp/icchp2008.html#MayerhoferPW08 http://dblp.uni-trier.de/db/conf/icchp/icchp2008.html#MayerhoferPW08 https://graz.pure.elsevier.com/en/publications/odilia-a-mobility-concept-for-the-visually-impaired https://link.springer.com/chapter/10.1007/978-3-540-70540-6_166 https://link.springer.com/content/pdf/10.1007%2F978-3-540-70540-6_166.pdf https://doi.org/10.1007/978-3-540-70540-6_166,http://dx.doi.org/10.1007/978-3-540-70540-6_166,,10.1007/978-3-540-70540-6_166,1613506192,,1,003-090-571-373-742; 042-348-220-328-560; 051-612-966-213-045; 057-427-285-007-231; 067-304-564-682-259; 093-743-527-930-29X; 173-906-282-690-390; 198-777-023-439-918,14,false,,
074-166-065-494-166,Visual rehabilitation,2023-04-25,2023,journal article,Kerala Journal of Ophthalmology,09766677; 26673983,Medknow,,K.C Rajini,"Visual impairment is an important public health problem and poses an enormous global financial burden due to productivity losses. Given the high financial burden of low vision and blindness in its own population, India has a great need to strengthen its low-vision services and better equip its eye specialists. In a global initiative, the World Health Organization (WHO) launched VISION 2020: The Right to Sight, which prioritized preventable blindness. This initiative sought and still seeks to increase awareness of primarily preventable eye diseases; its large-scale aim is to fully eliminate avoidable blindness. Although awareness of low-vision services among ophthalmic professionals in India has increased, these services are not robust and strong programs have not yet materialized. Globally, at least 2.2 billion people have a near vision or distant vision impairment. In at least one billion of these cases, vision impairment was preventable or remained untreated. The most common causes of low vision in adults are the following: age-related macular degeneration, glaucoma, cataracts, and diabetic retinopathy. The International Classification of Diseases 11 (2018) classifies vision impairment into two groups: distance and near-presenting vision impairment. Distant vision impairment is classified on the following scale: Mild—visual acuity worse than 6/12 to 6/18 Moderate—visual acuity worse than 6/18 to 6/60 Severe—visual acuity worse than 6/60 to 3/60 Blindness—visual acuity worse than 3/60. Near vision impairment is classified as near visual acuity worse than N6. Despite progress in therapeutic approaches, many diseases of the eyes and visual pathways still cause persistent visual deficits that make everyday life for those suffering from visual impairment rife with challenges. Apart from direct functional impact on the sufferer, visual impairment increases the risk of injury and of contracting further miscellaneous disorders. Moreover, when vision loss coexists with other health problems such as hearing loss or cognitive deficiency, the resulting consequences are usually severe. While the majority of people with vision impairment and blindness are over the age of 50 years, vision loss can affect people of all ages. In children, refractive errors and squinting are the most common expressions of visual impairment. In addition to affecting their health, children with visual impairments face many damaging social problems. These commonly include: being asked hurtful questions, being bullied by their peers, being pitied as a child with a disability, and being overprotected by parents, which can further hurt a child’s social development. Research has found that 65% of the general population are visual learners, meaning they need to see information to retain it. Hence, reduced visual input resulting from vision impairment can present a significant barrier to a child’s development and thus to their future success. It is clear that failing to win the global struggle against visual impairment creates financial and personal difficulties in the present. Worse, this issue is not stagnant. It is highly likely that the number of people living with low vision will increase over the coming decade. While visual impairment is a complex issue, rehabilitation aims to compensate for these limitations by optimizing residual vision. Rehabilitation measures in blind or visually impaired patients include a wide array of options, including the following: vision aids, tactile aids, and acoustic aids. These aids are for reading impairment, obstacle detection, and electronic guidance systems for orientation impairment. Training given to visually impaired people typically centers on learning how to use aids, learning possible compensatory behaviors, orientation and mobility training, and saccade training. Organizations and healthcare institutions, however, do not stop assisting visually impaired individuals at this stage of rehabilitation. Social counseling can be just as important as training visually impaired people on how to use these rehabilitation measures. Social counseling for visually impaired people can include managing visual impairment’s effect on life at school, at work, in leisure activities and independent living; for registering as disabled, in seeking state support due to blindness and in providing access to self-help groups. In India, the state of Kerala has made commendable progress in the prevention of avoidable blindness over the past decade. However, accurate data on the accessibility and effectiveness of low-vision services from the state are currently lacking. The challenge of rehabilitating the increasing number of visually impaired is enormous and requires appropriate strategic planning and efficient use of available resources. Still, the Kerala Government, Kerala Society of Ophthalmic Surgeons (KSOS), and some NGOs have launched statewide projects to address visual impairment in the populations they serve. The Kerala government launched the “Kazcha” project to empower visually impaired people. Most visually impaired people are unaware of technological advances, like screen reading, that can be of great help to them. Such technologies can help them overcome many difficulties. The “Kazcha” project will also teach visually impaired people how to use mobile applications that can be of help in their day-to-day lives and equip them with the latest technologies. The KSOS envisages starting Low Vision Centers in all districts in Kerala as a part of its Low Vision Project, which was launched during Drishti 2018, the annual conference of KSOS at Kannur. KSOS will bear the expense of training optometrists at Aravind Eyecare Systems, Madurai, and help partner hospitals to set up Low Vision Centers. “Punarjyothi,” the low vision project is the brainchild of Professor P.S. Girijadevi, former Director of the Regional Institute of Ophthalmology (RIO), Trivandrum. It is being taken forward in a big way by the alumni of RIO. The Society for the Rehabilitation of the Visually Challenged (SRVC), a registered NGO, has been working in the field of rehabilitation of the visually challenged (VC) since 2002. SRVC has taken a different approach by pursuing new avenues for employing the VC in non-conventional areas; taking advantage of existing technology; and offering the VC necessary training. Through empowering VC individuals and by creating public awareness, SRVC also aims to play an active role in creating an inclusive atmosphere and thus aid the VC in social integration. Projects and possible occupations identified for VC individuals under this project are orchestra, data entry, telemarketing, medical transcription, call center jobs, physiotherapy, counseling, foreign language assistance, tea tasting, wine tasting, assaying, and promoting sporting abilities in blind football, blind cricket, powerlifting, and other athletic disciplines. Tiffany Brar is an Indian community service worker who became blind as an infant due to oxygen toxicity. She is the founder of the Jyothirgamaya Foundation, a non-profit organization that teaches life skills to blind people of all ages. As has been shown, the spectrum of visual rehabilitation has become much wider in recent years and is no longer limited to the eyes. It now includes a new understanding of functional compensatory cortical plasticity in the adult visual system. This understanding gives healthcare professionals a bigger toolkit for providing treatment and rehabilitation options. For example, electronic canes can help patients to detect nearby objects. More recently, optical character recognition devices translate visual information—such as text, monetary denominations, and faces—into spoken words. Unobtrusively designed, the device includes a miniature camera clipped onto the wearer’s eyeglass frame. Further, several studies report promising results from repetitive trans-cranial stimulation, which is an example of a noninvasive method for restoring vision. Trans-cranial direct current stimulation is another technique to induce changes in cortical excitability. If repeated in daily sessions, it may modulate brain network plasticity, helping to restore impaired vision. Direct treatments, like retinal implants, appear to increase vision within acceptable safety profiles. Patients who have received retinal implants report significant improvement in their quality of life. Artificial vision has already shown efficacy in patients suffering from retinal photoreceptors degeneration. After a thorough diagnostic evaluation of a person’s individual visual impairment and analysis of its effects, the chosen rehabilitative measures can usually restore reading ability and improve spatial orientation, thereby enhancing the patient’s independence and quality of life. As the demand for visual rehabilitation is increasing, healthcare institutions and medical professionals must take steps to make treatment possibilities more widely available. Furthermore, as the scientific basis for visual rehabilitation is currently inadequate in some areas, more research in the field will be needed. However, the field is ripe for advancement. Optometrists along with ophthalmologists and social therapists can effectively tackle these demands. We have paramedics posted even in Primary Health Centres, India’s state-owned healthcare facilities whose assistance is valuable. Efficient utilization of the potential of all the stakeholders can bring great changes in the lives of people with visual impairment and blindness. Financial support and sponsorship Nil. Conflicts of interest There are no conflicts of interest.",35,1,5,7,Rehabilitation; Psychology; Physical medicine and rehabilitation; Medicine; Neuroscience,,,,,,http://dx.doi.org/10.4103/kjo.kjo_134_22,,10.4103/kjo.kjo_134_22,,,0,,0,true,cc-by-nc-sa,gold
074-319-407-634-910,Vision based localization stereo rig calibration and rectification,,2017,conference proceedings article,2017 International Conference on Optimization of Electrical and Electronic Equipment (OPTIM) & 2017 Intl Aegean Conference on Electrical Machines and Power Electronics (ACEMP),,IEEE,,Titus Iulian Ciocoiu; Florin Moldoveanu,"Lately, the 3D applications have become a more popular topic in robotics, computer vision or augmented reality. Using cameras and computer vision techniques, it is possible to obtain accurate 3D models of large-scale environments, such as cities. Furthermore, the cameras are low-cost, non-intrusive sensors compared to other sensors such as laser scanners and also offer rich information about the environment. One of the applications of great interest in this way is the vision-based localization in a prior 3D map. Robots need to perform tasks in the environment autonomously, and for this purpose, is very important to know precisely the location of the robot in the map. In the same way, providing accurate information about the location and spatial orientation of the user in a large-scale environment can be of benefit for those who suffer from visual impairment problems. A safe and autonomous navigation in unknown or known environments can be a great challenge for those who are blind or are visually impaired. Most of the commercial solutions for visually impaired localization and navigation assistance are based on the satellite Global Positioning System (GPS), which are not suitable enough for the visually impaired community in urban-environments. The errors are about of the order of several meters and there are also other problems such GPS signal loss or line-of-sight restrictions. Thus, taking into consideration all of the above mentioned problems, it is important to do further research on new more robust and accurate localization systems.",,,1013,1018,Engineering; GPS signals; Augmented reality; Artificial intelligence; Computer vision; Robotics; Visualization; Global Positioning System; Simultaneous localization and mapping; Robot; Orientation (computer vision),,,,,http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7975103 http://xplorestaging.ieee.org/ielx7/7963868/7974934/07975103.pdf?arnumber=7975103,http://dx.doi.org/10.1109/optim.2017.7975103,,10.1109/optim.2017.7975103,2735638502,,0,000-371-951-472-939; 001-437-805-803-782; 004-738-038-027-820; 006-639-160-290-55X; 011-279-408-558-148; 013-379-793-910-566; 031-082-977-157-674; 035-060-772-682-887; 049-946-067-877-233; 052-949-561-819-671; 078-263-231-973-323; 089-193-689-056-058; 136-578-110-194-188,0,false,,
074-513-900-078-507,"The Semiotic Challenges of Guide Dog Teams: the Experiences of German, Estonian and Swedish Guide Dog Users",2015-03-24,2015,journal article,Biosemiotics,18751342; 18751350,Springer Science and Business Media LLC,Germany,Riin Magnus,,9,2,267,285,Semiotics; Sociology; German; Affordance; Sociocultural evolution; Citizen journalism; Perception; Estonian; Context (language use); Public relations; Social psychology,,,,,https://link.springer.com/article/10.1007/s12304-015-9233-4,http://dx.doi.org/10.1007/s12304-015-9233-4,,10.1007/s12304-015-9233-4,2108322373,,0,003-689-875-638-886; 013-017-807-564-003; 016-624-834-350-530; 018-469-786-593-516; 020-024-040-091-86X; 021-213-435-954-075; 029-604-087-942-21X; 029-723-766-132-341; 030-872-184-919-712; 042-890-892-821-163; 043-641-341-926-996; 059-942-923-543-901; 063-480-415-985-069; 086-281-434-326-360; 088-217-813-430-674; 092-131-519-189-477; 102-555-702-781-751; 119-121-601-252-42X; 119-335-243-993-917; 129-363-941-584-628; 138-570-247-887-328; 140-564-570-982-051; 160-766-723-268-168; 163-975-214-848-834; 166-648-715-793-553; 193-218-549-215-158,10,false,,
075-113-904-846-298,Understanding visually impaired people’s experiences of social signal perception in face-to-face communication,2019-11-04,2019,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Shi Qiu; Pengcheng An; Jun Hu; Ting Han; Matthias Rauterberg,,19,4,873,890,Sensory cue; Sight; Psychology; Cognitive psychology; Face-to-face interaction; Facial expression; Gesture; Perception; Modalities; Signal perception,,,,,https://www.narcis.nl/publication/RecordID/oai%3Apure.tue.nl%3Apublications%2F7d6fe6c5-0acd-4da5-8a06-568f4a98cee3 https://dblp.uni-trier.de/db/journals/uais/uais19.html#QiuAHHR20 https://doi.org/10.1007/s10209-019-00698-3 https://link.springer.com/article/10.1007/s10209-019-00698-3 http://doi.org/10.1007/s10209-019-00698-3,http://dx.doi.org/10.1007/s10209-019-00698-3,,10.1007/s10209-019-00698-3,2987996268,,0,003-750-902-459-323; 007-080-394-784-095; 007-259-941-892-512; 010-061-498-865-02X; 010-099-351-805-43X; 010-719-927-108-990; 013-057-892-490-470; 016-902-629-485-96X; 018-340-778-394-181; 018-392-325-202-807; 019-077-296-062-433; 019-545-013-102-509; 021-036-852-459-724; 024-650-588-612-943; 025-161-798-409-993; 025-968-486-135-433; 031-129-726-595-210; 035-694-988-159-328; 036-011-916-817-310; 037-467-971-254-452; 038-076-824-391-033; 039-563-822-976-868; 041-490-683-969-672; 046-262-856-533-736; 046-388-325-059-59X; 047-724-565-341-128; 048-311-943-094-349; 051-891-992-884-218; 053-051-899-847-069; 054-925-004-108-240; 056-286-101-356-443; 056-320-293-478-563; 056-946-397-124-860; 059-398-290-385-221; 059-898-334-060-789; 062-648-870-318-081; 064-511-753-426-046; 066-145-866-996-227; 070-422-763-831-291; 070-987-216-003-479; 072-693-002-839-050; 073-129-864-755-634; 078-895-332-513-778; 080-336-475-026-839; 080-986-412-266-178; 082-309-842-872-957; 088-420-187-228-420; 099-241-020-794-047; 100-940-893-056-795; 102-277-409-869-888; 102-804-639-516-967; 104-823-048-240-547; 105-790-942-783-392; 105-986-387-709-891; 106-957-066-003-920; 107-815-275-098-420; 133-787-783-024-011; 134-788-672-466-192; 135-908-853-701-649; 138-340-449-294-068; 144-319-233-985-223; 144-452-403-048-99X; 145-577-348-295-941; 148-147-612-350-364; 159-008-393-918-894; 169-998-371-992-872; 170-806-049-189-674; 188-140-376-217-893; 188-258-270-148-088,14,false,,
075-245-568-050-489,Building a pedagogy around action and emotion: experiences of Blind Opera of Kolkata,2006-03-10,2006,journal article,AI & SOCIETY,09515666; 14355655,Springer Science and Business Media LLC,Germany,Biswatosh Saha; Shubhashis Gangopadhyay,"Contemporary knowledge systems have given too much importance to visual symbols, the written word for instance, as the repository of knowledge. The primacy of the written word and the representational world built around it is, however, under debate—especially from recent insights derived from cognitive science that seeks to bring back action, intent and emotion within the core of cognitive science (Freeman and Nunez in J Consciousness Stud 6(11/12), 1999). It is being argued that other sensory experiences, apart from the visual, along with desires (or intent) and emotions—like pain, pleasure, sorrow or joy—constitute equally important building blocks that shape an individual’s cognition of the world around. This multi-sensory cognition colored by emotions inspire action and hence is valid knowledge. This is probably nowhere more apparent than in the world of the visually impaired. Deprived of visual sensory capability, they have to perforce depend on other senses. But the dominant discourse in wider society plays a major role in determining what they (the blind) can do. A society built around visual symbols and the written word underplays other elements of cognition and in the process undervalues them. This also gets reflected in the construction of social artifacts of various kinds, such as the educational certification system (The Braille system is an attempt to make the written world accessible to the blind through tactile signals—so that words are ‘felt’ and ‘read.’ But it is quite cumbersome. For instance, even a blind highly skilled at writing in Braille would not be able to match the writing speed of an ordinary visually endowed literate person. Effective and efficient computer-based voice–text–voice converters might solve this problem better.)-based primarily on skills over the written word. Linguistic ability becomes most valuable and at another level the written word gets salience over the spoken word. The blind hardly has a chance, therefore, except through concessions or piety. A practice built around the imagery of an empowered blind person, therefore, must depart from mainstream conceptualization—for power is derived from what one has rather than from what one lacks. It must begin by tapping and valorizing one’s own endowments. This paper is an attempt to identify such a departure based on the experience of Blind Opera—a theatre group of the blind working in Kolkata, India. It seeks to provide an exposition in written word of an experience that can only be partially captured within the confines of a text. It is an incomplete account, therefore, and may be taken as an attempt to reach out and seek an exchange of experiences and insights.",21,1,57,71,Sociology; Cognition; Salience (language); Cognitive psychology; Consciousness; Body language; Pleasure; Action (philosophy); Braille; Spoken word; Social psychology,,,,,https://rd.springer.com/article/10.1007/s00146-006-0038-5 https://link.springer.com/content/pdf/10.1007%2Fs00146-006-0038-5.pdf https://link.springer.com/10.1007/978-1-84628-927-9_23 https://philpapers.org/rec/SAHBAP https://paperity.org/p/7008266/building-a-pedagogy-around-action-and-emotion-experiences-of-blind-opera-of-kolkata https://link.springer.com/article/10.1007/s00146-006-0038-5/fulltext.html https://link.springer.com/article/10.1007/s00146-006-0038-5 https://dblp.uni-trier.de/db/journals/ais/ais21.html#SahaG07 https://core.ac.uk/display/81730878,http://dx.doi.org/10.1007/s00146-006-0038-5,,10.1007/s00146-006-0038-5,2078986836,,0,019-382-553-475-177; 040-310-691-659-714; 178-249-319-026-544; 187-530-337-553-912,2,true,cc-by-nc,hybrid
075-772-863-492-129,Obstacle Detection and Assistance for Visually Impaired Individuals Using an IoT-Enabled Smart Blind Stick,2023-06-30,2023,journal article,Revue d'Intelligence Artificielle,0992499x; 19585748,International Information and Engineering Technology Association,France,Sangam Malla; Prabhat Kumar Sahu; Srikanta Patnaik; Anil Kumar Biswal,"As technological advancements permeate various aspects of life, they offer renewed hope for individuals grappling with disabilities.This paper focuses on the visually impaired population, who face considerable challenges in mobility due to physiological or neurological conditions causing blindness.Despite a reliance on external aid, a growing preference for self-sufficiency is observed among these individuals.In response to this, a pioneering tool, the Smart Blind Stick (SBS), is proposed to alleviate their mobility-related difficulties.The SBS is an advanced adaptive tool, designed to address daily navigation challenges faced by visually impaired individuals.The device operates by identifying obstacles and accurately calculating their distances using an integrated system of an Arduino UNO controller, Viola Jones algorithm, ultrasonic and water sensors.The SBS is equipped with a camera and advanced ultrasonic sensors, along with enhanced coding systems, enabling users to detect objects and navigate through challenging terrains.The SBS distinguishes itself from conventional aids by serving as an autonomous navigation companion, alerting the user of potential hazards such as water bodies, walls, staircases, or uneven surfaces via a headset connected to their phone.This paper elaborates on the development, functionality, and anticipated impact of the SBS in fostering greater autonomy among visually impaired individuals.",37,3,783,794,Obstacle; Internet of Things; Computer science; Visually impaired; Human–computer interaction; Computer vision; Physical medicine and rehabilitation; Artificial intelligence; Embedded system; Medicine; Geography; Archaeology,,,,,https://iieta.org/download/file/fid/103023 https://doi.org/10.18280/ria.370327,http://dx.doi.org/10.18280/ria.370327,,10.18280/ria.370327,,,0,,2,true,cc-by,hybrid
075-924-456-891-35X,Visually Impaired People Positioning Assistance System Using Artificial Intelligence,2023-04-01,2023,journal article,IEEE Sensors Journal,1530437x; 15581748; 23799153,Institute of Electrical and Electronics Engineers (IEEE),United States,Rui Lima; Luís Barreto; António Amaral; Sara Paiva,"Blindness and visual impairment are commonly associated with social and functional limitations, almost 45 million people in the world have blindness, and 135 million have any visual impairment. This condition has a significant impact on the quality of life and brings many challenges to the individual, one of which is the navigation and positioning tasks. Although there are already apps capable of helping visually impaired people (VIP) for mobility purposes, most of them focus on detecting obstacles and, therefore, on avoiding dangerous situations. However, mobility of VIP involves many more tasks, such as knowing their exact position and staying informed along an entire route. For this purpose, a standalone and customizable solution is proposed that uses traditional visual recognition of landmarks to process the surroundings of the current location of the visually impaired person using a smartphone and informing about the nearby places assuring the user a sense of the site. For feature detection, it used the oriented features from accelerated segment test (FAST) and rotated binary robust-independent elementary feature (BRIEF) (ORB) algorithm, and for feature matching, it used the brute-force method with the k-nearest neighbor (KNN) algorithm. Results show that the proposed solution can analyze pictures in fractions of a second with satisfactory accuracy.",23,7,7758,7765,Computer science; Feature (linguistics); Visual impairment; Computer vision; Artificial intelligence; Focus (optics); Blindness; Process (computing); Orb (optics); Matching (statistics); Position (finance); Mathematics; Psychology; Image (mathematics); Medicine; Philosophy; Linguistics; Physics; Statistics; Finance; Psychiatry; Optometry; Optics; Economics; Operating system,,,,,,http://dx.doi.org/10.1109/jsen.2023.3244128,,10.1109/jsen.2023.3244128,,,0,004-384-337-778-565; 009-200-062-110-912; 010-475-144-749-494; 010-949-460-362-648; 011-226-283-906-532; 023-050-798-970-583; 023-702-930-157-09X; 029-240-128-956-606; 029-327-214-479-561; 034-259-321-454-457; 035-703-396-844-665; 041-029-894-072-800; 042-125-442-624-332; 049-216-935-829-166; 061-019-902-905-734; 081-592-951-501-485; 098-440-015-522-27X; 125-253-620-953-16X; 172-216-712-535-207,5,false,,
076-199-062-556-070,A Concept of Dynamically Reconfigurable Real-time Vision System for Autonomous Mobile Robotics,2008-05-23,2008,journal article,International Journal of Automation and Computing,14768186; 17518520,Springer Science and Business Media LLC,Germany,Aymeric De Cabrol; Thibault Garcia; Patrick Bonnin; Maryline Chetto,"This paper describes specific constraints of vision systems that are dedicated to be embedded in mobile robots. If PC-based hardware architecture is convenient in this field because of its versatility, flexibility, performance, and cost, current real-time operating systems are not completely adapted to long processing with varying duration, and it is often necessary to oversize the system to guarantee fail-safe functioning. Also, interactions with other robotic tasks having more priority are difficult to handle. To answer this problem, we have developed a dynamically reconfigurable vision processing system, based on the innovative features of Cleopatre real-time applicative layer concerning scheduling and fault tolerance. This framework allows to define emergency and optional tasks to ensure a minimal quality of service for the other subsystems of the robot, while allowing to adapt dynamically vision processing chain to an exceptional overlasting vision process or processor overload. Thus, it allows a better cohabitation of several subsystems in a single hardware, and to develop less expensive but safe systems, as they will be designed for the regular case and not rare exceptional ones. Finally, it brings a new way to think and develop vision systems, with pairs of complementary operators.",5,2,174,184,Scheduling (computing); Engineering; Embedded system; Artificial intelligence; Mobile robot; Real-time operating system; Robotics; Quality of service; Fault tolerance; Hardware architecture; Robot,,,,,https://hal.archives-ouvertes.fr/hal-00542196 https://link.springer.com/content/pdf/10.1007%2Fs11633-008-0174-0.pdf http://ijac.xml-journal.net/en/article/doi/10.1007/s11633-008-0174-0 https://link.springer.com/article/10.1007%2Fs11633-008-0174-0 https://hal.archives-ouvertes.fr/hal-00542196/document,http://dx.doi.org/10.1007/s11633-008-0174-0,,10.1007/s11633-008-0174-0,1965130515,,0,002-552-743-077-534; 015-125-471-838-277; 019-662-479-138-709; 045-251-603-041-020; 047-681-176-696-163; 062-425-832-386-886; 070-344-836-555-208; 073-022-805-339-534; 080-514-854-879-224; 085-789-154-716-971; 089-252-301-942-967; 099-881-732-489-871; 198-477-451-245-55X,13,true,,green
076-696-638-219-606,Introduction to the focused section on sensing and perception for autonomous and networked robotics,2017-12-05,2017,journal article,International Journal of Intelligent Robotics and Applications,23665971; 2366598x,Springer Science and Business Media LLC,,Xiang Chen; Mathieu Grossard; Naoyuki Kubota; Dirk Wollherr; Simon X. Yang; Song Zhang,,1,4,369,371,Human–computer interaction; Artificial intelligence; Perception; Section (archaeology); Robotics; Computer science,,,,,https://link.springer.com/content/pdf/10.1007/s41315-017-0040-8.pdf https://dblp.uni-trier.de/db/journals/ijira/ijira1.html#ChenGKWYZ17 https://link.springer.com/article/10.1007%2Fs41315-017-0040-8 https://rd.springer.com/article/10.1007/s41315-017-0040-8,http://dx.doi.org/10.1007/s41315-017-0040-8,,10.1007/s41315-017-0040-8,2774017700,,0,,1,true,,bronze
076-781-057-771-298,Shared control framework applied to a robotic aid for the blind,,1999,journal article,IEEE Control Systems,1066033x; 1941000x,Institute of Electrical and Electronics Engineers (IEEE),United States,P. Aigner; B.J. McCarragher,"Demonstrates how the shared discrete event control system is applied to the Robotic Cane, a system which aids the visually impaired navigate through obstacle rich environments. It is also shown how the control framework operates to avoid obstacles with or without human interaction. During experiments the cane effectively helped the user to maneuver through an indoor environment. With a little practice the user could follow the direction in which the cane steered. This shows that the control mapping was defined effectively. The only command that was not very intuitive was the left and brake command used during a weak conflict. However, a different kinematic solution of the cane could achieve the turning and braking as desired. In times of strong conflict between the user input and the autonomous control command the human remained in control. The audio warning during these conflicts was effective in alerting the user.",19,2,40,46,Human–computer interaction; Engineering; Control engineering; Control system; Mobile robot; Control (management); Obstacle; Collision avoidance (spacecraft); Brake; Kinematics; Event (computing),,,,,http://ieeexplore.ieee.org/document/753934/ https://ieeexplore.ieee.org/document/753934/ https://ieeexplore.ieee.org/abstract/document/753934,http://dx.doi.org/10.1109/37.753934,,10.1109/37.753934,2167229922,,0,028-257-285-537-207; 040-795-186-876-484; 047-561-820-010-737; 065-992-777-809-97X; 076-434-297-159-026; 079-543-111-048-048; 115-053-288-089-779; 167-735-118-898-64X,38,false,,
076-835-312-916-636,Machine Learning for Cataract Classification/Grading on Ophthalmic Imaging Modalities: A Survey,2022-05-28,2022,journal article,Machine Intelligence Research,2731538x; 27315398,Springer Science and Business Media LLC,,Xiao-Qing Zhang; Yan Hu; Zun-Jie Xiao; Jian-Sheng Fang; Risa Higashita; Jiang Liu,,19,3,184,208,,,,,,,http://dx.doi.org/10.1007/s11633-022-1329-0,,10.1007/s11633-022-1329-0,,,2,000-120-607-999-268; 000-173-824-502-245; 001-321-117-388-242; 001-722-984-524-529; 001-885-527-306-039; 002-611-673-379-162; 002-998-315-217-824; 003-381-981-343-925; 003-929-134-274-800; 004-087-580-093-40X; 006-301-498-000-099; 006-398-579-900-260; 008-484-228-086-977; 009-079-983-265-87X; 009-275-812-506-200; 009-437-630-055-348; 009-604-757-222-049; 009-853-252-166-325; 010-614-796-764-267; 010-642-207-792-963; 011-541-349-133-633; 011-791-475-403-210; 011-920-603-781-821; 012-063-243-409-622; 012-708-531-109-101; 013-120-031-362-134; 013-156-255-523-712; 013-941-303-704-205; 014-110-324-653-91X; 015-485-644-683-955; 015-869-619-226-640; 015-941-973-379-539; 016-262-662-123-108; 016-493-575-231-731; 016-844-409-185-460; 017-320-767-050-370; 018-023-148-049-675; 018-070-591-410-199; 018-205-879-816-61X; 018-216-266-871-804; 018-302-830-647-936; 018-822-581-363-834; 019-079-030-604-580; 019-244-736-478-13X; 021-283-906-539-112; 021-600-334-563-665; 022-057-930-466-17X; 022-210-190-561-750; 022-899-697-591-871; 024-126-858-205-192; 024-836-365-252-042; 025-105-992-605-553; 025-536-634-412-633; 026-343-137-860-991; 027-918-723-742-446; 028-304-024-463-173; 028-696-653-010-949; 028-706-231-462-124; 028-892-921-362-560; 028-987-787-485-974; 030-440-500-148-114; 034-700-373-790-377; 035-100-177-852-11X; 035-636-402-997-653; 036-141-458-570-503; 036-320-588-660-332; 036-690-859-348-294; 036-734-955-027-636; 037-182-329-824-13X; 037-200-858-914-858; 037-377-111-918-15X; 038-860-514-401-282; 039-870-287-507-874; 039-933-184-572-951; 040-394-990-874-722; 040-604-464-476-946; 040-902-445-612-389; 042-192-102-730-681; 042-701-277-948-159; 043-857-858-221-879; 046-542-998-676-712; 047-499-602-406-894; 047-969-485-734-568; 048-006-903-794-374; 048-696-574-271-089; 052-361-233-342-360; 053-135-185-237-793; 053-259-162-795-631; 053-413-243-927-176; 053-855-051-218-170; 055-252-281-270-943; 057-275-975-082-01X; 057-479-137-786-856; 058-641-292-073-185; 060-834-273-039-056; 062-032-331-606-767; 064-517-902-309-220; 064-629-757-461-889; 065-591-745-183-848; 066-932-043-117-992; 067-587-112-140-46X; 067-796-020-731-378; 070-700-440-273-122; 071-682-961-256-918; 072-161-057-667-201; 073-779-514-841-019; 075-093-116-594-354; 075-102-874-313-55X; 075-519-293-886-898; 075-938-856-454-973; 076-231-538-797-898; 076-282-062-483-85X; 078-377-260-732-583; 078-990-936-852-442; 079-310-940-223-758; 081-059-477-555-455; 082-004-104-351-642; 082-077-698-701-335; 082-354-022-519-456; 086-581-544-792-631; 087-052-309-721-406; 090-993-370-576-407; 093-214-549-109-175; 094-792-552-535-133; 095-727-071-343-144; 099-451-642-714-334; 101-253-543-260-173; 101-366-592-780-82X; 102-637-094-158-702; 104-814-281-468-05X; 106-767-922-263-403; 108-250-372-373-579; 115-452-576-217-315; 124-922-619-603-217; 124-950-881-499-847; 126-128-111-973-203; 128-453-310-203-182; 129-691-646-981-850; 130-745-014-036-435; 131-961-641-708-061; 137-278-916-918-423; 139-552-118-652-140; 140-193-251-818-668; 144-801-681-029-185; 149-200-421-075-546; 149-393-296-870-578; 156-301-472-166-68X; 164-574-194-622-123; 165-058-384-346-831; 165-770-578-544-421; 166-516-509-327-952; 179-441-113-851-588; 180-559-374-049-445; 192-816-296-671-705; 193-683-203-859-558,36,false,,
077-101-929-493-550,ISVC - Object Detection to Assist Visually Impaired People: A Deep Neural Network Adventure,2018-11-10,2018,book chapter,Lecture Notes in Computer Science,03029743; 16113349,Springer International Publishing,Germany,Fereshteh S. Bashiri; Eric LaRose; Jonathan C. Badger; Roshan M. D'Souza; Zeyun Yu; Peggy L. Peissig,"Blindness or vision impairment, one of the top ten disabilities among men and women, targets more than 7 million Americans of all ages. Accessible visual information is of paramount importance to improve independence and safety of blind and visually impaired people, and there is a pressing need to develop smart automated systems to assist their navigation, specifically in unfamiliar healthcare environments, such as clinics, hospitals, and urgent cares. This contribution focused on developing computer vision algorithms composed with a deep neural network to assist visually impaired individual’s mobility in clinical environments by accurately detecting doors, stairs, and signages, the most remarkable landmarks. Quantitative experiments demonstrate that with enough number of training samples, the network recognizes the objects of interest with an accuracy of over 98% within a fraction of a second.",11241,,500,510,Human–computer interaction; Object detection; Blindness; Visually impaired; Computer science; Adventure; Artificial neural network,,,,,https://link.springer.com/content/pdf/10.1007%2F978-3-030-03801-4_44.pdf https://link.springer.com/chapter/10.1007%2F978-3-030-03801-4_44 https://jglobal.jst.go.jp/en/detail?JGLOBAL_ID=201902235239875528 https://dblp.uni-trier.de/db/conf/isvc/isvc2018.html#BashiriLBDYP18,http://dx.doi.org/10.1007/978-3-030-03801-4_44,,10.1007/978-3-030-03801-4_44,2900443499,,1,000-144-960-787-465; 015-324-500-912-571; 020-298-001-549-194; 023-805-082-979-008; 029-367-841-837-782; 029-596-977-717-535; 030-376-794-797-30X; 039-323-203-585-355; 040-171-522-307-139; 059-149-073-001-124; 064-663-965-968-384; 075-519-293-886-898; 076-111-507-166-077; 079-310-940-223-758; 132-564-289-502-036; 139-114-429-196-103; 148-824-702-763-782; 168-996-992-127-149; 195-091-532-336-938,31,false,,
077-216-158-138-867,Implicit and Explicit Knowledge_Based Deep learning Technique for Indoor Wayfinding Assistance Navigation,2023-05-22,2023,preprint,,,Research Square Platform LLC,,Mouna Afif; Riadh Ayachi; Said Yahia; Mohamed Atri,"<jats:title>Abstract</jats:title>;         <jats:p>indoor objects and recognition present a very important task in artificial intelligence (AI) and computer vision fields. This task is an increasingly important especially for blind and visually impaired (BVI) indoor assistance navigation. An increasing interest is addressed for building new assistance technologies used to improve the daily life technologies used to improve the daily life activities qualities for BVI persons. To fulfill this need we propose in this work a new deep learning based techniques used for indoor wayfinding assistance navigation. we propose to use in this paper a new deep learning-based technique based on You Only Learn One Representation YOLOR network. This network enables a combination between implicit and explicit learning and knowledge just like the human brain can do. By introducing the implicit knowledge, the neural network is able to generate a unified representation that can serve for different tasks. In order to train and test the proposed indoor wayfinding assistance system, we proposed to work with the proposed indoor signage dataset. Based on the conducted experiments, the proposed indoor wayfinding system has demonstrated very interesting results. We applied different optimizations techniques in order to reduce the network size and parameters number to make the proposed model suitable for implementation on embedded devices.  As a detection performance, we obtained 95.62% mAP for the original version of YOLOR network and 93.12% mAP for the compressed version and 28 FPS as detection speed.</jats:p>",,,,,Computer science; Task (project management); Artificial intelligence; Deep learning; Signage; Artificial neural network; Human–computer interaction; Machine learning; Representation (politics); Engineering; Art; Systems engineering; Politics; Law; Political science; Visual arts,,,,,https://www.researchsquare.com/article/rs-2949041/latest.pdf https://doi.org/10.21203/rs.3.rs-2949041/v1,http://dx.doi.org/10.21203/rs.3.rs-2949041/v1,,10.21203/rs.3.rs-2949041/v1,,,0,004-216-069-855-831; 005-135-800-876-298; 016-415-106-921-058; 026-994-702-158-53X; 030-273-939-499-337; 045-145-212-436-990; 049-369-704-409-401; 061-817-185-971-97X; 073-216-818-844-578; 086-731-195-129-866; 089-822-552-618-426; 098-888-570-180-630; 149-679-295-863-636; 192-486-333-277-813,0,true,cc-by,green
077-396-994-149-458,SNAVI: A Smart Navigation Assistant for Visually Impaired,2023-11-18,2023,book chapter,"Intelligent Control, Robotics, and Industrial Automation",18761100; 18761119,Springer Nature Singapore,Germany,Madhu R Seervi; Adwitiya Mukhopadhyay,"Our lives are made easier by automated solutions based on the Internet of Things (IoT). Navigating from one place to another can be challenging for blind people. IoT can increase navigational confidence while simultaneously decreasing reliance on others. The goal is to make the device less bulky and to aid them with two ultrasonic sensors and a flame sensor, which will be cost-effective for blind people who can comfortably travel both indoors and outdoors with minimum sensor use. SNAVI: A Smart Navigation Assistant for the Visually Impaired might be molded into a stick for visually impaired persons so that they receive a notification through their headphones when a barrier is identified in front of them via two ultrasonic sensors, as well as the height of the obstacle. The obstruction might be stationary or moving, and the system can detect fire through a flame IR sensor and send voice alerts. This method helps vision-impaired people travel about with less stress.",,,893,905,Ultrasonic sensor; Obstacle; Visually impaired; Headphones; Internet of Things; Computer science; Human–computer interaction; Computer vision; Real-time computing; Engineering; Artificial intelligence; Simulation; Embedded system; Acoustics; Electrical engineering; Geography; Physics; Archaeology,,,,,,http://dx.doi.org/10.1007/978-981-99-4634-1_70,,10.1007/978-981-99-4634-1_70,,,0,000-259-572-898-34X; 011-126-524-756-136; 012-004-532-920-822; 028-744-192-669-778; 029-927-540-699-18X; 036-261-548-880-915; 048-604-471-171-148; 056-374-807-238-30X; 064-100-103-232-577; 070-765-623-587-543; 079-341-203-025-956; 079-379-821-351-832; 080-304-413-097-782; 082-613-956-060-347; 099-334-901-486-872; 101-637-146-335-197; 126-240-265-487-347; 131-752-337-934-186; 140-522-290-521-853; 149-273-136-335-633; 150-546-942-169-55X; 160-616-288-373-428; 179-357-837-631-431,0,false,,
077-455-969-276-663,A computer vision system that ensure the autonomous navigation of blind people,,2013,conference proceedings article,2013 E-Health and Bioengineering Conference (EHB),,IEEE,,Ruxandra Tapu; Bogdan Mocanu; Titus Zaharia,"In this paper we introduce a real-time obstacle recognition framework designed to alert the visually impaired people/blind of their presence and to assist humans to navigate safely, in indoor and outdoor environments, by handling a Smartphone device. Static and dynamic objects are detected using interest points selected based on an image grid and tracked using the multiscale Lucas-Kanade algorithm. Next, we activated an object classification methodology. We incorporate HOG (Histogram of Oriented Gradients) descriptor into the BoVW (Bag of Visual Words) retrieval framework and demonstrate how this combination may be used for obstacle classification in video streams. The experimental results performed on various challenging scenes demonstrate that our approach is effective in image sequence with important camera movement, including noise and low resolution data and achieves high accuracy, while being computational efficient.",,,1,4,Artificial intelligence; Bag-of-words model in computer vision; Histogram of oriented gradients; Grid; Automatic image annotation; Obstacle; Computer vision; Computer science; Object (computer science); Contextual image classification; Noise (video),,,,,https://hal.archives-ouvertes.fr/hal-00944808 https://ieeexplore.ieee.org/document/6707267/,http://dx.doi.org/10.1109/ehb.2013.6707267,,10.1109/ehb.2013.6707267,2001821765,,7,017-547-799-577-579; 022-388-626-459-019; 023-438-345-162-875; 023-868-294-867-153; 030-767-617-573-338; 034-551-334-308-190; 035-291-375-623-97X; 039-274-384-165-289; 044-949-263-620-170; 048-395-692-930-517; 053-574-289-747-231; 060-173-817-564-022; 094-907-387-316-750; 137-707-963-300-012; 156-096-618-588-00X; 170-387-846-090-147; 192-623-080-180-332,43,false,,
077-783-752-404-604,Indoor Navigation Systems for Blind People,2018-12-27,2018,preprint,,,MDPI AG,,Ali Hojjat,"<jats:p>Indoor navigation systems must deal with absence of GPS signals, since they are only available in outdoor environments. Therefore, indoor systems have to rely upon other techniques for positioning users. Recently various indoor navigation systems have been designed and developed to help visually impaired people. In this paper an overview of some existing indoor navigation systems for visually impaired people are presented and they are compared from different perspectives. The evaluated techniques are ultrasonic systems, RFID-based solutions, computer vision aided navigation systems, ans smartphone-based applications.</jats:p>",,,,,Artificial intelligence; Computer vision; Sensor fusion; Computer science,,,,,http://www.preprints.org/manuscript/201803.0058/v2 https://www.preprints.org/manuscript/201803.0058/v1 https://www.preprints.org/manuscript/201803.0058/v1/download,http://dx.doi.org/10.20944/preprints201803.0058.v2,,10.20944/preprints201803.0058.v2,2789544237,,0,003-634-369-751-95X; 006-949-360-093-028; 011-687-393-000-151; 030-127-369-356-741; 048-603-106-043-151; 062-853-383-553-423; 065-032-907-680-425; 066-037-919-140-345; 069-706-555-672-053; 071-806-151-845-245; 074-324-981-337-749; 077-153-557-219-474; 078-736-735-476-395; 082-559-093-627-752; 083-294-350-082-331; 106-052-130-152-052; 122-730-377-867-156; 137-879-509-731-243; 141-489-442-115-69X; 152-183-517-423-628; 152-635-762-456-831; 173-236-675-369-848; 176-116-956-395-13X,0,true,cc-by,green
078-096-843-615-046,Embedded system based smart walking assistance device for visually impaired people,,2022,conference proceedings article,"INDUSTRIAL, MECHANICAL AND ELECTRICAL ENGINEERING",0094243x,AIP Publishing,,Swapnil Saurav; Gangireddy Prabhakar Reddy; B. Ravi Prasad; S. K. Mouleeswaran,"These papers set out recommendations to prepare a framework for the regeneration of neurons from the stroke patient by present solutions, which normally concentrates the reconstruction robots in order to reduce the patient’s influences as a result of stroke disease. It has an ultrasonic blind handle for speech reproduction. Visually disabled persons have traditionally used a stick to see if barriers are there. But in different respects this stick is inefficient and the individual who uses it must face many problems. The purpose of this article is to provide a better navigational stick for visually disabled persons. The ultrasonic blind handle is more powerful than the current handle as the use of sensors facilitates target tracking. Speech replay helps the blind individual take the right steps to reach the required target and the obstacles. By using a sensor on the stick, pit can be detected.",,,,,Computer science; Human–computer interaction; Set (abstract data type); Robot; Visually impaired; Computer vision; Artificial intelligence; Face (sociological concept); Ultrasonic sensor; Medicine; Social science; Sociology; Programming language; Radiology,,,,,https://aip.scitation.org/doi/pdf/10.1063/5.0109631 https://doi.org/10.1063/5.0109631,http://dx.doi.org/10.1063/5.0109631,,10.1063/5.0109631,,,0,053-978-018-079-228; 064-650-229-775-354; 102-104-931-289-725,0,true,,bronze
078-351-054-393-693,Development of robot kit for visually impaired,2017-06-01,2017,,,,,,Fang Lit Chee,"Visual impairment is the loss of the ability to see either gradually or totally lost sight, this situation also call blind. People who suffer with this lost are called visually impaired. They are experienced many inconvenience and one of them is their mobility is totally affected where they camot travel like ordinary people because their visual information is less compare with other. Therefore, a guidance robot which can guide the visually impaired away from obstacle and wall is required. With this avoidance, visually impaired can less concern about their defective vision because they can follow the guidance path of the robot. The robot is equipped with three ultrasonic sensors which is function as the ""eyes"" of visually impaired for gather the distance information in an area in front them. All the distance information is used as condition to alter the ratio of pulse width modulation applied to each motor of the robot. Thus, the speed of the wheels is based on the condition of environment. To test the functionality of the obstacle avoidance algorithm, the guidance robot is tested in three different situations. The overall success rate is around 83.33 percent. Apart from these, biomaterial is attempted to use to construct the body structure of the robot where the shape of the robot's body structure is successfully fabricated out.",,,,,Sight; Artificial intelligence; Obstacle; Robot kit; Visual impairment; Obstacle avoidance algorithm; Visually impaired; Computer vision; Computer science; Robot,,,,,http://umpir.ump.edu.my/id/eprint/25752/,http://umpir.ump.edu.my/id/eprint/25752/,,,2973487269,,0,,0,false,,
078-629-138-903-244,Control of bidirectional physical human---robot interaction based on the human intention,2016-09-24,2016,journal article,Intelligent Service Robotics,18612776; 18612784,Springer Science and Business Media LLC,Germany,Paulo Leica; Flavio Roberti; Matias Monllor; Juan Marcos Toibero; Ricardo Carelli,,10,1,31,40,Stability (learning theory); Human–robot interaction; Mechanical impedance; Control system; Mobile robot; Computer science; Simulation; Lyapunov function; Robot control; Robot,,,,Universidad Nacional de San Juan (AR); Consejo Nacional de Investigaciones Científicas y Técnicas; Fondo para la Investigación Científica y Tecnológica,https://link.springer.com/content/pdf/10.1007%2Fs11370-016-0207-4.pdf https://ri.conicet.gov.ar/handle/11336/63486 https://dblp.uni-trier.de/db/journals/isrob/isrob10.html#LeicaRMTC17,http://dx.doi.org/10.1007/s11370-016-0207-4,,10.1007/s11370-016-0207-4,2522919216,,1,000-880-849-548-437; 005-219-164-642-420; 017-622-720-413-115; 019-835-463-713-775; 020-620-665-286-908; 024-361-293-650-909; 030-127-369-356-741; 032-372-873-449-244; 033-356-145-695-419; 034-303-413-262-853; 038-123-119-441-786; 040-522-996-787-666; 042-201-537-299-730; 047-412-228-484-950; 052-460-783-563-655; 067-428-959-679-454; 071-285-924-249-509; 072-596-952-686-700; 087-838-014-190-104; 091-750-130-550-291; 094-345-973-475-55X; 099-151-403-226-347; 107-887-647-672-757; 111-170-066-201-152; 112-731-483-510-777; 120-093-276-975-914; 139-608-765-722-029; 161-079-685-736-646; 162-911-539-083-690; 185-076-665-900-936,13,false,,
079-030-710-068-047,"CARS 2021: Computer Assisted Radiology and Surgery Proceedings of the 35th International Congress and Exhibition Munich, Germany, June 21-25, 2021.",2021-06-04,2021,journal article,International journal of computer assisted radiology and surgery,18616429; 18616410,Springer Science and Business Media LLC,Germany,,"CARS 2021 and Rethinking the Hospital and OR of the FutureIn addition to the traditional CARS topics and presentations, this year's CARS Congress is revisiting and widening its scope towards both, the Digital Operating Room of the Future and the Hospital of the Future.This is expressed in the CARS 2021 overall theme: ''Rethinking the Hospital and OR''.In past CARS Congresses these themes have been addressed only as focused events, the ''Hospital of the Future'' as early as in 1995 [1] and the ''OR 2020-The Operating Room of the Future'' in CARS 2004 in close cooperation with Kevin Cleary and other colleagues in the USA [2], see Fig. 1.This resulted for example, also in the foundation of WG 24 DICOM in Surgery and the IHE Surgery domain.CARS being mainly concerned about the presence and future of the clinical domains of radiology and surgery recognizes, however, that fundamental developments in IT impact not only these two specific disciplines, but also the hospital of the future and even the healthcare system as a whole, in which radiology and surgery are expected to play a major role.Specifically, the goal of a newly established workshop for CARS 2021 ''OR 2030 and Beyond'' is to identify the clinical and technical requirements for the next generation operating room.OR 2030 is a sequel to OR 2020 [2] but also addressing themes of related developments.The next 10 years with the horizon of 2030 are selected as a target timeframe, but with a possible glance also at what might be expected beyond this period.Below are some of the proposed themes the OR 2030 initiative will focus on in corresponding four working groups:• Group 1: Surgical",16,Suppl 1,1,119,Exhibition; Medicine; Library science; General surgery; Art history; Art; Computer science,,,,NHLBI NIH HHS (R43 HL147747) United States,https://link.springer.com/content/pdf/10.1007/s11548-021-02375-4.pdf https://doi.org/10.1007/s11548-021-02375-4,http://dx.doi.org/10.1007/s11548-021-02375-4,34085172,10.1007/s11548-021-02375-4,,PMC8175231,1,,2,true,,green
079-034-384-481-925,Voice-based e-mail for Information systems: An aid to visually impaired,2023-10-06,2023,journal article,E3S Web of Conferences,22671242; 25550403,EDP Sciences,,P. Surekha; M. Ganga Pavani; B. Jahanavi; B. Siri; Tilottama Singh,"<jats:p>Artificial intelligence has the potential to revolutionize the way that visually impaired individuals interact with technology. With this in mind, a voice-based email system for visually impaired individuals has been proposed as a solution to provide a convenient and accessible way of managing email. The system leverages advanced Natural language processing (NLP)and Automatic Speech Recognition(ASR) technologies to convert spoken commands into email actions. The system supports voice commands for dictating emails, text-to-speech functionality for reading emails, and speech-to-text for writing emails, navigating the inbox, giving the count of unseen emails, and managing emails. This system is designed to be user-friendly, intuitive, and accessible, ensuring that blind individuals can easily navigate and use it with ease. Implementation of this system has the potential to greatly improve the daily lives of visually impaired individuals by providing a more convenient and accessible way to manage emails. This project presents an automation system for AI-powered voice-based email designed specifically information passing system to help blind individuals.</jats:p>",430,,1062,01062,Computer science; Voice command device; Reading (process); Human–computer interaction; Speech synthesis; Visually impaired; Multimedia; World Wide Web; Speech recognition; Political science; Law,,,,,https://www.e3s-conferences.org/articles/e3sconf/pdf/2023/67/e3sconf_icmpc2023_01062.pdf https://doi.org/10.1051/e3sconf/202343001062,http://dx.doi.org/10.1051/e3sconf/202343001062,,10.1051/e3sconf/202343001062,,,0,022-281-348-775-854; 057-072-924-132-769; 148-145-315-395-831; 169-143-929-181-312; 191-322-231-707-715,0,true,cc-by,gold
079-458-787-945-818,A Convolutional Neural Network based Live Object Recognition System as Blind Aid,2018-11-26,2018,preprint,arXiv: Computer Vision and Pattern Recognition,,,,Kedar Potdar; Chinmay D. Pai; Sukrut Akolkar,"This paper introduces a live object recognition system that serves as a blind aid. Visually impaired people heavily rely on their other senses such as touch and auditory signals for understanding the environment around them. The act of knowing what object is in front of the blind person without touching it (by hand or some other tool) is very difficult. In some cases, the physical contact between the person and object can be dangerous, and even lethal. ; This project employs a Convolutional Neural Network for recognition of pre-trained objects on the ImageNet dataset. A camera, aligned with the system's predetermined orientation serves as input to the computer system, which has the object recognition Neural Network deployed to carry out real-time object detection. Output from the network can then be parsed to present to the visually impaired person either in the form of audio or Braille text.",,,,,Artificial intelligence; Parsing; Object detection; Braille; Computer vision; Computer science; Artificial neural network; Object (computer science); Cognitive neuroscience of visual object recognition; Convolutional neural network; Orientation (computer vision),,,,,https://dblp.uni-trier.de/db/journals/corr/corr1811.html#abs-1811-10399 https://arxiv.org/abs/1811.10399 https://ui.adsabs.harvard.edu/abs/2018arXiv181110399P/abstract https://arxiv.org/pdf/1811.10399.pdf,https://dblp.uni-trier.de/db/journals/corr/corr1811.html#abs-1811-10399,,,2900841004,,0,004-269-574-716-057; 024-560-277-271-476; 067-978-433-455-065; 192-697-687-929-866,5,true,,unknown
079-574-573-024-51X,Orientation and mobility assessment in retinal prosthetic clinical trials.,,2012,journal article,Optometry and vision science : official publication of the American Academy of Optometry,15389235; 10405488,Lippincott Williams and Wilkins,United States,Duane R. Geruschat; Ava K Bittner; Gislin Dagnelie,"As the companies developing the various types of prosthetic vision devices move towards commercialization, the importance of measuring the impact of these technologies on performance of activities of daily living (ADL) has become apparent. One of the daily activities that has been identified as potentially benefiting from prosthetic vision is orientation and mobility (O&M).; ; It is useful to acknowledge that vision is not required for safe and independent travel. People who are totally blind, walking with the aid of a long cane or a dog guide, travel to and from work, places of worship, grocery stores, physician’s offices, and the homes of family and friends on a daily basis. They do this safely and independently. Therefore what would be the expected effect and/or benefit of the introduction of vision (for the congenitally blind) or re-introduction of vision (for the adventitiously blind) on their independent travel? One answer to this question comes from observing young children attending a residential school for the blind. You will often see groups of children walking down the hallway with one child in the lead and the other children holding hands while walking in single file. The only difference between the lead child and the followers is that the lead child has light projection or form perception – ultra low vision1. All of the children have had O&M instruction and they are all capable of walking independently through the hallways of the school, yet their collective behavior suggests they recognize that even the limited amount of vision of the lead child makes it is easier to travel. The lead child is capable of using light projection to see the lights in the ceiling and to visually trail these lights to maintain a straight line of travel in the hallway. Arriving at the intersecting corridor, the child can locate the ceiling lights for the intersecting hallway and visually trail those lights. While O&M instructors teach echolocation to listen for the walls and to hear the opening of an intersecting corridor, and O&M instruction can teach kinesthetic “muscle memory” to anticipate the length of a familiar hallway, a minimal amount of vision can enhance mobility, making it easier to travel. From this perspective can prosthetic vision improve mobility?; ; The purpose of this paper is to describe a study on the O&M of retinitis pigmentosa (RP) subjects in a treatment trial involving a subretinally implanted microchip, and to use the experience to propose a research agenda for prosthetics and other experimental vision-restoring treatments in the context of O&M. The study involved patients who had an Optobionics artificial silicon retina (ASR) device implanted subretinally 15-25° temporal to the fovea in a randomly selected eye. The ASR is a 2 mm diameter, 25 μm thick, light-activated array with ~5000 microphotodiode pixels, implanted with the intention of inducing neurotrophic rescue of various aspects of vision anywhere throughout the retina, not necessarily overlying the device. Unlike candidates for most retinal prosthetic devices who have virtually no remaining functional vision, the participants in this study had a wide range of functional vision pre-operatively, and any gains in vision were hypothesized to potentially occur anywhere throughout the retina with viable, dormant photoreceptors, not necessarily over the implant as with other retinal prostheses. Our experience with mobility evaluations during this trial provides the foundation for the proposed O&M research agenda.",89,9,1308,1315,Optometry; Form perception; Artificial intelligence; Psychology; Perspective (graphical); Orientation and Mobility; Visual prosthesis; Context (language use); Computer vision; Kinesthetic learning; Activities of daily living; Orientation (computer vision),,Adult; Contrast Sensitivity/physiology; Follow-Up Studies; Humans; Male; Middle Aged; Orientation/physiology; Psychomotor Performance/physiology; Retina/physiopathology; Retinitis Pigmentosa/physiopathology; Visual Prosthesis; Walking/physiology,,NEI NIH HHS (R01 EY021220) United States,https://jhu.pure.elsevier.com/en/publications/orientation-and-mobility-assessment-in-retinal-prosthetic-clinica-4 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3888774 https://www.ncbi.nlm.nih.gov/pubmed/22902422 https://pubmed.ncbi.nlm.nih.gov/22902422/ http://europepmc.org/abstract/MED/22902422,http://dx.doi.org/10.1097/opx.0b013e3182686251,22902422,10.1097/opx.0b013e3182686251,2057338286,PMC3888774,0,002-708-048-102-708; 006-321-115-294-56X; 012-871-966-995-552; 016-833-070-579-597; 018-959-876-479-647; 029-688-466-750-016; 031-842-427-416-191; 038-046-586-077-577; 040-582-624-072-621; 043-121-042-008-969; 057-275-450-748-982; 085-872-401-520-843; 102-836-027-739-066; 142-426-241-123-210,26,true,,green
079-849-611-602-318,Enabling Efficient Web Data-Record Interaction for People with Visual Impairments via Proxy Interfaces,2023-09-11,2023,journal article,ACM Transactions on Interactive Intelligent Systems,21606455; 21606463,Association for Computing Machinery (ACM),United States,Javedul Ferdous; Hae-Na Lee; Sampath Jayarathna; Vikas Ashok,"<jats:p>;             Web data records are usually accompanied by auxiliary webpage segments, such as filters, sort options, search form, and multi-page links, to enhance interaction efficiency and convenience for end users. However, blind and visually impaired (BVI) persons are presently unable to fully exploit the auxiliary segments like their sighted peers, since these segments are scattered all across the screen, and as such assistive technologies used by BVI users, i.e., screen reader and screen magnifier, are not geared for efficient interaction with such scattered content. Specifically, for blind screen reader users, content navigation is predominantly one-dimensional despite the support for skipping content, and therefore navigating to-and-fro between different parts of the webpage is tedious and frustrating. Similarly, low vision screen magnifier users have to continuously pan back-and-forth between different portions of a webpage, given that only a portion of the screen is viewable at any instant due to content enlargement. The extant techniques to overcome inefficient web interaction for BVI users have mostly focused on general web-browsing activities, and as such they provide little to no support for data record-specific interaction activities such as filtering and sorting – activities that are equally important for facilitating quick and easy access to;             <jats:italic>desired</jats:italic>;             data records. To fill this void, we present InSupport, a browser extension that: (i) employs custom machine learning-based algorithms to automatically extract auxiliary segments on any webpage containing data records; and (ii) provides an instantly accessible proxy;             <jats:italic>one-stop</jats:italic>;             interface for easily navigating the extracted auxiliary segments using either basic keyboard shortcuts or mouse actions. Evaluation studies with 14 blind participants and 16 low vision participants showed significant improvement in web usability with InSupport, driven by increased reduction in interaction time and user effort, compared to the state-of-the-art solutions.;           </jats:p>",13,3,1,27,Web page; Computer science; World Wide Web; Screen reader; Web navigation; Information retrieval; sort; Web accessibility; Human–computer interaction; Visually impaired; Web standards,,,,,https://dl.acm.org/doi/pdf/10.1145/3579364 https://doi.org/10.1145/3579364,http://dx.doi.org/10.1145/3579364,,10.1145/3579364,,,0,000-599-413-585-507; 001-924-376-165-900; 004-017-242-451-24X; 005-626-635-219-716; 011-797-116-120-363; 013-829-872-250-493; 014-004-324-093-087; 015-241-504-166-242; 016-603-843-023-846; 017-345-885-993-100; 022-705-697-906-740; 025-563-184-853-047; 027-314-311-233-234; 027-952-067-004-03X; 036-604-544-426-40X; 041-001-013-913-135; 046-295-832-232-067; 048-224-053-996-02X; 054-419-028-169-02X; 054-602-769-239-115; 054-925-004-108-240; 056-621-206-285-122; 058-094-178-011-394; 060-772-936-796-955; 064-308-485-412-962; 064-445-578-026-21X; 065-028-643-010-329; 075-703-200-177-225; 079-591-993-853-060; 082-843-996-139-846; 083-783-057-855-718; 083-843-363-636-574; 089-455-503-872-071; 091-578-168-871-304; 096-650-595-949-693; 125-897-318-346-362; 130-869-193-302-093; 134-988-695-366-940; 137-982-078-958-643; 147-528-900-617-88X; 158-276-284-210-613; 167-377-823-610-990; 181-649-480-422-771,1,true,,bronze
079-865-682-254-103,A review and reappraisal of information technologies within a conceptual framework for individuals with disabilities,,2001,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Julie A. Jacko; Holly S. Vitense,,1,1,56,76,Information technology; World Wide Web; Conceptual framework; Information system; The Conceptual Framework; User profile; Functional abilities; Computer science; User modeling; Universal design,,,,,https://link.springer.com/article/10.1007%2Fs102090100003 https://link.springer.com/10.1007/s102090100003 https://dblp.uni-trier.de/db/journals/uais/uais1.html#JackoV01 https://doi.org/10.1007/s102090100003,http://dx.doi.org/10.1007/s102090100003,,10.1007/s102090100003,170118551,,0,000-069-249-354-951; 000-826-029-769-368; 001-947-468-300-307; 002-476-797-404-615; 005-192-026-246-449; 006-013-937-579-762; 008-193-910-387-166; 010-424-945-127-417; 020-482-923-383-24X; 020-864-146-382-608; 020-935-182-477-36X; 022-262-433-392-774; 025-164-305-708-236; 027-747-492-293-466; 028-309-914-125-961; 028-558-466-568-773; 031-640-409-568-348; 032-358-690-137-124; 036-382-362-388-86X; 037-445-883-493-712; 038-798-796-772-406; 039-344-317-760-783; 039-716-494-693-184; 046-020-456-277-524; 046-823-809-489-952; 048-247-314-159-759; 056-626-032-463-007; 058-900-970-357-456; 061-469-922-280-549; 065-506-519-680-409; 070-166-437-977-666; 071-677-146-536-538; 077-566-166-438-966; 079-845-275-337-376; 084-112-657-944-721; 087-070-389-034-153; 092-441-606-809-930; 093-529-083-151-086; 093-934-165-598-104; 098-064-306-958-747; 104-516-464-203-470; 104-622-711-168-318; 107-647-186-103-670; 112-041-390-787-306; 112-785-472-414-302; 113-329-402-643-596; 117-847-948-577-282; 118-402-522-355-433; 120-051-926-613-072; 120-124-170-926-742; 126-396-107-353-010; 129-029-481-927-947; 136-148-140-323-040; 138-817-228-069-663; 142-444-051-884-659; 143-313-045-203-69X; 145-922-982-322-42X; 150-969-036-202-404; 157-466-370-015-962; 158-494-209-874-001; 160-586-493-692-992; 187-991-264-478-561; 188-479-439-994-052; 190-233-309-022-10X,44,false,,
079-994-587-327-088,Blind Image Deblurring via Adaptive Optimization with Flexible Sparse Structure Control,2019-05-10,2019,journal article,Journal of Computer Science and Technology,10009000; 18604749,Springer Science and Business Media LLC,China,Risheng Liu; Cai-Sheng Mao; Zhi-Hui Wang; Haojie Li,,34,3,609,621,Inverse problem; Adaptive optimization; Artificial intelligence; Prior probability; Pattern recognition; Deblurring; Sparse image; Kernel (image processing); Computer science; Convolutional neural network; Maximum a posteriori estimation,,,,,https://dblp.uni-trier.de/db/journals/jcst/jcst34.html#LiuMWL19 https://link.springer.com/content/pdf/10.1007/s11390-019-1930-z.pdf https://doi.org/10.1007/s11390-019-1930-z https://link.springer.com/article/10.1007/s11390-019-1930-z,http://dx.doi.org/10.1007/s11390-019-1930-z,,10.1007/s11390-019-1930-z,2945795552,,0,003-423-472-394-495; 008-125-304-772-289; 008-832-738-297-828; 015-123-368-247-242; 018-021-831-872-173; 018-483-538-435-333; 019-809-574-066-452; 020-964-346-013-40X; 028-803-329-967-096; 035-734-779-742-166; 038-609-532-994-616; 042-957-030-719-302; 044-066-550-665-065; 045-057-496-556-862; 052-168-751-772-497; 057-781-962-435-877; 066-533-125-203-239; 067-938-098-870-926; 075-801-396-334-083; 083-366-896-780-334; 083-781-768-661-654; 089-116-090-132-019; 090-615-044-178-317; 094-231-917-123-554; 096-513-975-787-357; 101-256-053-753-112; 104-053-627-754-069; 125-742-281-517-810; 127-511-557-118-760; 140-065-464-159-321; 156-765-044-924-988; 163-349-207-811-146; 172-112-498-122-981,3,false,,
080-160-352-580-315,Vision-based entrance detection in outdoor scenes,2018-03-05,2018,journal article,Multimedia Tools and Applications,13807501; 15737721,Springer Science and Business Media LLC,Netherlands,Mehdi Talebi; Abbas Vafaei; Amirhassan Monadjemi,,77,20,26219,26238,Artificial intelligence; Texture (music); Vision based; Computer vision; Computer science; Object (computer science); Robot; Image processing,,,,,https://dblp.uni-trier.de/db/journals/mta/mta77.html#TalebiVM18 http://iranaict.ir/journal/browse.php?a_id=1103&sid=1&slc_lang=en https://link.springer.com/article/10.1007%2Fs11042-018-5846-3 https://link.springer.com/content/pdf/10.1007%2Fs11042-018-5846-3.pdf,http://dx.doi.org/10.1007/s11042-018-5846-3,,10.1007/s11042-018-5846-3,2793572633,,2,008-119-010-540-246; 008-127-927-561-647; 010-831-034-611-281; 014-822-926-376-521; 015-620-703-326-093; 016-189-044-190-823; 019-925-596-666-983; 023-438-345-162-875; 041-814-549-374-676; 045-913-645-533-074; 047-256-631-224-356; 051-425-281-185-199; 057-567-246-168-045; 066-133-135-091-645; 069-126-955-887-642; 071-919-797-291-262; 072-800-669-480-155; 077-216-334-371-79X; 084-758-706-498-733; 089-733-804-554-103; 098-440-015-522-27X; 100-260-517-243-811; 100-877-498-649-831; 102-954-408-672-018; 111-335-307-016-090; 120-254-462-558-799; 125-112-083-666-544; 132-602-463-268-72X; 136-263-796-565-556; 136-724-921-224-160; 166-416-203-580-156; 196-611-996-038-384,10,false,,
080-176-188-069-385,The Deep Learning based Smart Navigational Stick for Blind People,2023-03-22,2023,journal article,UMT Artificial Intelligence Review,27911276; 27911268,University of Management and Technology,,Muhammad Sulaman; null S.U.Bazai; null Muhammad AKram; null Muhammad Akram Khan,"<jats:p>Blind and visually impaired people find difficulty in detecting obstacles and recognizing people in their way, which makes it dangerous for them to walk, to work, or to go in a crowded area/place. They have to be cautious all the time to move, while avoiding any solid obstacles in their way. Typically, they use different aid devices to reach their destination or to accomplish their daily task. The normal stick is useless for blind and visually impaired people since it cannot detect barriers or people's faces. Visually impaired individuals are unable to distinguish between different types of objects in front of them. They are unable to gauge the size of an object or its distance from them. Several works have been done by public individuals and scientific investigators but their work is dearth in technological aspect. This technological aspect need to be addressed by adding artificial intelligence (AI). This prototype aims to help blind and visually impaired individuals in several aspects to simply obtain/perform everyday tasks and help these individuals to live with the same confidence as sighted people live.Therefore, this study inclined deep learning Mobile-Net Single Shot MultiBox detection (SSD) algorithm for object recognition and Dlib library for face recognition. Subsequently, the proposed solution is using an Open CV and Python. Additionally, Ultrasonic sensors are used for distance measurement, which can be a great help for visually impaired people. These components are grouped together to work effectively and efficiently for the development of visually impaired people. The recognition procedure was revealed through headphones, which notifies the visually impaired when face or any object get recognized. Inclusively, the innovative solution would be a great aid for the blind and visually impaired individuals. As a result, to test and validate the accuracy of the smart navigational stick, several experiments have been conducted on a range of objects and faces. Hence, this study’s modified navigational system was adequate and valid for visually impaired people.</jats:p>",2,No 2,,,Computer science; Visually impaired; Human–computer interaction; Task (project management); Artificial intelligence; Python (programming language); Classifier (UML); Deep learning; Computer vision; Engineering; Systems engineering; Operating system,,,,,,http://dx.doi.org/10.32350/umtair.22.05,,10.32350/umtair.22.05,,,0,,0,true,cc-by,hybrid
080-255-005-963-25X,An Efficient Deep Reinforcement Learning Algorithm for Mapless Navigation with Gap-Guided Switching Strategy,2023-06-26,2023,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Heng Li; Jiahu Qin; Qingchen Liu; Chengzhen Yan,,108,3,,,Planner; A priori and a posteriori; Controller (irrigation); Reinforcement learning; Computer science; Artificial intelligence; Robot; Field (mathematics); Training (meteorology); Machine learning; Mathematics; Philosophy; Physics; Epistemology; Meteorology; Pure mathematics; Agronomy; Biology,,,,,,http://dx.doi.org/10.1007/s10846-023-01888-1,,10.1007/s10846-023-01888-1,,,0,001-107-809-647-295; 006-955-399-018-536; 007-974-088-726-818; 009-857-706-508-966; 011-813-664-094-802; 027-052-566-433-505; 028-210-385-967-046; 029-625-599-393-00X; 035-646-249-149-293; 036-349-823-816-579; 041-875-804-904-400; 042-735-686-367-115; 048-311-911-914-832; 052-075-906-602-220; 058-944-309-204-662; 062-255-717-371-794; 068-286-242-197-322; 077-168-546-058-179; 085-661-717-905-985; 087-934-844-725-374; 093-154-043-148-829; 103-255-068-627-002; 110-999-784-816-782; 116-406-441-633-492; 147-021-700-588-591; 177-847-265-139-423,5,false,,
081-088-371-396-065,An image processing approach for blind mobility facilitated through visual intracortical stimulation.,2012-03-16,2012,journal article,Artificial organs,15251594; 0160564x,Wiley,United Kingdom,Hossein Mahvash Mohammadi; Ebrahim Ghafar-Zadeh; Mohamad Sawan,"This article presents an image processing approach dedicated for a blind mobility aid facilitated through visual intracortical electrical stimulation. The method examines a display framework based on the distances related to a scene. The distances of objects to the walker are measured using a size perspective method which uses only one camera without any occlusion effect. The method extracts the information of the closest object to the camera and transfers a sense of distance to a blind walker. The proposed image processing method can estimate the distances of objects within 7.5 m of the walker, and alert the presence of the closest object to the person. This new method offers the advantages of information reduction and scene understanding suitable for visual prosthesis.",36,7,616,628,Perspective (graphical); Artificial intelligence; Visual prosthesis; Mobility aid; Distance measurement; Computer vision; Computer science; Simulation; Object (computer science); Image processing; Reduction (complexity),,"Algorithms; Blindness/therapy; Electric Stimulation; Humans; Image Processing, Computer-Assisted/methods; Phosphenes; Photic Stimulation/methods; Visual Prosthesis; Visually Impaired Persons; Walking",,,https://europepmc.org/article/MED/22428560 https://www.ncbi.nlm.nih.gov/pubmed/22428560 https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1525-1594.2011.01421.x https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1525-1594.2011.01421.x,http://dx.doi.org/10.1111/j.1525-1594.2011.01421.x,22428560,10.1111/j.1525-1594.2011.01421.x,1596917570,,0,000-492-601-966-120; 002-657-142-595-521; 003-828-997-049-673; 005-162-984-874-281; 006-849-558-638-590; 011-292-203-087-510; 011-634-635-004-878; 018-236-436-916-269; 018-683-650-533-423; 021-790-259-513-202; 031-799-548-880-483; 034-196-646-262-503; 034-350-308-512-672; 037-950-622-180-985; 039-232-446-202-853; 043-269-569-868-43X; 051-773-770-357-263; 055-132-919-152-975; 061-602-095-833-890; 063-048-719-705-828; 064-309-126-323-022; 076-911-364-196-257; 078-681-532-069-655; 082-797-888-377-703; 095-093-707-340-095; 098-811-589-762-176; 101-072-746-521-887; 108-101-226-480-553; 119-671-161-711-675; 120-827-890-232-053; 128-288-848-632-871; 137-707-963-300-012; 148-224-993-792-516; 148-649-574-089-264; 187-332-213-279-862,8,false,,
081-146-706-373-635,Exploring Levels of Control for a Navigation Assistant for Blind Travelers,2023-03-13,2023,conference proceedings article,Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction,,ACM,,Vinitha Ranganeni; Mike Sinclair; Eyal Ofek; Amos Miller; Jonathan Campbell; Andrey Kolobov; Edward Cutrell,"Only a small percentage of blind and low-vision people use traditional mobility aids such as a cane or a guide dog. Various assistive technologies have been proposed to address the limitations of traditional mobility aids. These devices often give either the user or the device majority of the control. In this work, we explore how varying levels of control affect the users' sense of agency, trust in the device, confidence, and successful navigation. We present Glide, a novel mobility aid with two modes for control: Glide-directed and User-directed. We employ Glide in a study (N=9) in which blind or low-vision participants used both modes to navigate through an indoor environment. Overall, participants found that Glide was easy to use and learn. Most participants trusted Glide despite its current limitations, and their confidence and performance increased as they continued to use Glide. Users' control mode preferences varied in different situations; no single mode ""won"" in all situations.",,,,,Computer science; Control (management); Human–computer interaction; Multimedia; Artificial intelligence,,,,,https://dl.acm.org/doi/pdf/10.1145/3568162.3578630 https://doi.org/10.1145/3568162.3578630 https://arxiv.org/pdf/2301.02336 https://arxiv.org/abs/2301.02336,http://dx.doi.org/10.1145/3568162.3578630,,10.1145/3568162.3578630,,,0,003-634-369-751-95X; 024-335-765-549-544; 030-127-369-356-741; 062-360-299-763-06X; 076-781-057-771-298; 080-809-254-267-492; 110-831-712-365-194,6,true,,bronze
081-206-690-444-209,IPTA - Indoor vision-based auditory assistance for blind people in semi controlled environments,,2014,conference proceedings article,"2014 4th International Conference on Image Processing Theory, Tools and Applications (IPTA)",,IEEE,,Aurang Zeb; Sehat Ullah; Ihsan Rabbi,"This paper presents an indoor auditory navigation system for blind and visually impaired people using computer vision based approach. In our approach, fiducial markers augmented with audio information are placed in the environment. The blind user holds a webcam attached to the system in his/her hand and navigates through the environment. The audio assistance is provided to the user whenever a particular marker is detected by the camera and thus enables him/her to independently navigate in the environment. The proposed navigation system provides two types of guidance (1) free mode guidance (2) targeted mode guidance. In the free mode guidance, the user navigates freely in the prepared environment and the system informs him/her about his/her current position through the audio information about the surrounding based on the detected marker. In the targeted mode guidance the user is guided from a source (current) position to his/her desired destination in the environment through the shortest path. Five blind users evaluated the system (both modes) in a semi controlled environment. The results obtained from the experiments indicate that the proposed system is successful in assisting blind users inside a huge and complex building. In addition the solution is low cost and gives high accuracy rate.",,,1,6,Artificial intelligence; Navigation system; Vision based; Environment controlled; Computer vision; Computer science; Mode (computer interface); Global Positioning System,,,,,https://ieeexplore.ieee.org/document/7001996/ https://dblp.uni-trier.de/db/conf/ipta/ipta2014.html#ZebUR14,http://dx.doi.org/10.1109/ipta.2014.7001996,,10.1109/ipta.2014.7001996,2027334777,,1,005-991-142-819-380; 038-854-450-817-818; 043-342-420-332-512; 045-324-207-032-848; 048-603-106-043-151; 059-788-436-281-315; 061-190-872-413-287; 063-200-348-257-985; 070-116-981-693-266; 071-806-151-845-245; 092-391-305-190-481; 099-820-000-852-453; 119-516-706-529-460; 127-003-826-051-947; 128-230-459-731-903; 139-070-575-754-706; 141-489-442-115-69X; 195-118-303-953-083; 197-027-830-854-908,17,false,,
081-276-092-803-471,Virtual Assistant for the Blind,2022-12-31,2022,journal article,International Journal for Research in Applied Science and Engineering Technology,23219653,International Journal for Research in Applied Science and Engineering Technology (IJRASET),,Isshita Borkar; Prof. Asma Shaikh; Snehal Jadhav; Vaishnavi Khandade; Bhakti Nagpure,"<jats:p>Abstract: In this world, certain people are visually impaired and they face a lot of problems while doing thereday-to-day work. Physical movement is one of the biggest challenges for the visually impaired. People with complete blindness or low vision often have a difficult time in self-navigating unfamiliar environments. Hencethis work is aiming for developing a device which will help them as their personal assistant. The goal of the present project is to model an object detector to detect objects for visually impaired people and other commercial purposes by recognizing the objects at a particular distance. The object recognition deep learning model utilizes the You Only Look Once(YOLO) algorithm and a voice announcement is synthesized using text-to-speech (TTS) to make it easier for the blind to get information about objects. This system proposes conglomeration of technologies like Image processing, Speech processing etc, so the problems faced by blind people can be reduced to certain extent. Object recognition methods in computer vision, Image processing, Text to Speech conversion can be embedded in a single object : SMART GLASSES (spectacles)</jats:p>",10,12,2049,2053,Computer science; Object (grammar); Blindness; Human–computer interaction; Computer vision; Face (sociological concept); Artificial intelligence; Change blindness; Image processing; Object detection; Low vision; Cognitive neuroscience of visual object recognition; Movement (music); Multimedia; Speech recognition; Image (mathematics); Pattern recognition (psychology); Change detection; Medicine; Social science; Philosophy; Sociology; Optometry; Aesthetics,,,,,,http://dx.doi.org/10.22214/ijraset.2022.48401,,10.22214/ijraset.2022.48401,,,0,,0,true,,bronze
081-509-625-494-769,Virtual environment to evaluate multimodal feedback strategies for augmented navigation of the visually impaired,,2010,journal article,Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference,23757477,,United States,Masayuki Hara; Solaiman Shokur; Akio Yamamoto; Toshiro Higuchi; Roger Gassert; Hannes Bleuler,"This paper proposes a novel experimental environment to evaluate multimodal feedback strategies for augmented navigation of the visually impaired. The environment consists of virtual obstacles and walls, an optical tracking system and a simple device with audio and vibrotactile feedback that interacts with the virtual environment, and presents many advantages in terms of safety, flexibility, control over experimental parameters and cost. The subject can freely move in an empty room, while the position of head and arm are tracked in real time. A virtual environment (walls, obstacles) is randomly generated, and audio and vibrotactile feedback are given according to the distance from the subjects arm to the virtual walls/objects. We investigate the applicability of our environment using a simple, commercially available feedback device. Experiments with unimpaired subjects show that it is possible to use the setup to “blindly” navigate in an unpredictable virtual environment. This validates the environment as a test platform to investigate navigation and exploration strategies of the visually impaired, and to evaluate novel technologies for augmented navigation.",2010,,975,978,Engineering; Artificial intelligence; Virtual reality; Virtual machine; SIMPLE (military communications protocol); Flexibility (engineering); Computer vision; Audio feedback; Tactile sensor; Wireless; Robot,,"Blindness/diagnosis; Environment; Humans; Imaging, Three-Dimensional/methods; Locomotion; Physical Examination/methods; Therapy, Computer-Assisted/methods; User-Computer Interface; Video Games",,,https://infoscience.epfl.ch/record/149793 http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.ieee-000005627611 https://ieeexplore.ieee.org/document/5627611/ https://www.ncbi.nlm.nih.gov/pubmed/21096984,http://dx.doi.org/10.1109/iembs.2010.5627611,21096984,10.1109/iembs.2010.5627611,2068530498,,0,020-610-812-973-945; 065-992-777-809-97X; 073-928-284-197-323; 094-838-611-687-737; 098-973-813-838-131; 115-315-649-469-222; 122-145-518-053-811; 127-817-632-406-052; 128-288-848-632-871; 174-569-067-252-699,13,false,,
081-577-596-878-150,Camera Based Wearable Devices: A Strategic Survey from 2010 to 2021,2023-12-19,2023,journal article,Wireless Personal Communications,09296212; 1572834x,Springer Science and Business Media LLC,Netherlands,Sanman Singh Brar; Neeru Jindal,,133,1,667,681,Computer science; Wearable computer; Human–computer interaction; Computer vision; Multimedia; Embedded system,,,,,,http://dx.doi.org/10.1007/s11277-023-10787-5,,10.1007/s11277-023-10787-5,,,0,003-707-229-554-553; 008-594-311-535-590; 011-719-416-742-435; 013-710-177-162-653; 018-541-421-131-750; 019-874-418-874-82X; 020-520-719-824-954; 028-737-414-813-072; 032-282-416-536-335; 033-195-948-358-390; 039-629-532-730-148; 053-215-378-840-089; 067-816-237-118-756; 077-455-969-276-663; 077-835-418-269-736; 081-052-370-528-287; 118-077-903-308-474; 121-439-134-242-08X; 123-394-801-483-300; 124-015-310-605-709; 138-692-831-461-671; 144-992-181-445-718; 147-516-729-009-367; 196-020-652-036-022,1,false,,
081-798-432-341-655,How can basic research on spatial cognition enhance the visual accessibility of architecture for people with low vision,2021-01-07,2021,journal article,Cognitive research: principles and implications,23657464,Springer Science and Business Media LLC,England,Sarah H. Creem-Regehr; Erica M. Barhorst-Cates; Margaret R. Tarampi; Kristina M. Rand; Gordon E. Legge,"People with visual impairment often rely on their residual vision when interacting with their spatial environments. The goal of visual accessibility is to design spaces that allow for safe travel for the large and growing population of people who have uncorrectable vision loss, enabling full participation in modern society. This paper defines the functional challenges in perception and spatial cognition with restricted visual information and reviews a body of empirical work on low vision perception of spaces on both local and global navigational scales. We evaluate how the results of this work can provide insights into the complex problem that architects face in the design of visually accessible spaces.",6,1,1,18,Experimental psychology; Human–computer interaction; Architecture; Cognition; Perception; Spatial cognition; Spatial ability; Visual impairment; Population; Computer science,Design; Low vision; Space perception; Spatial cognition,"Cognition; Humans; Vision Disorders; Vision, Low; Vision, Ocular",,NEI NIH HHS (R01 EY017835) United States,https://www.scilit.net/article/636f295dd0d995f2c7fb59baeed039d4 https://cognitiveresearchjournal.springeropen.com/articles/10.1186/s41235-020-00265-y http://www.ncbi.nlm.nih.gov/pubmed/33411062 https://link.springer.com/article/10.1186/s41235-020-00265-y https://link.springer.com/content/pdf/10.1186/s41235-020-00265-y.pdf https://experts.umn.edu/en/publications/how-can-basic-research-on-spatial-cognition-enhance-the-visual-ac https://eric.ed.gov/?id=EJ1279883 https://pubmed.ncbi.nlm.nih.gov/33411062/ https://www.ncbi.nlm.nih.gov/pubmed/33411062,http://dx.doi.org/10.1186/s41235-020-00265-y,33411062,10.1186/s41235-020-00265-y,3118922028,PMC7790979,0,001-264-498-747-553; 003-473-950-897-114; 003-682-075-177-450; 004-258-344-775-461; 004-756-256-314-376; 005-238-828-688-514; 007-919-813-542-615; 008-695-992-231-893; 011-362-233-838-179; 013-332-052-383-457; 014-026-229-873-322; 014-423-869-181-577; 015-473-894-218-724; 016-209-364-096-807; 016-795-216-845-76X; 016-991-662-703-196; 017-742-594-687-575; 021-493-472-353-497; 025-306-351-240-479; 025-466-555-926-395; 026-856-492-464-41X; 026-889-021-545-88X; 028-805-932-151-293; 029-227-467-074-02X; 030-671-263-784-720; 030-756-926-535-69X; 031-160-271-128-821; 031-842-427-416-191; 031-975-734-302-804; 033-195-230-003-638; 033-407-587-718-988; 034-024-375-481-50X; 036-187-226-023-344; 036-622-530-432-269; 037-576-914-950-497; 039-706-676-603-996; 040-067-457-462-625; 041-876-734-285-265; 042-820-812-412-447; 043-302-068-546-354; 046-689-071-082-296; 048-862-212-976-082; 049-642-490-880-525; 052-079-862-351-789; 055-495-691-087-785; 055-971-127-661-437; 062-089-093-591-041; 067-179-520-865-723; 070-554-278-363-644; 071-760-799-702-449; 074-761-269-389-815; 075-479-005-417-98X; 075-519-293-886-898; 081-012-118-505-554; 086-661-607-077-02X; 087-548-762-566-861; 089-893-209-230-430; 093-500-669-863-559; 094-500-294-457-138; 095-910-213-633-987; 099-027-071-247-96X; 099-709-107-458-338; 100-861-316-427-642; 102-092-274-629-367; 106-052-130-152-052; 117-224-596-841-236; 122-284-482-900-387; 126-375-172-326-825; 135-789-377-643-126; 142-103-066-430-873; 146-629-612-317-395; 147-930-698-789-930; 156-079-243-322-920; 162-052-479-457-255; 178-366-591-164-658; 196-944-456-852-532,6,true,cc-by,gold
081-938-680-892-669,"A robotic guide for blind people. Part 1. A multi-national survey of the attitudes, requirements and preferences of potential end-users",2010-12-14,2010,journal article,Applied Bionics and Biomechanics,11762322; 17542103,Wiley,Egypt,Marion Hersh; Michael A. Johnson,"This paper reports the results of a multi-national survey in several different countries on the attitudes, requirements and preferences of blind and visually impaired people for a robotic guide. The survey is introduced by a brief overview of existing work on robotic travel aids and other mobile robotic devices. The questionnaire comprises three sections on personal information about respondents, existing use of mobility and navigation devices and the functions and other features of a robotic guide. The survey found that respondents were very interested in the robotic guide having a number of different functions and being useful in a wide range of circumstances. They considered the robot's appearance to be very important but did not like any of the proposed designs. From their comments, respondents wanted the robot to be discreet and inconspicuous, small, light weight and portable, easy to use, robust to damage, require minimal maintenance, have a long life and a long battery life.",7,4,277,288,Engineering; Personally identifiable information; Work (electrical); Visually impaired; Multi national; Multimedia; Requirements analysis; Robot; End user,,,,,http://downloads.hindawi.com/journals/abb/2010/252609.pdf https://core.ac.uk/display/9385622 https://www.tandfonline.com/doi/abs/10.1080/11762322.2010.523626 https://content.iospress.com/articles/applied-bionics-and-biomechanics/abb523626 https://www.hindawi.com/journals/abb/2010/252609/ http://eprints.gla.ac.uk/46418/ https://downloads.hindawi.com/journals/abb/2010/252609.pdf,http://dx.doi.org/10.1080/11762322.2010.523626,,10.1080/11762322.2010.523626,2158897393,,0,002-174-592-203-701; 003-692-902-674-709; 022-415-644-543-338; 028-387-086-616-610; 028-460-767-882-226; 030-127-369-356-741; 036-774-982-375-077; 037-491-194-003-965; 043-780-577-468-223; 059-171-081-643-056; 069-419-248-108-995; 070-255-125-242-470; 076-431-801-589-030; 077-882-518-530-163; 080-809-254-267-492; 086-869-440-167-789; 091-452-455-693-367; 097-450-016-403-70X; 104-694-342-259-021; 107-887-647-672-757; 115-168-206-177-741; 125-117-336-571-192; 171-916-888-913-890; 174-524-223-550-702; 185-176-691-674-344,21,true,cc-by,gold
082-446-003-232-36X,A Comprehensive Study on Supermarket Indoor Navigation for Visually Impaired using Computer Vision Techniques,2023-02-08,2023,conference proceedings article,2022 OPJU International Technology Conference on Emerging Technologies for Sustainable Development (OTCON),,IEEE,,S. Kayalvizhi; S Roshni; Riya Ponraj; S Priya Dharshini,"The ability to navigate is a fundamental skill that every person must have. People who are visually challenged need regular support when travelling from one place to another. It is difficult for them to have an autonomous shopping experience in a supermarket. The project’s goal is to make one of the daily life’s task easier for visually impaired persons. The project entails aiding visually impaired customers by guiding them to the destination of the corresponding product sections using an indoor navigation technique. The program aids in providing the visually impaired with shopping environment as well as assistance on how to buy their preferred goods. When logging into the Android app, the user or someone who is blind will speak about their shopping list preferences (voice input). The camera is turned on to provide the live-streaming video. The user can then begin navigating indoors to reach the section where the product is found. This incorporates the technology that is used in automated self-driving cars. Modeled highdefinition simulated maps of the supermarkets are used to navigate inside employing the shortest distance. The first item from the list is selected, and the user’s location is identified by matching the features in the image frame with the map and used to compute the path to the product section from there. It provides speech output for navigation. Someone who assists the people to get the products from shelves will help them to get the exact item. The buying process continues until the last item on the list is bought. As a result, this will make tasks comparatively easier for visually impaired persons and make indoor navigation possible.",,,,,Computer science; Product (mathematics); Visually impaired; Task (project management); Android (operating system); Process (computing); Human–computer interaction; Frame (networking); Shortest path problem; Multimedia; Computer vision; Graph; Engineering; Telecommunications; Geometry; Mathematics; Systems engineering; Theoretical computer science; Operating system,,,,,,http://dx.doi.org/10.1109/otcon56053.2023.10114030,,10.1109/otcon56053.2023.10114030,,,0,004-936-224-604-924; 007-223-103-783-187; 011-154-706-724-598; 016-003-012-779-846; 028-753-316-269-878; 034-222-058-985-816; 080-148-076-219-168; 084-629-788-787-334; 085-384-460-222-093; 089-202-515-237-82X; 197-135-579-602-943,2,false,,
082-526-599-146-20X,Artificial landmark-based underwater localization for AUVs using weighted template matching,2014-04-22,2014,journal article,Intelligent Service Robotics,18612776; 18612784,Springer Science and Business Media LLC,Germany,Dong-Hoon Kim; Donghwa Lee; Hyun Myung; Hyun-Taek Choi,,7,3,175,184,Monte Carlo localization; Image segmentation; Remotely operated underwater vehicle; Artificial intelligence; Template matching; Object detection; Landmark; Computer vision; Computer science; Image processing; Underwater,,,,,https://dblp.uni-trier.de/db/journals/isrob/isrob7.html#KimLMC14 https://doi.org/10.1007/s11370-014-0153-y https://koasas.kaist.ac.kr/handle/10203/201167 https://dx.doi.org/10.1007/s11370-014-0153-y https://link.springer.com/article/10.1007/s11370-014-0153-y,http://dx.doi.org/10.1007/s11370-014-0153-y,,10.1007/s11370-014-0153-y,2030806501,,0,002-133-909-875-442; 002-724-634-490-58X; 003-604-763-496-996; 005-186-419-929-098; 007-975-654-011-900; 009-584-423-817-071; 019-903-103-830-646; 023-438-345-162-875; 024-131-470-445-452; 024-881-309-381-23X; 033-295-070-837-251; 040-109-595-628-235; 041-128-375-370-914; 044-898-668-023-385; 050-028-968-793-339; 051-929-219-223-392; 052-325-752-661-891; 058-284-895-435-795; 058-896-508-713-642; 059-038-254-421-097; 062-311-647-901-959; 062-460-608-687-098; 063-236-457-733-682; 065-410-194-623-214; 065-676-855-919-708; 065-819-195-728-230; 068-446-370-600-681; 078-719-866-421-103; 090-761-972-976-114; 090-783-251-654-526; 098-440-015-522-27X; 107-622-496-392-657; 109-292-805-162-833; 111-112-239-367-789; 117-034-978-998-047; 117-935-838-738-17X; 134-575-107-149-873; 161-458-854-350-93X,50,false,,
082-716-665-111-206,EyeVista: An assistive wearable device for visually impaired sprint athletes,,2016,conference proceedings article,2016 IEEE International Conference on Information and Automation for Sustainability (ICIAfS),,IEEE,,Hiranya Peiris; Charitha Kulasekara; Hashan Wijesinghe; Basiru Kothalawala; Namalie Walgampaya; Dharshana Kasthurirathna,"On-going progressions of Information Technology increase the scope for computer vision-based interventions to facilitate efficient and promising technology for people with disabilities. This project aims to develop a wearable navigational assistive device, titled EyeVista, to facilitate visually impaired sprint athletes. It is a lightweight, easy-to-use, customizable and low-cost wearable jacket built-in with off-the-shelf based on computer vision techniques. Synthesis of research initially reflects the impact of the main barriers of a human guide and how to break down such barriers. In doing so, we hope to introduce an alternative to the current practice of having a human guide for blind athletes, overcoming the shortcomings of it. The designed system uses Raspberry Pi single board computer to process the real-time image captured by Raspberry Pi camera module to navigate the athletes within the assigned track and to avoid collisions. As a result, we believe the project EyeVista will empower the visually impaired sprint athletes to enhance their performance by easing their mobility by allowing the user to move within their relevant track lanes and avoid collisions without the support of a human guide and enhance the independence, safety along with the quality of life.",,,1,6,Information technology; Engineering; Wearable computer; Sprint; Single-board computer; Athletes; Visualization; Process (engineering); Multimedia; Camera module,,,,,https://ieeexplore.ieee.org/document/7946558/ http://ieeexplore.ieee.org/document/7946558/,http://dx.doi.org/10.1109/iciafs.2016.7946558,,10.1109/iciafs.2016.7946558,2625797039,,0,006-904-828-430-44X; 056-026-067-299-930; 095-111-972-476-58X; 102-613-833-005-683; 122-174-964-308-24X; 151-410-117-968-371; 161-434-631-146-513,9,false,,
082-799-452-749-704,"Design and implementation of a secure mobile phone-based route navigator (mGuide), adapted for the visually challenged people",2023-03-21,2023,journal article,Journal of Electrical Systems and Information Technology,23147172,Springer Science and Business Media LLC,,Nicholas Ososi Onkoba; Patrick Karimi; Paul Onkundi Nyangaresi,"<jats:title>Abstract</jats:title><jats:p>This paper is focused on the development of a turn-by-turn voice navigation mobile application “mGuide<jats:italic>”</jats:italic> adapted for visually challenged people (VCP). The application was developed in response to the challenges of outdoor navigation by VCP. The application optimizes five map servers to give not only the best real-time routing at any instance but also faster loading and retrieval of route data into the phone’s storage. The destination points of the user are obtained in real time using open street maps and stored in the database. Once the destination has been set and mode of travel chosen, the software gives turn-by-turn voice navigation till the destination is reached. In addition, the application can be able to read inbox messages as they are received. Furthermore, the application gives an audio alert to the user in case of lost route and redirects the user using alternative route. Testing of the application was done using 9 totally blind students and 1 partially blind student from Kenyatta University in Kenya, who were successfully guided. In testing the performance of the whole system, no sound alerts between 0 and 10 m from the middle of the road were heard. However, as the user deviated more than the 10 m from the path, sound alerts are heard. In conclusion, sound alert due to wrong turn is averagely 5.23 s.; </jats:p>",10,1,,,,,,,"National Commission for Science, Technology and Innovation",,http://dx.doi.org/10.1186/s43067-023-00087-0,,10.1186/s43067-023-00087-0,,,0,011-073-542-316-947; 016-229-654-191-835; 024-960-377-978-420; 037-658-848-464-827; 058-981-433-915-800; 090-718-197-956-927; 129-993-969-020-433; 132-381-455-643-538; 189-270-541-373-235; 199-945-948-832-048,0,true,cc-by,gold
082-998-570-539-478,Smart Assistive Navigator for the Blind using Image Processing,2023-06-14,2023,conference proceedings article,2023 International Conference on Sustainable Computing and Smart Systems (ICSCSS),,IEEE,,Mukund Kulkarni; Maitrey Chitale; Shreeshail Chitpur; Atharva Chivate; Pratiksha Chopade; Sharvari Deshmukh,"The life of an individual depends on the basic five senses, and the ability to see is probably the most important one. Visually impaired individuals lack a sense of vision. Hence, daily activities are hampered by their inability to perceive their surroundings. They often face constraints related to independent mobility and navigation. This can lead to difficulties that can only be temporarily subdued by some assisting personnel, creating a feeling of dependency in such an individual. The current methods used nowadays, such as a smart cane, have a limited reach and can only detect obstacles in close proximity to the user. However, to provide comprehensive assistance and enhance independence for visually impaired individuals, it is clear that more techniques and solutions need to be invented. This paper proposes a sophisticated IoT-enabled wearable solution that also encompasses deep learning mechanisms. The architectural design includes a camera module and the Raspberry Pi as the processing unit. It is also equipped with buzzers. The camera module is used to detect obstacles in the user’s route in real-time. Object detection is achieved using OpenCV and deep learning. The use of machine learning here enhances the effectiveness of the application model. Buzzers are interfaced to warn the user of any incoming obstacles. In this way, the system can guide and help the user reach his destination safely and independently, without any external aid.",,,,,Computer science; Wearable computer; Human–computer interaction; Artificial intelligence; Raspberry pi; Computer vision; Independence (probability theory); Object detection; Object (grammar); Internet of Things; Embedded system; Statistics; Mathematics; Pattern recognition (psychology),,,,,,http://dx.doi.org/10.1109/icscss57650.2023.10169767,,10.1109/icscss57650.2023.10169767,,,0,001-820-638-972-731; 013-056-435-015-274; 014-019-198-784-152; 017-022-234-761-14X; 028-744-192-669-778; 030-127-369-356-741; 032-661-597-680-28X; 037-163-533-301-156; 037-491-194-003-965; 038-326-446-864-583; 055-252-859-531-805; 068-526-647-480-345; 095-165-450-694-460; 161-932-258-966-266,0,false,,
083-299-642-195-322,ECCV Workshops (6) - ASSIST: Personalized Indoor Navigation via Multimodal Sensors and High-Level Semantic Information,2019-01-23,2019,book chapter,Lecture Notes in Computer Science,03029743; 16113349,Springer International Publishing,Germany,Vishnu Nair; Manjekar Budhai; Greg Olmschenk; William Seiple; Zhigang Zhu,"Blind & visually impaired (BVI) individuals and those with Autism Spectrum Disorder (ASD) each face unique challenges in navigating unfamiliar indoor environments. In this paper, we propose an indoor positioning and navigation system that guides a user from point A to point B indoors with high accuracy while augmenting their situational awareness. This system has three major components: location recognition (a hybrid indoor localization app that uses Bluetooth Low Energy beacons and Google Tango to provide high accuracy), object recognition (a body-mounted camera to provide the user momentary situational awareness of objects and people), and semantic recognition (map-based annotations to alert the user of static environmental characteristics). This system also features personalized interfaces built upon the unique experiences that both BVI and ASD individuals have in indoor wayfinding and tailors its multimodal feedback to their needs. Here, the technical approach and implementation of this system are discussed, and the results of human subject tests with both BVI and ASD individuals are presented. In addition, we discuss and show the system’s user-centric interface and present points for future work and expansion.",11134,,128,143,Beacon; Human–computer interaction; Interface (computing); Situation awareness; Point (typography); Face (geometry); Navigation system; Semantic information; Computer science; Cognitive neuroscience of visual object recognition,,,,,https://openaccess.thecvf.com/content_eccv_2018_workshops/w31/html/Nair_ASSIST_Personalized_indoor_navigation_via_multimodal_sensors_and_high-level_semantic_ECCVW_2018_paper.html https://openaccess.thecvf.com/content_ECCVW_2018/papers/11134/Nair_ASSIST_Personalized_indoor_navigation_via_multimodal_sensors_and_high-level_semantic_ECCVW_2018_paper.pdf https://dblp.uni-trier.de/db/conf/eccv/eccv2018w6.html#NairBOSZ18 https://rd.springer.com/chapter/10.1007/978-3-030-11024-6_9 https://par.nsf.gov/servlets/purl/10110459 https://par.nsf.gov/biblio/10110459-assist-personalized-indoor-navigation-via-multimodal-sensors-high-level-semantic-information https://link.springer.com/chapter/10.1007%2F978-3-030-11024-6_9,http://dx.doi.org/10.1007/978-3-030-11024-6_9,,10.1007/978-3-030-11024-6_9,2914074051,,0,013-498-406-708-010; 015-998-086-943-276; 023-857-818-702-963; 026-244-307-943-578; 032-594-560-834-970; 034-350-308-512-672; 040-660-173-610-646; 046-098-711-060-761; 048-147-692-721-499; 048-254-076-918-785; 048-443-739-272-544; 049-744-762-183-465; 064-043-611-038-916; 079-433-787-480-087; 090-884-510-770-832; 105-278-566-106-933; 131-228-773-487-14X; 140-905-134-073-524; 151-055-538-428-054; 186-804-131-799-529,14,false,,
083-645-473-035-14X,WorldCIST (2) - Using Online Artificial Vision Services to Assist the Blind - an Assessment of Microsoft Cognitive Services and Google Cloud Vision,2018-05-17,2018,book chapter,Advances in Intelligent Systems and Computing,21945357; 21945365,Springer International Publishing,,Arsénio Reis; Dennis Paulino; Vitor Filipe; João Barroso,"The visually impaired must face several well-known difficulties on their daily life. The use of technology in assistive systems can greatly improve their lives by helping with navigation and orientation, for which several approaches and technologies have been proposed. Lately, it has been introduced powerful online image processing services, based on machine learning and deep learning, promising truly cognitive assessment capacities. Google and Microsoft are two of these main players. In this work we built a device to be used by the blind in order to test the usage of the Google and Microsoft services to assist the blind. The online services were tested by researchers in a laboratory environment and by blind users on a large meeting room, familiar to them. This work reports on our findings regarding the online services effectiveness, the user interface and system latency.",,,174,184,Deep learning; Artificial intelligence; Work (electrical); Order (business); Test (assessment); Computer science; Multimedia; Cloud computing; Latency (audio); User interface; Orientation (computer vision),,,,,https://link.springer.com/chapter/10.1007/978-3-319-77712-2_17 https://dblp.uni-trier.de/db/conf/worldcist/worldcist2018-2.html#ReisPFB18 https://rd.springer.com/chapter/10.1007/978-3-319-77712-2_17 https://doi.org/10.1007/978-3-319-77712-2_17,http://dx.doi.org/10.1007/978-3-319-77712-2_17,,10.1007/978-3-319-77712-2_17,2793232175,,0,001-432-915-664-207; 005-708-970-476-858; 010-719-927-108-990; 036-601-484-186-729; 043-054-937-507-243; 043-219-548-326-273; 044-300-904-108-999; 049-617-718-320-459; 056-949-181-493-396; 067-424-296-647-735; 082-512-681-574-679; 090-884-510-770-832; 096-363-956-353-392; 097-332-691-671-559; 106-146-118-283-508; 113-240-447-443-60X; 127-006-931-445-95X; 131-894-416-339-960; 141-489-442-115-69X; 165-900-958-604-951,16,false,,
083-699-981-470-854,An Intelligent Voice Assistance System for Visually Impaired using Deep Learning,2022-03-23,2022,conference proceedings article,2022 International Conference on Decision Aid Sciences and Applications (DASA),,IEEE,,Renju Rachel Varghese; Pramod Mathew Jacob; Midhun Shaji; Abhijith R; Emil Saji John; Sebin Beebi Philip,"Unassisted navigation, object recognition, obstacle avoidance, and reading activities are extremely difficult for people who are completely blind. For those who are visually impaired, we present a new form of assistive technology. Raspberry Pi 3 Model B+ was selected to illustrate the proposed prototype's capability because of its inexpensive price, compact size, and ease of integration. Incorporated within the design is a camera, sensors for obstacle avoidance, and powerful image processing algorithms for detecting and classifying objects. Both the camera and the ultrasonic sensors are used to determine the user's distance from the impediment. The image-to-text converter, followed by audio feedback, is integrated into the system. A typical pair of eyeglasses can be used to mount the entire system, which is small, light, and simple to use. Using 60 completely blind people, researchers compare the suggested device to the classic white cane in terms of performance. Controlled environments based on real-world scenarios are used to conduct the evaluations. In comparison to a white cane, the proposed device provides higher accessibility and comfort, as well as simplicity of navigation for visually impaired people.",,,,,Computer science; Computer vision; Artificial intelligence; Obstacle; Reading (process); Obstacle avoidance; Visually impaired; Raspberry pi; Audio feedback; Object (grammar); Human–computer interaction; Embedded system; Engineering; Internet of Things; Electrical engineering; Political science; Robot; Law; Mobile robot,,,,,,http://dx.doi.org/10.1109/dasa54658.2022.9765171,,10.1109/dasa54658.2022.9765171,,,0,007-223-103-783-187; 007-305-095-446-752; 018-170-211-594-394; 027-720-558-803-335; 029-927-540-699-18X; 040-057-279-881-544; 053-567-501-093-090; 055-495-282-581-758; 059-644-292-671-783; 061-933-888-557-079; 112-365-549-669-557; 118-533-294-109-774,2,false,,
083-770-766-420-44X,"A Smart Service System for Spatial Intelligence and Onboard Navigation for Individuals with Visual Impairment (VIS4ION Thailand): study protocol of a randomized controlled trial of visually impaired students at the Ratchasuda College, Thailand.",2023-03-07,2023,journal article,Trials,17456215,Springer Science and Business Media LLC,United Kingdom,Mahya Beheshti; Tahereh Naeimi; Todd E Hudson; Chen Feng; Pattanasak Mongkolwat; Wachara Riewpaiboon; William Seiple; Rajesh Vedanthan; John-Ross Rizzo,"<AbstractText Label=""BACKGROUND"" NlmCategory=""BACKGROUND"">Blind/low vision (BLV) severely limits information about our three-dimensional world, leading to poor spatial cognition and impaired navigation. BLV engenders mobility losses, debility, illness, and premature mortality. These mobility losses have been associated with unemployment and severe compromises in quality of life. VI not only eviscerates mobility and safety but also, creates barriers to inclusive higher education. Although true in almost every high-income country, these startling facts are even more severe in low- and middle-income countries, such as Thailand. We aim to use VIS<sup>4</sup>ION (Visually Impaired Smart Service System for Spatial Intelligence and Onboard Navigation), an advanced wearable technology, to enable real-time access to microservices, providing a potential solution to close this gap and deliver consistent and reliable access to critical spatial information needed for mobility and orientation during navigation.</AbstractText>;           <AbstractText Label=""METHODS"" NlmCategory=""METHODS"">We are leveraging 3D reconstruction and semantic segmentation techniques to create a digital twin of the campus that houses Mahidol University's disability college. We will do cross-over randomization, and two groups of randomized VI students will deploy this augmented platform in two phases: a passive phase, during which the wearable will only record location, and an active phase, in which end users receive orientation cueing during location recording. A group will perform the active phase first, then the passive, and the other group will experiment reciprocally. We will assess for acceptability, appropriateness, and feasibility, focusing on experiences with VIS<sup>4</sup>ION. In addition, we will test another cohort of students for navigational, health, and well-being improvements, comparing weeks 1 to 4. We will also conduct a process evaluation according to the Saunders Framework. Finally, we will extend our computer vision and digital twinning technique to a 12-block spatial grid in Bangkok, providing aid in a more complex environment.</AbstractText>;           <AbstractText Label=""DISCUSSION"" NlmCategory=""CONCLUSIONS"">Although electronic navigation aids seem like an attractive solution, there are several barriers to their use; chief among them is their dependence on either environmental (sensor-based) infrastructure or WiFi/cell ""connectivity"" infrastructure or both. These barriers limit their widespread adoption, particularly in low-and-middle-income countries. Here we propose a navigation solution that operates independently of both environmental and Wi-Fi/cell infrastructure. We predict the proposed platform supports spatial cognition in BLV populations, augmenting personal freedom and agency, and promoting health and well-being.</AbstractText>;           <AbstractText Label=""TRIAL REGISTRATION"" NlmCategory=""BACKGROUND"">ClinicalTrials.gov under the identifier: NCT03174314, Registered 2017.06.02.</AbstractText>;           <CopyrightInformation>© 2023. The Author(s).</CopyrightInformation>",24,1,169,,Orientation and Mobility; Medicine; Randomized controlled trial; Quality of life (healthcare); Computer science; Nursing; Visually impaired; Pathology; Optometry,Accessibility; Adaptive mobility device; Assistive technology; Blind/low vision; Low- and/or middle-income countries; Navigation; Visual impairment,"Humans; Vision, Low; Quality of Life; Thailand; Universities; Intelligence; Randomized Controlled Trials as Topic",,NEI NIH HHS (R33 EY033689) United States; NEI NIH HHS (R21EY033689) United States,https://trialsjournal.biomedcentral.com/counter/pdf/10.1186/s13063-023-07173-8 https://doi.org/10.1186/s13063-023-07173-8 https://www.researchsquare.com/article/rs-2085749/latest.pdf https://doi.org/10.21203/rs.3.rs-2085749/v1,http://dx.doi.org/10.1186/s13063-023-07173-8,36879333,10.1186/s13063-023-07173-8,,PMC9990238,0,000-326-447-768-594; 000-765-133-155-52X; 002-073-096-554-999; 002-208-879-428-281; 006-953-356-670-058; 007-290-948-184-733; 010-816-667-303-267; 011-507-559-759-571; 011-925-783-478-178; 016-327-721-221-926; 018-392-325-202-807; 021-987-891-911-326; 022-097-310-762-599; 024-118-585-521-180; 027-438-755-330-646; 030-464-942-782-370; 030-617-400-771-313; 031-637-704-228-445; 034-884-619-681-066; 039-981-321-899-739; 049-313-676-820-38X; 050-406-469-952-732; 056-220-908-929-116; 062-051-374-404-149; 069-656-898-335-443; 073-756-777-122-700; 080-150-281-543-648; 083-406-127-121-987; 095-874-358-491-937; 107-614-613-778-779; 116-815-276-926-407; 119-770-768-617-54X; 132-004-495-686-897; 133-958-322-087-443; 159-232-067-136-703,4,true,"CC BY, CC0",gold
083-853-269-246-35X,Providing accessibility to blind people using GIS,2011-09-28,2011,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Hugo Fernandes; N. Conceição; Hugo Paredes; António Pereira; P. Araújo; João Barroso,,11,4,399,407,GIS and public health; Human–computer interaction; Interface (computing); World Wide Web; Work (electrical); Everyday life; Computer communication networks; Spatial knowledge; Computer science; Geographic information system; Enterprise GIS,,,,,https://link.springer.com/article/10.1007/s10209-011-0255-7 https://link.springer.com/content/pdf/10.1007%2Fs10209-011-0255-7.pdf https://dx.doi.org/10.1007/s10209-011-0255-7 http://dx.doi.org/10.1007/s10209-011-0255-7 https://rd.springer.com/article/10.1007/s10209-011-0255-7 https://doi.org/10.1007/s10209-011-0255-7,http://dx.doi.org/10.1007/s10209-011-0255-7,,10.1007/s10209-011-0255-7,1964050683,,0,042-467-816-580-902; 045-418-646-209-673; 051-254-947-000-266; 054-708-339-566-453; 057-890-353-263-937; 065-882-712-564-29X; 074-792-297-012-435; 078-191-959-374-104; 088-940-980-793-475; 101-333-962-375-602; 105-584-440-275-77X; 124-950-106-275-471; 140-091-865-029-447; 148-768-859-064-775; 151-015-279-636-660; 171-967-210-532-103; 199-584-584-713-214,29,false,,
083-923-091-144-911,YOLO glass: video-based smart object detection using squeeze and attention YOLO network,2023-12-31,2023,journal article,"Signal, Image and Video Processing",18631703; 18631711,Springer Science and Business Media LLC,Germany,T. Sugashini; G. Balakrishnan,,18,3,2105,2115,Computer science; Computer vision; Artificial intelligence; Object detection; Object (grammar); Obstacle; Overfitting; Artificial neural network; Pattern recognition (psychology); Political science; Law,,,,,,http://dx.doi.org/10.1007/s11760-023-02855-x,,10.1007/s11760-023-02855-x,,,0,000-394-661-764-185; 000-551-523-973-321; 009-829-120-033-790; 016-645-836-018-745; 017-159-670-613-140; 018-165-888-033-875; 018-843-508-574-144; 022-622-218-704-364; 025-028-410-791-784; 038-326-446-864-583; 041-495-168-079-15X; 051-263-461-368-647; 051-915-945-716-485; 052-287-464-667-493; 056-311-013-170-616; 059-493-893-682-992; 067-583-626-133-824; 091-910-369-114-547; 091-973-674-139-589; 096-015-424-229-162; 099-334-901-486-872; 107-997-344-407-769; 109-822-701-564-796; 127-115-170-544-365; 135-076-511-985-283; 135-283-186-881-799; 148-478-467-972-963; 149-162-653-144-382; 186-512-780-643-036; 187-559-896-608-974,4,false,,
084-079-943-504-821,Blind Assistant Navigation System,2012-11-16,2012,book,,,,,Akella S. Narasimharaju,"This book is submitted for MTech degree. The project presents a design to develop an ETA (Electronic Travel Aid) the visually impaired people to navigate independently within an enclosed environment. It is linked to the other components of this design through the RF wireless communication. 	The blind person can perceive obstacles in front by means of an ETA from head height to foot level. The vision sensor, which is mounted on the head of the user, captures the image that is in front of him. The server processes the captured image and enhances the significant vision data by employing a set of image processing procedures. The processed information is presented as a structured form of acoustic signal and is conveyed to the user through a set of earphones. 	The blind rod (guide cane) is used for foot level obstacle detection. The obstacle sensor senses the obstacle that lies ahead of the user and informs him. 	This system also incorporates position and direction information. The remote server does optimal path planning. The user issues the commands to ETA and receives direction response through audio signals. The directions are given to the user based on the environmental map",,,,,Signal; Artificial intelligence; Set (psychology); Obstacle; Navigation system; Computer vision; Computer science; Wireless; Audio signal; Motion planning; Image processing,,,,,https://www.amazon.com/Blind-Assistant-Navigation-System-Electronic/dp/3659294934,https://www.amazon.com/Blind-Assistant-Navigation-System-Electronic/dp/3659294934,,,2961480625,,0,,0,false,,
084-468-268-444-907,Artificial Intelligence for Face recognition and Assistance for Visually Impaired,2023-06-15,2023,conference proceedings article,"2023 5th International Conference on Energy, Power and Environment: Towards Flexible Green Energy Technologies (ICEPE)",,IEEE,,Likewin Thomas; Thara K L; Sunil Kumar H R,"In this world, many people are affected by blindness and they face so many difficulties in their daily life. According to World Health Organization (WHO), there are approximately 2.2 billion people who are completely blind. They need to depend on primitive solutions like white canes, trained dogs, or other people. But these helping hands cannot always assist them. The affected people need a smart assisting device that avoids bumping into an obstacle and helps in navigating from one place to another independently. This proposed work describes the smart walking stick which makes blind people walk safely. By using the latest technologies and IoT devices, this smart walking stick can be developed where it provides safe navigation to the user. The proposed system employs a novel solution for navigation in indoor with the help of deep learning algorithms. In case of panic situations or emergency conditions, the predefined message with the user's location will be sent to the caretaker using an API. This smart walking stick is affordable, durable and provides more convenience to the user to walk safely, and gives more confidence without depending on any other externals.",,,,,Computer science; Obstacle; Face (sociological concept); Human–computer interaction; SAFER; Blindness; Computer security; Internet privacy; Artificial intelligence; Medicine; Social science; Sociology; Political science; Optometry; Law,,,,,,http://dx.doi.org/10.1109/icepe57949.2023.10201487,,10.1109/icepe57949.2023.10201487,,,0,000-889-606-496-856; 003-245-163-059-395; 007-223-103-783-187; 016-343-563-055-343; 027-838-356-729-214; 028-200-507-219-056; 034-936-259-439-98X; 044-973-139-715-439; 053-567-501-093-090; 056-404-390-495-54X; 056-916-160-072-321; 062-957-059-042-10X; 064-100-103-232-577; 078-346-643-467-148; 084-621-956-547-983; 100-869-464-816-806; 119-517-530-171-868; 119-553-924-738-845,0,false,,
084-581-504-702-117,Route selection algorithm for Blind pedestrian,,2010,conference proceedings article,ICCAS 2010,,IEEE,,Slim Kammoun; Florian Dramas; Bernard Oriolaand Christophe Jouffrais,"The vast majority of existing route selection processes is designed for vehicle navigation. In this paper we describe an adapted routing algorithm for visually impaired pedestrians based on users needs. Our aim was to find the most adapted route that connects origin and destination points, and which can provide the Blind with a sparse but helpful mental representation of the itinerary and surroundings. Based on multiple brainstorming sessions and interviews with blind people and an orientation and mobility (O&M) instructor, different classes of objects were defined and tagged in the Geographical Information System. The optimal route was then selected using the Dijkstra algorithm. This method will be used in NAVIG (Navigation Assisted by Artificial VIsion and GNSS), an assistive device for the Blind, whose aim is to improve orientation, mobility and objects localization.",,,2223,2228,GNSS applications; Brainstorming; Artificial intelligence; Dijkstra's algorithm; Information system; Selection algorithm; Orientation and Mobility; Computer vision; Computer science; Selection (genetic algorithm); Orientation (computer vision),,,,,https://ieeexplore.ieee.org/abstract/document/5669846 https://hal.archives-ouvertes.fr/hal-02926588,http://dx.doi.org/10.1109/iccas.2010.5669846,,10.1109/iccas.2010.5669846,1549972920,,0,002-868-168-418-515; 006-256-120-472-553; 009-128-554-117-48X; 009-645-272-092-857; 018-731-709-957-102; 031-962-503-545-822; 047-049-570-765-252; 051-766-223-654-722; 058-468-795-176-403; 061-686-108-928-655; 064-014-343-490-355; 074-132-202-006-187; 075-828-199-349-85X; 082-069-548-410-540; 088-470-436-866-921; 126-039-525-595-674; 132-246-550-138-466; 145-523-738-709-309; 166-771-600-379-049; 198-777-023-439-918,29,false,,
084-616-732-951-059,Navigate-Me: Secure voice authenticated indoor navigation system for blind individuals,2021-12-02,2021,conference proceedings article,2021 21st International Conference on Advances in ICT for Emerging Regions (ICter),,IEEE,,D. M. L. V Dissanayake; R. G. M. D. R. P Rajapaksha; U. P Prabhashawara; S. A. D. S. P Solanga; J. A. D. C. Anuradha Jayakody,"The majority of blind people require assistance when navigating through unfamiliar places due to a lack of information about the building structure and encounterable obstacles. To address this aspect of the problem, this paper presents ""Navigate-Me"" as an approach for indoor navigation with maximum accessibility, usability, and security, reducing the problems that the user might encounter while navigating through indoor environments. As the targeted audience of this paper is blind or visually impaired people, Navigate-Me utilizes voice-based inputs from the user. In addition, this paper includes Bluetooth beacon integration for localization, White Cane with sensors for obstacle detection, a machine learning model for voice authentication, and an algorithm protocol for a secure connection between server and application integration-driven architecture to assist the visually impaired in navigating known and unknown indoor environments.",,,,,Computer science; Usability; Bluetooth; Obstacle; Human–computer interaction; Authentication (law); Protocol (science); Voice command device; Computer security; Wireless; Speech recognition; Telecommunications; Medicine; Alternative medicine; Pathology; Political science; Law,,,,,,http://dx.doi.org/10.1109/icter53630.2021.9774790,,10.1109/icter53630.2021.9774790,,,0,018-431-515-101-520; 018-712-284-944-248; 032-220-505-346-185; 036-866-670-963-804; 037-649-514-992-799; 039-274-384-165-289; 047-749-445-596-16X; 050-541-811-550-301; 051-760-708-047-253; 065-508-730-912-016; 066-526-177-885-125; 105-176-521-066-563; 110-557-429-389-124; 139-114-429-196-103; 157-948-327-233-050,2,false,,
084-928-200-229-754,Donald Redfield Griffin: the discovery of echolocation,,2005,journal article,Resonance,09718044; 0973712x,Springer Science and Business Media LLC,India,H. Raghuram; G. Marimuthu,,10,2,20,32,Art; Human echolocation; Griffin; Art history,,,,,https://link.springer.com/article/10.1007/BF02835920 http://repository.ias.ac.in/28302/ https://core.ac.uk/download/pdf/291521331.pdf,http://dx.doi.org/10.1007/bf02835920,,10.1007/bf02835920,2313560370,,3,010-851-597-850-372; 022-038-467-108-181; 048-154-558-981-877; 075-422-785-140-16X; 141-161-647-265-617; 171-473-224-552-324,8,false,,
084-999-446-796-140,Mobile Cloud Visual Media Computing - Assistive Text Reading from Natural Scene for Blind Persons,2015-11-24,2015,book,Mobile Cloud Visual Media Computing,,Springer International Publishing,,Chucai Yi; Yingli Tian,"Text information serves as an understandable and comprehensive indicator, which plays a significant role in navigation and recognition in our daily lives. It is very difficult to access this valuable information for blind or visually impaired persons, in particular, in unfamiliar environments. With the development of computer vision technology and smart mobile applications, many assistive systems are developed to help blind or visually impaired persons in their daily lives. This chapter focuses on the methods of text reading from natural scene as well as their applications to assist people who are visually impaired. With the research work on accessibility for the disabled, the assistive text reading technique for the blind is implemented in mobile platform, such as smart phone, tablet, and other wearable device. The popularity and interconnection of mobile devices would provide more low-cost and convenient assistance for blind or visually impaired persons.",,,219,241,Wearable computer; Mobile device; Popularity; Blind persons; Smart phone; Text reading; Computer science; Multimedia; Optical character recognition; Natural (music),,,,,https://link.springer.com/chapter/10.1007/978-3-319-24702-1_9/fulltext.html https://link.springer.com/10.1007/978-3-319-24702-1_9 https://dblp.uni-trier.de/db/books/collections/HH2015.html#YiT15 https://rd.springer.com/chapter/10.1007/978-3-319-24702-1_9 https://link.springer.com/chapter/10.1007/978-3-319-24702-1_9,http://dx.doi.org/10.1007/978-3-319-24702-1_9,,10.1007/978-3-319-24702-1_9,2288708059,,0,004-766-408-593-108; 005-003-427-120-577; 007-211-765-072-925; 007-975-066-276-195; 008-766-288-584-076; 011-534-607-577-283; 013-056-435-015-274; 014-279-588-505-48X; 015-413-114-192-646; 015-522-377-591-609; 017-994-776-387-929; 018-659-243-361-88X; 020-294-759-828-842; 022-391-584-853-129; 022-489-327-390-755; 023-438-345-162-875; 023-884-758-333-089; 025-037-929-508-222; 029-061-861-419-041; 029-360-136-216-567; 031-078-926-616-760; 034-350-308-512-672; 037-039-198-474-114; 038-858-720-835-740; 043-022-952-629-736; 044-659-512-375-773; 045-669-671-005-660; 051-970-317-843-151; 057-644-188-024-866; 057-731-809-827-780; 059-647-236-855-012; 060-041-403-626-524; 062-957-059-042-10X; 071-806-151-845-245; 074-025-785-307-622; 074-324-981-337-749; 085-102-500-348-494; 091-785-995-981-234; 093-256-914-961-610; 094-206-454-118-462; 096-014-228-878-398; 101-767-617-199-036; 105-969-976-015-690; 112-365-549-669-557; 114-888-284-970-47X; 115-260-444-372-632; 116-976-189-445-151; 123-491-827-241-873; 135-646-014-794-306; 186-097-088-188-860; 195-842-744-142-232,5,false,,
085-086-926-958-452,Theory and Application of High-Precision Preoperative Positioning for Cochlear Surgical Robot,2023-11-04,2023,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Hengjia Liu; Hongjian Yu; Zhijiang Du; Feng Liu; Xuanbo Fan; Lining Sun,,109,3,,,Robot; Cochlear implant; Jacobian matrix and determinant; Cochlear implantation; Computer science; Surgical robot; Surgical procedures; Robotic surgery; Simulation; Engineering; Artificial intelligence; Medicine; Surgery; Audiology; Mathematics; Applied mathematics,,,,National Key Research and Development Program of China; National Key Research and Development Program of China,,http://dx.doi.org/10.1007/s10846-023-02001-2,,10.1007/s10846-023-02001-2,,,0,000-883-008-610-260; 001-393-339-930-714; 003-352-796-438-030; 003-932-423-126-65X; 004-473-512-334-051; 006-403-286-663-410; 007-056-162-652-560; 008-252-226-724-610; 018-307-258-732-437; 018-856-194-385-924; 022-165-947-689-853; 022-502-714-411-476; 023-221-520-670-432; 023-449-779-446-166; 024-177-706-367-74X; 028-681-416-066-867; 029-657-838-116-250; 046-142-007-276-02X; 049-166-404-163-438; 052-056-992-126-852; 057-040-925-657-94X; 059-334-371-425-018; 065-362-887-559-32X; 072-375-145-488-477; 074-405-998-714-488; 075-595-388-038-693; 080-681-499-742-415; 084-196-920-485-042; 088-160-392-638-404; 095-125-969-491-313; 099-633-438-840-162; 102-182-278-590-69X; 104-208-613-793-423; 111-707-597-981-351; 111-868-648-553-902; 114-108-384-811-943; 121-493-508-913-982; 129-433-874-354-566; 133-203-994-238-405; 141-471-114-901-055; 144-248-577-215-93X; 155-745-111-020-301,0,false,,
085-238-005-402-636,A Robotic Walker Based on a Two-Wheeled Inverted Pendulum,2016-12-07,2016,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Airton Rosa da Silva; Frank C. Sup,,86,1,17,34,Stability (learning theory); Engineering; State variable; Control engineering; Position (vector); Balance (ability); Inverted pendulum; Minimum jerk; Control theory; Control theory; Linear-quadratic regulator,,,,,https://link.springer.com/article/10.1007/s10846-016-0447-8 https://dialnet.unirioja.es/servlet/articulo?codigo=6138652 https://rd.springer.com/article/10.1007/s10846-016-0447-8 https://dblp.uni-trier.de/db/journals/jirs/jirs86.html#SilvaS17,http://dx.doi.org/10.1007/s10846-016-0447-8,,10.1007/s10846-016-0447-8,2559910056,,0,002-820-086-025-993; 004-270-360-270-844; 005-860-407-120-613; 005-894-340-066-16X; 007-502-378-744-488; 010-559-840-961-382; 011-527-612-257-65X; 011-563-349-897-550; 013-781-767-448-596; 016-685-057-422-50X; 017-079-969-858-898; 029-836-611-065-448; 030-183-456-287-381; 034-288-543-223-630; 036-011-987-822-715; 039-656-762-288-651; 042-512-930-967-385; 047-699-650-849-303; 052-556-478-485-183; 056-023-957-788-732; 058-672-572-980-785; 059-960-596-726-880; 073-246-761-192-325; 073-956-281-330-262; 075-547-657-514-672; 076-121-436-407-96X; 076-245-893-523-119; 080-327-373-849-813; 087-010-522-156-64X; 093-729-346-953-977; 095-254-990-979-234; 096-991-584-990-741; 106-510-970-591-862; 109-995-584-155-584; 110-669-097-959-173; 114-314-372-656-39X; 115-239-129-109-15X; 123-469-567-475-145; 123-840-695-743-224; 131-157-483-160-290; 131-349-107-642-655; 133-515-897-499-008; 135-075-382-709-615; 139-065-746-954-723; 145-391-515-855-550; 150-370-655-149-533; 153-606-108-662-781; 169-511-368-373-117; 172-532-397-825-494,17,false,,
085-276-331-962-812,Point spread function estimation for blind image deblurring problems based on framelet transform,2022-04-20,2022,journal article,The Visual Computer,01782789; 14322315,Springer Science and Business Media LLC,Germany,Reza Parvaz,"One of the most important issues in image processing is the approximation of the image that has been lost due to the blurring process. These types of matters are divided into non-blind and blind problems. The second type of problem is more complex in terms of calculations than the first problem due to the unknown of original image and point spread function estimation. In the present paper, an algorithm based on coarse-to-fine iterative by $$l_{0}-\alpha l_{1}$$ regularization and framelet transform is introduced to approximate the spread function estimation. Framelet transfer improves the restored kernel due to the decomposition of the kernel to different frequencies. Also, in the proposed model, a fraction gradient operator is used instead of the ordinary gradient operator. The proposed method is investigated on different kinds of images such as text, face and natural. The output of the proposed method reflects the effectiveness of the proposed algorithm in restoring images from blind problems.",39,7,2653,2669,Deblurring; Regularization (linguistics); Kernel (algebra); Algorithm; Mathematics; Artificial intelligence; Computer science; Image (mathematics); Image restoration; Point spread function; Kernel density estimation; Pattern recognition (psychology); Image processing; Statistics; Combinatorics; Estimator,,,,,https://arxiv.org/pdf/2112.11004 https://arxiv.org/abs/2112.11004,http://dx.doi.org/10.1007/s00371-022-02484-4,,10.1007/s00371-022-02484-4,,,0,002-562-524-957-901; 002-919-236-716-205; 003-423-472-394-495; 004-703-358-470-063; 006-984-075-547-485; 007-194-610-049-876; 009-753-130-908-45X; 015-123-368-247-242; 015-592-990-851-994; 015-620-463-869-004; 017-943-179-428-809; 021-900-931-644-871; 027-220-792-311-776; 028-008-478-401-907; 030-589-162-408-096; 036-817-670-195-111; 038-609-532-994-616; 042-957-030-719-302; 045-057-496-556-862; 054-623-371-685-935; 055-875-369-089-859; 056-241-542-313-773; 057-466-585-443-619; 061-232-207-605-804; 066-533-125-203-239; 070-381-889-516-395; 071-721-948-499-025; 073-347-893-442-579; 076-652-370-404-047; 079-943-835-670-02X; 081-723-119-178-381; 084-436-657-942-591; 085-576-972-997-710; 088-279-936-205-956; 093-459-952-694-816; 094-668-148-367-547; 096-124-090-694-058; 096-618-630-937-015; 107-078-879-644-253; 118-097-044-874-539; 125-992-708-212-672; 127-511-557-118-760; 127-781-047-558-547; 136-289-129-138-177; 136-616-529-707-696; 140-065-464-159-321; 140-518-601-590-089; 143-677-714-856-24X; 148-405-815-699-178; 192-957-512-999-019,3,true,,green
085-411-210-988-556,Can we cover navigational perception needs of the visually impaired by panoptic segmentation,2020-07-20,2020,preprint,arXiv: Computer Vision and Pattern Recognition,,,,Wei Mao; Jiaming Zhang; Kailun Yang; Rainer Stiefelhagen,"Navigational perception for visually impaired people has been substantially promoted by both classic and deep learning based segmentation methods. In classic visual recognition methods, the segmentation models are mostly object-dependent, which means a specific algorithm has to be devised for the object of interest. In contrast, deep learning based models such as instance segmentation and semantic segmentation allow to individually recognize part of the entire scene, namely things or stuff, for blind individuals. However, both of them can not provide a holistic understanding of the surroundings for the visually impaired. Panoptic segmentation is a newly proposed visual model with the aim of unifying semantic segmentation and instance segmentation. Motivated by that, we propose to utilize panoptic segmentation as an approach to navigating visually impaired people by offering both things and stuff awareness in the proximity of the visually impaired. We demonstrate that panoptic segmentation is able to equip the visually impaired with a holistic real-world scene perception through a wearable assistive system.",,,,,Human–computer interaction; Deep learning; Panopticon; Artificial intelligence; Perception; Contrast (vision); Cover (telecommunications); Computer science; Object (computer science); Segmentation,,,,,https://arxiv.org/pdf/2007.10202 https://www.arxiv-vanity.com/papers/2007.10202/ http://export.arxiv.org/pdf/2007.10202 https://dblp.uni-trier.de/db/journals/corr/corr2007.html#abs-2007-10202 https://ui.adsabs.harvard.edu/abs/2020arXiv200710202M/abstract,https://arxiv.org/pdf/2007.10202,,,3044686633,,0,000-278-355-752-479; 003-813-258-014-256; 021-176-270-241-156; 022-940-138-590-719; 028-671-432-434-245; 032-663-005-151-800; 039-274-384-165-289; 041-314-165-476-904; 046-309-239-761-288; 049-950-886-666-327; 049-978-547-114-653; 050-806-108-968-280; 051-295-299-879-938; 051-773-212-592-656; 059-020-380-002-461; 060-147-659-950-016; 075-519-293-886-898; 097-683-710-853-476; 098-926-109-358-108; 100-038-263-739-253; 101-952-752-741-881; 103-728-042-717-787; 110-537-605-548-246; 115-177-947-973-248; 134-950-160-470-035; 139-552-118-652-140; 151-358-790-051-04X; 160-196-276-119-125; 163-062-635-352-814; 181-862-545-772-223; 193-811-945-822-686,5,true,,unknown
085-634-277-118-553,Selfredepth,2024-07-04,2024,journal article,Journal of Real-Time Image Processing,18618200; 18618219,Springer Science and Business Media LLC,Germany,Alexandre Duarte; Francisco Fernandes; João M. Pereira; Catarina Moreira; Jacinto C. Nascimento; Joaquim Jorge,"<jats:title>Abstract</jats:title><jats:p>Depth maps produced by consumer-grade sensors suffer from inaccurate measurements and missing data from either system or scene-specific sources. Data-driven denoising algorithms can mitigate such problems; however, they require vast amounts of ground truth depth data. Recent research has tackled this limitation using self-supervised learning techniques, but it requires multiple RGB-D sensors. Moreover, most existing approaches focus on denoising single isolated depth maps or specific subjects of interest highlighting a need for methods that can effectively denoise depth maps in real-time dynamic environments. This paper extends state-of-the-art approaches for depth-denoising commodity depth devices, proposing SelfReDepth, a self-supervised deep learning technique for depth restoration, via denoising and hole-filling by inpainting of full-depth maps captured with RGB-D sensors. The algorithm targets depth data in video streams, utilizing multiple sequential depth frames coupled with color data to achieve high-quality depth videos with temporal coherence. Finally, SelfReDepth is designed to be compatible with various RGB-D sensors and usable in real-time scenarios as a pre-processing step before applying other depth-dependent algorithms. Our results demonstrate our approach’s real-time performance on real-world datasets shows that it outperforms state-of-the-art methods in denoising and restoration performance at over 30 fps on Commercial Depth Cameras, with potential benefits for augmented and mixed-reality applications.</jats:p>",21,4,,,Computer science,,,,"Fundação para a Ciência e a Tecnologia; Fundação para a Ciência e a Tecnologia; Fundação para a Ciência e a Tecnologia; Fundação para a Ciência e a Tecnologia; United Nations Educational, Scientific and Cultural Organization; United Nations Educational, Scientific and Cultural Organization; United Nations Educational, Scientific and Cultural Organization; Universidade de Lisboa",,http://dx.doi.org/10.1007/s11554-024-01491-z,,10.1007/s11554-024-01491-z,,,0,000-662-597-208-560; 002-159-812-383-439; 006-736-886-265-067; 006-853-335-776-808; 007-342-358-750-178; 010-523-152-505-610; 017-687-522-081-41X; 020-835-195-775-106; 021-419-378-678-409; 023-021-199-124-032; 024-189-147-690-847; 027-580-544-983-708; 036-609-247-580-331; 037-799-293-816-778; 040-937-348-773-826; 046-666-506-794-89X; 047-398-452-171-264; 049-991-190-553-149; 053-502-067-830-504; 057-354-720-613-531; 066-915-988-959-465; 072-453-349-734-546; 075-372-863-827-221; 076-287-448-995-885; 079-310-940-223-758; 080-878-396-430-995; 084-197-601-345-91X; 086-213-687-933-375; 091-666-004-862-444; 092-143-822-722-332; 095-036-957-604-048; 095-129-224-182-717; 095-444-258-776-054; 096-744-042-374-663; 101-790-519-213-047; 103-984-449-085-841; 109-546-459-029-817; 118-325-683-596-717; 131-367-737-657-551; 131-649-215-775-10X; 144-695-870-330-413; 144-759-563-966-884; 152-466-227-218-722; 162-301-823-952-066; 166-719-743-438-458; 167-131-435-704-717; 173-821-502-933-049; 180-180-231-997-679; 199-901-798-886-934,0,true,cc-by,hybrid
085-636-508-391-397,Plenary lecture 1: assisted movement of visually impaired in outdoor environments - work directions and new results,2009-07-22,2009,conference proceedings,,,,,Virgil Tiponut,"There are approximately 45 million blind individuals world-wide according to the World Health Report. Vision loss limits their access to the educational opportunities, social events, public transportation and leads to a higher rate of unemployment than that of individuals with no functional limitations (58% and 18% respectively, according to the American Foundation for the Blind (AFB)). Many efforts have been invested in the last years, based on ingenious devices and information technology, to help people to overcome these barriers and to integrate them in the social and productive life.; ; In this talk, research efforts are presented to develop electronic travel aids (ETA) that increase the visual impaired people's independence in their working and living environment. These devices, based on sensor technology and signal processing, are capable to improve the mobility of blind users (in terms of safety and speed), in unknown or dynamically changing environment. In particular, an integrated environment that improves the mobility of blind persons in to a limited area is presented. The proposed solution includes wearable equipment, placed on the subject, who guides the blind user to navigate autonomous with obstacles avoidance and stationary, monitoring equipment, which supervises the motion in order to avoid some unexpected events. The 3D obstacles detection system, included in the wearable equipment is bio-inspired, i.e. the system detects obstacles in a similar way as a subject with normal sight is looking for obstacles in front of him. The monitoring equipment, based on a GPS and a GSM/GPRS communication system, is capable to track the movement of a group of visually impaired, each of them moving on a specified pathway, in order to reach the desired target. The man-machine interface exploits the remarkable abilities of the human hearing system in identifying sound source positions in 3D space. The proposed solution relies on the Acoustic Virtual Reality (AVR) concept, which can be considered as a substitute for the lost sight of blind and visually impaired individuals. According to the AVR concept, the presence of obstacles in the surrounding environment and the path to the target will be signalized to the subject by burst of sounds, whose virtual source position suggests the position of the real obstacles and the direction of movement, respectively.",,,23,24,Communications system; Human–computer interaction; Interface (computing); Engineering; Wearable computer; Sight; Virtual reality; Assisted GPS; Unexpected events; Telecommunications; General Packet Radio Service,,,,,http://dl.acm.org/citation.cfm?id=1627575.1627581,http://dl.acm.org/citation.cfm?id=1627575.1627581,,,2261288093,,0,,0,false,,
085-905-962-226-60X,DUAL-PURPOSE BLIND NAVIGATION BOX,2014-05-25,2014,journal article,International Journal of Research in Engineering and Technology,23217308; 23191163,eSAT Publishing House,,Sandeep Reddy; Abid Hussain Zaidi,"The combination of sensors and microcontrollers can be a boon for physically challenged population. Ultrasonic sensors have convincing features over other sensors. These sensors are widely used for range finding for indoor and outdoor applications in robotics. These sensors can be used along with microcontroller to sense the objects with the echoes principle. The proposed gadget has applications like obstacle detection and navigation; which can aid visually impaired individuals, who are unable to afford the service animals or hinge on other helpers for their regular walking or movements.",03,19,359,362,Gadget; Artificial intelligence; Microcontroller; Obstacle; Population; Range finding; Dual purpose; Computer vision; Robotics; Computer science; Ultrasonic sensor,,,,,http://ijret.org/volumes/v03/i19/ijret_110319064.pdf http://esatjournals.net/ijret/2014v03/i19/IJRET20140319064.pdf,http://dx.doi.org/10.15623/ijret.2014.0319064,,10.15623/ijret.2014.0319064,2313517727,,0,019-294-646-355-510; 041-571-183-213-822; 065-992-777-809-97X; 146-425-395-790-21X; 150-148-650-092-841,0,true,,bronze
086-117-306-404-382,Virtual Assistant and Navigation for Visually Impaired using Deep Neural Network and Image Processing,2023-05-02,2023,preprint,,,Research Square Platform LLC,,Charan Bhukhya; Kashyap Bhumireddy; Harsha Vardhan Reddy Palakonalu; Shashank Kumar Singh; Saurabh Bansod; Prashant Pal; Yogesh Kumar,"<jats:title>Abstract</jats:title>;         <jats:p>The quick development of technology promotes the use of the resources that are at hand to make simpler daily tasks and improve the standard and quality of life for those who are blind. This module suggests creating a system of virtual assistant glasses to help the visually impaired navigate around them. An obstacle detection module built into the device uses computer vision to find obstacles and inform the user through haptic feedback. The system also has a text recognition module that can turn any text it identifies into speech that the user can hear using built-in speakers in the user's glasses. Users are now able to access to printed materials like menus and signs in a way that was not previously feasible. A sign board recognition module, which converts text on signs into speech, is also part of the system. The proposed approach has the potential to enhance the liberty and standard of existence of blind individuals through integrating these attributes in a wearable device.</jats:p>",,,,,Computer science; Obstacle; Wearable computer; Human–computer interaction; Haptic technology; Quality (philosophy); Sign (mathematics); Screen reader; Visually impaired; Artificial intelligence; Embedded system; Mathematical analysis; Philosophy; Mathematics; Epistemology; Political science; Law,,,,,https://www.researchsquare.com/article/rs-2867156/latest.pdf https://doi.org/10.21203/rs.3.rs-2867156/v1,http://dx.doi.org/10.21203/rs.3.rs-2867156/v1,,10.21203/rs.3.rs-2867156/v1,,,0,033-413-289-966-933; 058-353-852-013-644; 080-074-010-642-95X; 081-128-204-159-306; 083-459-752-937-630; 102-824-808-685-058; 144-837-124-254-309; 153-901-973-746-818; 155-769-403-085-381; 174-268-712-752-698; 177-664-450-864-692,1,true,cc-by,green
086-137-681-504-623,Improved canny detection algorithm for processing and segmenting text from the images,2018-02-23,2018,journal article,Cluster Computing,13867857; 15737543,Springer Science and Business Media LLC,Netherlands,P. Arunkumar; S. P. Shantharajah; M. Geetha,,22,3,7015,7021,Interface (computing); Market segmentation; Artificial intelligence; Structure (mathematical logic); Computer vision; Computer science; Process (computing),,,,,https://dblp.uni-trier.de/db/journals/cluster/cluster22.html#ArunkumarSG19 https://link.springer.com/article/10.1007/s10586-018-2056-8,http://dx.doi.org/10.1007/s10586-018-2056-8,,10.1007/s10586-018-2056-8,2789154503,,0,001-166-385-239-959; 001-908-844-133-169; 013-124-142-390-910; 013-322-068-920-633; 019-855-389-906-226; 026-287-947-734-205; 031-744-138-796-272; 061-734-789-534-507; 075-086-716-712-187; 080-691-289-270-770; 141-785-408-851-876; 157-182-568-969-169; 169-478-549-790-022,7,false,,
086-233-836-751-566,A transformer based real-time photo captioning framework for visually impaired people with visual attention,2024-03-26,2024,journal article,Multimedia Tools and Applications,15737721; 13807501,Springer Science and Business Media LLC,Netherlands,Abubeker Kiliyanal Muhammed Kunju; S. Baskar; Sherin Zafar; Bushara A R; Rinesh S; Shafeena Karim A,,,,,,Closed captioning; Computer science; Visually impaired; Transformer; Visual attention; Speech recognition; Artificial intelligence; Computer vision; Human–computer interaction; Multimedia; Image (mathematics); Cognition; Electrical engineering; Voltage; Neuroscience; Biology; Engineering,,,,,,http://dx.doi.org/10.1007/s11042-024-18966-7,,10.1007/s11042-024-18966-7,,,0,011-103-262-264-489; 013-269-537-309-627; 018-587-980-373-530; 019-950-020-300-30X; 030-329-866-472-95X; 032-431-231-606-09X; 040-810-209-546-350; 048-683-153-209-970; 079-821-633-943-063; 095-221-867-919-745; 096-971-264-708-34X; 102-100-353-930-315; 111-810-340-893-925; 119-996-698-161-04X; 127-016-196-604-289; 130-575-462-009-319; 136-201-101-843-414; 138-337-811-532-86X; 140-921-627-035-630; 147-022-564-211-101; 149-984-952-788-543; 154-891-848-147-576; 158-538-948-748-254; 163-837-935-935-772; 182-409-922-356-390; 183-708-240-966-745; 185-312-638-200-637; 191-200-363-030-608; 191-306-047-040-375; 194-155-248-576-014,0,false,,
086-461-917-596-881,A Novel GA-FCM Strategy for Motion Learning and Prediction: Application in Wireless Tracking of Intelligent Subjects,2012-04-21,2012,journal article,Arabian Journal for Science and Engineering,13198025; 21914281,Springer Science and Business Media LLC,Saudi Arabia,Sai Hong Tang; O. Motlagh; Abdul Rahman Ramli; Napsiah Ismail; D. Nakhaei Nia,,37,7,1929,1958,Decision support system; Engineering; Artificial intelligence; Mobile device; Mobile robot; Computer vision; Wi-Fi; Wireless; Radiolocation; Robot; Robustness (computer science),,,,,https://link.springer.com/article/10.1007%2Fs13369-012-0274-6 https://link.springer.com/content/pdf/10.1007%2Fs13369-012-0274-6.pdf,http://dx.doi.org/10.1007/s13369-012-0274-6,,10.1007/s13369-012-0274-6,1973884127,,0,000-659-254-096-600; 001-159-184-276-045; 001-829-713-611-997; 003-331-463-163-38X; 005-509-154-817-536; 006-790-031-646-97X; 010-280-775-222-082; 012-158-559-928-236; 017-914-916-808-148; 018-943-111-503-610; 018-975-013-073-270; 019-121-830-985-509; 023-038-175-484-755; 029-004-785-824-035; 030-086-116-701-289; 037-178-026-292-867; 038-287-277-274-099; 041-514-160-356-66X; 045-078-247-510-479; 045-321-254-881-478; 050-036-996-905-385; 058-183-061-082-759; 061-438-806-872-529; 061-512-608-480-586; 065-584-478-497-117; 067-668-236-111-761; 071-361-890-564-907; 073-618-852-661-080; 075-577-491-748-280; 078-596-413-666-012; 081-814-587-437-026; 085-402-078-019-154; 085-738-277-978-20X; 086-987-281-802-980; 088-775-755-617-671; 097-279-945-071-984; 100-209-104-251-916; 100-802-346-603-512; 111-406-378-843-03X; 116-375-838-506-150; 130-253-017-784-788; 130-968-215-176-002; 154-751-089-924-826; 159-350-852-818-838; 165-202-725-040-851; 166-991-870-993-967; 169-686-037-163-355; 178-242-627-182-109,5,false,,
086-567-540-019-296,Accessible learning objects: a systematic literature review,2023-08-04,2023,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Michele dos Santos Soares; Cássio Andrade Furukawa; Maria Istela Cagnin; Débora Maria Barroso Paiva,,,,,,Computer science; Context (archaeology); Systematic review; Object (grammar); Learning object; Order (exchange); Scientific literature; Data science; Human–computer interaction; World Wide Web; Artificial intelligence; MEDLINE; Finance; Political science; Law; Economics; Biology; Paleontology,,,,,,http://dx.doi.org/10.1007/s10209-023-01025-7,,10.1007/s10209-023-01025-7,,,0,000-303-444-704-85X; 001-219-373-623-263; 001-941-974-661-642; 013-622-015-274-257; 013-705-994-971-657; 014-105-930-696-809; 016-640-044-754-418; 019-473-069-788-503; 019-511-427-190-443; 020-015-626-302-906; 020-109-742-122-74X; 020-454-084-849-054; 024-264-024-616-943; 026-769-211-801-729; 029-803-077-917-493; 031-043-109-927-219; 032-262-564-821-172; 032-618-140-774-186; 033-451-289-805-74X; 036-895-827-642-260; 037-647-677-799-61X; 038-114-596-461-304; 039-283-231-270-613; 039-905-166-476-159; 043-801-961-298-433; 044-552-828-246-724; 049-011-708-214-023; 059-898-334-060-789; 063-670-066-881-807; 067-797-582-067-08X; 072-546-469-277-135; 077-647-694-348-440; 088-581-250-265-96X; 092-169-451-136-187; 095-901-448-348-30X; 096-617-727-315-576; 097-623-517-337-053; 098-874-325-350-010; 101-694-777-622-011; 105-202-616-457-758; 106-935-938-460-567; 107-515-313-277-653; 114-488-938-308-029; 121-364-136-978-425; 122-752-499-927-898; 133-805-415-820-854; 134-838-445-163-020; 138-629-384-553-264; 140-863-312-472-593; 145-902-098-209-92X; 173-181-653-100-738,0,false,,
086-568-108-954-220,Multimodal user interface for the communication of the disabled,2008-07-15,2008,journal article,Journal on Multimodal User Interfaces,17837677; 17838738,Springer Science and Business Media LLC,Germany,Savvas Argyropoulos; Konstantinos Moustakas; Alexey Karpov; Oya Aran; Dimitrios Tzovaras; Thanos Tsakiris; Giovanna Varni; Byungjun Kwon,,2,2,105,116,Human–computer interaction; Multimodal interaction; Haptic technology; Gesture recognition; Multimodal fusion; Computer science; Multimedia; Entertainment; Sign language; User interface,,,,,https://link.springer.com/article/10.1007%2Fs12193-008-0012-2 https://dblp.uni-trier.de/db/journals/jmui/jmui2.html#ArgyropoulosMKA08,http://dx.doi.org/10.1007/s12193-008-0012-2,,10.1007/s12193-008-0012-2,2114793269,,0,016-476-873-061-510; 017-890-198-886-454; 021-687-681-673-425; 035-293-284-303-558; 048-785-825-207-554; 063-520-943-541-705; 075-474-265-287-488; 080-091-760-037-884; 091-188-188-688-626; 096-478-130-067-808; 096-722-449-155-561; 106-977-353-081-639; 144-396-426-463-084; 162-893-659-409-943; 198-150-787-930-660,8,false,,
086-649-633-650-932,Intelligent Assistance System for Visually Impaired/Blind People (ISVB),2022-12-27,2022,conference proceedings article,"2022 5th International Conference on Communications, Signal Processing, and their Applications (ICCSPA)",,IEEE,,Noha Ghatwary; Ahmed Abouzeina; Ahmed Kantoush; Bahaa Eltawil; Mohamed Ramadan; Mohamed Yasser,"Visual impairments pose a parsing need to develop new automated systems to assist persons presenting visual impairments. The visual impairments have trouble interacting and sensing their surroundings. Their movement is limited and has to rely on a guided stick for them to move safely from one place to another. However, traditional canes have the disadvantage of failing to detect far-away obstacles and small objects. Therefore, this project is proposed to design and develop an Intelligent Assistance System for Visually Impaired People (ISVB). Our proposed system is composed of three interconnected parts, a smart cap, a 3D-printed intelligent cane and a mobile application that connects the system through an online server. The smart cap uses the Raspberry Pi and camera module, along with a deep learning object detection module for obstacle detection. The intelligent cane will provide the feasibility for the visually impaired person to walk without encountering problems by analyzing the surrounding environment through a microcontroller with multiple sensors and a bluetooth module. The mobile application interacts with the cap and the cane. Additionally, it will provide virtual navigation to help visually impaired people in their movement. To evaluate the performance of the system, different experiments for object detection, sensors and mobile applications have been conducted. The overall performance of the model showed an efficiency of 94.6 %.",,,,,Computer science; Bluetooth; Microcontroller; Obstacle; Object detection; Human–computer interaction; Embedded system; Object (grammar); Computer vision; Visually impaired; Artificial intelligence; Wireless; Telecommunications; Segmentation; Political science; Law,,,,,,http://dx.doi.org/10.1109/iccspa55860.2022.10019201,,10.1109/iccspa55860.2022.10019201,,,0,004-071-983-923-934; 004-841-247-614-218; 012-344-135-607-729; 023-718-896-573-896; 028-143-078-747-070; 028-744-192-669-778; 048-254-076-918-785; 066-560-690-089-073; 069-406-097-962-330; 072-715-264-095-324; 127-930-877-687-993; 165-556-947-058-430,4,false,,
086-675-849-046-792,Electronic Travel Aid for Bus Detection and Bus-Route Number Recognition for Blind People,2023-09-30,2023,book chapter,Advances in Data Science and Computing Technologies,18761100; 18761119,Springer Nature Singapore,Germany,Shripad Bhatlawande; Harshita Agrawal; Adhiraj Jagdale; Anusha Agrawal; Swati Shilaskar,"This paper proposes a novel camera-based bus detection and route number recognition system for visually impaired people. Visually impaired people face several challenges in their daily life of which mobility is a major issue. A major challenge is safe and independent commuting by making use of public transportation systems like buses. Visually impaired people are unable to recognize a particular public bus and read the route numbers. The aids currently available have limited functionalities and are based on technologies that have a steep learning curve. The main feature of the proposed system is that it is effectively able to detect the bus and recognize the bus number in a cluttered and noisy scene using SIFT features, OCR, and various normalization processes. It then tells the destination of the bus as an audio output to the visually impaired person. Steps were also taken to ensure feature scaling is done, so that no feature gets overlooked. The five algorithms discussed in the paper are decision tree which gave an accuracy of 87.63%, random forest that provided an accuracy of 83.84%, support vector machine that gave an accuracy of 83.75%, K-nearest neighbor that provided an accuracy of 82.68%, and logistic regression that gave an accuracy of 83.59%.",,,71,78,Computer science; Normalization (sociology); Support vector machine; Artificial intelligence; Scale-invariant feature transform; Decision tree; Computer vision; Random forest; Feature (linguistics); Pattern recognition (psychology); Feature extraction; Linguistics; Philosophy; Sociology; Anthropology,,,,,,http://dx.doi.org/10.1007/978-981-99-3656-4_8,,10.1007/978-981-99-3656-4_8,,,0,004-251-488-773-586; 005-362-076-845-297; 008-558-848-807-295; 030-884-979-386-25X; 043-376-650-343-976; 050-966-354-214-276; 055-371-440-082-002; 061-933-888-557-079; 067-936-687-916-954; 071-294-997-400-390; 082-438-955-220-577; 085-693-431-075-118; 089-926-237-820-92X; 135-312-662-527-804,0,false,,
087-154-803-997-480,Sonar glass—Artificial vision: Comprehensive design aspects of a synchronization protocol for vision based sensors,,2023,journal article,Measurement,02632241; 1873412x; 15366359,Elsevier BV,Netherlands,Amutha Balakrishnan; Kadiyala Ramana; Gokul Ashok; Wattana Viriyasitavat; Sultan Ahmad; Thippa Reddy Gadekallu,,211,,112636,112636,Computer vision; Sonar; Obstacle; Artificial intelligence; Computer science; Obstacle avoidance; Global Positioning System; Machine vision; Mobile robot; Political science; Robot; Law; Telecommunications,,,,,,http://dx.doi.org/10.1016/j.measurement.2023.112636,,10.1016/j.measurement.2023.112636,,,0,001-704-442-629-618; 003-772-960-648-066; 017-958-983-798-378; 024-591-739-547-572; 025-697-759-605-284; 027-677-726-158-590; 028-149-820-987-827; 031-166-974-618-429; 039-723-658-854-227; 045-206-353-366-638; 049-921-334-736-611; 058-768-199-347-004; 067-975-544-022-146; 076-939-128-536-963; 091-215-257-662-640; 093-379-831-122-539; 128-640-816-601-130; 149-227-403-461-961,4,false,,
087-272-456-866-28X,Navigation System for the Blind: Auditory Display Modes and Guidance,,1998,journal article,Presence: Teleoperators and Virtual Environments,10547460; 15313263,MIT Press - Journals,United States,Jack M. Loomis; Reginald G. Golledge; Roberta L. Klatzky,"The research we are reporting here is part of our effort to develop a navigation system for the blind. Our long-term goal is to create a portable, self-contained system that will allow visually impaired individuals to travel through familiar and unfamiliar environments without the assistance of guides. The system, as it exists now, consists of the following functional components: (1) a module for determining the traveler's position and orientation in space, (2) a Geographic Information System comprising a detailed database of our test site and software for route planning and for obtaining information from the database, and (3) the user interface. The experiment reported here is concerned with one function of the navigation system: guiding the traveler along a predefined route. We evaluate guidance performance as a function of four different display modes: one involving spatialized sound from a virtual acoustic display, and three involving verbal commands issued by a synthetic speech display. The virtual display mode fared best in terms of both guidance performance and user preferences.",7,2,193,203,Human–computer interaction; Artificial intelligence; Auditory display; Navigation system; Software; Computer vision; Computer science; Geographic information system; Turn-by-turn navigation; Mode (computer interface); Function (engineering); User interface,,,,,https://dl.acm.org/doi/10.1162/105474698565677 http://dx.doi.org/10.1162/105474698565677 https://dx.doi.org/10.1162/105474698565677 https://ieeexplore.ieee.org/document/6788010/ http://monet.cs.columbia.edu/courses/mobwear/resources/loomis-presence98.pdf https://dl.acm.org/citation.cfm?id=1246758 https://doi.org/10.1162/105474698565677 https://dblp.uni-trier.de/db/journals/presence/presence7.html#LoomisGK98 https://www.mitpressjournals.org/doi/abs/10.1162/105474698565677 https://direct.mit.edu/pvar/article/7/2/193/18185/Navigation-System-for-the-Blind-Auditory-Display,http://dx.doi.org/10.1162/105474698565677,,10.1162/105474698565677,2017967298,,2,013-523-374-269-739; 014-015-592-708-563; 015-476-928-897-846; 016-795-216-845-76X; 016-869-853-016-912; 025-423-906-830-062; 026-048-436-398-74X; 034-443-036-134-243; 040-690-288-101-937; 042-020-395-330-153; 044-192-308-604-739; 050-027-658-244-495; 058-468-795-176-403; 059-084-198-148-336; 067-932-436-626-540; 071-405-244-808-059; 080-270-063-470-184; 086-288-728-578-303; 088-470-436-866-921; 093-500-669-863-559; 103-769-217-393-19X; 127-769-262-221-224; 157-402-782-455-46X; 190-621-332-208-185,330,false,,
087-292-069-990-908,A 2D Mapping Method Based on Virtual Laser Scans for Indoor Robots,2021-06-04,2021,journal article,International Journal of Automation and Computing,14768186; 17518520,Springer Science and Business Media LLC,Germany,Xuyang Shao; Guohui Tian; Ying Zhang,,18,5,747,765,Field of view; Artificial intelligence; Projection (set theory); Metric (mathematics); Obstacle; Computer vision; Computer science; Occupancy grid mapping; RGB color model; Simultaneous localization and mapping; Robot,,,,,https://doi.org/10.1007/s11633-021-1304-1 http://www.ijac.net/en/article/doi/10.1007/s11633-021-1304-1 https://dblp.uni-trier.de/db/journals/ijautcomp/ijautcomp18.html#ShaoTZ21 https://link.springer.com/content/pdf/10.1007/s11633-021-1304-1.pdf http://ir.ia.ac.cn/handle/173211/45461 http://ijac.xml-journal.net/en/article/doi/10.1007/s11633-021-1304-1,http://dx.doi.org/10.1007/s11633-021-1304-1,,10.1007/s11633-021-1304-1,3166282679,,0,001-488-870-478-576; 008-403-597-435-186; 008-800-278-762-840; 011-794-886-103-048; 012-735-168-671-581; 013-455-036-449-006; 016-498-533-511-332; 017-819-671-909-34X; 019-288-649-597-848; 025-210-861-992-842; 025-530-996-736-590; 026-672-312-718-982; 030-431-481-851-60X; 035-068-852-538-718; 039-195-005-710-430; 041-333-506-109-883; 042-878-847-356-424; 044-486-903-186-790; 046-031-714-105-068; 047-518-969-230-056; 049-831-461-287-223; 052-389-842-490-021; 059-502-599-743-127; 061-101-033-659-713; 064-824-427-051-119; 066-238-126-613-238; 067-000-945-686-55X; 067-644-621-121-687; 067-877-640-759-421; 068-558-012-027-543; 077-418-416-591-134; 077-673-803-334-785; 089-156-568-698-078; 091-772-372-760-184; 099-351-606-902-321; 100-733-292-935-098; 101-187-047-185-669; 106-914-580-487-833; 125-866-920-756-856; 126-902-966-250-537; 141-077-134-230-230; 155-191-040-621-705; 179-779-983-362-76X,3,false,,
087-701-946-411-432,CAIP (2) - An Electronic Travel Aid to Assist Blind and Visually Impaired People to Avoid Obstacles,2015-08-26,2015,book chapter,Computer Analysis of Images and Patterns,03029743; 16113349,Springer International Publishing,Germany,Filippo Luigi Maria Milotta; Dario Allegra; Filippo Stanco; Giovanni Maria Farinella,"When devices and applications provide assistance to people they become part of assistive technology. If the assistance is given to impaired people, then it is possible to refer those technologies as adaptive technologies. The main aims of these systems are substitution of physical assistants and the improvement of typical tools already available for impaired people. In this paper some benefits and examples of adaptive technology applications will be discussed. Moreover we present an adaptive technology framework to avoid obstacles to be exploited by visually impaired and blind people. The proposed assistive technology has been designed to perform vision substitution; specifically it provides Electronic Travel Aid ETA capabilities through the processing of information acquired with a depth sensor such that the user can avoid obstacles during the environment exploration. In the proposed system we require to know just the height of the sensor with respect to the ground floor to calibrate the ETA system. Experiments are performed to asses the proposed system.",,,604,615,Human–computer interaction; Substitution (logic); Ground floor; Assistive technology; Visually impaired; Computer science; Simulation; Obstacle avoidance,,,,,https://link.springer.com/10.1007/978-3-319-23117-4_52 https://rd.springer.com/chapter/10.1007/978-3-319-23117-4_52 https://dblp.uni-trier.de/db/conf/caip/caip2015-2.html#MilottaASF15 https://link.springer.com/chapter/10.1007/978-3-319-23117-4_52,http://dx.doi.org/10.1007/978-3-319-23117-4_52,,10.1007/978-3-319-23117-4_52,2296194749,,0,002-890-928-346-413; 009-028-177-897-186; 010-004-081-294-240; 012-535-467-714-805; 013-056-435-015-274; 013-847-555-426-159; 052-236-623-665-955; 054-466-795-082-525; 060-257-784-475-928; 080-976-141-160-515; 138-528-673-324-553,8,false,,
088-024-421-732-373,Smart Voice Navigation and Object Perception for Individuals with Visual Impairments,2023-10-11,2023,conference proceedings article,"2023 7th International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)",,IEEE,,Sheetalrani Rukmaji Kawale; Shruti Mallikarjun; Dankan Gowda V; K D V Prasad; Anusha M N; Anil Kumar N,"Technological advancements have brought about substantial changes to the accessibility alternatives that cater to those with diverse abilities. The integration of artificial intelligence (AI) into assistive technology has presented unprecedented opportunities for enhancing the autonomy and quality of life of those with disabilities. The focus of rigorous investigation is on ""Smart Blind Sticks,"" which are novel equipment designed to enhance the mobility and safety of those with visual impairments. This article presents a comprehensive account of the conceptualization, design, and implementation of a technologically advanced blind stick. The primary objective of this innovative device is to detect and promptly respond to impediments in the immediate environment, hence facilitating navigation for those with vision impairments. The proposed system utilizes advanced sensor technologies and advanced data processing techniques to provide precise obstacle detection. Moreover, it employs state-of-the-art navigation systems to provide instantaneous guidance, guaranteeing seamless and secure transportation at all instances. The distinguishing characteristics of the smart blind stick set it apart from conventional white canes, hence affording its users more mobility. Experimental evidence showcases the practical use of this approach, therefore emphasizing its capacity to significantly enhance accessibility and promote autonomy among individuals with visual impairments.",,,,,Computer science; Obstacle; Human–computer interaction; Conceptualization; Perception; Autonomy; Set (abstract data type); Visualization; Artificial intelligence; Psychology; Neuroscience; Political science; Law; Programming language,,,,,,http://dx.doi.org/10.1109/i-smac58438.2023.10290607,,10.1109/i-smac58438.2023.10290607,,,0,016-343-563-055-343; 031-532-771-330-967; 056-734-407-356-215; 064-100-103-232-577; 084-621-956-547-983; 086-525-213-924-875; 100-294-171-241-127; 111-432-353-239-994; 122-447-093-474-63X; 126-524-218-035-094; 138-833-826-742-547,6,false,,
088-040-125-905-954,Obstacle-free paths detection by means of disparity maps for navigation assistance to visually impaired people,,2011,,,,,,Nuria Ortigosa Araque,"The detection of surrounding obstacle-free areas is an essential task for many intelligent automotive and robotic applications. It is also of interest in navigation tasks for Electronic Travel Aid devices, which help in mobility and independence to users with physical handicaps, such as blind or partially sighted people. In the last years, one of the most researched topics in intelligent systems has been the obstacle detection, due to the increasing interest in a variety of applications, from autonomous robots to automatic driving systems. Particularly, in the computer vision research area, each year many new references that perform obstacle detection appear, most of them by means of stereo vision or by combining single-cameras with other kind of sensors, such as ultrasound, sonar or lidar systems. This PhD dissertation undertakes the task of developing different methods to solve a related problem, which is the detection of obstacle-free paths in outdoor scenarios by using the information of disparity maps. These disparity maps are obtained by processing stereo correspondences from the left and right images captured by a stereo vision system. Given that the implemented detection algorithms aim to be integrated in a navigation system to support mobility assistance to any kind of visually impaired person, they must fulfil the computational complexity and memory requirements to assure that they are suitable to be processed at real-time frame rates with no-extra dedicated hardware. In the studied methods, fuzzy logic is used to provide certainties of being an obstacle-free path of the analyzed areas, to allow to deal with gradual transitions between obstacles and free paths, and to allow more flexibility and robustness. Experimental results show that the implemented methods provide good results and that they fulfil the system device constraints.",,,,,Artificial intelligence; Obstacle; Navigation system; Flexibility (engineering); Computer vision; Frame rate; Computer science; Intelligent decision support system; Stereopsis; Robot; Robustness (computer science),,,,,https://dialnet.unirioja.es/servlet/tesis?codigo=250915,https://dialnet.unirioja.es/servlet/tesis?codigo=250915,,,3003428783,,0,,0,false,,
088-052-168-243-979,Robotic manipulation and the role of the task in the metric of success,2019-08-09,2019,journal article,Nature Machine Intelligence,25225839,Springer Science and Business Media LLC,,Valerio Ortenzi; Marco Controzzi; Francesca Cini; Juxi Leitner; Matteo Bianchi; Maximo A. Roa; Peter Corke,,1,8,340,346,Human–computer interaction; Perspective (graphical); Metric (mathematics); Judgement; Conversation; Action (philosophy); Quality (business); Task (project management); Object (philosophy); Computer science,,,,RCUK | Engineering and Physical Sciences Research Council; EC | Horizon 2020 Framework Programme; EC | Horizon 2020 Framework Programme; EC | Horizon 2020 Framework Programme; EC | Horizon 2020 Framework Programme; Department of Education and Training | Australian Research Council; Department of Education and Training | Australian Research Council,https://www.nature.com/articles/s42256-019-0078-4.pdf https://www.nature.com/articles/s42256-019-0078-4 https://qa-eprints.qut.edu.au/200125/ https://eprints.qut.edu.au/200125/ https://arpi.unipi.it/handle/11568/1016363?mode=full.428,http://dx.doi.org/10.1038/s42256-019-0078-4,,10.1038/s42256-019-0078-4,2968849627,,0,000-640-272-423-53X; 000-785-637-953-793; 002-317-677-307-441; 003-098-300-201-54X; 004-304-512-259-963; 005-288-666-259-148; 005-553-789-511-205; 010-652-958-127-154; 012-931-722-365-281; 013-756-588-425-407; 014-241-147-658-655; 017-018-603-922-972; 017-409-603-350-888; 021-278-831-836-625; 021-523-523-387-422; 023-948-224-809-150; 028-252-636-304-905; 031-346-467-672-648; 032-303-086-184-997; 032-723-951-347-676; 033-760-756-699-034; 038-049-324-005-26X; 040-115-916-201-824; 040-872-252-749-763; 041-288-015-953-721; 042-436-594-482-669; 043-004-723-068-964; 043-054-344-977-696; 048-627-965-548-891; 048-694-350-809-178; 050-213-667-390-75X; 054-960-123-075-190; 055-143-210-098-012; 058-537-596-488-128; 060-661-936-749-233; 061-152-511-217-291; 062-806-307-892-106; 064-337-648-580-964; 066-708-070-852-451; 067-126-958-476-408; 067-388-265-838-576; 068-343-474-988-113; 072-340-865-663-923; 073-301-543-756-97X; 082-251-189-233-810; 084-303-497-995-766; 087-107-038-476-503; 088-452-944-232-512; 089-400-065-328-712; 101-497-865-118-031; 102-555-702-781-751; 104-617-971-424-705; 104-838-914-197-496; 104-968-465-100-77X; 106-745-468-671-222; 137-398-549-154-703; 140-662-689-773-31X; 141-961-524-396-866; 156-934-295-862-210; 159-885-989-866-655; 163-260-655-868-436,27,false,,
088-092-343-860-584,Indoor objects detection system implementation using multi-graphic processing units,2021-09-25,2021,journal article,Cluster Computing,13867857,Kluwer Academic Publishers,Netherlands,Mouna Afif; Riadh Ayachi; Mohamed Atri,"Indoor objects detection and recognition plays an important role in computer science and artificial intelligence fields. This task plays also a crucial role especially for blind and visually impaired persons (VIP) assistance navigation. Aiming to address this problem, we propose in this paper to develop a new indoor object detection system based on deep learning algorithms. Unfortunately, this type of algorithms requires heavy computational resources, and energy consumption. To address this problem, we propose a CUDA multi-GPU framework implementation of the proposed system. Generally deep learning based algorithms require huge amount of data to train and test networks. We propose to develop a new indoor dataset which consists of 11,000 indoor images containing 25 indoor landmark objects highly recommended for blind and VIP navigation. Based on the obtained results, the developed system shows big efficiency in terms of detection accuracy as well as processing time.",,,1,15,Deep learning; Artificial intelligence; Implementation; Object detection; CUDA; Energy consumption; Landmark; Task (project management); Visually Impaired Persons; Computer science; Real-time computing,,,,,https://link.springer.com/article/10.1007/s10586-021-03419-9/tables/10,https://link.springer.com/article/10.1007/s10586-021-03419-9/tables/10,,,3203390680,,0,003-270-934-605-038; 003-567-756-778-880; 007-759-025-744-822; 020-233-013-143-936; 021-360-232-245-594; 021-433-934-874-859; 025-522-385-395-133; 026-400-078-913-130; 026-994-702-158-53X; 035-277-967-833-996; 037-182-418-585-695; 041-074-700-593-708; 044-396-615-675-996; 045-309-399-228-849; 047-040-726-932-715; 049-093-905-733-529; 049-317-239-158-314; 054-876-301-922-486; 056-307-484-095-075; 061-579-383-511-500; 061-866-612-132-646; 062-800-469-258-59X; 066-835-465-051-044; 070-103-498-360-506; 070-146-203-962-535; 075-533-194-094-730; 077-689-906-335-058; 077-761-069-745-482; 083-477-719-417-886; 084-497-516-230-848; 085-507-633-373-250; 108-396-749-438-718; 109-806-014-448-128; 111-951-712-498-571; 133-703-464-031-598; 134-048-572-801-690; 138-852-374-862-575; 139-552-118-652-140; 171-091-935-415-285; 172-226-429-921-38X; 192-697-687-929-866; 198-200-840-981-591,0,false,,
088-424-322-032-673,Multi-Sound-Source Localization Using Machine Learning for Small Autonomous Unmanned Vehicles with a Self-Rotating Bi-Microphone Array,2021-10-27,2021,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Deepak Gala; Nathan Lindsay; Liang Sun,"While vision-based localization techniques have been widely studied for small autonomous unmanned vehicles (SAUVs), sound-source localization capabilities have not been fully enabled for SAUVs. This paper presents two novel approaches for SAUVs to perform three-dimensional (3D) multi-sound-sources localization (MSSL) using only the inter-channel time difference (ICTD) signal generated by a self-rotating bi-microphone array. The proposed two approaches are based on two machine learning techniques viz., Density-Based Spatial Clustering of Applications with Noise (DBSCAN) and Random Sample Consensus (RANSAC) algorithms, respectively, whose performances were tested and compared in both simulations and experiments. The results show that both approaches are capable of correctly identifying the number of sound sources along with their 3D orientations in a reverberant environment.",103,3,52,,RANSAC; Machine learning; Noise; Artificial intelligence; Microphone array; SIGNAL (programming language); Time difference; Spatial clustering; Computer science; DBSCAN; Acoustic source localization,,,,,https://link.springer.com/article/10.1007/s10846-021-01481-4,http://dx.doi.org/10.1007/s10846-021-01481-4,,10.1007/s10846-021-01481-4,3209660386,,0,001-918-817-219-986; 004-366-253-982-914; 005-270-746-979-617; 011-719-936-244-130; 013-984-778-015-315; 015-123-168-160-214; 020-300-626-066-205; 024-870-807-790-876; 025-068-861-828-765; 026-705-067-877-91X; 028-303-905-295-179; 031-849-838-040-821; 033-420-900-329-636; 033-448-926-376-187; 037-133-000-624-051; 047-816-016-957-796; 054-064-204-792-110; 058-555-265-703-363; 064-744-073-098-226; 071-932-077-495-458; 072-356-231-598-062; 073-615-451-555-910; 074-904-078-918-185; 075-270-990-119-167; 075-676-736-042-723; 081-108-028-675-898; 082-483-011-740-607; 083-067-500-712-176; 086-852-740-493-067; 108-206-953-532-779; 108-891-351-325-164; 113-324-850-940-559; 120-168-593-922-393; 122-931-772-959-211; 123-661-040-589-066; 124-331-219-122-778; 129-411-542-565-167; 131-588-885-939-058; 131-647-938-626-993; 141-995-630-420-618; 143-847-506-100-615; 144-264-859-561-312; 153-212-970-009-771; 153-769-667-564-957; 162-509-740-935-570; 176-135-438-456-485; 185-911-304-241-628,5,true,,green
088-627-895-347-429,Soil friction coefficient estimation using CNN included in an assistive system for walking in urban areas,2023-08-10,2023,journal article,Journal of Ambient Intelligence and Humanized Computing,18685137; 18685145,Springer Science and Business Media LLC,Germany,Oleksiy Gensytskyy; Pratyush Nandi; Martin J.-D. Otis; Clinton Enow Tabi; Johannes C. Ayena,,14,10,14291,14307,Computer science; Convolutional neural network; Artificial intelligence; Segmentation; Frame (networking); Computer vision; Computation; Simulation; Telecommunications; Algorithm,,,,Mitacs; Mitacs,,http://dx.doi.org/10.1007/s12652-023-04667-w,,10.1007/s12652-023-04667-w,,,0,001-156-189-299-613; 003-470-952-647-684; 004-216-069-855-831; 005-427-234-377-412; 006-355-537-489-40X; 007-907-758-104-429; 008-361-673-204-978; 012-370-328-340-986; 013-791-696-430-151; 014-153-330-736-922; 016-415-106-921-058; 017-159-670-613-140; 017-499-516-520-553; 018-843-508-574-144; 019-174-068-077-363; 020-233-013-143-936; 020-561-455-331-064; 021-366-799-226-391; 025-670-117-400-666; 027-418-298-137-518; 027-712-485-636-459; 028-143-078-747-070; 030-507-740-370-455; 032-102-165-870-302; 035-804-512-361-234; 042-251-157-585-319; 051-401-403-330-133; 053-413-243-927-176; 056-404-390-495-54X; 065-078-747-021-140; 067-583-626-133-824; 069-480-623-909-181; 075-703-478-713-600; 081-599-292-131-338; 083-671-887-012-721; 089-539-771-278-544; 090-408-646-543-535; 091-023-860-675-167; 091-910-369-114-547; 093-548-249-489-049; 094-381-859-252-65X; 099-334-901-486-872; 106-474-978-181-66X; 108-755-107-259-605; 119-358-620-680-732; 121-439-923-704-187; 137-032-010-877-936; 141-649-000-612-322; 164-630-824-349-584; 183-569-371-693-516,1,false,,
088-726-154-436-006,Sampling-based Non-Holonomic Path Generation for Self-driving Cars,2021-12-28,2021,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Sotirios Spanogiannopoulos; Yahya Zweiri; Lakmal Seneviratne,,104,1,,,Holonomic; Motion planning; Benchmark (surveying); Path (computing); Nonholonomic system; Computer science; Planner; Collision; Sampling (signal processing); Real-time computing; Robot; Mobile robot; Mathematical optimization; Artificial intelligence; Mathematics; Computer vision; Computer security; Geodesy; Filter (signal processing); Programming language; Geography,,,,,,http://dx.doi.org/10.1007/s10846-021-01440-z,,10.1007/s10846-021-01440-z,,,0,007-478-713-467-223; 008-647-462-378-293; 014-809-086-655-889; 015-518-986-871-954; 017-513-000-012-644; 018-205-185-779-340; 024-922-468-284-494; 025-043-162-854-365; 025-604-118-174-626; 027-458-286-566-610; 028-077-300-073-845; 028-180-432-662-479; 028-305-374-308-469; 028-417-294-744-11X; 031-187-847-362-080; 032-765-070-015-343; 032-967-752-252-456; 034-112-545-889-154; 036-439-762-038-337; 039-877-758-156-460; 040-345-004-226-432; 047-944-660-547-524; 048-525-818-743-526; 049-178-577-170-191; 050-403-923-589-168; 064-275-782-281-876; 064-478-161-033-757; 068-923-735-219-517; 069-025-449-385-460; 072-747-827-097-11X; 077-673-803-334-785; 081-510-263-955-940; 090-323-482-842-981; 096-771-279-576-62X; 098-030-755-037-147; 099-327-271-738-979; 103-580-620-629-706; 104-162-117-391-125; 106-786-092-132-394; 113-150-128-914-671; 113-570-449-525-07X; 118-346-853-250-78X; 125-189-868-563-76X; 129-638-983-274-599; 134-184-688-780-146; 138-480-515-139-264; 139-426-480-491-493,9,false,,
088-912-328-596-763,"The Authenticity of Machine-Augmented Human Intelligence: Therapy, Enhancement, and the Extended Mind",2020-10-24,2020,journal article,Neuroethics,18745490; 18745504,Springer Science and Business Media LLC,Netherlands,Allen Coin; Veljko Dubljević,,14,2,283,290,Human intelligence; Human–computer interaction; Interface (computing); Psychology; Externalism; Cognition; Praise; The Extended Mind; Human enhancement; Mathematical ability,,,,,https://link.springer.com/article/10.1007/s12152-020-09453-5,http://dx.doi.org/10.1007/s12152-020-09453-5,,10.1007/s12152-020-09453-5,3094556985,,0,001-787-381-860-782; 002-885-480-884-89X; 003-385-140-117-809; 006-533-321-797-330; 016-635-272-350-005; 017-122-130-814-209; 019-080-831-514-399; 019-837-466-048-458; 023-097-431-986-413; 024-801-865-678-402; 028-433-126-722-851; 034-196-646-262-503; 034-337-821-710-593; 039-617-781-200-801; 045-054-383-104-874; 045-397-140-835-511; 054-130-522-192-228; 079-426-774-617-19X; 080-079-936-024-664; 083-930-593-105-627; 092-778-206-174-941; 096-471-721-390-007; 097-012-326-303-974; 097-409-790-924-315; 105-718-270-657-628; 122-477-396-881-926; 128-288-848-632-871; 129-227-734-759-740; 132-658-938-284-751; 141-219-648-794-771; 151-221-259-746-173; 162-234-848-484-409; 187-613-808-508-928,25,false,,
089-188-444-759-648,Personalized Dynamics Models for Adaptive Assistive Navigation Systems,2018-04-11,2018,preprint,arXiv: Learning,,,,Eshed Ohn-Bar; Kris M. Kitani; Chieko Asakawa,"Consider an assistive system that guides visually impaired users through speech and haptic feedback to their destination. Existing robotic and ubiquitous navigation technologies (e.g., portable, ground, or wearable systems) often operate in a generic, user-agnostic manner. However, to minimize confusion and navigation errors, our real-world analysis reveals a crucial need to adapt the instructional guidance across different end-users with diverse mobility skills. To address this practical issue in scalable system design, we propose a novel model-based reinforcement learning framework for personalizing the system-user interaction experience. When incrementally adapting the system to new users, we propose to use a weighted experts model for addressing data-efficiency limitations in transfer learning with deep models. A real-world dataset of navigation by blind users is used to show that the proposed approach allows for (1) more accurate long-term human behavior prediction (up to 20 seconds into the future) through improved reasoning over personal mobility characteristics, interaction with surrounding obstacles, and the current navigation goal, and (2) quick adaptation at the onset of learning, when data is limited.",,,,,Human–computer interaction; Dynamics (music); Transfer of learning; Haptic technology; Personal mobility; Computer science; Adaptation (computer science); Reinforcement learning,,,,,https://arxiv.org/abs/1804.04118 http://ui.adsabs.harvard.edu/abs/2018arXiv180404118O/abstract,https://arxiv.org/abs/1804.04118,,,2895143214,,0,,4,true,,unknown
089-234-869-099-897,Indoor visual mapping and navigation for blind people,2022-03-05,2022,conference proceedings article,Fourteenth International Conference on Machine Vision (ICMV 2021),,SPIE,,Darius Plikynas,"Admittedly, machine vision-based assistive applications are beneficial for blind and visually impaired (BVI) persons. Such a need has numerous already implemented outdoor assistive solutions. However, there are much less effective solutions for indoor navigation and orientation. It is due to the absence of GPS signals and the need for infrastructural investments (such as WI-FI signals, beamers, RFID tags). In this paper, we present another way - a wearable electronic traveling aid (ETA) system for the BVI persons using outsourcing, i.e., volunteers’ mapping of buildings indoor routes. Volunteers use the proposed wearable ETA device to record indoor routes stored in the web cloud database using web services. Smartphones’ IMU and other sensors, stereo and depth camera, audio and haptic devices, computer vision algorithms, and computational intelligence are employed for objects detection and recognition, and consequently, intelligent routing and mapping of indoor spaces. Integration of semantic data of points of interest (such as stairs, doors, WC, entrances/exits) and building (evacuation) schemes makes the proposed approach even more attractive to the BVI users. The presented approach can also be employed to crowdsourcing real-time help in complex navigational situations such as dead reckoning, avoiding various obstacles, or unforeseen situations.",,,,,Computer science; Inertial measurement unit; Global Positioning System; Crowdsourcing; Doors; Wearable computer; Computer vision; Stairs; Cloud computing; Cloud database; Obstacle; Dead reckoning; Augmented reality; Artificial intelligence; Human–computer interaction; Real-time computing; Embedded system; World Wide Web; Engineering; Telecommunications; Civil engineering; Law; Political science; Operating system,,,,,,http://dx.doi.org/10.1117/12.2623893,,10.1117/12.2623893,,,0,016-415-106-921-058; 036-262-770-234-628; 046-097-009-151-183; 057-192-756-123-973; 061-729-705-527-929; 064-043-611-038-916; 074-405-882-074-984; 082-877-344-337-340; 089-822-552-618-426; 097-395-382-637-910; 102-952-660-914-374; 122-835-118-849-552; 163-650-743-266-789,0,false,,
089-539-771-278-544,A Hybrid System to Assist Visually Impaired People,2021-06-11,2021,journal article,SN Computer Science,2662995x; 26618907,Springer Science and Business Media LLC,,Leela Pravallika Siriboyina; Venkata Sainath Gupta Thadikemalla,,2,4,1,9,Human–computer interaction; Guidance system; Obstacle; Auditory feedback; White cane; Visually impaired; Computer science; Hybrid system; Cognitive neuroscience of visual object recognition; Coping (psychology),,,,,https://link.springer.com/article/10.1007/s42979-021-00703-8 https://link.springer.com/content/pdf/10.1007/s42979-021-00703-8.pdf https://dblp.uni-trier.de/db/journals/sncs/sncs2.html#SiriboyinaT21,http://dx.doi.org/10.1007/s42979-021-00703-8,,10.1007/s42979-021-00703-8,3169972792,,0,008-361-673-204-978; 018-843-508-574-144; 021-176-270-241-156; 030-884-979-386-25X; 032-661-597-680-28X; 037-168-339-082-036; 057-847-555-860-541; 069-750-894-920-400; 071-338-626-777-780; 073-928-284-197-323; 078-736-735-476-395; 109-525-288-959-67X; 111-356-166-368-094; 176-619-480-167-936; 186-810-372-950-411,1,false,,
089-540-520-725-483,A realistic elastic rod model for real-time simulation of minimally invasive vascular interventions,2010-06-01,2010,journal article,The Visual Computer,01782789; 14322315,Springer Science and Business Media LLC,Germany,Wen Tang; Pierre Lagadec; Derek A. Gould; Tao Ruan Wan; Jianhua Zhai; Thien How,,26,9,1157,1165,Real-time simulation; Blood vessel walls; Elastic rods; Medical instruments; Computer science; Simulation; Computation; Interventional radiology; Computer graphics,,,,,https://dl.acm.org/doi/10.1007/s00371-010-0442-1 https://rd.springer.com/article/10.1007/s00371-010-0442-1 https://core.ac.uk/display/9440018 https://link.springer.com/article/10.1007/s00371-010-0442-1 https://dblp.uni-trier.de/db/journals/vc/vc26.html#TangLGWZH10,http://dx.doi.org/10.1007/s00371-010-0442-1,,10.1007/s00371-010-0442-1,2083498294,,0,001-121-083-135-67X; 005-746-757-156-182; 007-678-414-060-024; 008-367-974-797-073; 008-461-457-062-512; 010-578-288-532-982; 014-814-144-487-622; 017-283-305-506-265; 031-164-499-025-325; 036-364-234-524-645; 041-992-588-369-003; 044-787-238-427-29X; 044-916-462-661-230; 047-546-020-955-788; 053-445-393-794-731; 059-799-456-049-453; 063-726-974-128-052; 065-268-077-175-171; 081-227-436-528-901; 086-715-032-455-667; 095-165-828-979-856; 108-621-617-724-761; 113-956-282-491-278; 115-946-860-878-954; 122-001-162-781-654; 148-872-129-746-811; 162-809-980-536-637,57,false,,
089-568-337-357-496,"Universal Access in the Information Society: Methods, Tools, and Interaction Technologies",,2001,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Constantine Stephanidis; Anthony Savidis,,1,1,40,55,Human–computer interaction; User interface design; User experience design; Relation (database); Information society; Computer science; User modeling; Adaptation (computer science); User interface; Universal design,,,,,https://dblp.uni-trier.de/db/journals/uais/uais1.html#StephanidisS01 https://link.springer.com/article/10.1007/s102090100008,http://dx.doi.org/10.1007/s102090100008,,10.1007/s102090100008,114612135,,0,001-700-271-591-484; 002-372-947-841-217; 004-891-391-225-829; 005-192-026-246-449; 008-208-412-087-872; 011-679-964-026-452; 011-968-358-769-548; 012-325-922-595-401; 015-846-267-053-632; 019-858-836-073-646; 036-158-946-964-924; 036-382-362-388-86X; 038-798-796-772-406; 050-050-882-453-879; 050-396-621-180-110; 052-157-348-485-478; 065-687-461-623-06X; 068-291-693-089-108; 072-120-979-657-906; 074-333-018-198-95X; 076-282-074-993-312; 077-566-166-438-966; 087-070-389-034-153; 091-209-147-597-779; 097-479-544-306-03X; 100-096-417-325-164; 102-286-034-287-72X; 106-724-299-687-342; 109-579-510-749-432; 132-387-925-312-619; 188-123-573-342-621,194,false,,
089-568-871-323-077,Welcome to the Cyborg Olympics,2016-08-03,2016,journal article,Nature,14764687; 00280836,Springer Science and Business Media LLC,United Kingdom,Sara Reardon,The Cybathlon aims to help disabled people navigate the most difficult course of all: the everyday world.,536,7614,20,22,Biotechnology; Psychology; MEDLINE; Exoskeleton Device; Electric stimulation therapy; Competitive behavior; Disabled people; Engineering ethics; Activities of daily living,,Activities of Daily Living; Algorithms; Competitive Behavior; Diffusion of Innovation; Disabled Persons/rehabilitation; Electric Stimulation Therapy; Exoskeleton Device; Humans; Man-Machine Systems; Motivation; Prostheses and Implants; Reward; Robotics/instrumentation; Switzerland,,,https://EconPapers.repec.org/RePEc:nat:nature:v:536:y:2016:i:7614:d:10.1038_536020a https://europepmc.org/article/MED/27488784 http://ui.adsabs.harvard.edu/abs/2016Natur.536...20R/abstract https://www.ncbi.nlm.nih.gov/pubmed/27488784 https://www.nature.com/articles/536020a https://pubmed.ncbi.nlm.nih.gov/27488784/,http://dx.doi.org/10.1038/536020a,27488784,10.1038/536020a,2486166888,,0,028-960-880-639-472,2,true,,bronze
089-625-986-822-245,Ultrasonic blind walking stick,,2017,journal article,Mangalmay Journal of Management & Technology,09737251,,,Bejayeta Manna; Reeti Pandey; Saima Jan; Ankit Singh,God gifted sense of vision to the human being is an important aspect of our life. But there are some unfortunate people who lack the ability of visualizing things. The visually impaired have to face many challenges in their daily life. The problem gets worse when there is an obstacle in front of them. Blind stick is an innovative stick designed for visually disabled people for improved navigation. The paper presents a theoretical system concept to provide a smart ultrasonic aid for blind people. The system is intended to provide overall measures-Artificial vision and object detection. The aim of the overall system is to provide a low cost and efficient navigation aid for a visually impaired person who gets a sense of artificial vision by providing information about the environmental scenario of static and dynamic objects around them. Ultrasonic sensors are used to calculate distance of the obstacles around the blind person to guide the user towards the available path. Output is in the form of sequence of beep sound which the blind person can hear.,7,2,241,249,Human–computer interaction; Face (geometry); Object detection; Obstacle; Artificial vision; Navigation aid; Walking stick; Disabled people; Computer science; Ultrasonic sensor,,,,,https://www.indianjournals.com/ijor.aspx?target=ijor:mjmt&volume=7&issue=2&article=026,https://www.indianjournals.com/ijor.aspx?target=ijor:mjmt&volume=7&issue=2&article=026,,,3208134249,,0,,0,false,,
089-796-474-980-887,Accessible wayfinding and navigation: a systematic mapping study,2021-09-08,2021,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Catia Prandi; Barbara Rita Barricelli; Silvia Mirri; Daniela Fogli,"<jats:title>Abstract</jats:title><jats:p>Urban environments, university campuses, and public and private buildings often present architectural barriers that prevent people with disabilities and special needs to move freely and independently. This paper presents a systematic mapping study of the scientific literature proposing devices, and software applications aimed at fostering accessible wayfinding and navigation in indoor and outdoor environments. We selected 111 out of 806 papers published in the period 2009–2020, and we analyzed them according to different dimensions: at first, we surveyed which solutions have been proposed to address the considered problem; then, we analyzed the selected papers according to five dimensions: context of use, target users, hardware/software technologies, type of data sources, and user role in system design and evaluation. Our findings highlight trends and gaps related to these dimensions. The paper finally presents a reflection on challenges and open issues that must be taken into consideration for the design of future accessible places and of related technologies and applications aimed at facilitating wayfinding and navigation.</jats:p>",22,1,185,212,,,,,Alma Mater Studiorum - Università di Bologna,,http://dx.doi.org/10.1007/s10209-021-00843-x,,10.1007/s10209-021-00843-x,,,0,000-308-931-127-853; 000-310-080-063-70X; 003-986-547-484-030; 005-832-287-518-660; 006-941-596-710-313; 007-202-382-837-877; 008-435-171-123-261; 008-713-968-971-388; 009-604-493-900-415; 010-132-131-695-891; 010-737-690-567-329; 011-556-861-729-761; 011-917-707-351-44X; 011-924-474-053-85X; 012-370-328-340-986; 012-867-926-708-131; 012-977-902-721-152; 013-056-435-015-274; 013-108-056-949-247; 014-413-195-694-640; 014-609-893-033-350; 014-778-253-308-501; 015-008-890-486-381; 015-075-700-493-012; 016-353-677-841-536; 017-741-284-860-905; 018-262-121-105-137; 018-767-375-770-819; 018-774-283-654-517; 018-843-508-574-144; 019-775-185-370-343; 020-598-437-914-425; 021-025-510-815-780; 021-839-984-814-002; 024-520-735-445-351; 030-002-779-817-417; 030-455-603-509-463; 030-986-135-488-173; 032-284-166-524-962; 032-813-098-485-949; 033-707-686-495-876; 034-085-976-948-717; 034-222-058-985-816; 034-402-642-303-122; 034-763-340-353-646; 037-502-776-410-748; 037-566-006-652-399; 038-058-919-295-985; 039-921-446-667-451; 040-264-280-218-709; 040-322-596-688-021; 042-547-247-654-317; 044-944-331-539-794; 045-599-200-378-144; 045-734-952-699-353; 048-142-650-411-086; 048-602-286-235-606; 049-889-294-002-623; 051-191-701-687-744; 051-252-295-573-245; 052-242-496-076-509; 054-398-342-264-076; 055-562-043-219-927; 055-673-950-572-320; 058-441-759-443-033; 061-084-974-994-693; 061-435-037-206-64X; 061-611-750-218-70X; 063-020-732-359-873; 065-229-290-625-82X; 068-304-145-715-104; 068-526-647-480-345; 069-689-572-820-466; 070-987-995-415-981; 073-785-113-194-202; 074-395-732-414-174; 075-840-103-183-093; 076-056-710-552-791; 077-112-329-361-11X; 078-544-250-112-589; 079-433-787-480-087; 080-810-141-031-297; 083-853-269-246-35X; 084-669-569-682-846; 084-728-686-418-03X; 085-855-388-539-878; 086-308-757-277-927; 087-408-536-412-018; 087-747-103-290-058; 088-856-257-261-363; 090-506-797-178-634; 091-723-105-357-269; 092-374-505-118-21X; 098-855-505-069-409; 099-843-284-027-827; 099-873-453-549-026; 100-846-994-797-815; 102-219-700-158-525; 105-111-935-790-124; 105-926-119-689-682; 111-360-462-814-708; 114-876-249-653-016; 115-998-456-191-40X; 117-677-849-214-196; 119-036-310-031-196; 119-526-303-815-066; 119-651-051-183-500; 121-980-294-085-592; 125-560-999-676-098; 126-287-456-856-107; 127-982-809-141-600; 128-341-284-062-649; 128-697-094-967-022; 132-997-113-420-890; 135-497-997-808-708; 136-438-153-973-435; 136-732-042-076-069; 136-909-504-607-970; 139-928-818-160-262; 139-986-942-713-317; 145-362-557-827-803; 146-197-245-984-789; 168-036-345-097-619; 168-099-649-007-625; 179-067-951-617-101; 186-053-612-941-659,21,true,cc-by,hybrid
089-832-968-992-306,Modern Cane Enabling Obstacle Detection and Navigation Based on Human Gesture using Voice Board,2019-12-19,2019,,,,,,Saeed Kamali; V. Malathy; M. Anand,"There are six sense organs for human being. Out of six sense organs, vision sense is an important sense for human life. But, the people who are visually challenged may find difficulty to lead their daily life. Their difficulty will become a problem when they have an object or an obstacle in front of them. Therefore, a special design of a stick is required for the use visually challenged human to help for their navigation. In this paper, the obstacle detection is proposed to assist the blind people by giving them an ultrasonic aid.The purpose of this paper is to develop a cane system to give a sense of artificial vision and to detect the objects for the sake of blind people. It is also of low cost and provides efficient navigation assistance. The visually challenged people get information about the surrounding where static and moving objects present. The distance between the blind person and the object is measured by ultrasonic sensors. With that measurement, the blind person can move in the path. The ultrasonic sensor indicates the existence of an object by giving an output of beep sounds which can be heard by the user of the white system.The increasing advanced technologies must be utilized to get the benefits soon.The proposed system helps the blind people to avoid collision with others and navigating them according to the obstacles. And also the system consists of sensors to avoid person from falling. This provides the person from falling and provides grip to the person. The indication is provided to the concerned person. The system also provides the stick location indication system and navigation system with gesture based control techniques.",81,,3900,3906,Human–computer interaction; Gesture; Control (management); Obstacle; Navigation system; Falling (sensation); Artificial vision; Navigation assistance; Object (philosophy); Computer science,,,,,https://www.testmagzine.biz/index.php/testmagzine/article/download/518/469 https://www.testmagzine.biz/index.php/testmagzine/article/view/518,https://www.testmagzine.biz/index.php/testmagzine/article/view/518,,,3016480056,,0,,1,false,,
090-042-328-255-171,HSI - Human-Robot Interaction for Assisted Wayfinding of a Robotic Navigation Aid for the Blind,,2019,book,2019 12th International Conference on Human System Interaction (HSI),,IEEE,,He Zhang; Cang Ye,"This paper introduces a new robotic navigation aid (RNA) for the visually impaired (VI). Two fundamental functions—wayfinding and human-robot interaction (HRI)—are presented for assisted wayfinding. The problem of wayfinding involves planning a path from the RNA's current location to the destination and following the path to get to the destination. To address the problem, we developed a new visual inertial odometry to estimate the RNA's pose by using the image and depth data from an RGB-D camera and the inertial data of an IMU. The estimated pose is used for path planning. To guide the user to follow the planned path, we designed an HRI interface with two guiding modes—the robocane mode and white-came mode. In the robocane mode, the RNA uses a motorized rolling tip to steer itself into the desired direction of travel (DDT) for the user to follow and track the planned path. In the white-cane mode, the RNA uses its speech interface to indicate the DDT to the user by audio messages. In this mode, the user swings the RNA just like using a conventional white cane. To make mode selection effortless, we developed a human intent detection (HID) method based on the decision tree mode. The method can detect the user intent and automatically select the appropriate mode according to the detected intent. Experimental results demonstrate the efficacies of the VIO, HRI, and HID methods for assisted wayfinding.",,,137,142,Interface (computing); Human–robot interaction; Artificial intelligence; PATH (variable); Computer vision; Computer science; Mode (computer interface); Odometry; Inertial measurement unit; Motion planning; Decision tree,,,,,https://dblp.uni-trier.de/db/conf/hsi/hsi2019.html#ZhangY19 https://ieeexplore.ieee.org/document/8942612 https://doi.org/10.1109/HSI47298.2019.8942612,http://dx.doi.org/10.1109/hsi47298.2019.8942612,,10.1109/hsi47298.2019.8942612,2998357093,,0,003-086-871-090-793; 022-940-138-590-719; 029-360-136-216-567; 037-403-741-955-39X; 050-095-187-695-437; 050-137-510-002-999; 064-521-070-547-235; 065-276-856-923-220; 074-006-536-890-200; 075-519-293-886-898; 077-683-848-099-335; 105-822-868-525-267; 153-697-872-076-389; 170-387-846-090-147; 193-123-800-849-774,7,false,,
090-260-266-886-781,Combining Invariant Features and the ALV Homing Method for Autonomous Robot Navigation Based on Panoramas,2011-05-10,2011,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Arnau Ramisa; Alex Goldhoorn; David Aldavert; Ricardo Toledo; Ramon López de Mántaras,"Biologically inspired homing methods, such as the Average Landmark Vector, are an interesting solution for local navigation due to its simplicity. However, usually they require a modification of the environment by placing artificial landmarks in order to work reliably. In this paper we combine the Average Landmark Vector with invariant feature points automatically detected in panoramic images to overcome this limitation. The proposed approach has been evaluated first in simulation and, as promising results are found, also in two data sets of panoramas from real world environments.",64,3,625,649,Artificial intelligence; Landmark; Autonomous robot navigation; Invariant feature; Visual homing; Computer vision; Computer science; Invariant (mathematics),,,,,https://digital.csic.es/handle/10261/138217 https://rd.springer.com/article/10.1007/s10846-011-9552-x https://core.ac.uk/display/21099921 https://link.springer.com/article/10.1007/s10846-011-9552-x https://doi.org/10.1007/s10846-011-9552-x https://www.iri.upc.edu/files/scidoc/1289-Combining-Invariant-Features-and-the-ALV-Homing-Method-for-Autonomous-Robot-Navigation-Based-on-Panoramas.pdf http://www.iiia.csic.es/~mantaras/alv.pdf https://www.iiia.csic.es/~mantaras/alv.pdf http://digital.csic.es/bitstream/10261/138217/1/JIRS_64%283-4%292011_625-49.pdf https://dialnet.unirioja.es/servlet/articulo?codigo=3788779 https://dblp.uni-trier.de/db/journals/jirs/jirs64.html#RamisaGATM11 https://core.ac.uk/download/80858951.pdf,http://dx.doi.org/10.1007/s10846-011-9552-x,,10.1007/s10846-011-9552-x,1967078514,,0,003-086-124-970-834; 004-543-946-163-844; 012-072-831-423-273; 013-759-719-034-849; 017-019-672-760-676; 018-804-175-799-297; 021-210-495-111-519; 021-620-148-568-444; 022-498-187-122-349; 024-942-406-825-505; 027-947-651-014-900; 028-037-989-331-413; 030-038-057-873-868; 034-851-341-170-18X; 043-913-804-677-225; 046-458-856-872-96X; 047-613-815-313-583; 049-854-285-982-316; 057-677-139-752-948; 058-253-338-790-418; 058-549-972-772-156; 064-533-587-679-976; 064-656-864-047-492; 067-277-157-471-04X; 069-936-267-138-597; 072-642-562-642-803; 075-250-724-827-816; 082-506-727-442-815; 093-927-745-338-426; 093-968-778-174-625; 095-264-022-660-794; 098-440-015-522-27X; 101-239-112-689-68X; 102-649-804-561-773; 102-897-962-893-469; 108-135-700-298-624; 112-718-412-269-959; 114-986-063-105-174; 133-943-846-582-224; 165-992-253-476-719; 169-041-348-073-455; 176-033-436-856-162; 181-797-068-816-79X; 183-107-700-653-747; 192-470-792-724-76X; 192-623-080-180-332,23,true,,green
090-327-449-932-625,Zebra Crossing Spotter: Automatic Population of Spatial Databases for Increased Safety of Blind Travelers - eScholarship,2015-10-01,2015,,,,,,Dragan Ahmetovic; Roberto Manduchi; James M. Coughlan; Sergio Mascetti,"Zebra Crossing Spotter: Automatic Population of Spatial Databases for Increased Safety of Blind Travelers Dragan Ahmetovic Roberto Manduchi Universita degli Studi di Milano University of California Santa Cruz dragan.ahmetovic@unimi.it James M. Coughlan manduchi@soe.ucsc.edu Smith-Kettlewell Eye Research Institute Universita degli Studi di Milano coughlan@ski.org sergio.mascetti@unimi.it Sergio Mascetti ABSTRACT In this paper we propose a computer vision-based technique that mines existing spatial image databases for discovery of zebra crosswalks in urban settings. Knowing the loca- tion of crosswalks is critical for a blind person planning a trip that includes street crossing. By augmenting existing spatial databases (such as Google Maps or OpenStreetMap) with this information, a blind traveler may make more in- formed routing decisions, resulting in greater safety during independent travel. Our algorithm first searches for zebra crosswalks in satel- lite images; all candidates thus found are validated against spatially registered Google Street View images. This cas- caded approach enables fast and reliable discovery and lo- calization of zebra crosswalks in large image datasets. While fully automatic, our algorithm could also be complemented by a final crowdsourcing validation stage for increased accu- racy. Independent travel can be extremely challenging without sight. Many blind persons learn (typically with the help of an Orientation and Mobility, or OM needs to follow the route safely while being aware of his or her lo- cation at all times; and needs to adapt to contingencies, for example if a sidewalk is undergoing repair and is not acces- sible. Each one of these tasks has challenges of its own. In particular, the lack of visual access to landmarks (for exam- ple, the location and layout of a bus stop or the presence of a pedestrian traffic light at an intersection) complicates the wayfinding process. Thus, a straightforward walk for a sighted person could become a complex, disorienting, and potentially hazardous endeavor for a blind traveler. Technological solutions for the support of blind wayfind- ing exist. Outdoors, where GPS can be relied upon for ap- proximate self-localization, a blind person can use accessible navigation apps. While these apps cannot substitute proper OM I.4.8 [IMAGE PRO- CESSING AND COMPUTER VISION]: Scene Analy- sis—Object recognition; K.4.2 [COMPUTERS AND SO- CIETY]: Social Issues—Assistive technologies for persons with disabilities General Terms Algorithms, Human Factors Keywords Orientation and Mobility, Autonomous navigation, Visual impairments and blindness, Satellite and street-level im- agery, Crowdsourcing Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. ASSETS’15, October 26–28, 2015, Lisbon, Portugal. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-3400-6/15/10 ...$15.00. DOI: http://dx.doi.org/10.1145/2700648.2809847 . INTRODUCTION http://mutcd.fhwa.dot.gov",,,,,Engineering; Pedestrian; Orientation and Mobility; Permission; Population; Zebra crossing; Global Positioning System; Crowdsourcing; Database; Server,,,,,https://escholarship.org/content/qt39x5w6fp/qt39x5w6fp.pdf https://escholarship.org/uc/item/39x5w6fp,https://escholarship.org/uc/item/39x5w6fp,,,2765652156,,0,,0,false,,
090-567-303-142-789,Simplification of Visual Rendering in Simulated Prosthetic Vision Facilitates Navigation.,2017-03-21,2017,journal article,Artificial organs,15251594; 0160564x,Wiley,United Kingdom,Victor Vergnieux; Marc J.-M. Macé; Christophe Jouffrais,"Visual neuroprostheses are yet limited and Simulated Prosthetic Vision (SPV) is used to evaluate potential and forthcoming functionality of these implants. SPV has been used to evaluate the minimum requirement on visual neuroprostheses characteristics to restore various functions such as reading, objects and face recognition, objects grasping, etc. Some of these studies focused on obstacle avoidance but only a few investigated orientation or navigation abilities with prosthetic vision. The resolution of current arrays of electrodes is not sufficient to allow navigation tasks without additional processing of the visual input. In this study, we simulated a low resolution array (15x18 electrodes, similar to a forthcoming generation of arrays) and evaluated the navigation abilities restored when visual information was processed with various computer vision algorithms to enhance the visual rendering. Three main visual rendering strategies were compared to a control rendering in a wayfinding task within an unknown environment. The control rendering corresponded to a resizing of the original image onto the electrode array size, according to the average brightness of the pixels. In the first rendering strategy, vision distance was limited to three, six or nine meters respectively. In the second strategy, the rendering was not based on the brightness of the image pixels, but on the distance between the user and the elements in the field of view. In the last rendering strategy, only the edges of the environments were displayed, similar to a wireframe rendering. All the tested renderings, except the 3 m limitation of the field of view, improved navigation performance and decreased cognitive load. Interestingly, the distance-based and wireframe renderings also improved the cognitive mapping of the unknown environment. These results show that low resolution implants are usable for wayfinding if specific computer vision algorithms are used to select and display appropriate information regarding the environment.",41,9,852,861,Field of view; Artificial intelligence; Pixel; Cognitive map; Rendering (computer graphics); Facial recognition system; Computer vision; Computer science; Cognitive load; Obstacle avoidance; Electrode array,Blind; Computer vision; Navigation; Retinal implant; Spatial cognition; Visual neuroprostheses; Wayfinding,"Adult; Algorithms; Comprehension; Electrodes; Female; Healthy Volunteers; Humans; Image Processing, Computer-Assisted; Male; Phosphenes; Prosthesis Design; Spatial Navigation; Vision Disorders/surgery; Vision, Ocular/physiology; Visual Prosthesis; Young Adult",,,https://onlinelibrary.wiley.com/doi/full/10.1111/aor.12868 https://core.ac.uk/display/146504084 https://oatao.univ-toulouse.fr/18822/ https://www.ncbi.nlm.nih.gov/pubmed/28321887 https://hal.archives-ouvertes.fr/hal-01692744 https://hal.archives-ouvertes.fr/hal-01692744/document https://hal-univ-tlse2.archives-ouvertes.fr/hal-01692744v1,http://dx.doi.org/10.1111/aor.12868,28321887,10.1111/aor.12868,2601877418,,0,006-166-582-003-71X; 006-849-558-638-590; 008-522-737-873-914; 010-510-046-702-511; 011-016-442-810-672; 024-144-206-344-655; 024-369-065-483-244; 030-459-735-956-167; 031-702-469-587-138; 034-196-646-262-503; 042-422-564-298-433; 049-870-351-843-307; 051-472-067-465-255; 052-104-972-727-451; 059-241-397-256-336; 063-048-719-705-828; 066-766-534-595-787; 075-984-009-876-515; 103-383-394-341-647; 120-224-001-278-543; 129-917-318-455-906; 139-982-071-483-668; 153-692-034-125-302; 157-469-267-875-941; 182-367-197-133-641,24,true,,green
090-864-728-761-623,Enhanced Cane for Blind People Mobility Assistance,2024-05-16,2024,conference proceedings article,"2024 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)",,IEEE,,Adrian Mocanu; Valentin Sita; Camelia Avram; Adina Aştilean,"The mobility issue for individuals with visual impairments is largely debated, persisting with no definitive resolution. Despite ongoing discussions, limited solutions or applications are available to address this challenge. In this context, exploring effective, innovative technologies in enhancing the mobility of visually impaired people remains an active area of research and development. Also, the rise of smart cities worldwide can come with significant support to solve the problems regarding the travel assistance of blind people. The present paper proposes a reworked enhanced cane to guide impaired visual individuals in daily activities, like reaching from one point to another. Various possible solutions were analysed to offer them increased comfort and minimise the effects of factors that bring undesired perturbations to their familiar environment.",,,,,Cane; Computer science; Sugar; Biochemistry; Chemistry,,,,,,http://dx.doi.org/10.1109/aqtr61889.2024.10554132,,10.1109/aqtr61889.2024.10554132,,,0,007-223-103-783-187; 012-028-214-945-449; 020-302-288-518-801; 026-922-910-952-305; 030-500-337-935-178; 036-552-223-342-289; 054-879-961-055-393; 057-580-969-583-660; 065-983-543-415-033; 067-781-225-869-731; 117-677-849-214-196; 118-533-294-109-774; 175-969-824-862-857; 176-339-454-003-376; 183-947-709-586-992,0,false,,
091-012-686-507-374,Future of computer aided surgery by collaborating academy and industry,2006-05-19,2006,journal article,International Journal of Computer Assisted Radiology and Surgery,18616410; 18616429,Springer Science and Business Media LLC,Germany,Hiroshi Iseki; Yoshihiro Muragaki; Ryoichi Nakamura; Tomokatsu Hori; Kintomo Takakura,,1,S1,293,295,Engineering; Medical physics; Intraoperative MRI; Computer aided surgery; Simulation,,,,,https://waseda.pure.elsevier.com/ja/publications/future-of-computer-aided-surgery-by-collaborating-academy-and-ind,http://dx.doi.org/10.1007/s11548-006-0023-y,,10.1007/s11548-006-0023-y,2606850553,,0,,2,false,,
091-280-821-743-216,A Flow-based Motion Perception Technique for an Autonomous Robot System,2013-11-09,2013,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Andry Maykol Pinto; A. Paulo Moreira; Miguel V. Correia; Paulo G. Costa,,75,3,475,492,Engineering; Structure from motion; Artificial intelligence; Optical flow; Autonomous robot; Observer (special relativity); Robotic systems; Visual motion perception; Computer vision; Computation; Motion perception,,,,,https://dblp.uni-trier.de/db/journals/jirs/jirs75.html#PintoMCC14 https://dialnet.unirioja.es/servlet/articulo?codigo=4783339 https://repositorio.inesctec.pt/handle/123456789/3505 https://doi.org/10.1007/s10846-013-9999-z https://link.springer.com/article/10.1007/s10846-013-9999-z,http://dx.doi.org/10.1007/s10846-013-9999-z,,10.1007/s10846-013-9999-z,2014081735,,0,000-994-204-290-585; 003-562-285-967-825; 014-022-090-843-779; 014-022-845-459-282; 017-207-873-454-753; 017-315-906-883-131; 018-846-230-795-906; 020-389-576-436-477; 021-164-045-109-45X; 023-551-314-781-803; 023-834-074-121-055; 030-160-171-033-909; 033-928-519-356-675; 035-056-742-132-989; 035-314-783-067-625; 036-663-192-985-83X; 042-857-445-500-750; 048-243-595-848-594; 049-639-325-252-974; 057-082-941-059-208; 060-031-222-934-422; 063-421-430-654-137; 066-620-035-651-850; 066-914-348-292-717; 073-333-788-980-79X; 074-006-536-890-200; 075-110-336-024-910; 076-187-888-953-402; 076-629-050-909-630; 082-040-105-278-676; 083-498-199-531-628; 083-756-134-424-441; 083-855-011-969-939; 097-283-319-226-805; 107-482-414-560-484; 110-209-503-112-465; 119-411-392-734-371; 124-411-545-803-229; 134-495-725-289-224; 145-892-258-885-120; 146-791-585-661-144; 147-516-912-068-269; 150-661-098-718-27X; 179-325-465-138-497,15,false,,
091-299-041-525-154,IWINAC (1) - Vision Substitution Experiments with See ColOr,,2013,book chapter,Natural and Artificial Models in Computation and Biology,03029743; 16113349,Springer Berlin Heidelberg,Germany,Guido Bologna; Juan Diego Gomez; Thierry Pun,"See ColOr is a mobility aid for visually impaired people that uses the auditory channel to represent portions of captured images in real time. A distinctive feature of the See ColOr interface is the simultaneous coding of colour and depth. Four main modules were developed, in order to replicate a number of mechanisms present in the human visual systems. In this work, we first present the main experiments carried out in the first years of the project; among them : the avoidance of obstacles, the recognition and localization of objects, the detection of edges and the identification of coloured targets. Finally, we introduce new undergoing experiments in Colombia with blind persons, whose purpose is (1) to determine and to touch a target; (2) to navigate and to find a person; and (3) to find particular objects. Preliminary results illustrate encouraging results.",,,83,93,Artificial intelligence; Coding (social sciences); Distinctive feature; Mobility aid; Replicate; Blind persons; 3d vision; Visually impaired; Computer vision; Computer science,,,,,https://rd.springer.com/chapter/10.1007/978-3-642-38637-4_9 https://link.springer.com/chapter/10.1007/978-3-642-38637-4_9 https://link.springer.com/content/pdf/10.1007%2F978-3-642-38637-4_9.pdf https://dblp.uni-trier.de/db/conf/iwinac/iwinac2013-1.html#BolognaGP13,http://dx.doi.org/10.1007/978-3-642-38637-4_9,,10.1007/978-3-642-38637-4_9,48146042,,0,000-783-495-538-375; 001-023-674-453-101; 013-847-555-426-159; 038-282-604-696-596; 072-278-195-125-805; 077-133-431-811-095; 083-550-403-744-467; 088-367-079-671-513; 129-098-405-599-274; 135-646-014-794-306; 154-101-288-248-272,2,false,,
091-393-785-318-271,Blind image quality assessment by simulating the visual cortex,2022-08-13,2022,journal article,The Visual Computer,01782789; 14322315,Springer Science and Business Media LLC,Germany,Rongtai Cai; Ming Fang,,39,10,4639,4656,Artificial intelligence; Computer science; Image quality; Visual cortex; Computer vision; Support vector machine; Pattern recognition (psychology); Human visual system model; Image (mathematics); Quality (philosophy); Visual perception; Perception; Philosophy; Epistemology; Neuroscience; Biology,,,,,,http://dx.doi.org/10.1007/s00371-022-02614-y,,10.1007/s00371-022-02614-y,,,0,001-484-504-895-566; 002-617-800-604-631; 003-040-709-965-367; 003-418-524-699-48X; 004-235-327-809-963; 004-903-691-297-061; 005-141-038-547-740; 006-101-455-300-124; 009-181-919-762-91X; 009-321-955-517-040; 010-157-738-409-581; 011-927-414-739-394; 014-614-391-804-430; 015-571-444-508-782; 015-867-542-444-041; 016-318-349-186-102; 023-579-839-563-336; 029-496-296-285-898; 033-305-653-493-936; 035-924-867-507-200; 036-013-153-822-566; 039-135-942-929-563; 040-484-775-336-912; 044-095-303-626-459; 048-375-126-277-164; 055-463-893-774-499; 058-261-304-093-358; 060-834-533-443-304; 061-376-829-020-31X; 061-599-028-823-778; 064-401-628-721-647; 066-506-142-974-306; 067-098-374-632-301; 072-194-589-240-824; 077-006-042-482-02X; 079-247-399-160-293; 082-654-175-872-896; 083-912-474-254-495; 084-314-938-523-363; 086-201-327-374-428; 089-021-066-825-948; 095-665-904-135-788; 096-717-380-007-985; 105-594-711-817-884; 107-154-223-114-876; 110-449-562-069-308; 119-036-352-986-417; 119-282-024-771-875; 121-456-195-261-238; 132-186-931-712-642; 132-463-207-932-268; 140-882-954-139-446; 152-987-724-399-716; 153-349-284-201-656; 153-943-351-085-440; 176-967-305-530-305; 198-053-173-313-495,8,false,,
091-395-527-770-635,Mobility enhancement using simulated artificial human vision,,2007,dissertation,,,,,Jason Dowling,"The electrical stimulation of appropriate components of the human visual system can result in the perception of blobs of light (or phosphenes) in totally blind patients. By stimulating an array of closely aligned electrodes it is possible for a patient to perceive very low-resolution images from spatially aligned phosphenes. Using this approach, a number of international research groups are working toward developing multiple electrode systems (called Artificial Human Vision (AHV) systems or visual prostheses) to provide a phosphene-based substitute for normal human vision. Despite the great promise, there are currently a number of constraints with current AHV systems. These include limitations in the number of electrodes which can be implanted and the perceived spatial layout and display frequency of phosphenes. Therefore the development of computer vision techniques that can maximise the visualisation value of the limited number of phosphenes would be useful in compensating for these constraints. The lack of an objective method for comparing different AHV system displays, in addition to comparing AHV systems and other blind mobility aids (such as the long cane), has been a significant problem for AHV researchers. Finally, AHV research in Australia and many other countries relies strongly on theoretical models and animal experimentation due to the difficult of prototype human trials. Because of this constraint the experiments conducted in this thesis were limited to simulated AHV devices with normally sighted research participants and the true impact on blind people can only be regarded as approximated. In light of these constraints, this thesis has two general aims. The first aim is to investigate, evaluate and develop effective techniques for mobility assessment which will allow the objective comparison of different AHV system phosphene presentation methods. The second aim is to develop a useful display framework to guide the development of AHV information presentation, and use this framework to guide the development of an AHV simulation device. The first research contribution resulting from this work is a conceptual framework based on literature reviews of blind and low vision mobility, AHV technology, and computer vision. This framework incorporates a comprehensive number of factors which affect the effectiveness of information presentation in an AHV system. Experiments reported in this thesis have investigated a number of these factors using simulated AHV with human participants. It has been found that higher spatial resolution is associated with accurate walking (reduced veering), whereas higher display rate is associated with faster walking speeds. In this way it has been demonstrated that the conceptual framework supports and guides the development of an adaptive AHV system, with the dynamic adjustment of display properties in real-time. The second research contribution addresses mobility assessment which has been identified as an important issue in the AHV literature. This thesis presents the adaptation of a mobility assessment method from the blind and low vision literature to measure simulated AHV mobility performance using real-time computer based analysis. This method of mobility assessment (based on parameters for walking speed, obstacle contacts and veering) is demonstrated experimentally in two different indoor mobility courses. These experiments involved sixty-five participants wearing a head-mounted simulation device. The final research contribution in this thesis is the development and evaluation of an original real-time looming obstacle detector, based on coarse optical flow, and implemented on a Windows PocketPC based Personal Digital Assistant (PDA) using a CF card camera. PDA based processors are a preferred main processing platform for AHV systems due to their small size, light weight and ease of software development. However, PDA devices are currently constrained by restricted random access memory, lack of a floating point unit and slow internal bus speeds. Therefore any real-time software needs to maximise the use of integer calculations and minimise memory usage. This contribution was significant as the resulting device provided a selection of experimental results and subjective opinions.",,,,,Engineering; Looming; Optical flow; Human visual system model; Software; Visualization; Simulation; Software development; Adaptation (computer science); Image processing,,,,,https://eprints.qut.edu.au/16380/1/Jason_Dowling_Thesis.pdf https://eprints.qut.edu.au/16380/,https://eprints.qut.edu.au/16380/,,,184934113,,0,000-865-628-176-62X; 001-767-840-740-256; 002-367-324-166-044; 003-366-555-089-471; 004-615-716-688-821; 004-756-256-314-376; 004-966-664-195-113; 005-327-478-596-058; 005-585-941-736-916; 007-919-813-542-615; 008-544-893-563-181; 010-246-777-790-754; 011-252-196-208-444; 011-277-668-215-329; 011-309-426-203-927; 011-845-015-646-247; 012-309-694-326-735; 013-368-513-529-725; 013-847-555-426-159; 014-015-592-708-563; 015-476-928-897-846; 015-489-907-186-458; 015-522-377-591-609; 016-972-325-725-809; 017-143-261-623-878; 017-312-938-936-230; 017-413-759-318-536; 017-638-478-647-912; 018-031-540-259-255; 018-094-489-878-074; 018-355-285-058-165; 018-424-322-161-003; 018-654-826-336-966; 018-819-188-935-149; 018-917-494-081-160; 019-530-920-940-440; 020-331-351-530-276; 020-684-609-770-394; 020-849-987-130-012; 021-408-707-817-630; 021-594-883-753-068; 023-747-070-514-665; 023-856-694-133-345; 024-101-086-297-03X; 024-369-065-483-244; 024-893-020-530-159; 024-948-820-292-297; 025-726-392-075-140; 026-187-830-635-985; 027-108-127-741-469; 027-722-755-784-523; 027-847-518-112-220; 028-805-932-151-293; 029-609-581-856-520; 031-181-587-843-197; 031-267-666-148-608; 031-407-549-066-074; 031-808-169-213-287; 031-842-427-416-191; 032-729-028-606-811; 033-195-230-003-638; 033-618-582-761-549; 034-232-878-371-173; 035-909-958-758-800; 036-245-312-337-81X; 036-362-141-712-656; 037-378-104-656-053; 038-145-742-148-159; 038-733-327-974-497; 041-041-858-186-300; 041-465-979-947-216; 041-474-580-376-436; 041-981-391-161-539; 042-811-767-289-667; 043-109-434-528-830; 043-121-042-008-969; 043-269-569-868-43X; 043-899-478-442-384; 044-318-997-140-730; 044-668-316-416-149; 044-930-518-244-999; 045-824-465-005-309; 047-081-444-387-991; 047-930-330-573-546; 048-611-739-852-004; 049-978-324-156-573; 050-859-041-353-124; 051-397-553-681-386; 052-198-427-054-087; 052-504-791-323-805; 052-691-150-775-867; 052-789-879-168-08X; 054-154-570-256-464; 055-100-021-382-030; 055-132-919-152-975; 056-125-347-120-409; 056-992-238-016-923; 059-482-749-135-704; 061-053-598-292-943; 061-405-445-106-456; 062-328-567-581-132; 063-523-491-864-813; 063-593-313-740-053; 063-923-383-739-743; 064-069-666-882-597; 065-343-644-415-942; 066-924-038-224-443; 067-190-146-612-677; 067-621-069-925-283; 070-075-898-281-676; 070-782-451-504-629; 070-928-995-766-167; 071-760-799-702-449; 072-170-487-675-943; 072-361-323-868-927; 072-719-431-639-594; 073-155-264-258-588; 073-891-703-684-446; 074-324-981-337-749; 075-733-426-878-806; 078-982-286-839-675; 079-604-216-078-83X; 083-135-613-306-133; 084-375-521-719-722; 087-369-533-052-257; 088-814-170-353-77X; 089-000-178-151-64X; 090-470-535-677-829; 090-573-118-422-229; 091-858-283-074-104; 092-338-509-426-246; 093-120-672-465-75X; 094-332-505-126-057; 095-077-689-032-530; 096-016-626-999-530; 096-391-569-947-076; 098-418-441-499-649; 099-600-325-014-082; 099-612-858-681-003; 100-130-700-238-45X; 101-737-054-906-229; 102-555-702-781-751; 103-071-915-627-411; 103-865-753-211-046; 104-602-992-365-300; 107-280-715-101-446; 107-725-434-701-399; 109-151-436-884-128; 114-683-350-855-519; 117-756-374-377-404; 118-384-547-716-200; 119-732-691-494-408; 120-742-669-010-74X; 121-618-132-195-615; 123-292-885-540-917; 124-222-594-665-093; 127-817-632-406-052; 128-331-208-279-855; 128-831-198-471-94X; 131-415-145-319-00X; 132-078-618-899-334; 134-495-725-289-224; 138-031-467-127-248; 139-109-671-123-221; 140-188-350-445-856; 141-719-052-316-710; 142-426-241-123-210; 145-032-090-481-201; 149-869-494-871-046; 150-532-270-321-442; 150-597-344-978-541; 153-994-342-017-219; 156-827-733-532-673; 160-321-090-931-529; 160-671-250-819-780; 165-202-725-040-851; 168-699-444-563-74X; 171-284-578-433-020; 176-327-704-557-382; 176-619-480-167-936; 182-018-490-989-080; 182-073-233-301-46X; 183-816-558-497-844; 185-266-364-086-800; 185-589-845-207-673; 186-624-489-795-807; 187-375-532-896-761; 187-488-585-098-860; 189-287-540-354-877; 189-964-429-526-197; 191-884-013-975-45X; 198-602-864-367-379; 198-662-427-258-865,4,false,,
091-491-861-595-289,A proposal toward the development of accessible e-learning content by human involvement,2006-06-21,2006,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Maria De Marsico; Stephen Kimani; Valeria Mirabella; L. Norman; Tiziana Catarci,,5,2,150,169,Usability; Software development process; E-learning (theory); Context (language use); Computer science; Process (engineering); Knowledge management; Universal design for instruction; Contextual design; Universal design,,,,,https://core.ac.uk/display/54357584 http://dx.doi.org/10.1007/s10209-006-0035-y https://dx.doi.org/10.1007/s10209-006-0035-y https://dblp.uni-trier.de/db/journals/uais/uais5.html#MarsicoKMNC06 https://link.springer.com/content/pdf/10.1007%2Fs10209-006-0035-y.pdf https://link.springer.com/article/10.1007%2Fs10209-006-0035-y https://link.springer.com/article/10.1007/s10209-006-0035-y/fulltext.html,http://dx.doi.org/10.1007/s10209-006-0035-y,,10.1007/s10209-006-0035-y,2072373135,,0,002-201-032-015-180; 005-880-973-524-100; 007-186-604-745-357; 007-601-788-408-018; 010-423-006-392-744; 018-922-235-731-524; 019-611-150-104-925; 021-226-504-676-092; 021-668-624-601-472; 039-894-933-300-023; 041-856-404-819-051; 044-361-316-218-322; 044-709-453-576-569; 053-271-843-163-893; 062-914-390-830-990; 065-565-020-625-793; 067-217-663-836-852; 074-988-362-684-540; 076-279-000-038-788; 079-403-449-604-57X; 081-423-356-818-421; 082-770-703-354-311; 086-033-967-796-020; 086-325-862-078-633; 090-145-625-816-635; 092-645-402-737-79X; 100-325-928-029-224; 101-726-962-582-502; 102-882-046-125-040; 120-237-352-847-309; 134-196-746-387-237; 136-317-156-689-754; 139-952-235-209-28X; 150-567-041-927-950; 170-997-648-286-868; 179-555-807-981-93X; 194-617-975-941-333,27,false,,
091-764-827-661-79X,Robotic assistance in indoor navigation for people who are blind,,2016,conference proceedings article,2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI),,IEEE,,Aditi Kulkarni; Allan Wang; Lynn Urbina; Aaron Steinfeld; Bernardine Dias,"In this paper, we describe the process of making a robot useful as a guide robot for people who are blind or visually impaired. For this group, the interactive audio feature of a robot assumes a very high level of importance. We have introduced some features that will help to make the robot sound natural and be more comfortable. We first addressed the question of the speaker placement to help the user determine the size and distance of the robot. After the initial meeting, user data will be retained by the robot so that their communication evolves with every interaction. The robot will also ask the users if they need to take a rest after a specified interval depending upon the user's age and the distance they need to cover. The next time they visit, all this information will be used to make the interaction more natural and customized for each individual user.",,,,,Robot; Computer science; Human–computer interaction; Ask price; Process (computing); Cover (algebra); Natural (archaeology); Feature (linguistics); Human–robot interaction; Mobile robot navigation; Artificial intelligence; Mobile robot; Multimedia; Robot control; Engineering; Mechanical engineering; Linguistics; Philosophy; Economy; Archaeology; Economics; History; Operating system,,,,,,http://dx.doi.org/10.1109/hri.2016.7451806,,10.1109/hri.2016.7451806,,,0,027-244-764-182-811; 061-472-591-708-148; 080-715-271-794-190,22,false,,
092-119-012-107-127,AI GUIDANCE FOR BLIND PEOPLE,2024-04-23,2024,journal article,INTERANTIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ENGINEERING AND MANAGEMENT,25823930,Indospace Publications,,DR. D. Vijaya Lakshmi,"<jats:p>Visually challenged people (VCP) struggle in their everyday life and have major difficulties in participating in cultural, tourist, family, and other types of outdoor activities especially those which are in unfamiliar surroundings. In modern days, synthetic intelligence is imparting a wide variety of answers for any hassle. This paper represents an guidance machine for blind . It is a wearable device which guides people efficiently and safely. This device is speedy and accurate for object detection by means of the digital camera and sensor for obstacle detection.An impaired person can wear this system and command this system for finding the things using voice command. This system recognizes these commands and gives a desirable output in voice. Also while travelling, it detects objects and obstacles and notifies about it to the user using voice output.     Key Words: Obstacle detection, Text recognition, wearable device.</jats:p>",8,4,1,5,Obstacle; Computer science; Wearable computer; Human–computer interaction; Key (lock); Variety (cybernetics); Object (grammar); Voice command device; Computer vision; Artificial intelligence; Multimedia; Speech recognition; Computer security; Embedded system; Political science; Law,,,,,https://ijsrem.com/download/ai-guidance-for-blind-people/?wpdmdl=30907&refresh=665bdeec72d4c1717296876 https://doi.org/10.55041/ijsrem31538,http://dx.doi.org/10.55041/ijsrem31538,,10.55041/ijsrem31538,,,0,,0,true,,bronze
092-221-200-723-817,Spatial cognition by the visually impaired: image processing with SIFT/BRISK-like detector and two-keypoint descriptor on Android CameraX,2023-12-31,2023,book chapter,Machine Learning in Medical Imaging and Computer Vision,,Institution of Engineering and Technology,,Dmytro Zubov; Ayman Aljarbouh; Andrey Kupin; Nurlan Shaidullaev,"Over 250 million people worldwide are visually impaired or blind today. Guide dogs and white canes are still the most preferred methods of aid because existing smart assistive devices are often unaffordable and/or not accurate/fast enough. In this work, Android phones are employed to provide an affordable and reliable solution to support the spatial cognition of the visually impaired. A new image processing approach has been proposed. In preprocessing, a two-dimensional Gaussian function downscales the image and removes noise. A pair of keypoints is selected from a maximum of 300 keypoints localized by the SIFT method. Their coordinates allow the calculation of the orientation and scale of the image descriptor, employing locations of these two keypoints and the distance between them. Then, the BRISK algorithm designs a two-keypoint binary descriptor of specific shape with 291 points located near two keypoints of interest (for example, two centers of the human eye pupils) in an image with a maximum dimension size of 180 pixels. The Hamming distance threshold value and the Hamming distance value after five AdaBoost weak classifiers were chosen to be 19,317 and 18,558, which shows a 99.5% recognition accuracy for the validation sample and 100% in real-life experiments. To implement the proposed approach, a multi-threaded Java Android application was developed to improve spatial cognition by the visually impaired and blind using the CameraX library. The execution time is approximately 0.8 s on the smartphone Doogee S96 Pro for a knowledge base with six template objects.",,,249,276,Artificial intelligence; Computer science; Computer vision; Scale-invariant feature transform; Android (operating system); Pixel; Hamming distance; Preprocessor; Pattern recognition (psychology); Feature extraction; Algorithm; Operating system,,,,,,http://dx.doi.org/10.1049/pbhe049e_ch12,,10.1049/pbhe049e_ch12,,,0,,0,false,,
092-320-226-202-271,LidSonic V2.0: LiDAR and Deep Learning-based Green Assistive Edge Device to Enhance Mobility for the Visually Impaired,2022-08-11,2022,preprint,,,MDPI AG,,Sahar Busaeed; Iyad Katib; Aiiad Albeshri; Juan M. Corchado; Tan Yigitcanlar; Rashid Mehmood,"<jats:p>Over a billion people around the world are disabled, among them, 253 million are visually impaired or blind, and this number is greatly increasing due to ageing, chronic diseases, poor environment, and health. Despite many proposals, the current devices and systems lack maturity and do not completely fulfill user requirements and satisfaction. Increased research activity in this field is required to encourage the development, commercialization, and widespread acceptance of low-cost and affordable assistive technologies for visual impairment and other disabilities. This paper proposes a novel approach using a LiDAR with a servo motor and an ultrasonic sensor to collect data and predict objects using deep learning for environment perception and navigation. We adopted this approach in a pair of smart glasses, called LidSonic V2.0, to enable the identification of obstacles for the visually impaired. The LidSonic system consists of an Arduino Uno edge computing device integrated into the smart glasses and a smartphone app that transmits data via Bluetooth. Arduino gathers data, operates the sensors on smart glasses, detects obstacles using simple data processing, and provides buzzer feedback to visually impaired users. The smartphone application collects data from Arduino, detects and classifies items in the spatial environment, and gives spoken feedback to the user on the detected objects. In comparison to image processing-based glasses, LidSonic uses far less processing time and energy to classify obstacles using simple LiDAR data, according to several integer measurements. We comprehensively describe the proposed system's hardware and software design, construct their prototype implementations, and test them in real-world environments. Using the open platforms, WEKA and TensorFlow, the entire LidSonic system is built with affordable off-the-shelf sensors and a microcontroller board costing less than $80. Essentially, we provide designs of an inexpensive, miniature, green device that can be built into, or mounted on, any pair of glasses or even a wheelchair to help the visually impaired. Our approach affords faster inference and decision-making using relatively low energy with smaller data sizes as well as faster communications for the edge, fog, and cloud computing.</jats:p>",,,,,Arduino; Computer science; Buzzer; Implementation; Enhanced Data Rates for GSM Evolution; Lidar; Embedded system; Artificial intelligence; Real-time computing; Human–computer interaction; Engineering; Programming language; Remote sensing; Geology; Aerospace engineering; ALARM,,,,,https://www.mdpi.com/1424-8220/22/19/7435/pdf?version=1664545959 https://doi.org/10.3390/s22197435 https://eprints.qut.edu.au/235416/1/115932010.pdf,http://dx.doi.org/10.20944/preprints202208.0215.v1,,10.20944/preprints202208.0215.v1,,,0,,3,true,,green
092-452-392-581-46X,Real-Time Object Detection for Visually Challenged People,,2020,conference proceedings article,2020 4th International Conference on Intelligent Computing and Control Systems (ICICCS),,IEEE,,Sunit Vaidya; Naisha Shah; Niti Shah; Radha Shankarmani,"One of the most important senses for a living is vision. Millions of people living in this world deal with visual impairment. These people encounter difficulties in navigating independently and safely, facing issues in accessing information and communication. The objective of the proposed work is to change the visual world into an audio world by notifying the blind people about the objects in their path. This will help visually impaired people to navigate independently without any external assistance just by using the real-time object detection system. The application uses image processing and machine learning techniques to determine real-time objects through the camera and inform blind people about the object and its location through the audio output. Inability to differentiate between objects has led to many limitations to the existing approach which includes less accuracy and lowperformance results. The main objective of the proposed work is to provide good accuracy, best performance results and a viable option for the visually impaired people to make the world a better place for them.",,,311,316,Human–computer interaction; Object detection; Visual impairment; Performance results; Visually impaired; Object (philosophy); Computer science; Image processing,,,,,https://ieeexplore.ieee.org/abstract/document/9121085,http://dx.doi.org/10.1109/iciccs48265.2020.9121085,,10.1109/iciccs48265.2020.9121085,3036488190,,0,001-942-404-050-182; 042-568-425-640-966; 066-133-135-091-645; 081-747-388-175-61X; 125-112-083-666-544,32,false,,
092-748-850-855-273,Design and Implementation of a Real-Time Color Recognition System for the Visually Impaired,2022-12-19,2022,journal article,Arabian Journal for Science and Engineering,2193567x; 21914281; 13198025,Springer Science and Business Media LLC,Saudi Arabia,Mohammed Samara; Mohamed Deriche; Jihad Al-Sadah; Yahya Osais,,48,5,6783,6796,Sonification; Computer science; Set (abstract data type); Computer vision; Perception; Artificial intelligence; HSL and HSV; Matching (statistics); Identification (biology); Human–computer interaction; Mathematics; Psychology; Virus; Statistics; Botany; Virology; Neuroscience; Biology; Programming language,,,,,,http://dx.doi.org/10.1007/s13369-022-07506-w,,10.1007/s13369-022-07506-w,,,0,000-462-024-208-188; 003-936-796-445-918; 007-573-197-769-65X; 008-553-939-609-320; 009-496-095-565-142; 011-343-097-521-041; 018-031-085-269-362; 024-888-117-412-240; 027-853-867-091-596; 028-745-837-982-10X; 032-077-673-631-306; 032-262-564-821-172; 032-616-534-832-351; 035-475-577-840-442; 035-714-269-213-741; 036-723-065-534-309; 037-254-825-753-510; 037-491-194-003-965; 038-741-844-338-018; 043-116-100-107-337; 045-871-809-765-910; 046-991-476-786-467; 049-130-224-418-304; 061-611-750-218-70X; 066-741-866-630-306; 077-178-038-619-709; 080-021-648-530-499; 081-051-243-555-763; 081-113-507-056-249; 083-671-887-012-721; 087-193-049-082-327; 088-367-079-671-513; 101-542-450-131-541; 104-432-532-310-721; 104-685-875-026-63X; 122-822-693-327-547; 134-119-662-497-130; 143-360-205-659-883; 170-893-437-482-766,0,false,,
092-833-605-309-363,A Versatile Personal Assistant Dedicated to B/VIP,2019-10-08,2019,journal article,International Journal of Robotics and Automation Technology,24099694,Zeal Press,,Paul Costache; Simona Riurean; Sebastian Rosca,"<jats:p>Today, there are different systems on the market that aim to help Blind or Visually Impaired People (B/VIP) to gain more independency and self-trust. From the classic white cane to various intelligent devices to guide people with vision problems, most of the systems are expensive, difficult to be used or with low efficiency. The subject is a very important one because, according to the World Health Organization, there are approximately 2.2 billion people in the world that have a vision impairment or blindness. Thus, the purpose of this work is to make a short survey on solutions available on the market, in order to develop a low-cost, efficient and easy-to-use system for both indoor and outdoor use. The versatile personal assistant dedicated to B/VIP aims to enhance their independence and also improved outdoor navigation. The smart warning system addressed here is an innovative device aiming to detect obstacles with the help of ultrasonic sensors and warning of their presence by means of a buzzer or vibration. The system, when used outdoor, also gives the possibility to send short message to the B/VIP’s own assistant, in the form of an SMS with the exact location of the B/VIP. Moreover, the SMS’ content can easily create the optimal route to the B/VIP’s location with the Google Maps support for a fast-time assistance.</jats:p>",6,,33,39,,,,,,http://dx.doi.org/10.31875/2409-9694.2019.06.4,http://dx.doi.org/10.31875/2409-9694.2019.06.4,,10.31875/2409-9694.2019.06.4,3216299691,,0,018-797-747-752-732; 028-683-545-086-724; 041-468-306-822-770; 052-154-945-556-605; 053-872-980-515-457; 057-088-066-723-066; 075-693-006-853-959; 076-056-710-552-791; 144-454-158-356-436,0,true,,gold
092-884-633-833-840,SERVICES - A Low-Vision Navigation Platform for Economies in Transition Countries,2020-12-21,2020,journal article,Proceedings IEEE World Congress on Services (SERVICES). IEEE World Congress on Services,2642939x; 23783818,,United States,John-Ross Rizzo; Chen Feng; Wachara Riewpaiboon; Pattanasak Mongkolwat,"An ability to move freely, when wanted, is an essential activity for healthy living. Visually impaired and completely blinded persons encounter many disadvantages in their day-to-day activities, including performing work-related tasks. They are at risk of mobility losses, illness, debility, social isolation, and premature mortality. A novel wearable device and computing platform called VIS4ION is reducing the disadvantage gaps and raising living standards for the visually challenged. It provides personal mobility navigational services that serves as a customizable, human-in-the-loop, sensing-to-feedback platform to deliver functional assistance. The platform is configured as a wearable that provides on-board microcomputers, human-machine interfaces, and sensory augmentation. Mobile edge computing enhances functionality as more services are unleashed with the computational gains. The meta-level goal is to support spatial cognition, personal freedom, and activities, and to promoting health and wellbeing. VIS 4 ION can be conceptualized as the dovetailing of two thrusts: an on-person navigational and computing device and a multimodal functional aid providing microservices through the cloud. The device has on-board wireless capabilities connected through Wi-Fi or 4/5G. The cloud-based microservices reduce hardware and power requirements while allowing existing and new services to be enhanced and added such as loading new map and real-time communication via haptic or audio signals. This technology can be made available and affordable in the economies of transition countries.",2020,,1,3,Economy; Wearable computer; Wireless sensor network; Mobile edge computing; Edge computing; Microservices; Personal mobility; Dovetailing; Computer science; Cloud computing,AI for Visually Impaired; Edge Computing; Mobility; Navigational System; Visually Impaired; component,,,NEI NIH HHS (R21 EY033689) United States,https://dblp.uni-trier.de/db/conf/services/services2020.html#Rizzo0RM20 https://doi.org/10.1109/SERVICES48979.2020.00013 https://nyuscholars.nyu.edu/en/publications/a-low-vision-navigation-platform-for-economies-in-transition-coun,http://dx.doi.org/10.1109/services48979.2020.00013,35983015,10.1109/services48979.2020.00013,3114110897,PMC9382852,0,000-326-447-768-594; 006-953-356-670-058; 049-313-676-820-38X; 080-150-281-543-648; 083-406-127-121-987; 138-100-780-829-672,2,true,,unknown
092-958-099-002-180,Object Detection for Visually Impaired People Using SSD Algorithm,2020-07-03,2020,conference proceedings article,"2020 International Conference on System, Computation, Automation and Networking (ICSCAN)",,IEEE,,K. Vijlyakumar; K. Ajitha; A. Alexia; M. Hemalashmi; S. Madhumitha,"Visually impaired people are unaware of the danger that they are facing in their life. They may face many challenges while performing their daily activity even in their familiar environments. Vision is the necessary human senses and it plays the important role in human perception about surrounding environment. Hence, there are variety of computer vision products and services which are used in the development of new electronic aids for those blind people. In this paper we designed to provide navigation to those people. It guides the people about the object as well as provides the distance of the object. The algorithm itself calculates the distance of the object. Here it also provides the audio jack to insist them about the object. Here we are using SSD Algorithm for object detection and calculating the distance of the object by using monodepth algorithm.",,,1,7,Deep learning; Statistical classification; Algorithm; Artificial intelligence; Task analysis; Face (geometry); Perception; Object detection; Computer science; Object (computer science); Cognitive neuroscience of visual object recognition,,,,,https://ieeexplore.ieee.org/document/9262344,http://dx.doi.org/10.1109/icscan49426.2020.9262344,,10.1109/icscan49426.2020.9262344,3108263496,,0,015-667-411-553-321; 017-407-877-112-562; 022-435-752-434-065; 040-941-067-536-586; 049-317-239-158-314,4,false,,
093-012-808-136-050,"Computer Assisted Head and Neck, and ENT Surgery",2009-05-07,2009,journal article,International Journal of Computer Assisted Radiology and Surgery,18616410; 18616429,Springer Science and Business Media LLC,Germany,,,4,S1,71,80,Head and neck; Head and neck surgery; Head (geology); Medicine; Otorhinolaryngology; Surgery; Computer science; Geology; Geomorphology,,,,,,http://dx.doi.org/10.1007/s11548-009-0325-y,,10.1007/s11548-009-0325-y,,,0,,1,false,,
093-219-699-117-536,Staying in the Crosswalk: A System for Guiding Visually Impaired Pedestrians at Traffic Intersections.,,2009,journal article,Assistive technology research series,1383813x; 18798071,IOS Press,Netherlands,null Ivanchenko; Coughlan J; Shen H,"Traffic intersections are among the most dangerous parts of a blind or visually impaired person’s travel. Our “Crosswatch” device [4] is a handheld (mobile phone) computer vision system for orienting visually impaired pedestrians to crosswalks, to help users avoid entering the crosswalk in the wrong direction and straying outside of it. This paper describes two new developments in the Crosswatch project: (a) a new computer vision algorithm to locate the more common – but less highly visible – standard “two-stripe” crosswalk pattern marked by two narrow stripes along the borders of the crosswalk; and (b) 3D analysis to estimate crosswalk location relative to the user, to help him/her stay inside the crosswalk (not merely pointing in the correct direction). Experiments with blind subjects using the system demonstrate the feasibility of the approach.",25,2009,69,73,Schema crosswalk; Artificial intelligence; Mobile device; Geography; Mobile phone; Wrong direction; Computer vision algorithms; 3d analysis; Visually impaired; Computer vision,,,,NEI NIH HHS (R01 EY018345) United States; NEI NIH HHS (R01 EY018345-01) United States,https://europepmc.org/article/MED/21037936 https://www.ski.org/sites/default/files/publications/aaate09-crosswalk.pdf https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2964893/,https://www.ncbi.nlm.nih.gov/pubmed/21037936,21037936,,2236013308,PMC2964893,0,046-262-856-533-736; 050-007-989-745-990; 100-404-039-254-355,20,true,,unknown
093-351-911-740-199,EmoAssist: emotion enabled assistive tool to enhance dyadic conversation for the blind,2016-03-15,2016,journal article,Multimedia Tools and Applications,13807501; 15737721,Springer Science and Business Media LLC,Netherlands,Akmmahbubur Rahman; Asm Iftekhar Anam; Mohammed Yeasin,,76,6,7699,7730,Valence (psychology); Human–computer interaction; Sensory cue; Machine learning; Empirical research; Social relation; Artificial intelligence; Usability; Conversation; Auditory feedback; Affect (psychology); Computer science,,,,,https://dblp.uni-trier.de/db/journals/mta/mta76.html#RahmanAY17 https://link.springer.com/article/10.1007/s11042-016-3295-4/fulltext.html https://link.springer.com/article/10.1007/s11042-016-3295-4 https://doi.org/10.1007/s11042-016-3295-4 http://dblp.uni-trier.de/db/journals/mta/mta76.html#RahmanAY17,http://dx.doi.org/10.1007/s11042-016-3295-4,,10.1007/s11042-016-3295-4,2300287379,,0,005-232-648-761-051; 018-987-613-739-265; 028-261-945-373-327; 029-162-444-345-996; 031-298-073-564-034; 031-744-407-103-607; 034-817-050-539-216; 041-491-759-825-961; 043-022-952-629-736; 043-467-506-615-926; 044-817-282-359-305; 048-054-201-833-814; 057-333-614-219-44X; 068-408-614-456-288; 076-293-094-228-670; 077-721-178-017-592; 081-548-603-926-788; 081-669-474-754-14X; 083-209-246-533-471; 090-788-443-096-947; 091-226-352-993-927; 098-563-281-490-848; 105-057-420-005-006; 105-615-783-668-558; 115-527-423-384-36X; 116-833-265-508-516; 136-349-724-236-048; 137-162-815-164-525; 140-810-696-589-766; 149-344-016-206-322; 157-013-225-398-483; 157-363-310-177-394; 160-914-692-204-293; 167-385-200-089-924; 172-572-166-912-802; 178-637-879-846-799; 179-595-169-274-902; 181-073-365-413-379; 187-967-014-093-095,16,false,,
093-352-896-616-812,Practical Structure and Motion from Stereo When Motion is Unconstrained,,2000,journal article,International Journal of Computer Vision,09205691; 15731405,Springer Science and Business Media LLC,Netherlands,Nicholas Molton; Michael Brady,,39,1,5,23,Motion estimation; Motion field; Structure from motion; Artificial intelligence; Stereo camera; Computer stereo vision; Computer vision; Computer science; Obstacle avoidance; Stereopsis; Match moving,,,,,https://doi.org/10.1023/A:1008191416557 https://link.springer.com/article/10.1023/A:1008191416557 https://dblp.uni-trier.de/db/journals/ijcv/ijcv39.html#MoltonB00 https://www.oncology.ox.ac.uk/publications/65003,http://dx.doi.org/10.1023/a:1008191416557,,10.1023/a:1008191416557,1598902384,,0,004-194-473-680-995; 010-560-311-479-636; 015-642-586-236-845; 018-911-741-393-317; 028-303-905-295-179; 034-907-068-665-252; 044-854-955-351-395; 055-327-604-009-867; 070-022-537-204-828; 071-339-071-250-911; 078-747-544-702-234; 081-561-685-507-937; 082-391-007-325-197; 084-679-921-430-947; 087-346-913-019-123; 087-830-636-050-900; 092-048-685-832-387; 103-978-905-569-962; 120-742-669-010-74X; 133-793-530-159-12X; 144-263-664-519-451; 148-809-664-181-902; 148-975-926-020-839,32,false,,
093-397-196-284-713,Simulating the obstacle avoidance behavior day and night based on the visible-infrared MoS2/Ge heterojunction field-effect phototransistor,2023-07-12,2023,journal article,Nano Research,19980124; 19980000,Springer Science and Business Media LLC,China,Zhao Han; Bo Wang; Jie You; Qiancui Zhang; Yichi Zhang; Tian Miao; Ningning Zhang; Dongdong Lin; Zuimin Jiang; Renxu Jia; Jincheng Zhang; Hui Guo; Huiyong Hu; Liming Wang,,16,8,11296,11302,Heterojunction; Infrared; Optoelectronics; Materials science; Obstacle; Visible spectrum; Computer science; Optics; Physics; Political science; Law,,,,,,http://dx.doi.org/10.1007/s12274-023-5816-6,,10.1007/s12274-023-5816-6,,,0,001-024-993-945-028; 001-876-688-628-798; 003-143-723-130-878; 005-385-342-666-040; 010-474-860-129-398; 012-128-315-521-256; 012-345-470-142-062; 012-675-047-185-120; 014-622-163-175-61X; 015-513-722-126-675; 018-031-085-269-362; 030-127-369-356-741; 039-062-907-303-915; 043-766-181-742-053; 044-745-223-800-105; 046-485-900-294-68X; 050-699-118-648-01X; 052-239-032-566-641; 055-586-661-176-203; 055-799-694-545-692; 058-760-900-936-389; 059-482-749-135-704; 063-406-407-025-556; 066-899-777-939-379; 073-763-532-948-462; 076-914-702-107-928; 084-216-105-135-543; 086-869-440-167-789; 090-718-197-956-927; 091-764-827-661-79X; 092-846-649-193-238; 096-012-395-376-564; 097-312-123-618-145; 100-575-932-125-341; 104-901-071-732-006; 133-502-888-772-188; 151-729-719-938-22X,0,false,,
093-548-249-489-049,Wearable assistive devices for visually impaired: A state of the art survey,,2020,journal article,Pattern Recognition Letters,01678655,Elsevier BV,Netherlands,Ruxandra Tapu; Bogdan Mocanu; Titus Zaharia,"Abstract Recent statistics of the World Health Organization (WHO), published in October 2017, estimate that more than 253 million people worldwide suffer from visual impairment (VI) with 36 million of blinds and 217 million people with low vision. In the last decade, there was a tremendous amount of work in developing wearable assistive devices dedicated to the visually impaired people, aiming at increasing the user cognition when navigating in known/unknown, indoor/outdoor environments, and designed to improve the VI quality of life. This paper presents a survey of wearable/assistive devices and provides a critical presentation of each system, while emphasizing related strengths and limitations. The paper is designed to inform the research community and the VI people about the capabilities of existing systems, the progress in assistive technologies and provide a glimpse in the possible short/medium term axes of research that can improve existing devices. The survey is based on various features and performance parameters, established with the help of the blind community that allows systems classification using both qualitative and quantitative measu.res of evaluation. This makes it possible to rank the analyzed systems based on their potential impact on the VI people life.",137,,37,52,Human–computer interaction; Wearable computer; Presentation; Quality of life (healthcare); Visual impairment; World health; Visually impaired; Computer science; State (computer science),,,,,https://doi.org/10.1016/j.patrec.2018.10.031 https://hal.archives-ouvertes.fr/hal-01993932 http://ui.adsabs.harvard.edu/abs/2020PaReL.137...37T/abstract https://www.sciencedirect.com/science/article/pii/S0167865518308602 https://dblp.uni-trier.de/db/journals/prl/prl137.html#TapuMZ20,http://dx.doi.org/10.1016/j.patrec.2018.10.031,,10.1016/j.patrec.2018.10.031,2898741879,,0,000-424-243-390-605; 001-461-750-787-745; 003-473-950-897-114; 003-729-766-539-575; 004-772-554-982-380; 006-349-410-091-743; 007-105-512-073-496; 008-361-673-204-978; 008-558-848-807-295; 009-452-541-325-559; 010-630-218-652-28X; 010-969-630-242-584; 012-081-521-655-479; 013-056-435-015-274; 013-498-406-708-010; 014-478-034-924-123; 015-174-884-664-544; 015-651-904-909-30X; 015-714-235-272-975; 016-344-423-220-225; 016-645-836-018-745; 017-739-144-960-987; 019-925-883-027-48X; 023-085-100-690-935; 023-868-294-867-153; 025-940-097-162-978; 029-083-674-347-975; 029-360-136-216-567; 030-127-369-356-741; 030-500-337-935-178; 032-471-666-440-419; 033-288-442-402-545; 034-412-649-320-055; 035-205-900-793-931; 035-395-409-643-765; 037-163-533-301-156; 041-418-746-776-514; 042-532-555-486-368; 043-536-853-133-602; 044-625-862-601-704; 046-540-172-422-610; 046-991-476-786-467; 048-298-689-305-353; 048-443-739-272-544; 049-592-217-207-783; 051-970-317-843-151; 052-795-626-916-431; 053-968-972-722-364; 054-776-184-641-037; 055-495-282-581-758; 055-863-575-118-552; 057-890-353-263-937; 059-384-037-009-460; 059-647-236-855-012; 060-023-913-733-902; 061-205-550-296-347; 061-427-166-076-036; 061-483-553-522-346; 063-367-028-428-831; 064-043-611-038-916; 064-298-392-230-768; 068-526-647-480-345; 070-068-111-472-212; 071-806-151-845-245; 072-240-298-491-062; 073-039-903-888-829; 075-402-037-032-739; 075-903-419-661-27X; 076-059-869-327-354; 076-406-569-127-303; 078-407-819-377-776; 079-433-787-480-087; 079-503-435-008-732; 080-828-369-606-62X; 083-146-360-435-079; 085-102-500-348-494; 090-884-510-770-832; 097-390-623-922-438; 097-395-382-637-910; 101-333-962-375-602; 102-952-660-914-374; 105-278-566-106-933; 107-659-830-473-823; 110-650-958-929-874; 112-365-549-669-557; 114-207-384-088-322; 116-194-090-701-243; 120-499-391-106-298; 124-326-712-632-53X; 125-897-318-346-362; 127-070-672-662-621; 127-483-973-270-088; 128-897-617-870-753; 134-448-474-253-195; 137-648-461-521-23X; 141-489-442-115-69X; 144-452-403-048-99X; 145-601-041-771-47X; 156-482-225-015-089; 168-445-619-792-756; 170-387-846-090-147; 184-028-341-914-351,106,true,,bronze
093-860-086-109-06X,"Electronic spatial sensing for the blind : contributions from perception, rehabilitation, and computer vision",,1985,book,,,,,David H. Warren; Edward R. Strelow,"Review Section.- Historical Overview.- A Review of Mobility Aids and Means of Assessment.- Technologies of Spatial Sensing.- On Mobility Aids for the Blind.- Computer Vision Requirements in Blind Mobility Aids.- Computer Vision for the Blind.- Sensory Aids to Spatial Perception for Blind Persons: Their Design and Evaluation.- Physical Principles Underlying Blind Guidance Prostheses with an Emphasis on the Ultrasonic Exploration of a Region of Space.- Comment: Three New Blind Guidance Prostheses and What they Teach Us.- Microprocessor Techniques Applied to Ultrasonic Pulse/Echo Travel Aids for the Blind.- Tactile Vision Substitution: Some Instrumentation and Perceptual Considerations.- Studies of the Use of Spatial Sensors.- Evaluating Mobility Aids: An Evolving Methodology.- Training the Use of Artificial Spatial Displays.- Sensory Substitution of Vision by Audition.- Sonar Sensory Aid and Blind Children's Spatial Cognition.- Spatial Awareness Training of Blind Children Using the Trisensor.- Sensory Substitution in Blind Children and Neonates.- Use of Sonar Sensors with Human Infants.- Developmental Brain Research, Deprivation, and Sensory Aids.- Comment: Animal Models of Plasticity and Sensory Substitution.- Rehabilitation Issues.- Vision Prosthesis and Aids: Readiness or Appropriateness.- Technology and the Blind Person: Corridors of Insensitivity.- Perceptual and Cognitive Considerations.- On Replacement and Problem Solving Potentials of Spatial Aids for the Blind.- Implications of Perceptual Theory for the Development of Non-Visual Travel Aids for the Visually Impaired.- Amodal Information and Transmodal Perception.- Understanding Perceived Spatial Layout of Scenes: A Prerequisite for Prostheses for Blind Travelers.- The Cognitive Foundations of Mobility.- Comment: Issues in Travel Aid Design.- Comment: Machine Visual Guidance for the Blind.- Mobility and Orientation Processes of the Blind.- Final Commentary.",,,,,Artificial intelligence; Psychology; Cognition; Amodal perception; Perception; Spatial cognition; Rehabilitation; Computer vision; Spatial contextual awareness; Sensory substitution; Orientation (computer vision),,,,,http://ci.nii.ac.jp/ncid/BA00215576,http://ci.nii.ac.jp/ncid/BA00215576,,,595126354,,0,,37,false,,
093-986-177-443-585,"Power Transmission Line Inspections: Methods, Challenges, Current Status and Usage of Unmanned Aerial Systems",2024-03-26,2024,journal article,Journal of Intelligent & Robotic Systems,15730409; 09210296,Springer Science and Business Media LLC,Netherlands,Faiyaz Ahmed; J. C. Mohanta; Anupam Keshari,"<jats:title>Abstract</jats:title><jats:p>Condition monitoring of power transmission lines is an essential aspect of improving transmission efficiency and ensuring an uninterrupted power supply. Wherein, efficient inspection methods play a critical role for carrying out regular inspections with less effort &amp; cost, minimum labour engagement and ease of execution in any geographical &amp; environmental conditions. Earlier various methods such as manual inspection, roll-on wire robotic inspection and helicopter-based inspection are preferably utilized. In the present days, Unmanned Aerial System (UAS) based inspection techniques are gradually increasing its suitability in terms of working speed, flexibility to program for difficult circumstances, accuracy in data collection and cost minimization. This paper reports a state-of-the-art study on the inspection of power transmission line systems and various methods utilized therein, along with their merits and demerits, which are explained and compared. Furthermore, a review was also carried out for the existing visual inspection systems utilized for power line inspection. In addition to that, blockchain utilities for power transmission line inspection are discussed, which illustrates next-generation data management possibilities, automating an effective inspection and providing solutions for the current challenges. Overall, the review demonstrates a concept for synergic integration of deep learning, navigation control concepts and the utilization of advanced sensors so that UAVs with advanced computation techniques can be analyzed with different aspects of implementation.</jats:p>",110,2,,,Current (fluid); Line (geometry); Transmission line; Power (physics); Electric power transmission; Transmission (telecommunications); Engineering; Power transmission; Electric power system; Computer science; Electrical engineering; Systems engineering; Telecommunications; Physics; Geometry; Mathematics; Quantum mechanics,,,,,https://link.springer.com/content/pdf/10.1007/s10846-024-02061-y.pdf https://doi.org/10.1007/s10846-024-02061-y,http://dx.doi.org/10.1007/s10846-024-02061-y,,10.1007/s10846-024-02061-y,,,0,000-222-655-599-899; 001-576-194-743-003; 002-272-091-430-390; 002-274-096-222-311; 002-938-717-543-193; 004-030-422-801-75X; 005-318-991-086-556; 005-375-950-611-271; 005-542-135-759-435; 007-809-685-130-685; 008-104-728-001-67X; 008-882-829-411-396; 008-894-150-160-664; 010-106-265-102-650; 010-138-201-517-052; 013-510-181-297-52X; 015-663-282-885-240; 016-578-970-510-944; 017-104-318-740-871; 018-880-931-976-59X; 020-087-682-191-326; 020-296-922-227-561; 022-007-927-656-997; 022-345-416-944-374; 023-133-537-535-501; 023-398-964-786-421; 023-456-728-291-083; 023-717-138-613-874; 024-197-081-130-715; 025-880-963-880-932; 026-717-662-186-599; 027-884-643-917-637; 029-209-362-539-872; 029-251-209-263-338; 032-303-625-283-223; 033-502-484-551-025; 033-992-942-858-716; 035-388-209-957-545; 035-589-627-626-088; 035-629-786-839-957; 035-924-867-507-200; 038-359-126-392-389; 039-903-710-968-394; 045-707-082-155-657; 047-055-954-897-36X; 047-779-282-109-474; 047-999-130-453-393; 048-144-498-043-379; 049-695-542-770-088; 049-969-679-817-428; 054-819-045-052-125; 055-219-827-308-380; 055-313-284-079-26X; 056-923-821-835-070; 059-206-857-015-151; 061-928-598-110-416; 062-875-001-628-803; 063-512-633-172-342; 063-993-639-233-396; 064-089-168-655-453; 065-299-682-519-747; 070-962-282-453-207; 075-629-967-252-742; 075-852-386-679-395; 077-745-738-253-398; 081-142-884-880-826; 081-914-506-159-73X; 083-446-152-631-029; 084-968-611-682-189; 088-539-734-190-168; 090-919-886-010-425; 093-084-154-424-186; 093-945-299-354-272; 098-582-798-859-488; 101-652-443-753-403; 103-709-755-059-602; 104-567-156-366-024; 109-305-411-120-999; 109-423-899-189-363; 110-634-749-285-996; 113-093-437-897-694; 113-438-558-381-548; 115-259-820-032-263; 115-837-468-787-684; 119-135-534-409-163; 122-795-451-982-265; 128-903-631-254-250; 135-452-875-019-939; 138-392-217-122-587; 140-700-368-294-646; 142-188-560-740-75X; 146-142-403-894-28X; 153-619-130-959-875; 160-695-376-585-699; 162-550-235-020-227; 163-340-674-637-721; 171-084-745-671-055; 183-394-290-588-560,4,true,cc-by,hybrid
094-079-582-213-545,Cultural heritage visits supported on visitors’ preferences and mobile devices,2019-06-18,2019,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Pedro J. S. Cardoso; João M. F. Rodrigues; João A. R. Pereira; Sergey Nogin; Joana Lessa; Célia M. Q. Ramos; Roman Bajireanu; Miguel Gomes; Paulo Bica,,19,3,499,513,World Wide Web; Point of interest; Augmented reality; Set (psychology); Mobile device; Pace; Adaptive user interface; Face (sociological concept); Computer science; Cultural heritage,,,,,https://doi.org/10.1007/s10209-019-00657-y https://link.springer.com/content/pdf/10.1007/s10209-019-00657-y.pdf https://link.springer.com/article/10.1007/s10209-019-00657-y https://dblp.uni-trier.de/db/journals/uais/uais19.html#CardosoRPNLRBGB20,http://dx.doi.org/10.1007/s10209-019-00657-y,,10.1007/s10209-019-00657-y,2949827225,,0,001-996-430-142-340; 004-254-839-119-716; 004-869-102-346-341; 006-134-643-338-411; 008-032-816-492-763; 009-647-425-609-714; 010-891-471-503-208; 011-688-487-518-802; 011-919-186-191-397; 012-795-704-703-589; 017-637-644-483-147; 020-134-924-501-761; 025-357-204-640-667; 025-540-534-199-996; 026-047-385-746-550; 038-089-951-848-641; 038-529-483-820-280; 039-766-229-943-758; 052-932-758-364-999; 055-056-263-762-567; 058-708-870-487-368; 059-781-475-575-090; 061-030-537-372-198; 061-705-953-560-414; 064-942-704-072-565; 077-043-520-479-021; 078-788-084-066-353; 084-415-272-770-968; 089-615-942-344-764; 090-153-671-291-606; 093-793-624-432-394; 099-565-998-121-287; 099-907-884-080-920; 105-810-803-790-416; 106-435-518-439-410; 106-766-517-518-914; 109-120-276-138-641; 116-153-682-946-69X; 120-377-913-401-943; 120-767-088-360-769; 124-157-539-574-879; 127-033-396-767-810; 130-841-337-778-53X; 134-692-988-441-892; 143-875-130-007-110; 162-672-591-271-298; 168-036-345-097-619,8,false,,
094-108-145-447-415,Improved head related transfer function generation and testing for acoustic virtual reality development,2010-07-22,2010,conference proceedings,,,,,Zoltan Haraszy; David-George Cristea; Virgil Tiponut; Titus Slavici,"The new Acoustic Virtual Reality (AVR) concept is often used as a man-machine interface in electronic travel aid (ETA), that help blind and visually impaired individuals to navigate in real outdoor environments. According to this concept, the presence of obstacles in the surrounding environment and the path to the desired target will be signalized to the blind subject by burst of sounds, whose virtual source position suggests the position of the real obstacles and the direction of movement, respectively. The practical implementation of the AVR concept requires the so-called Head Related Transfer Functions (HRTFs) to be known in every point of the 3D space and for each subject. These functions can be determined by using a quite complex procedure, which requires many measurements for each individual. In the present paper, an improved version of the previously proposed [12] artificial neural network (ANN) is presented and used, in order to obtain the HRTFs. The proposed method, valid for only one subject, speeds up the implementation of the AVR concept after the ANN training has been completed. Finally, the experimental setup for testing, some experimental results using the new ANNs, conclusions and further developments are also presented.",,,411,416,Interface (computing); Engineering; Position (vector); Head-related transfer function; Artificial intelligence; Virtual reality; Point (geometry); Computer vision; Simulation; Artificial neural network; Transfer function; Path (graph theory),,,,,http://dl.acm.org/ft_gateway.cfm?id=1981412&type=pdf,http://dl.acm.org/ft_gateway.cfm?id=1981412&type=pdf,,,69756498,,0,019-306-790-422-912; 019-431-359-682-592; 019-550-995-456-408; 030-127-369-356-741; 044-192-308-604-739; 050-966-266-525-728; 051-766-223-654-722; 063-957-925-086-43X; 066-114-366-503-885; 067-332-342-991-804; 071-405-244-808-059; 073-928-284-197-323; 077-113-362-690-85X; 084-533-882-547-840; 092-862-654-304-908; 099-646-885-015-028; 100-773-828-432-613; 190-621-332-208-185,14,false,,
094-570-122-957-172,Robust ground plane detection and tracking in stereo sequences using camera orientation,,2016,conference proceedings article,"2016 20th International Conference on System Theory, Control and Computing (ICSTCC)",,IEEE,,Paul Herghelegiu; Adrian Burlacu; Simona Caraiman,"The research on assisting visually impaired people to navigate in unknown environments commonly employs image processing or computer vision techniques. One key element that needs to be identified in the surrounding environment of a blind user is the ground plane. Its accurate detection is highly important because the user is able to move freely in that area. An assistive device should primarily assist the user in avoiding the obstacles that lie on the ground surface. The images that will be further processed are usually acquired using depth acquisition devices worn by the user. Therefore, in contrast with automotive or robotics applications, which also heavily rely on ground plane extraction for obstacle detection, the camera orientation has more degrees of freedom. This leads to the requirement of designing more complex solutions for the ground plane detection in the case of assistive systems for the visually impaired. In this paper we introduce an algorithm to detect the ground plane taking into consideration the orientation of the camera, namely its pitch and roll. The proposed algorithm is based on an efficient processing of the v-disparity map associated to each frame. A two-step decision making approach to determine the most suited area that corresponds to the ground plane is described. Experimental results with synthetic datasets are provided to prove the efficiency and robustness of the proposed approach.",,,514,519,Engineering; Artificial intelligence; Obstacle; Camera orientation; Assistive device; Computer vision; Robotics; Automotive industry; Ground plane; Image processing; Robustness (computer science),,,,,https://ieeexplore.ieee.org/document/7790717/,http://dx.doi.org/10.1109/icstcc.2016.7790717,,10.1109/icstcc.2016.7790717,2561255379,,1,014-090-386-279-841; 023-497-003-056-706; 033-626-844-682-323; 034-551-334-308-190; 036-013-311-274-842; 039-274-384-165-289; 041-064-453-599-993; 043-346-361-854-086; 053-212-609-843-102; 058-350-834-986-051; 060-669-484-901-332; 078-895-691-224-986; 086-350-514-739-970; 113-222-487-624-107; 116-653-785-043-953; 120-499-391-106-298; 131-853-990-412-535,15,false,,
094-699-066-960-285,Real-Time Computer Vision Based Autonomous Navigation System for Assisting Visually Impaired People using Machine Learning,2022-12-17,2022,conference proceedings article,2022 4th International Conference on Sustainable Technologies for Industry 4.0 (STI),,IEEE,,Md. Zahidul Hasan; Shovon Sikder; Muhammad Aminur Rahaman,"Visual impairment is a global problem and people without vision suffer more than other impaired people. A companion is always needed for the movement of blind people and there may not be anyone by their side in case of emergency. Walking alone on the street, detecting the person closest to him and avoiding obstacles are always problems. Researchers have been working for visually impaired people using sensor based distance measurement systems for years. This paper proposes a Computer Vision based system to navigate visually impaired people by using Artificial Intelligence, and also a novel distance measuring approach. The system will capture real time images through a camera placed inside a sun-glass then process the video frames by trained YOLO V3 model. After processing, the program will identify a total of 80 pre-trained objects and additional 7 objects including person, car, bicycle, and broken roads and then will produce a navigation command through headphones. A comparative evaluation with other similar works is performed, and the result represents the primary accomplishments of this article. Several testing and validation procedures were carried out in order to achieve optimal performance and accurate distance measurement. The proposed system outperforms the state-ofthe-art in terms of object detection, distance measurement, computational costs calculation, and accessibility for the visually impaired, according to the results, which were validated using mathematical calculations and the necessary measuring devices. Since Industry 4.0 demands smart automation, this system has a significant impact not just on disabled persons but also on the development of a smart city.",,,,,Process (computing); Computer science; Computer vision; Artificial intelligence; Visually impaired; Automation; Object (grammar); Headphones; Human–computer interaction; Machine vision; Object detection; Visual impairment; Simulation; Engineering; Pattern recognition (psychology); Mechanical engineering; Psychology; Electrical engineering; Psychiatry; Operating system,,,,Green University,,http://dx.doi.org/10.1109/sti56238.2022.10103268,,10.1109/sti56238.2022.10103268,,,0,002-397-792-416-419; 006-565-780-206-428; 007-223-103-783-187; 023-309-209-821-636; 039-274-384-165-289; 048-199-650-951-338; 052-046-438-827-353; 066-479-281-229-604; 067-894-235-988-508; 120-891-067-806-838; 155-344-434-360-450,1,false,,
095-459-008-366-935,Indoor Navigation Systems for Blind People,2018-03-08,2018,preprint,,,MDPI AG,,Ali Hojjat,"<jats:p>Indoor navigation systems must deal with absence of GPS signals, since they are only available in outdoor environments. Therefore, indoor systems have to rely upon other techniques for positioning users. Recently various indoor navigation systems have been designed and developed to help visually impaired people. In this paper an overview of some existing indoor navigation systems for visually impaired people are presented and they are compared from different perspectives. The evaluated techniques are ultrasonic systems, RFID-based solutions, computer vision aided navigation systems, ans smartphone-based applications.</jats:p>",,,,,Global Positioning System; Computer science; Visually impaired; Navigation system; Human–computer interaction; Real-time computing; Computer vision; Telecommunications,,,,,,http://dx.doi.org/10.20944/preprints201803.0058.v1,,10.20944/preprints201803.0058.v1,,,0,,0,true,cc-by,green
095-880-530-631-147,Heterogeneous Teams of Modular Robots for Mapping and Exploration,,2000,journal article,Autonomous Robots,09295593; 15737527,Springer Science and Business Media LLC,Netherlands,Robert Grabowski; Luis E. Navarro-Serment; Christiaan J. J. Paredis; Pradeep K. Khosla,,8,3,293,308,Modular design; Human–computer interaction; AISoy1; Artificial intelligence; Robotic sensing; Computer science; Ant robotics; Self-reconfiguring modular robot; Sonar; Occupancy grid mapping; Robot,,,,,https://doi.org/10.1023/A:1008933826411 http://lars.mec.ua.pt/public/LAR%20Projects/Laser3D/2003_MiguelMatos/Interessante/Heterogeneous%20Teams%20of%20Modular%20Robots%20for.pdf http://repository.cmu.edu/isr/566/ http://repository.cmu.edu/cgi/viewcontent.cgi?article=1547&context=isr http://csc.ucdavis.edu/%7Edynlearn/dynlearn/RoMADS/papers/mapandexplore.pdf https://link.springer.com/article/10.1023/A%3A1008933826411 http://web.cecs.pdx.edu/~mperkows/CLASS_479/S2006/mapandexplore_MAS_Grabowski.pdf https://link.springer.com/article/10.1023/A:1008933826411?error=cookies_not_supported&error=cookies_not_supported&code=7b43c0b8-6e4f-45e1-a7bf-41d4057cbd1a&code=7d2f2ad6-be90-45e0-a61e-9d957a816f25 http://www.ri.cmu.edu/pub_files/pub3/grabowski_robert_1999_1/grabowski_robert_1999_1.pdf https://dblp.uni-trier.de/db/journals/arobots/arobots8.html#GrabowskiNPK00 https://dialnet.unirioja.es/servlet/articulo?codigo=707638 http://www.ai.rug.nl/~feldbrug/cogrobot/CogRobPres_final_team-mapping.pdf,http://dx.doi.org/10.1023/a:1008933826411,,10.1023/a:1008933826411,2136247341,,0,000-450-135-588-509; 006-385-256-101-71X; 017-255-740-752-577; 022-425-939-641-217; 043-827-090-820-753; 046-841-903-000-681; 052-221-254-547-432; 055-224-290-217-847; 059-861-386-317-882; 061-343-909-070-172; 075-029-771-945-345; 077-521-929-868-835; 078-518-008-346-978; 083-967-961-875-940; 084-771-922-369-940; 088-157-080-695-090; 095-335-063-978-235; 116-274-995-307-611; 118-341-503-040-35X; 119-203-734-980-120; 137-681-539-641-19X; 141-995-630-420-618; 155-377-611-500-428; 170-274-379-608-662; 171-043-491-787-443; 179-189-463-390-361,181,false,,
096-339-751-374-050,"Survey on Object Detection, Distance Estimation and Navigation Systems for Blind People",2020-10-23,2020,book chapter,Machine Learning for Predictive Analysis,23673370; 23673389,Springer Singapore,,Bela Shah; Smeet Shah; Purvesh Shah; Aneri Shah,"Loss of vision is a huge problem that is faced by many of the people around the world either from birth or due to some accident or else disease. Due to this, they face many difficulties while interacting with surrounding. In this paper, we have given a brief case study on the existing systems for object detection, distance estimation and navigation for blind and visually impaired people. Many systems have been developed using the electronic sensors or using the concepts of machine learning and deep learning to assist them. These new techniques are far more efficient and reliable than the prior methods like walking cane, guiding dogs, etc. We have also proposed a system based on machine learning integrated with a voice assistant-mobile-based application and external camera using existing methodologies. It aims to help blind people to identify nearby objects along with their respective distances. To achieve this operation, we will be using existing technologies YOLOv3 and DisNet based on neural networks. The system also makes traveling task from one place to another easier by suggesting the fastest transportation system available at that specific time.",,,463,472,Human–computer interaction; Deep learning; Artificial intelligence; Face (geometry); Object detection; Task (project management); External camera; Specific time; Computer science; Artificial neural network; Estimation,,,,,https://link.springer.com/chapter/10.1007/978-981-15-7106-0_46,http://dx.doi.org/10.1007/978-981-15-7106-0_46,,10.1007/978-981-15-7106-0_46,3093514461,,0,005-747-780-254-594; 007-725-872-437-764; 017-159-670-613-140; 037-199-326-802-024; 053-567-501-093-090; 145-286-072-054-601; 179-654-625-487-90X,1,false,,
096-426-650-456-538,Universal multimedia information access,2003-06-01,2003,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Mark T. Maybury,,2,2,96,104,World Wide Web; Interface design; Social responsibility; Government; Automatic summarization; Information extraction; Information access; Grand Challenges; Multimedia information; Computer science,,,,,https://dblp.uni-trier.de/db/journals/uais/uais2.html#Maybury03 https://doi.org/10.1007/s10209-002-0043-5 https://link.springer.com/article/10.1007%2Fs10209-002-0043-5 https://dl.acm.org/doi/10.1007/s10209-002-0043-5,http://dx.doi.org/10.1007/s10209-002-0043-5,,10.1007/s10209-002-0043-5,2053860592,,0,011-015-976-677-453; 031-585-078-018-464; 037-824-565-385-804; 039-078-762-555-105; 039-828-146-979-588; 042-942-266-998-406; 043-440-044-480-886; 049-262-106-075-819; 056-359-125-334-235; 059-928-226-672-352; 071-694-569-900-076; 081-155-610-461-715; 082-227-958-913-838; 105-197-183-269-175; 112-707-395-881-092; 160-745-033-543-879,10,false,,
096-662-295-088-316,Implementation of Smart Hindrance Tracking System,2024-02-28,2024,journal article,International Research Journal of Computer Science,23939842,AM Publications,,Rishabh Raj,"<jats:p>The Smart Hindrance Tracking System for Blind Individuals is a mini-project aimed at enhancing the mobility and safety of visually impaired individuals in navigating their surroundings. Leveraging cutting-edge technologies, this system integrates computer vision, sensor networks, and mobile applications to provide real-time hindrance detection and navigation assistance. The primary objective is to empower blind individuals with a reliable and intelligent solution that enables them to identify and navigate around obstacles in their path. The system utilizes a combination of depth-sensing cameras and ultrasonic sensors to detect obstacles in the user's vicinity. These sensors feed data to a central processing unit, where a computer vision algorithm processes the information in real-time, identifying obstacles and their spatial locations. The processed data is then communicated to the user through a user-friendly mobile application. This research presents the daily challenges faced by visually impaired individuals but also fosters inclusivity and independence. The Smart Hindrance Tracking System stands at the intersection of technology and social impact, offering an innovative solution to improve the lives of those with visual impairments.</jats:p>",11,2,121,128,Tracking (education); Computer science; Environmental science; Psychology; Pedagogy,,,,,,http://dx.doi.org/10.26562/irjcs.2024.v1102.08,,10.26562/irjcs.2024.v1102.08,,,0,,0,true,,gold
097-057-226-522-546,Vehicle Artificial Intelligence System Based on Intelligent Image Analysis and 5G Network,2021-09-03,2021,journal article,International Journal of Wireless Information Networks,10689605; 15728129,Springer Science and Business Media LLC,United States,Baojing Liu; Chenye Han; Xinxin Liu; Wei Li,,,,1,17,Image (mathematics); Latency (engineering); Emerging technologies; 5G; Quality (business); Vehicle to everything; Road congestion; Artificial Intelligence System; Computer science; Telecommunications,,,,,https://link.springer.com/article/10.1007%2Fs10776-021-00535-6,http://dx.doi.org/10.1007/s10776-021-00535-6,,10.1007/s10776-021-00535-6,3196677700,,0,001-050-234-041-158; 001-187-806-935-403; 002-179-639-622-229; 003-541-437-088-916; 005-113-996-557-004; 007-272-916-898-054; 015-233-879-087-436; 016-711-564-471-118; 030-744-615-388-989; 031-105-311-584-791; 032-605-269-813-956; 042-858-374-476-558; 043-184-003-121-238; 043-768-060-792-686; 045-114-922-567-686; 049-793-478-737-388; 052-866-632-930-42X; 054-173-981-939-618; 056-715-954-494-875; 057-780-380-012-570; 061-798-671-902-100; 062-783-090-820-363; 067-146-552-484-828; 067-290-240-707-910; 073-727-639-273-655; 074-121-202-685-024; 076-804-334-501-894; 077-363-135-045-641; 078-954-282-194-453; 086-802-547-036-969; 087-941-937-580-683; 098-360-202-958-909; 100-991-699-079-280; 103-644-892-497-794; 106-778-762-534-967; 110-049-456-492-089; 130-441-982-493-892; 147-776-401-851-527; 157-326-385-429-581; 177-153-375-429-775; 180-216-260-822-28X; 181-507-613-514-140; 184-281-251-991-723,7,false,,
097-103-796-322-313,Feasibility of laser-guided percutaneous pedicle screw placement in the lumbar spine using a hybrid-OR,2017-02-10,2017,journal article,International journal of computer assisted radiology and surgery,18616429; 18616410,Springer Science and Business Media LLC,Germany,Peter H. Richter; Florian Gebhard; M. Salameh; Konrad Schuetze; Michael Kraus,,12,5,873,879,Radiology; Percutaneous; Nuclear medicine; Intraoperative imaging; Lumbar spine; Standard technique; Pedicle screw; Procedure time; Lumbar; Orthopedic surgery; Medicine,Accuracy; Hybrid-OR; Minimally invasive; Pedicle screw; Syngo iGuide,"Bone Screws; Equipment Design; Feasibility Studies; Fluoroscopy/methods; Humans; Lasers; Lumbar Vertebrae/surgery; Orthopedic Procedures/methods; Pedicle Screws; Surgery, Computer-Assisted/methods; Tomography, X-Ray Computed",,AO Foundation,https://link.springer.com/article/10.1007/s11548-017-1529-1 https://dblp.uni-trier.de/db/journals/cars/cars12.html#RichterGSSK17 https://www.ncbi.nlm.nih.gov/pubmed/28188485 https://rd.springer.com/article/10.1007/s11548-017-1529-1,http://dx.doi.org/10.1007/s11548-017-1529-1,28188485,10.1007/s11548-017-1529-1,2587750430,,0,002-195-404-794-034; 004-199-817-753-362; 006-823-267-668-620; 009-992-176-416-125; 010-061-767-317-006; 011-908-378-193-983; 012-745-868-579-654; 014-647-898-264-411; 018-363-777-151-649; 018-784-301-380-37X; 019-656-470-234-47X; 021-064-356-735-513; 022-236-797-636-080; 026-368-972-117-529; 033-769-294-080-106; 039-180-537-312-247; 057-064-865-752-254; 067-163-035-259-299; 067-806-452-987-250; 072-452-865-382-896; 072-946-070-435-86X; 077-926-875-097-46X; 078-700-766-446-530; 079-800-535-310-905; 088-786-161-167-654; 089-456-678-779-842; 104-069-875-618-847; 104-121-679-441-840; 105-883-618-896-697; 117-723-223-705-940; 131-786-333-753-818; 138-638-618-203-094; 145-656-193-919-937; 184-579-853-509-107,8,false,,
097-265-339-953-488,A New Generation of Intelligent Aid Cane for Blindness Based on STM32,2024-04-18,2024,book chapter,Lecture Notes in Electrical Engineering,18761100; 18761119,Springer Nature Singapore,Germany,Haoyang Zhang; Peidong Zhuang,"With the rapid development of urban traffic, the travel of the visually impaired is facing great challenges, and the traditional blind rod can no longer meet their needs for safe travel. At present, the development of front-end guide products is in a bottleneck period, and the application problems of traditional guide products are frequent, and there is a lack of innovation and ways to solve the problems. Learn from the popular intelligent devices in the last five years, thereby liberating the hands of the visually impaired, providing more accurate navigation services, improving the living standards of the blind, and ensuring the safety of the blind. In addition to visual impairment, blind people have the same intelligence as normal people. Normal travel is part of the independent life of disabled people, and they enjoy the same right to freedom as normal people. Providing travel freedom for the blind is conducive to equal participation in society and improving living standards. In order to solve the problem of difficult travel for the blind, we will design an intelligent guide cane, which is based on STM32 microcontroller and supported by GPS positioning module, ultrasonic ranging module, GSM module, voice module, etc., which can realize multiple functions such as real-time positioning, obstacle alarm, sending SMS, calling emergency contacts and so on. Walking stick adopts voice broadcasting and key operation to carry out man–machine interaction. The intelligent blind rod has complete functions, small size and low cost, and is easy to scale production.",,,389,397,STM32; Cane; Blindness; Computer science; Optometry; Medicine; Telecommunications; Chemistry; Food science; Chip; Sugar,,,,,,http://dx.doi.org/10.1007/978-981-99-7502-0_43,,10.1007/978-981-99-7502-0_43,,,0,,0,false,,
097-390-623-922-438,The SmartVision Navigation Prototype for Blind Users,2011-05-31,2011,journal article,International Journal of Digital Content Technology and its Applications,19759339; 22339310,AICIT,South Korea,J. M. H. du Buf; João Barroso; João M. F. Rodrigues; Hugo Paredes; Miguel Farrajota; Hugo Fernandes; João José; Victor Teixeira; Mário Saleiro,"The goal of the Portuguese project ""SmartVision: active vision for the blind"" is to develop a small, portable and cheap yet intelligent and reliable system for assisting the blind and visually impaired while navigating autonomously, both in- and outdoor. In this article we present an overview of the prototype, design issues, and its different modules which integrate GPS and Wi-Fi localisation with a GIS, passive RFID tags, and computer vision. The prototype addresses global navigation for going to some destiny, by following known landmarks stored in the GIS in combination with path optimisation, and local navigation with path and obstacle detection just beyond the reach of the white cane. The system does not replace the white cane but complements it, in order to alert the user to looming hazards. In addition, computer vision is used to identify objects on shelves, for example in a pantry or refrigerator. The user-friendly interface consists of a four-button hand-held box, a vibration actuator in the handle of the white cane, and speech synthesis. In the near future, passive RFID tags will be complemented by active tags for marking navigation landmarks, and speech recognition may complement or substitute the vibration actuator.",5,5,351,361,Human–computer interaction; Interface (computing); Looming; Speech synthesis; Artificial intelligence; Actuator; Active vision; Obstacle; PATH (variable); Computer vision; Computer science; Global Positioning System,,,,,https://core.ac.uk/display/61500023 https://www.researchgate.net/profile/Hugo_Paredes2/publication/216435207_The_SmartVision_Na_vigation_Prototype_for_Blind_Users/links/0912f509bac06c030f000000.pdf https://core.ac.uk/download/61500023.pdf,http://dx.doi.org/10.4156/jdcta.vol5.issue5.39,,10.4156/jdcta.vol5.issue5.39,1968406403,,0,000-631-028-807-828; 012-642-196-605-159; 019-775-185-370-343; 029-785-009-752-993; 049-789-660-442-971; 056-773-999-189-634; 059-914-108-891-872; 068-446-370-600-681; 077-703-665-866-285; 078-191-959-374-104; 095-077-362-337-471; 125-215-400-077-068; 130-552-169-471-000; 140-900-790-253-336; 148-768-859-064-775; 157-138-503-886-356; 171-967-210-532-103,45,true,,green
097-675-464-751-908,Poster Session: Computer Assisted Radiology and Surgery - 22nd International Congress and Exhibition,2008-05-16,2008,journal article,International Journal of Computer Assisted Radiology and Surgery,18616410; 18616429,Springer Science and Business Media LLC,Germany,,,3,S1,446,451,Session (web analytics); Exhibition; Medical physics; Computer science; Medicine; Multimedia; World Wide Web; Art history; Art,,,,,,http://dx.doi.org/10.1007/s11548-008-0209-6,,10.1007/s11548-008-0209-6,,,0,,2,false,,
097-880-577-297-409,GripSee: A Gesture-Controlled Robot for Object Perception and Manipulation,,1999,journal article,Autonomous Robots,09295593; 15737527,Springer Science and Business Media LLC,Netherlands,Mark Becker; Efthimia Kefalea; Eric Maël; Christoph von der Malsburg; Mike Pagel; Jochen Triesch; Jan C. Vorbrüggen; Rolf P. Würtz; Stefan Zadel,,6,2,203,221,Human–robot interaction; Artificial intelligence; Gesture recognition; Social robot; Gesture; Service robot; Computer vision; Computer science; Pose; Object (computer science); Robot,,,,,https://dblp.uni-trier.de/db/journals/arobots/arobots6.html#BeckerKMMPTVWZ99 https://www.ini.rub.de/PEOPLE/rolf/articles/gripsee-1999.pdf http://doi.org/10.1023/A:1008839628783 http://dx.doi.org/10.1023/A:1008839628783 https://link.springer.com/article/10.1023/A:1008839628783 https://dx.doi.org/10.1023/A:1008839628783 https://doi.org/10.1023/A:1008839628783,http://dx.doi.org/10.1023/a:1008839628783,,10.1023/a:1008839628783,1497334142,,3,000-022-554-622-619; 000-863-593-527-370; 005-217-494-185-382; 006-058-661-942-987; 008-989-773-861-565; 021-440-976-943-037; 030-119-548-218-731; 038-695-017-947-376; 050-896-524-141-735; 051-681-576-413-542; 052-463-361-333-386; 052-654-067-805-479; 054-816-590-413-615; 056-558-636-505-552; 060-182-208-525-209; 061-635-112-862-871; 068-354-193-203-889; 075-963-980-484-947; 081-053-831-670-863; 084-641-826-330-987; 085-956-926-103-653; 088-038-498-120-292; 091-772-543-127-78X; 093-703-604-622-674; 099-837-467-202-965; 109-936-433-946-573; 112-105-226-342-663; 112-480-455-497-014; 116-543-910-178-199; 119-453-550-731-367; 122-517-258-530-97X; 142-022-708-560-801; 156-753-912-008-536,57,false,,
098-053-146-056-504,Virtual reality based safety system for airborne platforms,2012-04-21,2012,journal article,International Journal on Interactive Design and Manufacturing (IJIDeM),19552513; 19552505,Springer Science and Business Media LLC,Germany,Paul Huang; Omar A. Khan,,6,2,131,137,Engineering; Industrial design; Embedded system; Aeronautics; Virtual reality; Extreme weather; Quality (business); Collision avoidance; Engineering design process; Process (engineering); Global Positioning System,,,,,http://web.thu.edu.tw/ylhuang/www/101_1/ip/ip_paper/01.pdf https://link.springer.com/article/10.1007%2Fs12008-012-0157-9 https://link.springer.com/content/pdf/10.1007%2Fs12008-012-0157-9.pdf,http://dx.doi.org/10.1007/s12008-012-0157-9,,10.1007/s12008-012-0157-9,2019272683,,0,039-012-736-680-672; 162-684-439-817-311,3,false,,
098-156-392-926-60X,RoadCompass: following rural roads with vision + ladar using vanishing point tracking,2008-02-16,2008,journal article,Autonomous Robots,09295593; 15737527,Springer Science and Business Media LLC,Netherlands,Christopher Rasmussen,,25,3,205,229,Gabor wavelet; Artificial intelligence; Mobile robot; Obstacle; Glare (vision); Computer vision; Computer science; Tracking (particle physics); Component (UML); Lidar; Vanishing point,,,,,https://link.springer.com/article/10.1007%2Fs10514-008-9091-x http://dx.doi.org/10.1007/s10514-008-9091-x https://dblp.uni-trier.de/db/journals/arobots/arobots25.html#Rasmussen08 https://dx.doi.org/10.1007/s10514-008-9091-x https://doi.org/10.1007/s10514-008-9091-x,http://dx.doi.org/10.1007/s10514-008-9091-x,,10.1007/s10514-008-9091-x,2069390654,,0,001-977-631-509-918; 005-497-878-220-991; 005-660-565-993-288; 011-969-064-214-381; 014-468-816-023-945; 017-999-077-393-089; 019-278-288-676-196; 024-550-497-968-452; 025-719-510-917-72X; 030-014-279-775-853; 031-714-785-711-928; 034-986-160-519-213; 038-258-320-042-590; 040-372-781-904-772; 042-540-922-383-492; 047-976-576-302-766; 052-483-351-090-489; 052-721-654-905-666; 061-243-669-600-825; 063-745-126-644-146; 071-180-817-619-657; 076-029-223-455-587; 078-674-891-030-079; 088-392-324-377-786; 092-029-784-601-900; 093-447-930-343-531; 095-846-834-551-171; 098-059-732-445-50X; 098-217-726-201-956; 099-529-763-244-874; 100-293-824-072-814; 102-222-833-712-515; 102-301-035-113-548; 102-878-081-844-866; 109-617-690-629-802; 112-102-868-041-922; 124-160-572-625-698; 124-559-628-961-266; 132-283-070-099-560; 134-372-781-281-569; 149-398-167-302-42X; 152-159-771-302-24X; 159-041-984-660-607; 163-682-334-840-862; 168-788-495-210-658; 169-227-601-108-021; 196-639-565-639-668,39,false,,
098-336-107-872-162,Deep panoramic depth prediction and completion for indoor scenes,2024-02-08,2024,journal article,Computational Visual Media,20960433; 20960662,Springer Science and Business Media LLC,,Giovanni Pintore; Eva Almansa; Armando Sanchez; Giorgio Vassena; Enrico Gobbetti,"<jats:title>Abstract</jats:title><jats:p>We introduce a novel end-to-end deep-learning solution for rapidly estimating a dense spherical depth map of an indoor environment. Our input is a single equirectangular image registered with a sparse depth map, as provided by a variety of common capture setups. Depth is inferred by an efficient and lightweight single-branch network, which employs a dynamic gating system to process together dense visual data and sparse geometric data. We exploit the characteristics of typical man-made environments to efficiently compress multi-resolution features and find short- and long-range relations among scene parts. Furthermore, we introduce a new augmentation strategy to make the model robust to different types of sparsity, including those generated by various structured light sensors and LiDAR setups. The experimental results demonstrate that our method provides interactive performance and outperforms state-of-the-art solutions in computational efficiency, adaptivity to variable depth sparsity patterns, and prediction accuracy for challenging indoor data, even when trained solely on synthetic data without any fine tuning.; </jats:p>",,,,,Computer graphics; Computer science; Artificial intelligence; Computer graphics (images); Computer vision,,,,,https://link.springer.com/content/pdf/10.1007/s41095-023-0358-0.pdf https://doi.org/10.1007/s41095-023-0358-0,http://dx.doi.org/10.1007/s41095-023-0358-0,,10.1007/s41095-023-0358-0,,,0,000-617-323-414-217; 002-191-750-985-513; 002-359-795-431-422; 002-491-135-613-421; 003-782-618-340-153; 006-736-886-265-067; 006-740-872-540-437; 007-772-713-007-566; 007-846-897-801-789; 008-783-981-093-093; 009-895-951-265-619; 010-037-102-410-599; 011-187-269-087-316; 016-136-553-382-947; 016-446-586-220-032; 017-744-969-953-681; 020-233-013-143-936; 020-368-387-449-823; 020-962-225-757-788; 021-599-579-431-147; 023-971-639-342-156; 023-977-658-432-971; 024-275-798-808-977; 024-487-750-761-705; 024-897-291-099-864; 026-322-154-050-779; 026-532-217-294-849; 027-341-363-165-185; 028-588-077-439-721; 030-489-803-226-623; 031-503-995-532-020; 036-549-275-474-294; 038-204-348-464-177; 038-453-856-102-508; 038-500-416-646-674; 038-907-227-962-197; 040-565-015-553-409; 043-606-468-399-253; 045-756-066-330-284; 047-326-510-583-783; 053-921-812-461-216; 061-708-237-293-611; 061-720-909-498-372; 063-457-289-996-190; 065-168-312-055-789; 073-277-651-666-368; 074-833-074-757-649; 076-935-880-279-119; 080-188-677-018-333; 088-120-630-082-987; 090-887-762-037-50X; 091-352-977-764-128; 095-000-759-079-663; 095-444-258-776-054; 098-131-223-729-815; 107-703-693-522-304; 109-583-962-992-929; 120-484-719-870-547; 120-827-498-101-307; 121-913-978-357-552; 137-739-792-705-774; 144-270-978-986-816; 144-512-643-782-140; 144-932-220-353-337; 145-228-454-863-185; 148-999-081-243-809; 159-140-600-015-478; 159-996-094-626-724; 160-314-075-482-678; 167-131-435-704-717; 187-266-834-123-366,1,true,cc-by,gold
098-497-595-587-383,Deep Learning based Object Detection and Recognition Framework for the Visually-Impaired,,2020,conference proceedings article,2020 Fourth International Conference on Computing Methodologies and Communication (ICCMC),,IEEE,,Swapnil Bhole; Aniket Dhok,"Vision impairment or blindness is one of the top ten disabilities in humans, and unfortunately, India is home to the world’s largest visually impaired population. In this study, we present a novel framework to assist the visually impaired in object detection and recognition, so that they can independently navigate, and be aware of their surroundings. The paper employs transfer learning on Single-Shot Detection (SSD) mechanism for object detection and classification, followed by recognition of human faces and currency notes, if detected, using Inception v3 model. SSD detector is trained on modified PASCAL VOC 2007 dataset, in which a new class is added, to enable the detection of currency as well. Furthermore, separate Inception v3 models are trained to recognize human faces and currency notes, thus making the framework scalable and adaptable according to the user preferences. Ultimately, the output from the framework can then be presented to the visually impaired person in audio format. Mean Accuracy and Precision (mAP) scores of standalone SSD detector of the added currency class was 67.8 percent, and testing accuracy of person and currency recognition of Inception v3 model were 92.5 and 90.2 percent respectively.",,,725,728,Deep learning; Currency; Transfer of learning; Artificial intelligence; Object detection; Speech recognition; Population; Computer science; Scalability; Convolutional neural network; Pascal (programming language),,,,,https://ieeexplore.ieee.org/document/9076530,http://dx.doi.org/10.1109/iccmc48092.2020.iccmc-000135,,10.1109/iccmc48092.2020.iccmc-000135,3019223839,,0,004-216-069-855-831; 006-565-780-206-428; 014-694-226-133-007; 031-218-334-653-826; 031-618-901-880-025; 034-001-383-747-521; 058-268-022-053-157; 064-043-611-038-916; 065-272-316-935-556; 071-895-492-147-282; 074-199-903-520-265; 079-458-787-945-818; 087-496-050-387-679; 097-841-790-469-802; 180-559-374-049-445,8,false,,
098-811-589-762-176,ICRA - Mobile robot obstacle avoidance in a computerized travel aid for the blind,,,conference proceedings article,Proceedings of the 1994 IEEE International Conference on Robotics and Automation,,IEEE Comput. Soc. Press,,Shraga Shoval; Johann Borenstein; Yoram Koren,"A blind traveler walking through an unfamiliar environment and a mobile robot navigating through a cluttered environment have an important feature in common: both have the kinematic ability to perform the motion, but are depended on a sensory system to detect and avoid obstacles. This paper describes the use of a mobile robot obstacle avoidance system as a guidance device for blind and visually impaired people. Just like electronic signals are sent to a mobile robot's motor controllers, auditory signals can guide the blind traveler around obstacles, or alternatively, they can provide an ""acoustic image"" of the surroundings. The concept has been implemented and tested in a new traveling aid for the blind, called the Navbelt. Experimental results of subjects traveling with the Navbelt in different surroundings are presented. >",,,2023,2028,Engineering; Artificial intelligence; Mobile robot; Mobile robot navigation; Detect and avoid; Computer vision; Kinematics; Motor controller; Robot control; Obstacle avoidance; Motion planning,,,,,https://dblp.uni-trier.de/db/conf/icra/icra1994-3.html#ShovalBK94 http://www-personal.umich.edu/~johannb/Papers/paper52.pdf http://ykoren.engin.umich.edu/wp-content/uploads/sites/122/2014/05/36-Mobile-Robot-Obstacle-Avoidance-in-a-Computerized-Travel-Aid-for-the-Blind1.pdf https://ieeexplore.ieee.org/abstract/document/351166/ https://www.cs.unc.edu/~welch/class/mobility/papers/NavBelt.pdf,http://dx.doi.org/10.1109/robot.1994.351166,,10.1109/robot.1994.351166,2161001478,,1,012-527-691-987-058; 044-693-314-510-582; 045-321-254-881-478; 073-722-974-726-137; 191-001-801-688-935,100,false,,
098-840-339-092-862,Autonomous Floor and Staircase Cleaning Framework by Reconfigurable sTetro Robot with Perception Sensors,2020-12-14,2020,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Anh Vu Le; Phone Thiha Kyaw; Rajesh Elara Mohan; Sai Htet Moe Swe; Ashiwin Rajendran; Kamalesh Boopathi; Nguyen Huu Khanh Nhan,,101,1,17,,Trajectory; Artificial intelligence; Boundary (real estate); Climb; Point (geometry); Computer vision; Computer science; Mode (computer interface); Plan (drawing); Convolutional neural network; Robot,,,,"the National Robotics Programme under its Robotics Enabling Capabilities and Technologies with Project Number RGAST1907 and administered by the Agency for Science, Technology and Research",https://jglobal.jst.go.jp/en/detail?JGLOBAL_ID=202102287094562787 https://dblp.uni-trier.de/db/journals/jirs/jirs101.html#LeKMSRBH21 https://link.springer.com/article/10.1007/s10846-020-01281-2,http://dx.doi.org/10.1007/s10846-020-01281-2,,10.1007/s10846-020-01281-2,3111208580,,0,005-207-159-804-271; 005-359-644-637-131; 006-130-750-959-100; 015-417-609-922-215; 018-280-957-941-88X; 019-831-866-121-506; 020-233-013-143-936; 022-839-587-252-946; 024-061-846-302-361; 024-951-049-603-086; 026-605-813-589-98X; 028-988-874-926-875; 029-231-540-151-434; 030-559-494-323-921; 031-218-334-653-826; 032-307-234-807-606; 036-946-001-965-191; 037-642-631-771-74X; 038-183-992-403-279; 042-251-157-585-319; 044-939-519-224-333; 046-368-088-129-735; 049-317-239-158-314; 053-269-050-693-832; 056-296-354-867-101; 059-149-073-001-124; 063-367-028-428-831; 063-428-227-332-011; 063-625-320-581-998; 066-133-135-091-645; 073-186-181-939-291; 075-622-082-197-668; 082-834-961-401-256; 096-613-752-810-722; 104-208-613-793-423; 104-481-834-363-037; 107-166-729-909-551; 116-030-405-883-678; 118-908-073-419-232; 121-606-915-606-769; 122-927-488-988-996; 123-396-297-619-190; 125-252-124-288-717; 127-800-491-023-697; 132-602-463-268-72X; 141-077-134-230-230; 145-846-060-526-90X; 146-093-154-688-224; 166-719-051-375-011; 166-762-627-551-406; 180-559-374-049-445; 195-091-532-336-938,22,false,,
098-888-570-180-630,Indoor-Guided Navigation for People Who Are Blind: Crowdsourcing for Route Mapping and Assistance,2022-01-05,2022,journal article,Applied Sciences,20763417,MDPI AG,,Darius Plikynas; Audrius Indriulionis; Algirdas Laukaitis; Leonidas Sakalauskas,"<jats:p>This paper presents an approach to enhance electronic traveling aids (ETAs) for people who are blind and severely visually impaired (BSVI) using indoor orientation and guided navigation by employing social outsourcing of indoor route mapping and assistance processes. This type of approach is necessary because GPS does not work well, and infrastructural investments are absent or too costly to install for indoor navigation. Our approach proposes the prior outsourcing of vision-based recordings of indoor routes from an online network of seeing volunteers, who gather and constantly update a web cloud database of indoor routes using specialized sensory equipment and web services. Computational intelligence-based algorithms process sensory data and prepare them for BSVI usage. In this way, people who are BSVI can obtain ready-to-use access to the indoor routes database. This type of service has not previously been offered in such a setting. Specialized wearable sensory ETA equipment, depth cameras, smartphones, computer vision algorithms, tactile and audio interfaces, and computational intelligence algorithms are employed for that matter. The integration of semantic data of points of interest (such as stairs, doors, WC, entrances/exits) and evacuation schemes could make the proposed approach even more attractive to BVSI users. Presented approach crowdsources volunteers’ real-time online help for complex navigational situations using a mobile app, a live video stream from BSVI wearable cameras, and digitalized maps of buildings’ evacuation schemes.</jats:p>",12,1,523,523,Computer science; Global Positioning System; Crowdsourcing; Doors; Wearable computer; Process (computing); Human–computer interaction; Multimedia; Real-time computing; World Wide Web; Embedded system; Telecommunications; Operating system,,,,European Regional Development Fund under grant agreement with the Research Council of Lithuania,https://www.mdpi.com/2076-3417/12/1/523/pdf?version=1641388792 https://doi.org/10.3390/app12010523,http://dx.doi.org/10.3390/app12010523,,10.3390/app12010523,,,0,008-264-080-460-955; 009-171-960-350-539; 016-415-106-921-058; 017-108-666-837-168; 019-719-320-041-002; 023-735-000-187-111; 038-679-046-196-030; 046-097-009-151-183; 057-192-756-123-973; 064-043-611-038-916; 070-690-290-610-376; 075-445-221-814-298; 080-054-356-948-958; 089-822-552-618-426; 094-047-556-512-350; 097-395-382-637-910; 102-089-014-752-716; 102-952-660-914-374; 109-574-591-857-730; 114-106-444-683-447; 128-772-800-889-015; 159-355-459-283-016; 163-650-743-266-789,7,true,cc-by,gold
098-910-337-444-079,"Navigation Systems for the Blind and Visually Impaired: Past Work, Challenges, and Open Problems.",2019-08-02,2019,journal article,"Sensors (Basel, Switzerland)",14248220; 14243210,Multidisciplinary Digital Publishing Institute (MDPI),Switzerland,Santiago Real; Alvaro Araujo,"Over the last decades, the development of navigation devices capable of guiding the blind through indoor and/or outdoor scenarios has remained a challenge. In this context, this paper’s objective is to provide an updated, holistic view of this research, in order to enable developers to exploit the different aspects of its multidisciplinary nature. To that end, previous solutions will be briefly described and analyzed from a historical perspective, from the first “Electronic Travel Aids” and early research on sensory substitution or indoor/outdoor positioning, to recent systems based on artificial vision. Thereafter, user-centered design fundamentals are addressed, including the main points of criticism of previous approaches. Finally, several technological achievements are highlighted as they could underpin future feasible designs. In line with this, smartphones and wearables with built-in cameras will then be indicated as potentially feasible options with which to support state-of-art computer vision solutions, thus allowing for both the positioning and monitoring of the user’s surrounding area. These functionalities could then be further boosted by means of remote resources, leading to cloud computing schemas or even remote sensing via urban infrastructure.",19,15,3404,,Human–computer interaction; Situation awareness; Wearable computer; Perspective (graphical); Context (language use); Computer science; Cloud computing,assisting systems; navigation systems; perception; situation awareness; visually impaired,Blindness/pathology; Geographic Information Systems; Humans; Radio Frequency Identification Device; Sensory Aids; Smartphone; User-Computer Interface; Vision Disorders/pathology; Wearable Electronic Devices,,,https://www.mdpi.com/1424-8220/19/15/3404 https://dblp.uni-trier.de/db/journals/sensors/sensors19.html#RealA19 https://www.mdpi.com/1424-8220/19/15/3404/pdf http://dblp.uni-trier.de/db/journals/sensors/sensors19.html#RealA19 https://pubmed.ncbi.nlm.nih.gov/31382536/ https://doi.org/10.3390/s19153404 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6696419/ https://europepmc.org/article/MED/31382536,http://dx.doi.org/10.3390/s19153404,31382536,10.3390/s19153404,2964842853,PMC6696419,0,003-473-950-897-114; 004-198-036-488-477; 004-216-069-855-831; 006-220-054-487-92X; 007-223-103-783-187; 008-686-549-442-050; 009-142-401-900-87X; 012-081-521-655-479; 013-589-881-594-538; 013-847-555-426-159; 014-387-823-436-92X; 016-074-555-557-834; 016-209-364-096-807; 017-312-938-936-230; 018-037-207-286-716; 022-377-931-639-450; 025-306-351-240-479; 026-048-436-398-74X; 026-835-137-329-413; 027-033-143-017-928; 027-060-080-984-50X; 028-527-614-710-566; 028-683-545-086-724; 029-669-699-174-861; 030-669-750-467-762; 034-350-308-512-672; 036-194-252-598-526; 037-491-194-003-965; 040-583-672-414-491; 040-940-319-245-132; 041-468-306-822-770; 043-794-164-324-472; 044-981-118-560-300; 046-991-476-786-467; 048-298-689-305-353; 053-968-972-722-364; 055-228-148-749-592; 055-660-300-282-556; 057-847-555-860-541; 057-854-913-554-209; 057-890-353-263-937; 060-578-904-142-843; 062-501-586-349-490; 062-595-017-409-561; 063-573-679-377-529; 064-309-126-323-022; 064-521-070-547-235; 065-992-777-809-97X; 066-526-177-885-125; 068-526-647-480-345; 070-057-253-482-738; 071-473-658-566-47X; 075-519-293-886-898; 079-433-787-480-087; 092-207-666-926-877; 093-548-249-489-049; 098-811-589-762-176; 099-072-913-456-358; 100-404-039-254-355; 100-661-796-902-521; 101-811-333-176-655; 107-983-224-543-339; 110-057-749-336-776; 110-650-958-929-874; 124-597-558-957-962; 127-070-672-662-621; 130-901-433-069-196; 135-646-014-794-306; 139-812-238-822-149; 141-489-442-115-69X; 144-452-403-048-99X; 152-285-151-328-833; 162-758-589-839-11X,123,true,cc-by,gold
099-126-918-799-895,Towards Richer Assisted Living Environments.,2021-12-08,2021,journal article,SN computer science,26618907; 2662995x,Springer Science and Business Media LLC,Singapore,Paulo A Condado; Fernando G Lobo; Tiago Carita,"This paper describes an ongoing research project which explores the design and use of inexpensive robotics, artificial intelligence techniques, and human-computer interaction methods, to enrich assisted living environments. Such environments provide help to the inhabitants of a home or office, assisting them to perform daily activities, helping them to socialize and interact with others, and to provide enhanced levels of security and safeness. We present the development of an inexpensive robotic solution to help people with disabilities and/or older adults to perform their daily activities. It can be used as a remote controlled surveillance system, and also as a personal assistant. It is able to recognize each inhabitant, his/her emotions, and detect abnormal situations such as falls and health problems. The whole system is designed to operate solely within a local network and special attention is given to the privacy and data protection of the users.",3,1,96,,Assisted living; Computer science; Robotics; Human–computer interaction; Activities of daily living; Independent living; Robot; Artificial intelligence; Computer security; Internet privacy; Psychology; Gerontology; Medicine; Psychiatry,Artificial intelligence; Assistive technologies; Domotics; Human–machine interaction; Low-cost robotics; Telepresence,,,Fundação para a Ciência e a Tecnologia; Fundação para a Ciência e a Tecnologia,https://link.springer.com/content/pdf/10.1007/s42979-021-00983-0.pdf https://doi.org/10.1007/s42979-021-00983-0,http://dx.doi.org/10.1007/s42979-021-00983-0,34901879,10.1007/s42979-021-00983-0,,PMC8653622,0,000-287-610-634-408; 001-429-449-366-925; 009-074-645-156-002; 012-781-258-686-602; 013-758-455-415-757; 013-994-236-320-699; 014-894-338-066-781; 018-019-939-974-064; 020-788-432-301-774; 021-771-046-587-546; 022-780-478-174-594; 023-486-721-286-211; 026-433-273-014-377; 031-955-093-251-323; 032-706-370-484-985; 033-736-534-707-764; 035-399-326-736-745; 036-254-706-670-927; 036-670-457-419-776; 050-107-109-061-223; 054-165-861-350-77X; 066-093-988-572-199; 069-473-677-894-493; 070-371-162-782-327; 071-730-070-885-968; 078-090-496-359-318; 079-300-437-731-609; 084-468-139-209-388; 087-092-119-732-880; 090-342-597-295-977; 092-058-590-871-171; 117-593-926-050-159; 149-321-171-221-491; 152-127-960-081-400; 159-747-197-333-660; 167-768-335-721-753; 172-650-412-381-13X,1,true,,green
099-658-271-768-218,Real-time model based geometric reasoning for vision-guided navigation,,1989,journal article,Machine Vision and Applications,09328092; 14321769,Springer Science and Business Media LLC,Germany,Uma Kant Sharma; Darwin Kuan,,2,1,31,44,Image segmentation; Raw data; Pattern recognition (psychology); Artificial intelligence; Boundary (real estate); Autonomous robot; Computer vision; Model-based reasoning; Computer science; Machine vision; Segmentation,,,,,https://link.springer.com/article/10.1007/BF01214395 https://trid.trb.org/view/492874 https://doi.org/10.1007/BF01214395 https://link.springer.com/content/pdf/10.1007/BF01214395.pdf,http://dx.doi.org/10.1007/bf01214395,,10.1007/bf01214395,2068052258,,0,012-887-274-543-199; 015-535-257-634-38X; 016-963-260-245-390; 041-544-828-058-556; 066-774-098-019-109; 067-930-434-946-920; 087-887-890-493-528; 088-064-342-134-03X; 111-255-381-201-69X; 132-874-640-853-997; 162-296-801-341-869,7,false,,
100-273-845-464-966,Review of substitutive assistive tools and technologies for people with visual impairments: recent advancements and prospects,2023-12-19,2023,journal article,Journal on Multimodal User Interfaces,17837677; 17838738,Springer Science and Business Media LLC,Germany,Zahra J. Muhsin; Rami Qahwaji; Faruque Ghanchi; Majid Al-Taee,"<jats:title>Abstract</jats:title><jats:p>The development of many tools and technologies for people with visual impairment has become a major priority in the field of assistive technology research. However, many of these technology advancements have limitations in terms of the human aspects of the user experience (e.g., usability, learnability, and time to user adaptation) as well as difficulties in translating research prototypes into production. Also, there was no clear distinction between the assistive aids of adults and children, as well as between “partial impairment” and “total blindness”. As a result of these limitations, the produced aids have not gained much popularity and the intended users are still hesitant to utilise them. This paper presents a comprehensive review of substitutive interventions that aid in adapting to vision loss, centred on laboratory research studies to assess user-system interaction and system validation. Depending on the primary cueing feedback signal offered to the user, these technology aids are categorized as visual, haptics, or auditory-based aids. The context of use, cueing feedback signals, and participation of visually impaired people in the evaluation are all considered while discussing these aids. Based on the findings, a set of recommendations is suggested to assist the scientific community in addressing persisting challenges and restrictions faced by both the totally blind and partially sighted people.</jats:p>",18,1,135,156,Computer science; Usability; Learnability; Popularity; Visual impairment; Assistive technology; Context (archaeology); Human–computer interaction; Adaptation (eye); Set (abstract data type); Blindness; Psychology; Medicine; Optometry; Social psychology; Paleontology; Neuroscience; Psychiatry; Biology; Programming language,,,,,https://link.springer.com/content/pdf/10.1007/s12193-023-00427-4.pdf https://doi.org/10.1007/s12193-023-00427-4,http://dx.doi.org/10.1007/s12193-023-00427-4,,10.1007/s12193-023-00427-4,,,0,002-056-722-378-626; 003-799-838-753-102; 004-216-069-855-831; 004-772-554-982-380; 005-296-969-070-422; 006-487-017-505-503; 007-223-103-783-187; 008-139-948-663-237; 008-361-673-204-978; 008-391-811-713-349; 008-558-848-807-295; 008-855-573-563-607; 009-123-423-709-941; 009-690-618-758-576; 009-923-289-249-387; 010-475-144-749-494; 011-459-978-528-790; 012-370-328-340-986; 012-891-794-926-558; 013-016-358-471-463; 013-322-105-152-072; 013-489-530-355-326; 013-829-437-174-959; 014-153-330-736-922; 015-141-950-069-532; 015-940-288-390-612; 018-031-085-269-362; 018-262-121-105-137; 019-162-440-728-196; 019-860-339-039-546; 020-581-037-678-426; 020-702-500-823-487; 020-935-356-605-174; 021-176-270-241-156; 021-310-385-124-034; 021-567-179-454-087; 021-657-554-837-75X; 022-557-628-555-690; 022-858-944-348-940; 022-940-138-590-719; 024-197-560-708-270; 024-674-099-372-674; 025-088-765-559-487; 025-276-160-799-62X; 026-000-229-605-790; 026-844-174-253-723; 028-165-225-355-198; 030-328-551-719-95X; 032-538-345-541-781; 032-607-209-452-107; 033-288-442-402-545; 033-310-282-700-479; 034-936-259-439-98X; 035-228-339-650-24X; 035-483-680-381-053; 036-320-983-932-780; 037-109-788-288-910; 037-297-171-524-582; 037-500-971-639-135; 038-037-768-189-508; 038-342-051-187-32X; 038-546-717-874-316; 039-001-223-547-151; 039-945-629-084-829; 040-011-425-844-511; 040-533-527-422-721; 043-364-403-755-49X; 047-499-628-736-170; 049-317-239-158-314; 049-324-523-155-079; 049-592-217-207-783; 052-008-106-369-555; 052-731-143-530-822; 053-567-501-093-090; 053-726-248-960-116; 055-495-282-581-758; 057-847-555-860-541; 057-855-069-190-300; 059-565-100-298-852; 060-025-465-346-149; 060-135-893-855-211; 061-730-853-418-888; 063-783-140-107-766; 064-140-744-806-266; 064-657-343-403-492; 065-078-747-021-140; 066-246-145-459-483; 066-748-640-288-896; 068-526-647-480-345; 068-681-027-105-640; 069-231-232-077-747; 070-606-003-984-347; 070-637-786-069-947; 070-777-575-483-27X; 071-281-258-323-085; 074-484-764-304-687; 075-825-766-649-469; 076-059-869-327-354; 077-816-271-850-898; 080-469-614-247-455; 081-728-252-369-796; 083-274-355-997-655; 083-671-887-012-721; 084-474-987-383-515; 084-831-611-614-437; 085-019-924-832-610; 088-755-092-328-87X; 088-822-320-203-25X; 089-727-416-707-488; 090-039-427-926-498; 092-726-040-947-902; 094-838-611-687-737; 096-095-070-620-14X; 096-363-956-353-392; 099-027-542-101-490; 099-334-901-486-872; 100-950-617-368-126; 101-310-804-847-326; 101-874-510-448-200; 103-415-396-808-910; 103-948-121-717-966; 105-278-566-106-933; 105-986-387-709-891; 107-033-964-168-870; 107-659-830-473-823; 111-620-943-951-912; 115-531-661-954-69X; 116-712-429-904-769; 121-240-214-873-975; 122-704-318-024-950; 124-695-108-834-438; 132-429-296-876-034; 136-084-494-835-342; 137-648-461-521-23X; 141-649-000-612-322; 142-554-745-154-754; 143-890-694-965-002; 145-103-385-375-073; 145-921-567-715-571; 157-071-774-392-979; 166-405-745-827-631; 170-167-018-558-387; 172-091-766-059-430; 178-809-894-909-274; 184-792-782-243-438; 186-810-372-950-411; 188-855-394-666-739; 189-439-205-453-579; 189-488-396-765-364; 190-593-710-020-775; 190-677-270-973-553; 196-295-387-293-716,4,true,cc-by,hybrid
100-348-545-876-970,Computer Assisted Radiology and Surgery,2010-05-26,2010,journal article,International Journal of Computer Assisted Radiology and Surgery,18616410; 18616429,Springer Science and Business Media LLC,Germany,Javier Sanchez-Castro; Claudio Pollo; Jean‐Philippe Thiran,,5,S1,404,426,Health informatics; Medicine; Medical physics; Computer science; Radiology; General surgery; Pathology; Public health,,,,,https://hal.science/hal-00509426/document https://hal.archives-ouvertes.fr/hal-00509426 https://hal.science/hal-00509426/file/SanchezCastro_CARS2010_S424.pdf https://hal.archives-ouvertes.fr/hal-00509426/document https://hal.archives-ouvertes.fr/hal-00509426/file/SanchezCastro_CARS2010_S424.pdf,http://dx.doi.org/10.1007/s11548-010-0472-1,,10.1007/s11548-010-0472-1,,,0,,0,true,,green
100-386-730-663-560,Machine-Learning-Based Accessibility System,2024-02-28,2024,journal article,SN Computer Science,26618907; 2662995x,Springer Science and Business Media LLC,,Kakoli Banerjee; Amarjeet Singh; Naved Akhtar; Indira Vats,,5,3,,,Computer science; Artificial intelligence; Human–computer interaction,,,,,,http://dx.doi.org/10.1007/s42979-024-02615-9,,10.1007/s42979-024-02615-9,,,0,001-549-228-353-510; 002-342-742-235-582; 002-916-588-132-357; 003-217-812-157-879; 004-269-574-716-057; 005-295-549-300-75X; 005-367-150-142-593; 008-859-163-960-907; 009-940-906-151-786; 011-247-970-780-217; 012-456-583-984-92X; 013-155-514-013-035; 013-678-271-109-499; 014-704-237-967-193; 015-318-511-735-135; 017-783-416-617-559; 019-215-610-025-597; 019-924-218-904-281; 020-759-387-332-017; 026-676-413-924-396; 026-733-932-019-02X; 028-637-071-339-71X; 029-610-834-019-455; 031-441-750-908-578; 033-591-895-385-938; 034-014-499-765-927; 035-496-113-866-428; 038-015-938-697-023; 041-190-312-414-69X; 054-178-012-630-825; 054-707-690-622-446; 054-819-045-052-125; 055-420-391-242-185; 059-176-684-868-070; 059-574-981-575-526; 061-498-680-791-452; 062-748-264-120-815; 066-542-638-897-20X; 067-575-282-924-285; 068-650-524-982-562; 068-675-812-546-766; 070-581-383-082-848; 071-491-960-177-044; 073-814-514-329-091; 073-941-906-316-072; 074-346-457-235-698; 076-016-719-826-975; 081-891-722-294-865; 085-812-384-193-902; 085-852-553-481-582; 086-069-304-736-152; 088-446-408-991-477; 089-249-926-805-148; 095-685-197-467-828; 096-348-680-804-635; 097-647-450-075-322; 098-554-808-275-884; 099-668-458-886-010; 101-637-582-441-287; 101-912-113-941-952; 103-170-105-553-522; 104-991-649-955-858; 110-187-238-275-962; 113-317-691-021-444; 119-832-922-608-969; 120-143-920-256-201; 121-707-574-802-675; 122-134-073-012-983; 123-707-217-226-449; 129-760-388-262-808; 133-675-309-388-719; 135-809-647-940-581; 145-000-909-423-805; 145-440-258-238-911; 147-702-454-676-839; 148-878-479-967-38X; 149-094-033-305-649; 149-750-526-461-351; 150-727-783-562-962; 179-794-699-193-650; 180-559-374-049-445,0,false,,
100-854-492-693-426,Neuro-cognitively inspired haptic user interfaces,2007-09-06,2007,journal article,Multimedia Tools and Applications,13807501; 15737721,Springer Science and Business Media LLC,Netherlands,Kanav Kahol; Sethuraman Panchanathan,,37,1,15,38,Human–computer interaction; Haptic technology; Usability; Perception; Haptic perception; Realization (linguistics); Human machine interaction; Information processor; Computer science; Multimedia,,,,,https://asu.pure.elsevier.com/en/publications/neuro-cognitively-inspired-haptic-user-interfaces-3 https://doi.org/10.1007/s11042-007-0167-y https://dblp.uni-trier.de/db/journals/mta/mta37.html#KaholP08 https://link.springer.com/content/pdf/10.1007%2Fs11042-007-0167-y.pdf https://link.springer.com/article/10.1007/s11042-007-0167-y https://rd.springer.com/article/10.1007/s11042-007-0167-y,http://dx.doi.org/10.1007/s11042-007-0167-y,,10.1007/s11042-007-0167-y,1985862230,,0,003-468-683-475-729; 005-759-369-930-618; 006-635-585-195-621; 007-553-541-862-987; 012-632-001-956-399; 024-855-215-002-295; 027-511-250-284-021; 034-174-463-230-384; 034-256-801-829-437; 039-120-363-178-943; 042-390-834-488-947; 043-380-401-330-631; 043-919-569-664-578; 048-934-941-786-174; 050-036-996-905-385; 058-531-954-531-310; 077-885-409-638-718; 078-699-200-092-432; 079-365-766-905-742; 080-100-658-973-31X; 085-405-799-670-741; 093-134-773-386-898; 098-774-171-718-637; 101-191-198-207-265; 101-394-405-570-420; 108-998-817-785-964; 115-940-056-507-071; 116-573-270-480-425; 119-038-142-418-672; 120-776-079-690-373; 129-622-141-884-722; 154-751-089-924-826; 180-486-143-295-222; 189-964-429-526-197,11,false,,
100-950-617-368-126,A Multimodal Assistive System for Helping Visually Impaired in Social Interactions,2017-10-24,2017,journal article,Informatik-Spektrum,01706012; 1432122x,Springer Science and Business Media LLC,Germany,M. Saquib Sarfraz; Angela Constantinescu; Melanie Zuzej; Rainer Stiefelhagen,"Access to non-verbal cues in social interactions is vital for people with visual impairment. It has been shown that non-verbal cues such as eye contact, number of people, their names and positions are helpful for individuals who are blind. In this paper, we present a real time multi-modal system that provides such non-verbal cues via audio and haptic interfaces. We assess the usefulness of the proposed system both quantitatively and qualitatively by gathering feedback with a focus group of visually impaired participants in a typical social interaction setting. The paper provides important insight about developing such technology for this significant part of society.",40,6,540,545,Human–computer interaction; Social relation; Haptic technology; Psychology; Eye contact; Visual impairment; Visually impaired; Significant part; Focus group; Usability; Multimedia,,,,,https://ui.adsabs.harvard.edu/abs/2017arXiv171110886S/abstract https://arxiv.org/abs/1711.10886 https://doi.org/10.1007/s00287-017-1077-7 https://publikationen.bibliothek.kit.edu/1000123983 https://dblp.uni-trier.de/db/journals/corr/corr1711.html#abs-1711-10886 https://link.springer.com/article/10.1007/s00287-017-1077-7 http://arxiv.org/abs/1711.10886,http://dx.doi.org/10.1007/s00287-017-1077-7,,10.1007/s00287-017-1077-7,2766288920; 3098809338,,1,003-506-530-057-475; 005-252-672-861-886; 020-385-212-908-164; 021-736-241-758-765; 025-161-798-409-993; 028-168-304-880-189; 029-731-550-451-757; 030-078-850-891-734; 031-294-750-698-550; 034-468-880-826-73X; 035-092-147-627-969; 037-039-198-474-114; 053-051-899-847-069; 054-925-004-108-240; 064-846-316-680-673; 072-710-510-756-106; 075-473-547-844-495; 085-102-500-348-494; 085-972-264-981-114; 086-432-007-245-879; 094-918-532-658-786; 105-790-942-783-392; 122-704-318-024-950; 130-321-222-024-616; 141-390-415-567-80X; 143-875-130-007-110; 168-036-345-097-619,20,true,,green
101-194-482-663-181,A Robust Analog VLSI Motion Sensor Based on the Visual System of the Fly,,1999,journal article,Autonomous Robots,09295593; 15737527,Springer Science and Business Media LLC,Netherlands,Reid R. Harrison; Christof Koch,"Sensing visual motion gives a creature valuable information about its interactions with the environment. Flies in particular use visual motion information to navigate through turbulent air, avoid obstacles, and land safely. Mobile robots are ideal candidates for using this sensory modality to enhance their performance, but so far have been limited by the computational expense of processing video. Also, the complex structure of natural visual scenes poses an algorithmic challenge for extracting useful information in a robust manner. We address both issues by creating a small, low-power visual sensor with integrated analog parallel processing to extract motion in real-time. Because our architecture is based on biological motion detectors, we gain the advantages of this highly evolved system: A design that robustly and continuously extracts relevant information from its visual environment. We show that this sensor is suitable for use in the real world, and demonstrate its ability to compensate for an imperfect motor system in the control of an autonomous robot. The sensor attenuates open-loop rotation by a factor of 31 with less than 1 mW power dissipation.",7,3,211,224,Motion (physics); Very-large-scale integration; Visual sensor network; Artificial intelligence; Mobile robot; Biological motion; Parallel processing (DSP implementation); Motion detector; Autonomous robot; Computer vision; Computer science,,,,,http://doi.org/10.1023/A:1008916202887 https://link.springer.com/article/10.1023/A%3A1008916202887 https://dblp.uni-trier.de/db/journals/arobots/arobots7.html#HarrisonK99 http://resolver.caltech.edu/CaltechAUTHORS:20130816-103144911 http://dx.doi.org/10.1023/A:1008916202887 https://dialnet.unirioja.es/servlet/articulo?codigo=478575 https://dx.doi.org/10.1023/A:1008916202887 http://authors.library.caltech.edu/40385/,http://dx.doi.org/10.1023/a:1008916202887,,10.1023/a:1008916202887,1549720765,,0,003-418-524-699-48X; 007-919-813-542-615; 009-588-766-603-719; 012-181-629-515-601; 015-226-492-949-868; 017-168-443-964-471; 021-808-546-687-89X; 024-859-610-944-440; 026-492-475-056-495; 027-710-143-314-872; 027-743-933-568-43X; 028-931-659-346-715; 033-972-633-551-421; 034-313-199-113-433; 038-644-445-197-604; 039-342-551-086-505; 043-876-909-991-691; 045-385-562-884-155; 050-606-686-219-365; 053-752-920-804-699; 054-526-402-379-742; 066-321-239-528-918; 066-403-653-604-698; 070-589-296-364-536; 073-495-407-090-665; 075-691-287-008-882; 078-808-858-217-174; 079-332-185-382-401; 080-411-234-888-072; 086-836-069-298-635; 088-562-843-438-850; 098-240-332-945-413; 099-397-964-883-910; 107-150-341-733-389; 109-357-693-813-332; 113-017-373-590-84X; 118-689-045-509-786; 119-321-490-917-375; 130-731-478-380-819; 133-684-476-017-522; 140-103-063-791-355; 150-285-410-059-541; 157-114-616-350-594; 158-528-949-919-343; 165-309-281-864-605,57,true,,green
101-295-208-484-052,Sensor fusion for visually impaired navigation in constrained spaces,,2016,conference proceedings article,2016 IEEE International Conference on Information and Automation for Sustainability (ICIAfS),,IEEE,,Chathurika S. Silva; Prasad Wimalaratne,"This work presents a multi-sensor fusion approach for an electronic navigation aid for the blind and visually impaired persons. This approach proposes to intelligently fuse the surrounding information senses via ultrasonic sensors and vision sensors. The intelligent component of the prototype serves in several facets including object detection and recognition. Extended Kalman filter is used to fuse the data emerging from homogeneous sensors and rule-based fusion is used to fuse the data from heterogeneous sensors. Feedback is provided via tactile and audio feedback. Critical obstacles of blind navigation like staircases are recognized by the Hough line detection in image processing. Rotations, which occur due to the body movements in the camera, correct using the fusion of data obtain by the inertial measurement unit which is connected to the camera. The results of the evaluations proved that the use of fusion of multiple homogeneous sensors improve the detection of a particular obstacle and fusion of vision and ultrasonic sensors improve the object detection identification of the obstacles. The current status of the work and the future developments are presented in this paper.",,,1,6,Engineering; Fuse (electrical); Artificial intelligence; Extended Kalman filter; Object detection; Electronic navigation; Computer vision; Sensor fusion; Inertial measurement unit; Ultrasonic sensor; Image processing,,,,,http://xplorestaging.ieee.org/ielx7/7939934/7946517/07946537.pdf?arnumber=7946537 http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7946537 http://ieeexplore.ieee.org/iel7/7939934/7946517/07946537.pdf,http://dx.doi.org/10.1109/iciafs.2016.7946537,,10.1109/iciafs.2016.7946537,2625164180,,0,029-360-136-216-567; 039-274-384-165-289; 044-949-263-620-170; 064-043-611-038-916; 071-472-632-444-357; 104-284-924-766-507; 123-935-744-308-31X; 130-097-955-008-505; 134-059-757-787-462,11,false,,
101-564-187-680-996,Psychoacoustic auditory display for navigation: an auditory assistance system for spatial orientation tasks,2018-11-24,2018,journal article,Journal on Multimodal User Interfaces,17837677; 17838738,Springer Science and Business Media LLC,Germany,Tim Ziemer; Holger Schultheis,,13,3,205,218,Artificial intelligence; Auditory display; Perception; Visual navigation; Invasive surgery; Computer vision; Computer science; Orientation (computer vision); Sonification; Psychoacoustics,,,,,https://link.springer.com/article/10.1007/s12193-018-0282-2 https://dblp.uni-trier.de/db/journals/jmui/jmui13.html#ZiemerS19 https://jglobal.jst.go.jp/en/detail?JGLOBAL_ID=201902220766732753 http://doi.org/10.1007/s12193-018-0282-2,http://dx.doi.org/10.1007/s12193-018-0282-2,,10.1007/s12193-018-0282-2,2901715600,,0,002-620-657-668-321; 004-698-059-305-240; 008-181-070-001-153; 011-576-299-911-117; 011-797-116-120-363; 014-012-922-540-000; 014-405-075-934-101; 016-104-903-803-654; 018-537-462-541-13X; 018-829-856-144-799; 020-270-691-966-758; 028-769-920-874-392; 035-665-520-639-833; 040-177-465-563-474; 043-616-807-560-697; 043-855-239-823-855; 047-930-087-747-314; 049-327-550-256-725; 051-713-845-046-300; 056-154-800-525-010; 057-158-749-321-34X; 059-621-672-265-849; 060-352-507-753-497; 065-247-364-027-427; 066-465-922-468-975; 067-016-604-275-855; 072-536-882-725-422; 076-011-531-861-522; 077-514-404-102-794; 088-486-787-687-85X; 096-198-966-272-805; 096-943-846-117-656; 097-510-231-073-848; 100-402-920-229-148; 112-578-968-866-909; 113-903-429-888-781; 122-104-622-847-936; 125-276-397-918-67X; 127-800-533-313-001; 131-311-853-960-344; 153-569-583-997-169,16,false,,
101-746-519-771-905,Image-Guided Craniofacial Surgery,2009-05-09,2009,journal article,International Journal of Computer Assisted Radiology and Surgery,18616410; 18616429,Springer Science and Business Media LLC,Germany,,,4,S1,213,221,Craniofacial; Image-guided surgery; Health informatics; Computer science; Medicine; General surgery; Artificial intelligence; Computer vision; Medical physics; Pathology; Public health; Psychiatry,,,,,,http://dx.doi.org/10.1007/s11548-009-0334-x,,10.1007/s11548-009-0334-x,,,0,,0,false,,
101-767-617-199-036,ICME Workshops - A primary travelling assistant system of bus detection and recognition for visually impaired people,,2013,conference proceedings article,2013 IEEE International Conference on Multimedia and Expo Workshops (ICMEW),,IEEE,,Hangrong Pan; Chucai Yi; Yingli Tian,"It is a challenging task for many blind and visually impaired passengers travelling by bus. To assist visually impaired passengers to travel more independently, we design a computer vision-based system to detect and recognize bus information from images captured by a camera at a bus stop. Our system is able to notify the visually impaired people in speech the information of the coming bus, and detect the route number and other related information which is depicted in the form of text. In bus detection, histogram of the oriented gradient (HOG) descriptor is employed to extract the image-based features of bus facade. Cascade SVM model is applied to train a bus classifier to detect the existence of bus facade in sliding windows. In bus route number recognition, we design a text detection algorithm on the basis of layout analysis and text feature learning, and then recognize the text codes from detected text regions for audio notification. This algorithm is able to compute the image regions containing text information. Experimental results demonstrate the effectiveness of our proposed algorithm for bus detection and route number recognition.",,,1,6,Artificial intelligence; Computer vision; Computer science; Classifier (UML),,,,,https://dblp.uni-trier.de/db/conf/icmcs/icmew2013.html#PanYT13 https://doi.org/10.1109/ICMEW.2013.6618346 https://ieeexplore.ieee.org/document/6618346/ http://ieeexplore.ieee.org/document/6618346/,http://dx.doi.org/10.1109/icmew.2013.6618346,,10.1109/icmew.2013.6618346,2062855687,,0,023-438-345-162-875; 023-928-444-110-407; 043-022-952-629-736; 044-659-512-375-773; 059-944-629-990-267; 071-454-753-954-688; 089-657-002-660-076; 105-969-976-015-690; 146-084-945-839-717,22,false,,
102-100-961-622-631,Smart Navigation Application for Visually Challenged People in Indoor Premises,2019-08-01,2019,book chapter,Lecture Notes on Data Engineering and Communications Technologies,23674512; 23674520,Springer International Publishing,,Ganesh Kotalwar; Jigisha Prajapati; Sharayu Patil; Dilip Moralwar; Kajal Jewani,"It is very difficult for a visually impaired person to perform its day to day job with ease. Since Mobile applications are largely used among people they have high potential in aiding blind people. In this paper, we are trying to present an application to assist visually disabled. The android application will be using Deep learning object detection and identification techniques such as YOLO, R-CNN etc. The need for navigation help among blind people and a broader look at the advanced technology becoming available in today’s world motivated us to develop this project. Technology is something which is there to ease tasks for human beings. Hence, in this project, we use technology to solve the problems of visually impaired people. The project aims to help users in navigation with the use of technology and our engineering profession motivates us to use the technology we have.",,,998,1004,Human–computer interaction; Deep learning; Artificial intelligence; Object detection; Engineering profession; Day to day; Use of technology; Android application; Visually impaired; Computer science; Identification (information),,,,,https://link.springer.com/chapter/10.1007%2F978-3-030-24643-3_119,http://dx.doi.org/10.1007/978-3-030-24643-3_119,,10.1007/978-3-030-24643-3_119,2964579115,,0,011-126-524-756-136; 027-934-550-270-689; 055-495-282-581-758; 058-268-989-258-793; 059-734-752-087-166; 071-338-626-777-780; 115-360-392-890-988; 131-503-960-053-375,2,false,,
102-403-081-902-959,Deep learning robotic guidance for autonomous vascular access,2020-02-14,2020,journal article,Nature Machine Intelligence,25225839,Springer Science and Business Media LLC,,Alvin I. Chen; Max L. Balter; Tim Maguire; Martin L. Yarmush,,2,2,104,115,Imaging phantom; Deep learning; Artificial intelligence; Vascular access; Task (computing); Difficult Vascular Access; Multimodal image; Computer vision; Computer science; Convolutional neural network; Robot,,,,U.S. Department of Health & Human Services | NIH | National Institute of Biomedical Imaging and Bioengineering; U.S. Department of Health & Human Services | NIH | National Institute of Biomedical Imaging and Bioengineering; U.S. Department of Health & Human Services | NIH | National Institute of Biomedical Imaging and Bioengineering; National Science Foundation; U.S. Department of Health & Human Services | NIH | National Institute of Biomedical Imaging and Bioengineering; U.S. Department of Health & Human Services | NIH | National Institute of Biomedical Imaging and Bioengineering; U.S. Department of Health & Human Services | NIH | National Institute of General Medical Sciences,https://www.nature.com/articles/s42256-020-0148-7 https://www.nature.com/articles/s42256-020-0148-7.pdf,http://dx.doi.org/10.1038/s42256-020-0148-7,,10.1038/s42256-020-0148-7,3006293443,,3,001-712-968-781-841; 001-731-030-961-116; 005-372-455-048-035; 006-251-707-023-738; 007-619-251-889-048; 007-707-213-749-95X; 008-252-226-724-610; 009-313-334-880-373; 009-372-613-532-494; 011-270-966-522-421; 013-108-799-318-518; 013-435-321-816-92X; 014-723-110-809-116; 016-269-848-088-621; 016-690-190-219-97X; 016-779-217-684-312; 017-175-370-344-866; 017-251-097-789-080; 017-399-952-874-51X; 022-904-628-723-379; 027-526-459-601-798; 028-789-858-993-11X; 030-313-615-485-645; 031-580-680-743-711; 032-734-709-752-108; 035-893-674-370-480; 037-447-676-299-182; 038-964-373-634-483; 039-501-300-252-674; 041-222-810-802-568; 041-314-165-476-904; 047-732-099-875-033; 049-883-826-815-039; 051-575-833-486-162; 052-006-540-437-638; 052-288-515-393-694; 052-602-682-348-009; 052-713-148-345-512; 055-950-000-454-358; 058-338-841-664-104; 059-149-073-001-124; 060-689-078-105-355; 066-955-828-741-703; 072-686-466-094-880; 073-060-212-503-957; 077-803-535-314-958; 078-590-585-660-904; 080-171-446-189-37X; 081-162-989-156-322; 084-510-831-812-921; 086-051-532-076-883; 088-509-279-651-534; 089-367-372-062-158; 091-172-020-013-67X; 093-483-334-059-890; 098-079-263-585-320; 098-772-037-960-925; 104-735-249-249-02X; 105-541-555-944-822; 105-924-795-463-006; 107-118-849-489-671; 107-654-529-952-38X; 109-111-937-618-622; 112-154-051-246-370; 121-311-175-015-649; 121-340-418-949-802; 145-108-829-550-497; 151-374-013-752-001; 151-489-947-969-554; 156-590-586-181-275; 167-131-435-704-717; 193-052-734-829-39X,107,false,,
102-547-236-141-107,Navigation Tool for Visually Impaired Hurdle Recognition using Image Processing and IoT,2023-06-22,2023,journal article,Journal of IoT and Machine Learning,,QTanalytics India (Publications),,Mohan N; Ravi Kumar L; Sharath Kumar DA; Sreeja L; Sushmitha S Dhisley,"<jats:p>A device made to help visually challenged people navigate their surroundings using image processing and the Internet of Things. Traditional walking aids are ineffective at detecting impediments, making it challenging for blind people to move around on their own. Obstacles are identified by the smart walking stick's camera and ultrasonic sensor, which also calculate the user's distance from the object. A sound is produced in the user's headphones when an obstruction is identified, alerting them to the obstacle. For visually impaired people who might need assistance navigating their environment, this technology is a useful aid. The stick has a camera that takes pictures of the surrounding area. These pictures are subsequently analyzed using image processing algorithms to find barriers and other important details. The stick is also linked to the Internet of Things (IoT), which enables it to interact with other devices and offer further functionality.</jats:p>",,,12,17,Obstacle; Computer science; Headphones; Internet of Things; Computer vision; Visually impaired; Image processing; Artificial intelligence; Object (grammar); The Internet; Human–computer interaction; Image (mathematics); Embedded system; Engineering; World Wide Web; Electrical engineering; Political science; Law,,,,,https://qtanalytics.in/publications/index.php/JoITML/article/download/58/25 https://doi.org/10.48001/joitml.2023.1112-17,http://dx.doi.org/10.48001/joitml.2023.1112-17,,10.48001/joitml.2023.1112-17,,,0,,0,true,,bronze
102-551-569-903-749,Multimodal sensing and intuitive steering assistance improve navigation and mobility for people with impaired vision.,2021-10-13,2021,journal article,Science robotics,24709476,American Association for the Advancement of Science (AAAS),United States,Patrick Slade; Arjun Tambe; Mykel J. Kochenderfer,"Globally, more than 250 million people have impaired vision and face challenges navigating outside their homes, affecting their independence, mental health, and physical health. Navigating unfamili...",6,59,eabg6594,,Mental health; Psychology; Independence; Face (sociological concept); MEDLINE; Impaired Vision; Physical health; Applied psychology,,Algorithms; Blindness/rehabilitation; Canes; Electronics; Equipment Design; Haptic Technology; Humans; Man-Machine Systems; Movement; Quality of Life; Robotics; Safety; Self-Help Devices; Visually Impaired Persons; Walking,,,https://dblp.uni-trier.de/db/journals/scirobotics/scirobotics6.html#SladeTK21 https://www.science.org/doi/10.1126/scirobotics.abg6594 http://www.science.org/doi/10.1126/scirobotics.abg6594 https://pubmed.ncbi.nlm.nih.gov/34644159/ https://doi.org/10.1126/scirobotics.abg6594,http://dx.doi.org/10.1126/scirobotics.abg6594,34644159,10.1126/scirobotics.abg6594,3207432148,,0,000-319-559-523-254; 003-544-606-128-039; 007-214-673-266-435; 011-797-116-120-363; 011-969-960-701-014; 013-056-435-015-274; 016-935-118-808-254; 018-843-508-574-144; 021-987-891-911-326; 022-939-915-291-88X; 024-335-765-549-544; 024-369-065-483-244; 024-964-397-377-997; 025-716-725-120-01X; 029-313-431-507-058; 029-378-346-519-254; 030-127-369-356-741; 031-223-487-502-873; 032-494-170-766-10X; 032-661-597-680-28X; 032-880-698-479-50X; 033-687-327-893-105; 034-222-058-985-816; 038-147-118-193-190; 041-346-284-111-904; 044-097-632-032-714; 045-599-200-378-144; 052-698-035-156-042; 056-353-641-915-733; 059-080-888-469-016; 059-321-941-695-210; 061-205-550-296-347; 064-521-070-547-235; 066-246-145-459-483; 071-183-084-885-707; 072-170-707-017-281; 073-938-112-385-378; 075-519-293-886-898; 080-650-893-954-801; 093-548-249-489-049; 096-041-542-728-826; 098-910-337-444-079; 102-795-246-835-316; 109-806-014-448-128; 133-451-470-922-209; 164-672-465-821-597; 172-095-244-078-296; 184-116-517-417-325,28,false,,
102-576-931-206-147,Survey and analysis of the current status of research in the field of outdoor navigation for the blind.,2023-07-04,2023,journal article,Disability and rehabilitation. Assistive technology,17483115; 17483107,Informa UK Limited,United Kingdom,Yue Lian; De-Er Liu; Wei-Zhen Ji,"<AbstractText Label=""PURPOSE"" NlmCategory=""UNASSIGNED"">In this article, we comprehensively review the current situation and research on technology related to outdoor travel for blind and visually impaired people (BVIP), given the diverse types and incomplete functionality of navigation aids for the blind. This aims to provide a reference for related research in the fields of outdoor travel for BVIP and blind navigation.</AbstractText>;           <AbstractText Label=""MATERIALS AND METHODS"" NlmCategory=""UNASSIGNED"">We compiled articles related to blind navigation, of which a total of 227 of them are included in the search criteria. One hundred and seventy-nine articles are selected from the initial set, from a technical point of view, to elaborate on five aspects of blind navigation: system equipment, data sources, guidance algorithms, optimization of related methods, and navigation maps.</AbstractText>;           <AbstractText Label=""RESULTS"" NlmCategory=""UNASSIGNED"">The wearable form of assistive devices for the blind has the most research, followed by the handheld type of aids. The RGB data class based on vision sensor is the most common source of navigation environment information data. Object detection based on picture data is also particularly rich among navigation algorithms and associated methods, indicating that computer vision technology has become an important study content in the field of blind navigation. However, research on navigation maps is relatively less.</AbstractText>;           <AbstractText Label=""CONCLUSIONS"" NlmCategory=""UNASSIGNED"">In the study and development of assistive equipment for BVIP, there will be an emphasis on prioritizing attributes, such as lightness, portability, and efficiency. In light of the upcoming driverless era, the research focus will be on the development of visual sensors and computer vision technologies that can aid in navigation for the blind.</AbstractText>",19,4,1657,1675,Computer science; Software portability; Human–computer interaction; Navigation system; Navigational aid; Wearable computer; Visually impaired; Field (mathematics); Computer vision; Augmented reality; Turn-by-turn navigation; Set (abstract data type); Object (grammar); Global Positioning System; Artificial intelligence; Multimedia; Engineering; Telecommunications; Embedded system; Robot control; Mathematics; Robot; Mobile robot; Pure mathematics; Programming language; Aerospace engineering,Blind; assistive devices; blind navigation; computer vision; outdoor,Humans; Self-Help Devices; Visually Impaired Persons/rehabilitation; Spatial Navigation; Blindness/rehabilitation; Algorithms; Wearable Electronic Devices,,,,http://dx.doi.org/10.1080/17483107.2023.2227224,37402242,10.1080/17483107.2023.2227224,,,0,000-092-841-555-619; 000-631-028-807-828; 000-761-776-336-588; 001-236-462-649-523; 001-443-399-861-469; 002-279-515-155-647; 002-315-319-511-451; 002-341-994-164-78X; 002-990-910-547-637; 003-787-428-155-830; 006-675-659-208-672; 007-530-851-453-706; 008-723-145-538-785; 010-305-498-226-998; 011-876-327-472-778; 012-344-135-607-729; 012-370-328-340-986; 013-056-435-015-274; 014-561-705-083-896; 014-763-356-516-379; 014-764-979-190-592; 015-230-770-571-570; 015-632-759-289-699; 016-353-889-913-511; 016-982-081-564-859; 016-988-681-008-984; 017-300-737-144-757; 017-499-516-520-553; 017-952-244-691-65X; 018-262-121-105-137; 018-437-881-959-268; 018-821-519-355-660; 019-166-213-222-249; 019-944-992-128-451; 023-884-758-333-089; 025-764-110-104-300; 026-712-179-033-83X; 027-026-193-850-672; 027-805-115-749-256; 028-497-636-919-112; 029-272-104-723-556; 030-200-876-346-95X; 030-442-794-214-787; 030-500-337-935-178; 030-846-671-006-321; 033-457-095-812-412; 033-940-482-248-436; 035-544-542-108-733; 037-811-687-057-734; 038-326-446-864-583; 038-679-046-196-030; 041-421-358-393-72X; 042-418-744-828-17X; 044-431-461-724-644; 045-541-754-099-882; 045-802-145-074-623; 046-098-711-060-761; 046-395-605-220-97X; 047-558-392-308-360; 049-150-188-083-612; 049-870-006-718-994; 051-056-518-521-154; 052-698-035-156-042; 052-853-944-262-608; 053-070-682-445-505; 053-219-839-728-849; 056-479-341-958-161; 057-647-661-980-575; 057-879-237-577-755; 058-268-989-258-793; 061-205-550-296-347; 062-128-353-400-109; 062-310-067-368-268; 063-848-176-030-726; 064-100-103-232-577; 064-964-065-389-714; 065-078-747-021-140; 066-034-963-566-331; 066-479-281-229-604; 069-843-577-264-465; 070-933-247-065-230; 072-449-487-905-209; 073-183-770-809-088; 074-004-764-807-22X; 074-884-578-675-436; 075-668-749-746-092; 077-044-689-183-714; 078-346-643-467-148; 078-910-203-148-420; 079-075-379-667-056; 079-184-007-572-729; 080-397-695-387-045; 080-589-710-669-822; 080-898-568-683-066; 082-036-797-529-285; 082-060-789-804-822; 082-505-202-910-319; 086-141-234-318-177; 087-335-989-871-837; 089-796-474-980-887; 092-218-935-664-70X; 096-363-956-353-392; 097-390-623-922-438; 098-811-589-762-176; 098-966-666-704-92X; 100-294-171-241-127; 101-637-146-335-197; 103-490-387-033-105; 104-685-875-026-63X; 104-694-110-788-442; 108-076-063-556-83X; 108-186-632-977-341; 108-697-687-762-596; 109-176-046-622-406; 110-697-170-045-887; 111-860-820-059-660; 113-203-444-442-959; 113-764-349-033-065; 114-906-535-117-147; 119-517-530-171-868; 119-892-455-283-484; 128-487-778-186-58X; 136-648-400-143-451; 141-489-442-115-69X; 141-649-000-612-322; 144-045-024-507-975; 146-327-810-117-104; 149-273-136-335-633; 150-333-770-505-967; 150-546-942-169-55X; 152-183-517-423-628; 153-111-007-440-51X; 153-471-712-074-630; 163-071-105-957-992; 168-511-383-336-908; 189-270-541-373-235; 190-485-029-949-876,0,false,,
102-776-490-783-032,Collision detection and prevention for the visually impaired using computer vision and machine learning,,2023,journal article,Advances in Engineering Software,09659978; 18735339,Elsevier BV,United Kingdom,Shivang Sunil Singh; Mayank Agrawal; M Eliazer,,179,,103424,103424,Computer science; Collision avoidance; Artificial intelligence; Machine vision; Cover (algebra); Service (business); Collision; Computer security; Object (grammar); White paper; Cloud computing; Computer vision; Human–computer interaction; Engineering; Business; Mechanical engineering; Archaeology; Marketing; History; Operating system,,,,,,http://dx.doi.org/10.1016/j.advengsoft.2023.103424,,10.1016/j.advengsoft.2023.103424,,,0,015-040-823-132-436; 026-994-702-158-53X; 031-408-091-120-255; 039-274-384-165-289; 045-309-399-228-849; 046-703-334-644-632; 070-606-003-984-347; 112-566-962-059-235,1,false,,
103-851-813-622-617,Computer Assisted Orthopaedic Surgery,2008-05-21,2008,journal article,International Journal of Computer Assisted Radiology and Surgery,18616410; 18616429,Springer Science and Business Media LLC,Germany,,,3,S1,94,102,,,,,,,http://dx.doi.org/10.1007/s11548-008-0179-8,,10.1007/s11548-008-0179-8,,,0,,0,false,,
103-961-181-480-397,Black phosphorous-based human-machine communication interface.,2023-01-03,2023,journal article,Nature communications,20411723,Springer Science and Business Media LLC,United Kingdom,Jayraj V Vaghasiya; Carmen C Mayorga-Martinez; Jan Vyskočil; Martin Pumera,"Assistive technology involving auditory feedback is generally utilized by those who are visually impaired or have speech and language difficulties. Therefore, here we concentrate on an auditory human-machine interface that uses audio as a platform for conveying information between visually or speech-disabled users and society. We develop a piezoresistive tactile sensor based on a black phosphorous and polyaniline (BP@PANI) composite by the facile chemical oxidative polymerization of aniline on cotton fabric. Taking advantage of BP's puckered honeycomb lattice structure and superior electrical properties as well as the vast wavy fabric surface, this BP@PANI-based tactile sensor exhibits excellent sensitivity, low-pressure sensitivity, reasonable response time, and good cycle stability. For a real-world application, a prototype device employs six BP@PANI tactile sensors that correspond to braille characters and can convert pressed text into audio on reading or typing to assist visually or speech-disabled persons. Overall, this research offers promising insight into the material candidates and strategies for the development of auditory feedback devices based on layered and 2D materials for human-machine interfaces.",14,1,2,,Computer science; Tactile sensor; Interface (matter); Gesture; Polyaniline; Human–computer interaction; Sensitivity (control systems); Materials science; Artificial intelligence; Polymerization; Electronic engineering; Engineering; Bubble; Maximum bubble pressure method; Parallel computing; Robot; Composite material; Polymer,,Humans; Communication; Touch; Speech Disorders; Speech,polyaniline,"Ministerstvo Školství, Mládeže a Tělovýchovy",https://www.nature.com/articles/s41467-022-34482-4.pdf https://doi.org/10.1038/s41467-022-34482-4,http://dx.doi.org/10.1038/s41467-022-34482-4,36596775,10.1038/s41467-022-34482-4,,PMC9810665,0,000-247-894-086-186; 003-569-836-574-596; 003-983-012-332-838; 004-142-809-303-794; 004-352-879-766-562; 007-065-019-912-010; 008-793-922-360-656; 009-960-027-289-971; 012-039-714-719-409; 013-735-699-073-51X; 013-925-238-237-320; 018-870-024-589-554; 020-164-487-708-242; 020-437-779-690-882; 021-526-065-064-012; 023-398-337-835-132; 026-344-962-111-960; 027-848-943-522-671; 027-959-936-265-373; 029-901-117-432-28X; 033-379-631-042-335; 035-607-382-315-877; 037-943-064-117-694; 043-477-725-427-027; 044-189-669-100-766; 044-489-677-018-855; 046-956-438-954-93X; 051-153-801-531-395; 056-180-395-313-573; 057-025-139-105-51X; 061-215-233-526-566; 061-850-298-377-736; 067-744-737-013-053; 068-693-478-550-282; 070-201-950-305-140; 074-000-035-762-430; 074-183-822-389-457; 074-317-769-910-688; 075-197-964-286-668; 083-918-789-062-348; 088-795-869-088-585; 097-804-408-019-331; 101-430-080-461-212; 102-491-311-049-651; 105-696-498-086-230; 108-510-851-363-536; 111-452-512-180-52X; 116-219-976-118-59X; 126-063-672-414-386; 126-572-556-263-117; 133-842-108-923-776; 153-036-430-995-953; 156-356-098-435-701; 170-780-277-942-530; 192-073-904-074-185,27,true,cc-by,gold
104-694-342-259-021,Robot-assisted wayfinding for the visually impaired in structured indoor environments,2006-06-15,2006,journal article,Autonomous Robots,09295593; 15737527,Springer Science and Business Media LLC,Netherlands,Vladimir Kulyukin; Chaitanya Gharpure; John Nicholson; Grayson Osborne,,21,1,29,41,Human–computer interaction; Human–robot interaction; Artificial intelligence; Assistive robotics; Visually impaired; Computer vision; Computer science; Obstacle avoidance; Robot,,,,,https://dl.acm.org/doi/10.1007/s10514-006-7223-8 https://link.springer.com/article/10.1007/s10514-006-7223-8 https://dialnet.unirioja.es/servlet/articulo?codigo=2073527 https://dblp.uni-trier.de/db/journals/arobots/arobots21.html#KulyukinGNO06 https://www.sciencedirect.com/science/article/pii/B9780128008812000049 https://dx.doi.org/10.1007/s10514-006-7223-8 http://dx.doi.org/10.1007/s10514-006-7223-8,http://dx.doi.org/10.1007/s10514-006-7223-8,,10.1007/s10514-006-7223-8,2110670751,,1,025-889-586-624-837; 031-612-744-612-651; 032-132-714-120-297; 046-039-876-846-615; 046-631-702-552-070; 047-362-948-871-331; 048-298-689-305-353; 049-716-464-360-111; 052-220-278-389-016; 061-190-872-413-287; 067-728-299-967-462; 069-419-248-108-995; 078-736-735-476-395; 081-927-652-906-388; 082-338-324-037-276; 086-720-900-772-539; 098-811-589-762-176; 101-709-814-035-182; 102-010-386-119-845; 107-887-647-672-757; 112-849-097-976-249; 116-357-396-234-936; 145-142-196-197-927; 152-107-747-981-76X; 157-544-671-747-498; 171-916-888-913-890,143,false,,
105-470-038-749-150,One step surgical scene restoration for robot assisted minimally invasive surgery.,2023-02-22,2023,journal article,Scientific reports,20452322,Springer Science and Business Media LLC,United Kingdom,Shahnewaz Ali; Yaqub Jonmohamadi; Davide Fontanarosa; Ross Crawford; Ajay K Pandey,"Minimally invasive surgery (MIS) offers several advantages to patients including minimum blood loss and quick recovery time. However, lack of tactile or haptic feedback and poor visualization of the surgical site often result in some unintentional tissue damage. Visualization aspects further limits the collection of imaged frame contextual details, therefore the utility of computational methods such as tracking of tissue and tools, scene segmentation, and depth estimation are of paramount interest. Here, we discuss an online preprocessing framework that overcomes routinely encountered visualization challenges associated with the MIS. We resolve three pivotal surgical scene reconstruction tasks in a single step; namely, (i) denoise, (ii) deblur, and (iii) color correction. Our proposed method provides a latent clean and sharp image in the standard RGB color space from its noisy, blurred, and raw inputs in a single preprocessing step (end-to-end in one step). The proposed approach is compared against current state-of-the-art methods that perform each of the image restoration tasks separately. Results from knee arthroscopy show that our method outperforms existing solutions in tackling high-level vision tasks at a significantly reduced computation time.",13,1,3127,,Computer science; Computer vision; Artificial intelligence; Visualization; Preprocessor; Segmentation; Haptic technology; Robot; RGB color model; Robotic surgery,,"Humans; Robotics/methods; Minimally Invasive Surgical Procedures/methods; Surgery, Computer-Assisted/methods",,Australia-India Strategic Research Fund,https://www.nature.com/articles/s41598-022-26647-4.pdf https://doi.org/10.1038/s41598-022-26647-4,http://dx.doi.org/10.1038/s41598-022-26647-4,36813821,10.1038/s41598-022-26647-4,,PMC9947129,0,000-063-508-210-171; 000-323-170-426-335; 003-499-035-235-702; 004-118-169-267-325; 007-130-159-127-944; 007-456-026-245-488; 007-738-059-437-975; 012-576-579-715-950; 014-211-368-344-118; 018-361-170-375-655; 018-822-580-983-660; 019-809-574-066-452; 020-233-013-143-936; 024-507-972-916-317; 031-837-119-396-225; 033-874-901-146-977; 036-359-420-092-077; 038-000-300-597-195; 038-609-532-994-616; 040-271-788-018-068; 047-000-233-106-021; 048-105-918-405-726; 049-160-926-535-510; 049-647-655-934-679; 051-040-537-727-290; 051-898-621-864-420; 052-168-751-772-497; 055-253-045-739-265; 055-875-369-089-859; 057-507-650-151-11X; 058-230-324-463-924; 059-618-439-195-322; 061-708-237-293-611; 062-709-405-569-279; 063-281-766-931-597; 063-641-017-343-137; 063-649-531-293-319; 064-147-656-657-506; 074-187-066-666-532; 075-086-734-221-525; 075-302-601-923-981; 080-433-417-106-771; 080-838-944-121-661; 085-871-251-583-448; 090-215-200-905-318; 092-279-612-947-587; 096-325-814-120-502; 102-610-680-169-113; 109-476-729-389-388; 118-429-269-490-761; 129-472-729-394-889; 130-203-640-460-35X; 140-065-464-159-321; 141-290-200-776-016; 145-681-799-860-049; 146-107-127-945-357; 148-390-745-267-02X; 156-073-374-063-773; 167-131-435-704-717; 167-329-358-698-889; 181-027-379-989-814; 188-347-926-339-325; 191-304-489-351-403,3,true,"CC BY, CC BY-NC-ND",gold
105-483-953-987-980,Game accessibility for visually impaired people: a review,2024-07-20,2024,journal article,Soft Computing,14327643; 14337479,Springer Science and Business Media LLC,Germany,Emanuele Agrimi; Chiara Battaglini; Davide Bottari; Giorgio Gnecco; Barbara Leporini,"<jats:title>Abstract</jats:title><jats:p>Playing games is an important way to promote the integration, inclusion, and socialization of participants. This is especially the case of persons with disabilities, such as visually impaired people. Unfortunately, very few games are accessible to such persons. Hopefully, in many digital games, this accessibility can be enabled in principle by assistive technologies, such as screen readers. The aim of this work consists in reviewing the recent literature on game accessibility for people with visual impairment and discussing benefits, limitations, and possible improvements of currently available accessibility solutions. After providing a definition of visual impairment and describing its relationship with gaming, the work reviews general techniques for designing more accessible games. Subsequently, it focuses on specific techniques based on replacing visual stimuli with auditory stimuli (e.g., sonification and sound-source simulation), also presenting some recently proposed sonification-mapping strategies. Then, the application of machine-learning techniques to the development of accessible interfaces for online versions of board games is illustrated by a recent case study. Finally, a discussion and some conclusions are provided, with a particular focus on policy implications of improvements in game accessibility for visually impaired people.</jats:p>",,,,,Visually impaired; Sonification; Visual impairment; Computer science; Human–computer interaction; Inclusion (mineral); Game design; Assistive technology; Work (physics); Socialization; Multimedia; Serious game; Psychology; Engineering; Social psychology; Mechanical engineering; Psychiatry,,,,Scuola IMT Alti Studi Lucca,,http://dx.doi.org/10.1007/s00500-024-09827-4,,10.1007/s00500-024-09827-4,,,0,010-237-806-158-244; 011-379-891-174-336; 014-145-885-588-310; 014-576-756-466-119; 015-005-228-755-192; 016-196-643-553-003; 016-553-489-463-642; 019-422-406-567-622; 019-924-218-904-281; 022-593-957-274-038; 023-500-764-288-581; 023-532-763-292-374; 025-401-091-044-88X; 027-315-101-797-917; 030-248-132-041-785; 035-378-156-699-490; 038-114-596-461-304; 039-720-388-801-921; 039-853-000-175-960; 040-225-983-203-587; 043-771-414-899-454; 043-881-041-208-469; 044-948-210-682-226; 045-175-114-910-238; 059-915-127-909-657; 061-337-103-646-581; 064-713-411-409-049; 072-150-198-244-09X; 073-450-199-671-377; 074-699-897-884-118; 076-306-748-157-067; 079-431-325-375-320; 079-823-216-832-636; 083-269-137-307-090; 087-452-847-569-31X; 089-396-434-513-245; 104-377-419-657-812; 114-921-445-235-823; 115-701-326-312-883; 121-862-743-659-233; 129-889-189-313-251; 134-269-980-706-042; 138-148-139-576-473; 140-553-953-844-074; 174-282-513-115-523; 175-128-950-938-799; 182-752-891-540-336; 184-355-804-558-831; 185-649-035-847-881; 193-663-337-578-905,0,true,cc-by,hybrid
105-947-935-113-350,A vision-based system to detect potholes and uneven surfaces for assisting blind people,,2016,conference proceedings,,,,,A Sridhara Rao; Jayavardhana Gubbi; Marimuthu Palaniswami; Elaine Wong,"© 2016 IEEE.Vision is one of the most advanced and important sensory input in humans. However, many people have vision problems due to birth defects, uncorrected errors, work nature, accidents, and aging. The white cane and guide dog are the most widely used means of navigation for the vision-impaired. With advancements in technology, electronic devices have been created using different sensors and technologies to help navigate the blind. Electronic Travel AIDS (ETAs) assist in navigating a person by collecting information about the environment and relaying this information in a form that allows a blind or vision-impaired person to understand the nature of the environment. However, there is still a lack of devices to detect potholes and uneven pavements, which inhibits mobility after dark. This pilot study proposes a computer vision based pothole and uneven surface detection approach to assist blind people in meeting their mobility needs. The system includes projecting laser patterns, recording the patterns through a monocular video, analyzing the patterns to extract features and then providing path cues for the blind user. With over 90% accuracy in detecting potholes, the proposed system aims to assist blind people in real-time navigation.",,,,,Human–computer interaction; Vision based; Monocular video; Sensory input; White cane; Assistive technology; Computer science,,,,,https://findanexpert.unimelb.edu.au/scholarlywork/1091736-a-vision-based-system-to-detect-potholes-and-uneven-surfaces-for-assisting-blind-people,https://findanexpert.unimelb.edu.au/scholarlywork/1091736-a-vision-based-system-to-detect-potholes-and-uneven-surfaces-for-assisting-blind-people,,,3189422676,,0,,0,false,,
105-949-429-785-399,A robot grasping detection network based on flexible selection of multi-modal feature fusion structure,2024-04-13,2024,journal article,Applied Intelligence,0924669x; 15737497,Springer Science and Business Media LLC,Netherlands,Yuhan Wang; Zhibo Guo; Yu Chen; Chaiqi Guo; Meizhen Xia; Tingyue Qi,,54,6,5044,5061,Computer science; GRASP; Modal; Artificial intelligence; Robustness (evolution); Encoder; Robot; Fusion; Feature (linguistics); Data mining; Pattern recognition (psychology); Computer vision; Linguistics; Philosophy; Polymer chemistry; Biochemistry; Chemistry; Gene; Programming language; Operating system,,,,Jiangsu Agricultural Science and Technology Innovation Fund; Teaching Reform Research Project of Yangzhou University; Medical Innovation Transformation Special Fund of Yangzhou University-New Medical Interdisciplinary Innovation Team,,http://dx.doi.org/10.1007/s10489-024-05427-9,,10.1007/s10489-024-05427-9,,,0,002-605-355-414-179; 010-881-186-573-195; 017-412-595-003-129; 018-957-298-905-501; 020-233-013-143-936; 021-360-232-245-594; 027-410-123-987-397; 028-161-820-875-296; 030-496-222-072-721; 031-218-334-653-826; 033-173-264-242-664; 034-476-380-034-058; 035-634-965-449-15X; 035-766-056-167-504; 036-635-202-361-919; 037-973-222-238-676; 040-904-005-449-943; 041-058-139-703-974; 041-632-100-094-060; 044-082-935-499-811; 049-317-239-158-314; 049-555-532-283-548; 057-567-246-168-045; 058-537-596-488-128; 059-168-358-860-68X; 063-852-163-804-704; 064-971-187-213-173; 065-374-293-809-860; 072-467-468-696-578; 074-237-814-340-136; 089-913-151-025-72X; 095-555-903-485-762; 104-208-613-793-423; 113-990-981-681-117; 115-794-333-641-736; 116-850-249-828-856; 118-987-155-975-927; 121-707-574-802-675; 121-817-966-057-579; 125-112-083-666-544; 125-653-450-925-758; 126-860-936-499-62X; 127-206-654-228-119; 129-187-662-955-203; 141-079-389-471-246; 143-821-509-033-731; 145-026-648-809-851; 145-126-980-585-43X; 146-398-762-650-454; 154-904-756-100-498; 162-350-609-751-491; 166-272-606-070-943,0,false,,
105-978-737-228-765,GUIDED MICROCONTROLLER FOR BLIND PEOPLE,,2012,,,,,,S. Shafiulla Basha; C. Hariprasad,"With the development of radar and ultrasonic technologies, when combined with the robotic technology and bioengineering, gave rise to new series of devices, known as “electronic travel aids (ETAs). It operates similar to a radar system, sends a laser or an ultrasonic beam, which after striking the object reflects back and is detected by the sensors, and so the corresponding distance from the object is calculated. In particular, these devices are used to help people organ failure and people with disabilities, such as visual impairment, deafness etc. This paper is about an instrument, which is the outcome of robotics and bioengineering, and it is called “Guided Microcontroller”. It is a robotics-based obstacle-avoidance system for the blind and visually impaired. A device, called Guided Microcontroller, uses the mobile robotics technology is a wheeled device pushed ahead of the user via an attached cane. When the Guided Microcontroller detects an obstacle, it steers around it. The user immediately feels this steering action and can follow the device’s new path easily without any conscious effort. I. INTRODUCTION This paper is about “GUIDED MICROCONTROLLER (Guide Cane)”, which is computerized device based on advanced mobile robotic navigation for obstacle avoidance useful for visually impaired people. This is “Bioengineering for people with disabilities”. The Guided Microcontroller uses the mobile robotics technology is a wheeled device pushed ahead of the user via an attached cane. When the device detects an obstacle, it steers around it. The user immediately feels this steering action and can follow the device’s new path easily without any conscious effort. The mechanical, electrical and software components, user machine interface and the prototypes of the device are described below. II. MOBILE ROBOTICS TECHNOLOGIES FOR BLIND PEOPLE With the development of radar and ultrasonic technologies, a new series of devices, known as Electronic Travel Aids (ETA’s) was developed. Obstacle Avoidance Systems (OAS) originally developed for mobile robots; lend themselves well to incorporation in Electronic Travel Aids for the visually impaired. An OAS for mobile robots typically comprises a set of, ultrasonic or other sensors and the computer algorithm that uses the sensor data to compute the safe path around detected obstacle. One such algorithm is the Vector Field Histogram (VFH). The VFH method is based on information perceived by an array of ultrasonic sensors (also called Sonar’s) and a fast statistical analysis of that information. The VFH method builds and continuously upgrades a local map of its immediate surroundings based on recent Sonar data history. The algorithm then computes a momentary steering direction and travel speed and sends this information to the mobile robot. The ultrasonic sensors are controlled by the Error-Eliminating Rapid Ultrasonic Firing (EERUF) method. This method allows Sonar’s to fire at rates that are five to ten times faster than conventional methods. In the VHF method, the local map is represented by a two-dimensional (2D) array, called a Histogram Grid. The 2D Histogram Grid is reduced to a one-dimensional Polar Histogram that is constructed around the robot’s momentary location. The Polar Histogram provides an instantaneous 360˚panoramic view of the immediate environment, in which elevations suggest the presence of obstacles, and valleys suggest that the corresponding directions are free of obstacles. The Polar Histogram has 72 sectors that are each 5˚ wide. The numeric values associated with each sector are called Obstacle Density Values. Figure (1), shows the Polar Histogram created from an actual experiment, wherein, high Obstacle Density Values are shown as taller bars in the bar chart type representation. Hence, the Polar Histogram provides comprehensive information about the environment (with regard to obstacles).",,,,,Engineering; Artificial intelligence; Mobile robot; Obstacle; Computer vision; Robotics; Vector Field Histogram; Histogram; Sonar; Obstacle avoidance; Robot,,,,,,,,,2378133422,,0,012-220-148-772-614; 073-722-974-726-137,2,false,,
106-297-500-176-971,The contribution of virtual reality to research on sensory feedback in remote control,2006-03-03,2006,journal article,Virtual Reality,13594338; 14349957,Springer Science and Business Media LLC,United Kingdom,Barry Lund Richardson; Mark Anthony Symmons; Dianne Beryl Wuillemin,"Here we consider research on the kinds of sensory information most effective as feedback during remote control of machines, and the role of virtual reality and telepresence in that research. We argue that full automation is a distant goal and that remote control deserves continued attention and improvement. Visual feedback to controllers has developed in various ways but autostereoscopic displays have yet to be proven. Haptic force feedback, in both real and virtual settings, has been demonstrated to offer much to the remote control environment and has led to a greater understanding of the kinesthetic and cutaneous components of haptics, and their role in multimodal processes, such as sensory capture and integration. We suggest that many displays using primarily visual feedback would benefit from the addition of haptic information but that much is yet to be learned about optimizing such displays.",9,4,234,242,Human–computer interaction; Visual servoing; Haptic technology; Teleoperation; Virtual reality; Perception; Remote operation; Computer science; Simulation; Kinesthetic learning; Remote control,,,,,https://link.springer.com/article/10.1007/s10055-006-0020-z https://link.springer.com/article/10.1007/s10055-006-0020-z/fulltext.html https://dblp.uni-trier.de/db/journals/vr/vr9.html#RichardsonSW06 https://research.monash.edu/en/publications/the-contribution-of-virtual-reality-to-research-on-sensory-feedba,http://dx.doi.org/10.1007/s10055-006-0020-z,,10.1007/s10055-006-0020-z,1977265386,,1,000-794-091-336-949; 000-944-813-355-81X; 003-008-443-486-667; 004-745-889-840-946; 005-019-439-268-380; 007-661-172-037-321; 007-797-333-936-671; 010-945-214-923-835; 011-800-151-221-64X; 016-302-448-343-859; 016-308-925-840-59X; 016-467-774-934-326; 017-811-108-259-847; 018-980-800-164-178; 024-626-058-020-932; 025-539-918-450-74X; 027-469-876-965-244; 027-482-867-808-809; 027-867-169-362-210; 028-382-080-566-269; 029-077-481-507-062; 032-915-576-231-271; 033-878-188-733-794; 034-440-231-851-295; 035-522-803-991-994; 038-147-681-652-675; 043-050-542-638-128; 043-282-147-155-997; 044-314-700-224-856; 047-917-965-615-600; 049-512-100-716-012; 049-606-570-007-664; 050-483-037-308-846; 057-494-562-457-91X; 061-442-260-304-897; 062-118-733-361-368; 069-991-963-793-773; 070-368-171-667-956; 071-381-037-022-818; 078-278-427-385-437; 080-726-093-114-693; 080-834-250-842-730; 082-249-730-131-458; 084-792-667-481-224; 089-939-410-078-494; 091-094-640-354-343; 091-488-042-763-739; 096-282-093-985-620; 102-267-942-611-675; 112-360-849-172-62X; 121-748-808-096-20X; 123-885-773-107-071; 126-200-021-700-475; 159-344-488-231-940; 162-427-094-849-838; 166-420-078-918-614; 174-232-612-382-621; 177-241-423-658-960,7,true,"CC BY, CC BY-NC-ND",gold
106-307-065-687-000,A Systematic Review on Blind and Visually Impaired Navigation Systems,2024-02-27,2024,book chapter,Advances in Intelligent System and Smart Technologies,23673370; 23673389,Springer International Publishing,,Mohamed Bakali El Mohamadi; Adnan Anouzla; Nabila Zrira; Khadija Ouazzani-Touhami,"Visually impaired people face particular navigation challenges in daily life. Indoor and outdoor areas have increasingly become complex environments due to the wide range of agents and entities that make up the ecosystem. Many assistive mobility tools have been developed over the years to overcome this problem. However, these solutions do not satisfy the needs of visually impaired individuals in terms of efficient guidance and real-time processing, as well as the difficulties encountered when handling and operating the entire system. The purpose of this paper is to provide a comprehensive overview of strategies and approaches for developing navigational devices based on 3D computer vision in order to evaluate the detection and recognition accuracy of each system’s software, and to examine the hardware components needed for easy and convenient navigation. With this purpose in mind, we attempted to present a broad synopsis of existing studies using the systematic literature review methodology. The results obtained shed light on CNN-based, cloud computing-based, smartphone-based, and R-GBD-based systems. This review gives helpful information on technological equipment for developing electronic navigation assistive devices in both indoor and outdoor environments.",,,151,160,Visually impaired; Computer science; Physical medicine and rehabilitation; Psychology; Medicine; Human–computer interaction,,,,,,http://dx.doi.org/10.1007/978-3-031-47672-3_17,,10.1007/978-3-031-47672-3_17,,,0,012-344-135-607-729; 022-489-327-390-755; 027-763-143-738-105; 051-263-461-368-647; 067-894-235-988-508; 098-910-337-444-079; 141-489-442-115-69X; 141-649-000-612-322; 148-478-467-972-963; 149-488-483-135-976,0,false,,
106-679-073-847-868,"Driver information system: a combination of augmented reality, deep learning and vehicular Ad-hoc networks",2017-08-03,2017,journal article,Multimedia Tools and Applications,13807501; 15737721,Springer Science and Business Media LLC,Netherlands,Lotfi Abdi; Aref Meddeb,,77,12,14673,14703,Human–computer interaction; Vehicle Information and Communication System; Augmented reality; Information system; Road traffic safety; Traffic congestion; Computer security; Computer science; Advanced Traffic Management System; Intelligent transportation system; Floating car data,,,,,https://dblp.uni-trier.de/db/journals/mta/mta77.html#AbdiM18 https://link.springer.com/10.1007/s11042-017-5054-6 https://link.springer.com/article/10.1007/s11042-017-5054-6,http://dx.doi.org/10.1007/s11042-017-5054-6,,10.1007/s11042-017-5054-6,2742738902,,8,002-663-734-321-306; 003-335-972-879-114; 009-672-392-151-819; 010-212-728-959-861; 014-195-520-388-67X; 020-233-013-143-936; 021-326-533-295-625; 025-227-466-503-19X; 027-712-353-678-737; 030-359-720-170-892; 031-098-452-075-699; 031-133-947-905-667; 031-218-334-653-826; 032-134-337-320-216; 032-773-218-843-517; 036-498-269-062-587; 036-509-252-163-999; 040-214-281-474-370; 042-669-408-757-241; 047-466-380-442-169; 049-317-239-158-314; 050-209-691-402-050; 052-520-917-480-92X; 057-037-636-095-146; 057-547-064-622-616; 061-033-358-084-944; 061-282-694-328-300; 066-133-135-091-645; 066-307-946-649-478; 066-537-888-180-555; 067-860-941-275-569; 068-150-528-289-633; 070-426-575-437-092; 072-828-799-053-287; 073-732-290-782-671; 076-705-973-880-073; 077-621-344-741-829; 082-675-607-047-578; 084-309-315-768-464; 085-188-114-318-57X; 086-768-965-261-150; 087-049-757-998-188; 093-270-973-518-484; 104-578-134-691-508; 106-264-944-278-433; 117-651-317-384-703; 118-982-335-255-035; 119-893-157-949-940; 125-112-083-666-544; 132-045-443-670-181; 138-745-908-104-258; 153-363-162-681-947; 156-048-119-213-215; 166-818-352-186-584; 171-091-935-415-285; 196-056-987-604-567; 199-420-906-525-929,33,false,,
107-078-534-201-078,Electronic Travel Aid for Visually Impaired People based on Computer Vision and Sensor Nodes using Raspberry Pi,2016-01-20,2016,journal article,Indian journal of science and technology,09745645; 09746846,Indian Society for Education and Environment,India,Ram Tirlangi; Ch. Ravi Sankar,"Objectives: To make robust, reliable and affordable Electronic Travel Aid (ETA) which is composed of ultrasonic sensor nodes, smart cane stick and object detection device used in synergy used by blind and visually impaired persons for navigation and object recognition in indoor and outdoor environments. Methods: For the navigation in indoor environment, four ultrasonic sensor nodes which measure proximity of surrounding obstacles and if any object is too close then vibration feedback is given to the user. Smart cane stick will detect wet floor and stairs and if detected then vibration feedback is given at cane handle. In object recognition device we have used Raspberry pi 3 and camera which will detect defined objects in front of the blind user and describe it using audio feedback via headphones in which we have used computer vision techniques from opencv libraries like SURF extraction and haar cascades used with neural networks for cognitive machine learning which makes object detection and recognition robust. Findings: Using ultrasonic sensor nodes at ankles, wrist and waist, yielded better navigation in indoor environment because the ultrasonic sensors are more accurate, fast and distortion less as compared to the other sensors. A simple cane stick gives less information about the environment while smart cane stick gives information about stairs, wet floor and unevenness of floor to the blind user to avoid collision and also better range of detection as compared to simple cane stick. For object recognition, SURF feature extraction and haar cascades are used which is best suited for object detection and recognition of defined objects. And using machine learning increases the accuracy of the object recognition for unknown objects so like this the blind user can understand what kind of object is there infront of him, which was not possible using distance sensors alone. Using the three devices in synergy the blind person can navigate and recognize the objects in indoor and outdoor environments more easily and makes our ETA more accurate, efficient and affordable. Application: For visualisation of the surroundings for blind and visually impaired people.",9,47,1,5,Artificial intelligence; Object detection; Computer vision; Visualization; Computer science; Artificial neural network; Feature extraction; Object (computer science); Cognitive neuroscience of visual object recognition; Ultrasonic sensor,,,,,https://www.indjst.org/index.php/indjst/article/view/106850 http://www.indjst.org/index.php/indjst/article/view/106850,https://www.indjst.org/index.php/indjst/article/view/106850,,,2572620154,,0,,2,false,,
107-178-287-094-567,Developing a Smart Clothing System for Blinds Based on Information Axiom,,2013,journal article,International Journal of Computational Intelligence Systems,18756883; 18756891,Springer Science and Business Media LLC,United Kingdom,Senem Kursun Bahadir; Selcuk Cebi; Cengiz Kahraman; Fatma Kalaoglu,"Abstract In this paper, a novel approach is proposed to determine the electronic system components of a smart clothing system design developed for blinds. The integration of diverse components which are developed and produced by different technologies and materials is a major challenge in a smart clothing system technology. Therefore, an integrated methodology based on fuzzy analytic hierarchy process (AHP) and fuzzy information axiom is used for the solution of this problem.",6,2,279,292,Industrial engineering; Axiom; Electronic systems; Fuzzy analytic hierarchy process; Systems design; Computer science; Clothing; Fuzzy logic; Axiomatic design; Analytic hierarchy process,,,,,https://core.ac.uk/display/153652999 https://www.tandfonline.com/doi/pdf/10.1080/18756891.2013.769766 https://www.atlantis-press.com/journals/ijcis/25868386 https://dblp.uni-trier.de/db/journals/ijcisys/ijcisys6.html#BahadirCKK13 https://doi.org/10.1080/18756891.2013.769766,http://dx.doi.org/10.1080/18756891.2013.769766,,10.1080/18756891.2013.769766,2072844030,,0,000-740-058-509-664; 002-391-643-288-41X; 009-814-775-936-569; 010-223-699-418-436; 010-415-207-268-088; 010-762-934-910-213; 011-052-759-230-690; 017-520-346-325-475; 022-517-000-547-234; 023-056-476-749-672; 023-680-889-449-204; 032-899-514-894-566; 034-373-226-233-399; 034-742-688-109-756; 038-983-402-285-520; 038-984-255-913-340; 039-079-110-624-640; 045-321-254-881-478; 048-454-930-534-243; 052-461-910-461-761; 053-742-727-978-057; 054-270-646-889-152; 060-229-919-441-197; 061-190-872-413-287; 062-957-059-042-10X; 066-526-177-885-125; 067-341-125-020-782; 067-448-216-802-587; 071-183-084-885-707; 072-490-660-724-917; 074-132-202-006-187; 081-229-170-313-186; 088-760-390-340-267; 090-884-510-770-832; 096-150-304-170-09X; 098-418-441-499-649; 098-811-589-762-176; 100-072-520-952-880; 102-633-298-647-083; 109-136-385-975-779; 114-831-834-284-881; 127-483-973-270-088; 137-351-591-563-449; 139-923-031-684-57X; 145-367-063-968-334; 145-923-335-180-378; 145-986-433-757-263; 151-295-136-828-154; 159-342-518-767-22X; 189-153-834-666-012; 193-325-954-560-370,7,true,cc-by,gold
107-182-842-424-273,Guidance device for visually impaired people based on ultrasonic signals and open hardware,,2024,journal article,International Journal of Reconfigurable and Embedded Systems (IJRES),27222608; 20894864,Institute of Advanced Engineering and Science,,Ricardo Yauri; Kevin Alvarez; Junior Cotaquispe; Jordy Ynquilla; Oscar Llerena,"<jats:p>&lt;span&gt;Visual impairment is a complex challenge that affects people of all ages, and it is estimated that around 2.2 billion people worldwide lack adequate access to medical treatment and support. In Latin America, there is a lack of attention to people with visual disabilities, evidenced by poor urban infrastructure and lack of compliance with inclusion laws. Some projects stand out for the use of prototypes with artificial vision technology, global positioning system (GPS) and smart canes. Therefore, the objective of the project is to use ultrasonic sensors and a low-cost electronic device coupled to canes, for obstacle detection and mobility using an open hardware embedded system. The results confirmed the efficiency in the detection and operation of the ultrasonic sensor by activating the light emitting diode (LED), the buzzer and the vibrating motor according to the programmed distances. Challenges were identified, such as adapting the sensor to the tilt of the cane and the importance of accurate calibration of the ultrasonic sensor. The system met its objectives by detecting objects in a range of 2 to 50 cm and providing sound alerts to improve the perception of blind people.&lt;/span&gt;</jats:p>",13,3,520,520,Visually impaired; Ultrasonic sensor; Computer science; Computer hardware; Computer vision; Human–computer interaction; Embedded system; Acoustics; Physics,,,,,,http://dx.doi.org/10.11591/ijres.v13.i3.pp520-527,,10.11591/ijres.v13.i3.pp520-527,,,0,,0,false,,
107-323-305-593-908,Evaluation of Tactile Situation Awareness System as an Aid for Improving Aircraft Control During Periods of Impaired Vision,2009-06-01,2009,,,,,,James S. Brown,"Abstract : This thesis describes the use of a prototype Tactile Situational Awareness System (TSAS) as an approach to aid pilot performance following simulated laser blindness modeled during a virtual approach in an SH-60 helicopter. Situational awareness and spatial awareness remain critical factors for successful control of manned aircraft. Helicopters and fixed winged aircraft pilots react to spatial orientation challenges during take-off, and landing phases of flight. U.S. and NATO aircraft pilot surveys examined the human machine interaction and revealed degraded vision as an important human factor contributing to mishaps or near mishaps. Vision was identified as an information chokepoint limiting command and control of the aircraft. Fortunately, vision can be augmented with an available technology called ""haptics"" during restricted or limited human vision. Therefore, an experiment using X-Plane output for haptics-generated input from a torso-worn TSAS was developed. Participants received haptic cues during runway approaches after experiencing simulated loss of vision. Participant performance after simulated laser blinding with and without the TSAS compared time advantage and navigation accuracy. Simulator performance data indicated pilots using TSAS following simulated laser blindness responded to haptic cues, had more time to prevent the aircraft from obtaining an unsafe pitch or roll condition, and could position the aircraft closer to the landing zone.",,,,,Engineering; Situation awareness; Haptic technology; Visual perception; Impaired Vision; Flight simulator; Simulation; Aviation safety; Spatial contextual awareness; Runway,,,,,https://apps.dtic.mil/dtic/tr/fulltext/u2/a501130.pdf,https://apps.dtic.mil/dtic/tr/fulltext/u2/a501130.pdf,,,317657875,,0,,1,false,,
107-745-071-480-045,Two Types of Demonstration Through Guided Touch with Cane: Instruction Sequences in Orientation and Mobility Training for a Person with Visual Impairments,2023-12-12,2023,journal article,Human Studies,01638548; 1572851x,Springer Science and Business Media LLC,Netherlands,Yasusuke Minami; Hiro Yuki Nisisawa; Mitsuhiro Okada; Rui Sakaida,"<jats:title>Abstract</jats:title><jats:p>Persons with visual impairments (hereafter PVI) detect and discover obstacles and road conditions by touching with a white cane when walking on the streets. In one training session, an Orientation and Mobility specialist (hereafter SPT) guided a PVI by grasping and moving the cane that the PVI was holding. We conducted a multimodal analysis of two instruction sequences, one a ""proving and achieving"" demonstration (Sacks in Lectures on conversation, Blackwell, 1992) and the other a ""learnable"" (Zemel and Koschmann, in Discourse Stud 16:163–183, 2014) demonstration. The achieving demonstration proved the assessment of the PVI's performance. In the ""learnable"" demonstration, the PVI was able to receive and perform the most critical part of the ""learnable"" of the long contact touch without the aid of talk. Sharing a single cane touch is an efficient way for both the guiding SPT and the guided PVI to jointly experience and understand the environmental features. The SPT did not need to verbally confirm that the guided touch was accountable to the PVI and seemed confident that intersubjectivity with the PVI had been established. A unique form of being with others and achieving intersubjectivity in society was identified. In traditional learning instruction, it has been assumed that the learnable is presented and communicated visually and audibly. However, through guided touch learnable is presented and conveyed effectively in the cases of this paper. It seems that the sense of touch has been considered to be just for the occasion, but this is an example of something that is not just for the occasion but is consequential, that is, usable for further occasions. The data is in Japanese.</jats:p>",46,4,723,756,Intersubjectivity; Conversation; Orientation (vector space); Turn-taking; Psychology; Session (web analytics); Orientation and Mobility; Communication; Computer science; Cognitive psychology; Human–computer interaction; Artificial intelligence; Sociology; Mathematics; Visually impaired; World Wide Web; Social science; Geometry,,,,Japan Society for the Promotion of Science; Japan Society for the Promotion of Science,https://link.springer.com/content/pdf/10.1007/s10746-023-09690-6.pdf https://doi.org/10.1007/s10746-023-09690-6,http://dx.doi.org/10.1007/s10746-023-09690-6,,10.1007/s10746-023-09690-6,,,0,007-104-356-838-99X; 020-286-662-321-826; 032-241-320-900-955; 033-119-450-330-839; 039-754-645-748-179; 042-226-025-256-586; 046-652-435-343-698; 058-818-921-626-40X; 079-769-311-398-124; 084-111-906-521-347; 085-428-386-239-875; 086-527-055-355-388; 091-093-550-937-727; 091-422-389-682-418; 102-938-321-479-83X; 111-762-914-436-907; 120-695-984-380-717; 123-925-698-947-751; 145-836-335-173-922; 183-189-085-638-87X; 183-227-583-887-849,0,true,cc-by,hybrid
107-849-851-790-34X,How body motion influences echolocation while walking.,2018-10-24,2018,journal article,Scientific reports,20452322,Springer Science and Business Media LLC,United Kingdom,Alessia Tonelli; Claudio Campus; Luca Brayda,"This study investigated the influence of body motion on an echolocation task. We asked a group of blindfolded novice sighted participants to walk along a corridor, made with plastic sound-reflecting panels. By self-generating mouth clicks, the participants attempted to understand some spatial properties of the corridor, i.e. a left turn, a right turn or a dead end. They were asked to explore the corridor and stop whenever they were confident about the corridor shape. Their body motion was captured by a camera system and coded. Most participants were able to accomplish the task with the percentage of correct guesses above the chance level. We found a mutual interaction between some kinematic variables that can lead to optimal echolocation skills. These variables are head motion, accounting for spatial exploration, the motion stop-point of the person and the amount of correct guesses about the spatial structure. The results confirmed that sighted people are able to use self-generated echoes to navigate in a complex environment. The inter-individual variability and the quality of echolocation tasks seems to depend on how and how much the space is explored.",8,1,15704,15704,Motion (physics); Human echolocation; Cognitive psychology; Space (commercial competition); Task (project management); Dead end; Kinematics; Computer science,,"Adult; Biomechanical Phenomena; Distance Perception; Female; Head Movements; Humans; Male; Motion; Sound Localization/physiology; Time Factors; Vision, Ocular; Walking/physiology; Young Adult",,,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6200730/ http://ui.adsabs.harvard.edu/abs/2018NatSR...815704T/abstract https://www.nature.com/articles/s41598-018-34074-7.pdf https://europepmc.org/article/MED/30356175 https://www.nature.com/articles/s41598-018-34074-7,http://dx.doi.org/10.1038/s41598-018-34074-7,30356175,10.1038/s41598-018-34074-7,2897449776,PMC6200730,0,001-069-756-144-955; 002-708-048-102-708; 003-782-769-185-824; 004-974-056-848-790; 008-053-447-086-155; 013-196-453-981-278; 014-790-339-235-851; 016-709-058-509-432; 022-183-457-930-690; 024-873-718-242-898; 025-855-630-627-133; 027-823-915-833-13X; 028-306-786-135-490; 028-626-974-113-857; 031-696-287-727-760; 033-219-570-769-289; 038-429-789-443-678; 042-508-473-438-274; 047-065-239-959-460; 049-435-675-206-981; 051-987-563-680-010; 057-275-450-748-982; 078-277-260-226-121; 079-559-118-932-246; 080-031-266-744-052; 081-880-438-021-348; 083-095-642-169-058; 084-448-260-056-274; 084-750-380-873-575; 095-180-219-281-052; 098-544-319-160-155; 106-603-492-993-738; 133-300-802-059-627; 160-650-934-403-441; 166-888-560-041-543,8,true,"CC BY, CC BY-NC-ND",gold
107-863-525-487-741,Development of a robotic application with voice to guide vision impairment people indoor environments based on Robotic Operating System,2021-03-22,2021,conference proceedings article,2021 IEEE International Conference on Automation/XXIV Congress of the Chilean Association of Automatic Control (ICA-ACCA),,IEEE,,L. Pari; Christian Mamani,"Blindness is one of the most widespread disabilities in people around the world, these people cannot lead a normal life due to the limitation of their movements even in their homes. Most of them, need the assistance of someone or something. The navigation robots have the potential to overcome this limitations, they are presented as a good alternative for them, since they can be controllated by voice, buttons and others, they can also be adapted to the requirements of vision impairmed people. In this article is developed a voice-comands robotic aplication to indicate the indoor destination and then associate it to a coordinate in the map based on Turtlebot3, Pocketsphinx, Robotic Operating System (ROS) and Text to Speech (TTS) like Talkey, of this way the robot guides the patient to his destination through a path that this traces it. Finally, tests were carried out in six patients applying the usability test, it was possible to obtain an average greater than eighty, we found that application's performance was very well, exceeding expectations in patient trust and safety.",,,9465292,,Operating system; Speech synthesis; Usability; Voice command device; Test (assessment); Visual impairment; Visualization; Computer science; Robot kinematics; Robot,,,,,http://xplorestaging.ieee.org/ielx7/9465172/9465173/09465292.pdf?arnumber=9465292 https://pure.unsa.edu.pe/es/publications/development-of-a-robotic-application-with-voice-to-guide-vision-i http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=9465292,http://dx.doi.org/10.1109/icaacca51523.2021.9465292,,10.1109/icaacca51523.2021.9465292,3181791170,,0,000-783-429-447-993; 013-056-435-015-274; 019-150-105-799-327; 027-026-193-850-672; 029-313-431-507-058; 031-175-485-676-104; 035-108-881-019-179; 039-274-384-165-289; 047-417-556-221-888; 048-665-782-773-575; 056-839-169-895-213; 068-708-501-300-740; 069-523-876-223-951; 070-717-243-164-26X; 071-183-084-885-707; 078-021-533-108-528; 085-469-792-005-426; 137-032-010-877-936; 139-267-595-049-632; 165-646-371-179-035; 196-866-598-969-196,1,false,,
107-873-316-412-005,Public Participation in the Development Process of a Mobility Assistance System for Visually Impaired Pedestrians,2019-04-27,2019,journal article,Societies,20754698,MDPI AG,,Nora Weinberger; Markus Winkelmann; Karin Müller; Sebastian Ritterbusch; Rainer Stiefelhagen,"Blind and visually impaired people have to cope with the safe movement through public space and the (lack of) knowledge of spatial issues and walkable routes. These challenges often lead to a fear of accidents and collisions, frequently also of disorientation. This, in turn, can result in a reduced radius of action, restricted mobility, and later on, in social isolation. Against this background, the project TERRAIN aims at developing a technical guidance system for orientation and navigation in urban space. For the development of this assistance system, the project pursues an approach in which reflexive, responsive, and deliberative dimensions have been integrated to address the ethical, legal and social implications (ELSI) in a co-design process. This paper focuses on the participation of citizens independent of vision impairments in the project which provided a variety of relevant indications of impacts and potential technical adaptations from an ‘outer’ point of view. In addition, conclusions can be drawn about the existing desirability and acceptance of the technical solution among the potential users as well as their social environment of potential users. In addition, it turned out that the citizen participation process raised different expectations among the project partners. Therefore, this article evaluates the participation results from the perspective of the technology developers and the technology assessors.",9,2,32,,Internet privacy; Social environment; Reflexivity; Variety (cybernetics); Psychology; Perspective (graphical); Social isolation; Public participation; Action (philosophy); Public space,,,,"Bundesministerium für Bildung, Wissenschaft, Forschung und Technologie",https://www.mdpi.com/2075-4698/9/2/32/pdf https://ideas.repec.org/a/gam/jsoctx/v9y2019i2p32-d226396.html https://www.mdpi.com/2075-4698/9/2/32 https://publikationen.bibliothek.kit.edu/1000095761 https://publikationen.bibliothek.kit.edu/1000095761/31557427,http://dx.doi.org/10.3390/soc9020032,,10.3390/soc9020032,2940878177,,0,005-085-530-933-762; 005-187-563-285-957; 006-807-297-896-480; 019-864-198-596-020; 025-825-210-608-908; 058-443-047-291-990; 075-519-293-886-898; 088-470-436-866-921; 111-468-073-654-696; 148-826-291-255-474; 183-812-177-691-266,6,true,cc-by,gold
107-887-647-672-757,A Robotic Travel Aid for the Blind,,1998,book chapter,Robotics Research,,Springer London,,Hideo Mori; Shinji Kotani,"We have been developing Robotic Travel Aid(RoTA) “HARUNOBU” to guide the visually impaired in the sidewalk or campus. RoTA is a motor wheel chair equipped with vision system, sonar, differential GPS system, dead reckoning system and a portable GIS. We estimate the performance of RoTA in two viewpoints, the viewpoint of guidance and the viewpoint of safety. RoTA is superior to the guide dog in the navigation function, and is inferior to the guide dog in the mobility. It can show the route from the current location to the destination but can not walk up and down stairs. RoTA is superior to the portable navigation system in the orientation, obstacle avoidance and physical support to keep balance of walking, but is inferior in portability.",,,237,245,Dead reckoning; Artificial intelligence; Mobile robot; Stairs; Navigation system; Differential GPS; Navigation function; Computer vision; Computer science; Simulation; Machine vision; Obstacle avoidance,,,,,https://link.springer.com/chapter/10.1007/978-1-4471-1580-9_22 https://rd.springer.com/chapter/10.1007/978-1-4471-1580-9_22,http://dx.doi.org/10.1007/978-1-4471-1580-9_22,,10.1007/978-1-4471-1580-9_22,2264621948,,0,014-440-173-656-399; 021-495-309-385-383; 068-538-710-738-336; 072-400-524-676-429; 073-211-104-818-914; 088-470-436-866-921; 117-614-663-243-046; 125-224-278-103-756; 139-791-546-659-910,39,false,,
107-913-756-844-390,The development of a visual system for the detection of obstructions for visually impaired people,2009-10-14,2009,journal article,Journal of Mechanical Science and Technology,1738494x; 19763824,Springer Science and Business Media LLC,South Korea,Mitsuhiro Okayasu,,23,10,2776,2779,Engineering; Artificial intelligence; Control system; Base (geometry); Development (differential geometry); Ultrasonic actuator; Visually impaired; Computer vision; Frame rate; Object (computer science); Image sensor,,,,,https://okayama.pure.elsevier.com/en/publications/the-development-of-a-visual-system-for-the-detection-of-obstructi https://link.springer.com/article/10.1007/s12206-009-0729-1 https://link.springer.com/content/pdf/10.1007%2Fs12206-009-0729-1.pdf https://jglobal.jst.go.jp/en/detail?JGLOBAL_ID=201302269610819498,http://dx.doi.org/10.1007/s12206-009-0729-1,,10.1007/s12206-009-0729-1,2077227475,,0,017-674-895-589-096; 065-344-358-340-26X,4,false,,
107-983-224-543-339,HCI (6) - Integrating Computer Vision Object Recognition with Location Based Services for the Blind,,2014,book chapter,Lecture Notes in Computer Science,03029743; 16113349,Springer International Publishing,Germany,Hugo Fernandes; Paulo Costa; Hugo Paredes; Vitor Filipe; João Barroso,"The task of moving from one place to another is a difficult challenge that involves obstacle avoidance, staying on street walks, finding doors, knowing the current location and keeping on track through the desired path. Nowadays, navigation systems are widely used to find the correct path, or the quickest, between two places. While assistive technology has contributed to the improvement of the quality of life of people with disabilities, people with visual impairment still face enormous limitations in terms of their mobility. In recent years, several approaches have been made to create systems that allow seamless tracking and navigation both in indoor and outdoor environments. However there is still an enormous lack of availability of information that can be used to assist the navigation of users with visual impairments as well as a lack of sufficient precision in terms of the estimation of the user's location. Blavigator is a navigation system designed to help users with visual impairments. In a known location, the use of object recognition algorithms can provide contextual feedback to the user and even serve as a validator to the positioning module and geographic information system of a navigation system for the visually impaired. This paper proposes a method where the use of computer vision algorithms validate the outputs of the positioning system of the Blavigator prototype.",,,493,500,Human–computer interaction; Artificial intelligence; Location-based service; Mobile robot navigation; Navigation system; Positioning system; Computer vision; Validator; Computer science; Turn-by-turn navigation; Cognitive neuroscience of visual object recognition; Obstacle avoidance,,,,,https://link.springer.com/10.1007/978-3-319-07446-7_48 https://rd.springer.com/chapter/10.1007/978-3-319-07446-7_48 https://link.springer.com/content/pdf/10.1007%2F978-3-319-07446-7_48.pdf https://link.springer.com/chapter/10.1007/978-3-319-07446-7_48,http://dx.doi.org/10.1007/978-3-319-07446-7_48,,10.1007/978-3-319-07446-7_48,181846293,,0,029-785-009-752-993; 039-079-110-624-640; 040-660-173-610-646; 090-884-510-770-832; 097-390-623-922-438; 125-867-883-586-61X; 144-045-024-507-975; 151-015-279-636-660,11,true,,bronze
108-046-314-468-15X,Image-Guided Implantology and Bone Assessment,2006-05-19,2006,journal article,International Journal of Computer Assisted Radiology and Surgery,18616410; 18616429,Springer Science and Business Media LLC,Germany,,,1,S1,413,426,Computer science; Dentistry; Medical physics; Computer vision; Medicine,,,,,,http://dx.doi.org/10.1007/s11548-006-0025-9,,10.1007/s11548-006-0025-9,,,0,,1,false,,
108-143-765-116-713,Cardiovascular Surgery,2008-05-16,2008,journal article,International Journal of Computer Assisted Radiology and Surgery,18616410; 18616429,Springer Science and Business Media LLC,Germany,Ayyaz Ali; Yasir Abu-Omar; Amit N. Patel; Ziad A. Ali; Ahmad Y. Sheikh; Asim Akhtar; Aleksandra Pavlović; Panagiotis Theodorou; Thanos Athanasiou; John Pepper,"Structural valve deterioration (SVD) limits the long-term durability of homograft aortic valve replacement (AVR).Valves are implanted predominantly using two techniques, the free-hand sub-coronary (SC) technique or aortic root replacement (RR).Our objective was to identify risk factors associated with the development of SVD or ascending aortic dilatation.In particular we strived to determine whether the mode of implantation had an independent effect. Methods and resultsDemographic and pre-operative clinical data were obtained retrospectively through case-note review.All operations were performed by a single surgeon.Actuarial freedom from 2þ AR (aortic regurgitation), elevated trans-valvular gradient (TVG) (25 mmHg) and ascending aortic dilatation (4.0 cm) were assessed using Kaplan -Meier curves and multivariable Cox proportional hazards regression.A propensity analysis was carried out using a non-parsimonius logistic regression model for implantation with SC vs. RR.Between 1 January 1991 and 1 January 2001, 215 patients underwent AVR with a homograft.The SC technique was used in 131 (61%) patients and 84 (39%) patients underwent RR.Technique was not an independent predictor for 2þ AR (adjusted hazard ratio 1.9; 95% CI 0.56 -6.16, P ¼ 0.31), elevated TVG (adjusted hazard ratio; 0.99; 95% CI 0.15-6.71,P ¼ 0.99) or ascending aortic dilatation (adjusted hazard ratio 2.01; 95% CI 0.50 -8.25, P ¼ 0.33).One and 5 year actuarial freedom from 2þ AR (logrank -P ¼ 0.09) and ascending aortic dilatation (log-rank -P ¼ 0.88) were not significantly different between groups. ConclusionThe incidence of SVD and ascending aortic dilatation is not affected by the method of implantation of the aortic homograft.All homografts are prone to SVD which is responsible for a progressive increase in the prevalence of these changes over time.",3,S1,86,93,Medicine; General surgery; Computer science,,,,,http://eurheartj.oxfordjournals.org/content/29/11/1454.full.pdf http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.560.6343 http://eurheartj.oxfordjournals.org/content/ehj/early/2014/05/16/eurheartj.ehu192.full.pdf http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.653.1531,http://dx.doi.org/10.1007/s11548-008-0178-9,,10.1007/s11548-008-0178-9,,,0,,0,true,,green
108-186-632-977-341,Voice Navigation Based guiding Device for Visually Impaired People,2021-03-25,2021,conference proceedings article,2021 International Conference on Artificial Intelligence and Smart Systems (ICAIS),,IEEE,,P. Chitra; V. Balamurugan; M. Sumathi; N. Mathan; K. Srilatha; R. Narmadha,"Navigation of visually impaired people is one of the important disputes that requires significant research consideration. The visually impaired users generally use white canes for obstacle detection by remembering all the familiar locations. In a new-fangled and unacquainted environment, they totally depend on individuals passing by to enquire for certain places. In this contemporary world along with various sensors, there should be a system with the most basic invention to make their life a bit tranquil. A contactless, hands free, LVU (Lidars and Vibrotactile Units), discrete wearable device was designed in this proposed work that allows blind people to detect obstacles. To provide a safe mobility for the impaired people, a suitable mobile assistance device is necessary. This paper propose a safe wearable device with audio output for benign local navigation in both inside and outdoor environment which help in assisting the user to discriminate free space from obstacles. The device presented is composed of wearable strap with sensors. By using TOF sensor attached in the front of the belt worn by the users, the pulses from the LiDAR provide a reliable and correct measurements of the distances between the handler and obstacles. The image captured by the camera is processed and classified by the convolution neural network algorithm. The identified image is given as an audio input to the audio jockey. The vibratory motor and voice intimation by audio jockey provides the haptic feedback when the disabled person reach the obstacle. The vibration motor is placed with a pretension point-loaded applicator to transmit the isolated vibrations to incapacitate person. Distance between the obstacle and the disabled person is measured by using LiDAR sensor and it will be given as a feedback to the visually impaired person with the voice input. Thus, this wearable device helps in assisting the visually impaired people in a more comfortable way than white canes.",,,911,915,Human–computer interaction; Direct voice input; Wearable computer; Haptic technology; Obstacle; Vibration motor; Hands free; Visually impaired; Computer science; Convolutional neural network,,,,,https://ieeexplore.ieee.org/document/9395981/,http://dx.doi.org/10.1109/icais50930.2021.9395981,,10.1109/icais50930.2021.9395981,3152621422,,0,013-932-260-305-671; 015-490-373-388-693; 034-463-620-626-325; 043-536-853-133-602; 045-205-438-482-211; 050-047-706-859-59X; 066-246-145-459-483; 084-621-956-547-983; 085-102-500-348-494; 092-391-305-190-481; 092-575-825-025-769; 116-715-441-276-690,8,false,,
108-396-749-438-718,ACM Multimedia - Context-based indoor object detection as an aid to blind persons accessing unfamiliar environments,2010-10-25,2010,conference proceedings article,Proceedings of the 18th ACM international conference on Multimedia,,ACM,,Xiaodong Yang; Yingli Tian; Chucai Yi; Aries Arditi,"Independent travel is a well known challenge for blind or visually impaired persons. In this paper, we propose a computer vision-based indoor wayfinding system for assisting blind people to independently access unfamiliar buildings. In order to find different rooms (i.e. an office, a lab, or a bathroom) and other building amenities (i.e. an exit or an elevator), we incorporate door detection with text recognition. First we develop a robust and efficient algorithm to detect doors and elevators based on general geometric shape, by combining edges and corners. The algorithm is generic enough to handle large intra-class variations of the object model among different indoor environments, as well as small inter-class differences between different objects such as doors and elevators. Next, to distinguish an office door from a bathroom door, we extract and recognize the text information associated with the detected objects. We first extract text regions from indoor signs with multiple colors. Then text character localization and layout analysis of text strings are applied to filter out background interference. The extracted text is recognized by using off-the-shelf optical character recognition (OCR) software products. The object type, orientation, and location can be displayed as speech for blind travelers.",,,1087,1090,Artificial intelligence; Object model; Object detection; Computer vision; Computer science; Optical character recognition; Geometric shape; Orientation (computer vision),,,,,https://dl.acm.org/citation.cfm?doid=1873951.1874156 https://doi.org/10.1145/1873951.1874156 https://core.ac.uk/display/103016477 https://dblp.uni-trier.de/db/conf/mm/mm2010.html#YangTYA10,http://dx.doi.org/10.1145/1873951.1874156,,10.1145/1873951.1874156,2077958504,,0,003-050-926-366-503; 014-950-710-125-076; 021-652-866-302-313; 054-374-371-905-358; 089-733-804-554-103; 091-785-995-981-234; 100-260-517-243-811; 114-064-723-555-816; 126-327-812-713-131; 136-263-796-565-556,32,false,,
108-494-601-296-636,Voice Assisted Image Captioning and VQA For Visually Challenged Individuals,2022-11-24,2022,conference proceedings article,2022 IEEE 19th India Council International Conference (INDICON),,IEEE,,Kristen Pereira; Rushil Patel; Joy Almeida; Rupali Sawant,"Vision is one of the most vital senses for a person’s well being. More than 285 million people suffer from the problem of visual impairment. This count is predicted to be three fold in coming 30 years. Independent navigation and safe travel are difficult for these individuals, as they have difficulty perceiving information from their surrounding and communicating. Rather than relying on visual cues to guide blind people, the proposed project will help them navigate the world through the use of audio means. It will empower visually impaired individuals to explore independently by using the system to detect objects in their vicinity and without any outside assistance. Within our proposed research, we have built a mobile application which leverages the power of image processing, and deep learning techniques to identify and describe the current scene through the camera and inform it to the user by audio cues. As a result of not being able to distinguish between different objects, the already existing approaches have been limited, resulting in low performance and accuracy. With this we attempt to provide enhanced performance, better accuracy and hence a more practicable and reliable alternative.",,,,,Closed captioning; Computer science; Artificial intelligence; Visually impaired; Computer vision; Sensory cue; Human–computer interaction; Mobile device; Visualization; Image (mathematics); Multimedia; World Wide Web,,,,,,http://dx.doi.org/10.1109/indicon56171.2022.10040196,,10.1109/indicon56171.2022.10040196,,,0,038-062-146-318-998; 040-706-584-850-653; 042-568-425-640-966; 071-074-526-761-451; 081-747-388-175-61X; 090-432-106-031-504; 196-216-771-093-34X,0,false,,
108-603-812-108-826,Analysis of Deep Learning Techniques for Prediction of Eye Diseases: A Systematic Review,2023-08-18,2023,journal article,Archives of Computational Methods in Engineering,11343060; 18861784,Springer Science and Business Media LLC,Spain,Akanksha Bali; Vibhakar Mansotra,,31,1,487,520,Deep learning; Artificial intelligence; Computer science; Identification (biology); Convolutional neural network; Machine learning; Biology; Botany,,,,,,http://dx.doi.org/10.1007/s11831-023-09989-8,,10.1007/s11831-023-09989-8,,,0,000-218-494-580-078; 001-727-951-806-24X; 002-130-520-878-692; 002-523-050-188-95X; 002-924-890-832-40X; 003-460-980-096-622; 004-186-648-370-228; 004-679-186-247-190; 004-818-729-058-351; 005-003-264-004-220; 006-408-727-567-508; 006-571-154-271-91X; 007-024-634-458-154; 007-628-655-149-892; 009-823-064-160-244; 010-144-227-179-46X; 010-217-568-139-831; 011-055-345-753-049; 011-346-127-092-879; 012-247-264-752-733; 013-165-381-692-298; 013-533-616-574-592; 014-936-648-385-465; 017-069-807-457-494; 020-046-145-126-683; 021-385-988-736-052; 023-813-664-007-110; 024-150-603-419-340; 025-212-025-365-068; 026-489-656-595-415; 027-604-370-848-250; 028-046-252-638-415; 028-064-888-546-435; 028-304-024-463-173; 029-821-764-192-948; 029-893-141-129-048; 031-687-647-460-365; 032-638-376-514-012; 033-417-610-436-487; 033-641-809-378-970; 034-247-279-757-287; 034-712-744-754-331; 035-396-229-095-022; 037-745-221-573-451; 038-684-901-913-157; 038-758-050-791-532; 041-389-060-499-380; 041-616-294-784-608; 042-559-001-183-621; 042-709-712-012-117; 045-980-364-913-388; 047-283-134-651-254; 047-863-624-601-744; 048-085-691-460-257; 048-292-064-974-349; 051-142-623-830-267; 051-819-243-984-119; 051-883-851-750-224; 052-176-743-885-133; 053-642-094-685-458; 054-205-808-854-876; 055-640-721-228-242; 056-771-950-252-584; 059-411-946-044-686; 059-849-870-110-224; 061-034-184-980-70X; 061-475-121-262-704; 065-373-627-828-54X; 065-591-745-183-848; 067-230-548-783-595; 067-632-178-226-223; 068-985-212-740-26X; 069-400-790-387-680; 069-851-282-938-856; 071-396-811-405-327; 071-398-807-064-477; 075-093-116-594-354; 075-408-445-901-46X; 076-519-002-473-29X; 076-703-094-587-584; 076-911-590-074-151; 077-535-508-872-892; 077-552-637-317-849; 077-864-110-481-987; 077-906-335-921-771; 079-722-660-299-417; 080-124-934-637-299; 080-924-463-158-958; 081-558-483-145-600; 082-283-693-656-990; 083-403-131-267-651; 084-607-435-272-557; 085-096-534-798-153; 087-524-269-634-211; 088-267-656-527-071; 088-487-274-027-981; 088-524-206-626-501; 090-148-273-599-449; 092-109-419-882-628; 093-163-558-256-137; 095-533-555-857-904; 098-468-134-539-239; 098-666-413-641-700; 101-059-820-039-444; 104-810-517-818-987; 110-365-354-056-23X; 112-177-297-805-314; 113-567-438-754-785; 114-070-088-774-462; 115-178-018-965-424; 117-033-885-349-043; 120-273-280-311-377; 121-178-557-072-024; 125-744-670-781-892; 126-248-154-713-424; 130-725-700-213-384; 145-088-179-500-520; 148-961-558-252-929; 149-463-549-256-674; 149-654-933-654-759; 151-659-635-969-757; 155-298-779-849-476; 158-622-733-499-264; 160-133-777-402-553; 161-233-412-349-635; 161-237-664-288-910; 163-502-723-443-456; 165-937-900-448-342; 168-117-435-927-449; 182-200-840-773-206; 188-426-330-073-429; 193-148-709-247-976; 198-923-324-170-863,4,false,,
108-623-076-527-130,A Qualitative and Quantitative Analysis of Research in Mobility Technologies for Visually Impaired People,,2023,journal article,IEEE Access,21693536,Institute of Electrical and Electronics Engineers (IEEE),United States,Jyoti Madake; Shripad Bhatlawande; Anjali Solanke; Swati Shilaskar,"Assistive technology in rehabilitation programs is vital for people with vision impairments worldwide.The term ""blind assistive technology"" refers to mobility devices specifically designed to provide position, orientation and mobility assistance for visually impaired individuals during indoor and outdoor activities.The paper presents a comprehensive evaluation of 140 research articles published over the past 75 years (1946 to 2022).This research analyses the evolution of assistive technology aids in depth, in terms sensing technique followed, algorithms employed for obstacle detection, localization, object recognition, depth estimation and scene understanding.It also includes, the functional attributes of the aid, feedback type, and assistive solutions embedded in aid.It evaluates the assistive aids for their usability index, portability, battery life, feedback type, and aesthetics.The survey findings reveal that optical and sonic sensor-based aids prioritize speed, weight, and battery life but lack major functionalities, achieving an average performance score of 62%.Stereo, monocular, SLAM, and 3-D point cloud-based aids excel in obstacle distance estimation and avoidance but require greater memory resources, with a lower performance score of 41%.Artificial intelligence and cloud-based aids offer comprehensive scene details but demand complex computational capabilities, achieving a performance score of 44%.However, the most suitable technology for developing state-of-the-art solutions for blind individuals is the multisensor fusion-based and guide robot-based aids, providing a majority of the essential assistive functions with a performance score of 51%.The study highlights possible challenges associated with implementing assistive technology aids, emphasizes the importance of user acceptability, and stresses the need for real-time evaluation of blind aids.The paper lays a concrete foundation and direction for future development, emphasizing the critical challenges faced by blind users, including boarding trains, traveling on public transport, shopping in a supermarket, avoiding dynamic obstacles, and real-time understanding of the surrounding scene.Addressing these key concerns is crucial for the continued development and improvement of assistive technology aids for the visually impaired, leading to enhanced independence, mobility, and ultimately, a higher quality of life.",11,,82496,82520,Orientation and Mobility; Computer science; Usability; Software portability; Human–computer interaction; Obstacle; Artificial intelligence; Assistive technology; Assistive device; Computer vision; Physical medicine and rehabilitation; Visually impaired; Medicine; Political science; Law; Programming language,,,,,https://ieeexplore.ieee.org/ielx7/6287639/6514899/10168878.pdf https://doi.org/10.1109/access.2023.3291074,http://dx.doi.org/10.1109/access.2023.3291074,,10.1109/access.2023.3291074,,,0,000-676-430-732-026; 000-678-811-523-155; 000-729-381-857-952; 001-420-206-231-830; 001-932-879-647-813; 002-397-792-416-419; 004-772-554-982-380; 005-303-872-644-71X; 005-427-234-377-412; 006-409-052-293-645; 006-557-756-014-856; 008-684-469-671-014; 009-142-401-900-87X; 009-160-989-834-373; 009-210-734-467-949; 009-310-889-054-661; 009-645-272-092-857; 009-748-285-068-532; 009-864-381-071-748; 010-616-069-097-815; 011-737-861-055-472; 011-821-531-136-685; 012-344-135-607-729; 013-651-178-224-273; 015-672-084-912-775; 016-353-889-913-511; 018-170-211-594-394; 018-797-747-752-732; 018-828-594-955-588; 019-474-434-639-720; 019-545-013-102-509; 020-373-201-301-684; 020-458-465-756-853; 020-538-097-044-472; 020-678-057-737-391; 023-237-215-374-844; 023-309-209-821-636; 023-829-577-001-809; 024-627-299-891-290; 024-964-397-377-997; 027-720-558-803-335; 027-893-571-523-130; 028-094-088-190-140; 028-460-767-882-226; 029-360-136-216-567; 030-127-369-356-741; 033-490-815-544-255; 034-412-649-320-055; 034-463-620-626-325; 034-936-259-439-98X; 037-039-198-474-114; 037-163-533-301-156; 040-057-279-881-544; 042-156-772-128-560; 044-170-635-000-193; 044-314-299-753-89X; 044-912-780-668-165; 049-361-709-307-125; 050-137-510-002-999; 051-065-738-489-461; 052-152-856-946-645; 053-574-289-747-231; 053-968-972-722-364; 054-279-559-477-855; 056-698-843-057-822; 058-013-370-483-44X; 058-169-716-619-430; 058-443-047-291-990; 060-041-403-626-524; 064-376-268-713-288; 066-037-919-140-345; 066-246-145-459-483; 066-821-243-144-379; 067-894-235-988-508; 068-104-342-801-953; 069-116-650-623-84X; 070-740-216-883-969; 073-938-112-385-378; 074-289-099-974-07X; 075-465-285-579-483; 075-519-293-886-898; 077-455-969-276-663; 078-736-735-476-395; 079-349-286-645-136; 079-885-279-778-204; 083-645-473-035-14X; 083-915-762-724-168; 086-498-058-492-978; 087-408-536-412-018; 087-711-076-115-099; 088-874-304-672-173; 089-606-647-937-546; 092-984-297-325-959; 093-596-358-012-81X; 094-720-052-773-714; 095-466-674-437-104; 096-077-311-058-372; 096-803-870-115-728; 098-430-297-950-379; 099-164-837-383-131; 099-334-901-486-872; 101-173-483-983-547; 101-718-374-273-666; 104-694-342-259-021; 106-052-130-152-052; 106-709-302-557-800; 107-751-267-320-642; 109-525-288-959-67X; 109-822-701-564-796; 110-831-712-365-194; 116-429-844-417-075; 118-307-824-434-940; 121-261-690-038-295; 122-246-600-364-064; 126-359-559-763-114; 132-448-374-033-555; 133-006-378-605-168; 134-448-474-253-195; 135-646-014-794-306; 137-648-461-521-23X; 139-563-108-359-374; 145-898-655-711-084; 151-015-279-636-660; 151-608-461-049-845; 170-387-846-090-147; 171-189-370-546-346; 172-273-670-197-780; 173-235-651-240-332; 184-116-855-210-992; 186-512-780-643-036,7,true,"CC BY, CC BY-NC-ND",gold
108-722-741-642-547,Visually Guided Cooperative Robot Actions Based on Information Quality,,2005,journal article,Autonomous Robots,09295593; 15737527,Springer Science and Business Media LLC,Netherlands,Vivek A. Sujan; Steven Dubowsky,,19,1,89,110,Artificial intelligence; Metric (mathematics); Constraint (information theory); Task (project management); Field (computer science); Computer science; Information quality; Component (UML); Information theory; Robot,,,,,https://link.springer.com/article/10.1007/s10514-005-6013-2 https://link.springer.com/content/pdf/10.1007/s10514-005-6013-2.pdf http://doi.org/10.1007/s10514-005-6013-2 https://dblp.uni-trier.de/db/journals/arobots/arobots19.html#SujanD05 https://doi.org/10.1007/s10514-005-6013-2 http://scripts.mit.edu/~robots/robots/publications/papers/2005_07_Suj_Dub.pdf https://dialnet.unirioja.es/servlet/articulo?codigo=1187675,http://dx.doi.org/10.1007/s10514-005-6013-2,,10.1007/s10514-005-6013-2,2009523931,,1,000-873-713-751-158; 000-878-095-301-047; 001-311-811-596-712; 003-724-771-545-678; 006-756-619-780-96X; 010-423-042-177-734; 011-377-335-026-237; 014-665-440-618-176; 015-104-375-041-337; 015-490-888-427-487; 017-795-547-360-448; 018-212-702-095-699; 019-607-628-727-549; 020-373-501-751-88X; 023-025-291-268-790; 025-274-652-201-875; 025-815-458-443-896; 028-672-941-726-953; 033-077-793-876-064; 039-957-731-915-682; 043-253-159-526-879; 050-975-757-910-254; 052-317-575-761-604; 052-343-129-390-70X; 055-723-984-186-930; 056-242-564-041-099; 058-517-493-677-518; 058-776-477-740-384; 060-006-097-995-335; 062-006-994-347-779; 062-779-893-197-497; 067-552-869-026-407; 067-891-642-402-101; 068-453-360-771-263; 069-703-062-700-403; 070-079-676-494-949; 073-356-791-847-000; 074-114-110-867-941; 082-421-064-042-446; 085-876-803-713-160; 086-496-181-995-491; 095-794-813-276-076; 095-797-100-007-451; 099-804-708-771-069; 101-648-763-046-597; 106-099-514-429-987; 120-452-352-304-664; 122-227-563-647-036; 123-192-450-175-991; 125-632-488-086-23X; 125-924-024-213-979; 130-169-136-291-730; 135-003-614-633-902; 135-998-476-584-476; 143-667-999-764-12X; 145-804-086-994-546; 146-414-474-456-179; 150-535-137-239-785; 159-674-517-377-378; 171-633-484-596-024; 189-627-529-590-973,14,false,,
109-176-046-622-406,Blind guide,2015-08-11,2015,journal article,Journal on Multimodal User Interfaces,17837677; 17838738,Springer Science and Business Media LLC,Germany,Bálint Sövény; Gábor Kovács; Zsolt T. Kardkovács,,9,4,287,297,Computer science; Wearable computer; Visually impaired; Perception; Human–computer interaction; Real-time computing; Computer vision; Embedded system; Neuroscience; Biology,,,,,,http://dx.doi.org/10.1007/s12193-015-0191-6,,10.1007/s12193-015-0191-6,,,0,016-554-897-381-94X; 042-348-220-328-560; 076-121-436-407-96X; 087-272-456-866-28X; 141-489-442-115-69X,11,false,,
109-403-290-836-059,Multi-Sensor Data Fusion Solutions for Blind and Visually Impaired: Research and Commercial Navigation Applications for Indoor and Outdoor Spaces.,2023-06-07,2023,journal article,"Sensors (Basel, Switzerland)",14248220; 14243210,Multidisciplinary Digital Publishing Institute (MDPI),Switzerland,Paraskevi Theodorou; Kleomenis Tsiligkos; Apostolos Meliones,"Several assistive technology solutions, targeting the group of Blind and Visually Impaired (BVI), have been proposed in the literature utilizing multi-sensor data fusion techniques. Furthermore, several commercial systems are currently being used in real-life scenarios by BVI individuals. However, given the rate by which new publications are made, the available review studies become quickly outdated. Moreover, there is no comparative study regarding the multi-sensor data fusion techniques between those found in the research literature and those being used in the commercial applications that many BVI individuals trust to complete their everyday activities. The objective of this study is to classify the available multi-sensor data fusion solutions found in the research literature and the commercial applications, conduct a comparative study between the most popular commercial applications (Blindsquare, Lazarillo, Ariadne GPS, Nav by ViaOpta, Seeing Assistant Move) regarding the supported features as well as compare the two most popular ones (Blindsquare and Lazarillo) with the BlindRouteVision application, developed by the authors, from the standpoint of Usability and User Experience (UX) through field testing. The literature review of sensor-fusion solutions highlights the trends of utilizing computer vision and deep learning techniques, the comparison of the commercial applications reveals their features, strengths, and weaknesses while Usability and UX demonstrate that BVI individuals are willing to sacrifice a wealth of features for more reliable navigation.",23,12,5411,5411,Usability; Sensor fusion; Visually impaired; Global Positioning System; Human–computer interaction; Computer science; Field (mathematics); Screen reader; Strengths and weaknesses; Data science; Artificial intelligence; Telecommunications; Philosophy; Mathematics; Epistemology; Pure mathematics,assistive technologies; comparison analysis; computer vision; deep learning; remote sensing; sensor fusion techniques; usability and user experience,Humans; Blindness; Visually Impaired Persons; Self-Help Devices; Sensory Aids,,,https://www.mdpi.com/1424-8220/23/12/5411/pdf?version=1686908243 https://doi.org/10.3390/s23125411,http://dx.doi.org/10.3390/s23125411,37420578,10.3390/s23125411,,PMC10301813,0,000-281-887-252-27X; 000-551-523-973-321; 003-923-369-046-370; 006-363-788-276-273; 007-202-382-837-877; 007-893-587-247-77X; 008-938-334-809-968; 013-501-956-117-041; 013-647-160-902-302; 015-008-890-486-381; 015-440-630-508-33X; 015-628-872-152-561; 015-714-235-272-975; 017-559-000-135-867; 019-569-358-201-015; 023-281-952-251-359; 025-171-784-787-749; 027-298-301-487-030; 027-924-735-524-754; 028-171-260-452-285; 031-643-780-327-702; 032-661-597-680-28X; 032-841-083-055-581; 033-457-095-812-412; 040-834-320-822-882; 043-196-700-803-427; 043-555-167-620-711; 044-973-971-344-592; 047-271-748-480-72X; 049-915-507-735-342; 050-261-233-769-785; 053-452-893-789-459; 060-147-659-950-016; 061-501-173-208-984; 064-962-460-142-421; 073-393-946-682-259; 074-456-796-365-929; 074-546-616-086-99X; 077-907-290-656-920; 080-161-825-469-284; 080-902-679-821-627; 081-744-667-649-108; 083-722-482-242-606; 090-604-948-038-479; 094-492-344-298-140; 098-910-337-444-079; 099-334-901-486-872; 101-107-146-050-45X; 101-176-260-587-612; 101-295-208-484-052; 105-366-647-059-243; 105-986-387-709-891; 107-248-048-866-831; 109-356-782-692-064; 109-822-701-564-796; 110-887-908-077-277; 111-492-289-873-695; 117-320-452-579-582; 117-749-296-378-35X; 120-599-494-370-890; 125-608-691-280-273; 136-016-112-914-359; 136-920-597-759-54X; 137-395-677-516-752; 161-012-758-615-452; 166-158-315-750-560; 188-270-236-313-299; 189-901-479-982-379; 195-196-708-451-173,4,true,cc-by,gold
109-467-808-338-787,Obstacle Detection and Navigation for The Visually Impaired,2023-03-17,2023,conference proceedings article,2023 9th International Conference on Advanced Computing and Communication Systems (ICACCS),,IEEE,,Deepa J; Maria Adeline P; Sai Madhumita S S; Pavalaselvi N,"Because restrictiveness poses a serious problem for those who are visually impaired, independent living is becoming more and more crucial today. People who are visually impaired have difficulties because they require manual aid to learn about their surroundings. People with visual impairments find it difficult to carry out most of their everyday activities, such as eating, taking a walk, conversing with others, etc., because visual information provides the foundation for many actions. Thanks to technological breakthroughs, it is now possible to assist those who are blind or visually impaired. This study may be a survey of a device that helps the blind by using sensors and voice commands. Finding the distance, we wish to measure is difficult these days. In any event, the tape is a straightforward option, but this type of technology is only susceptible to human error. A rangefinder module had already been created by engineers. The module has a lot of problems, including a distance limit and varying outcomes with different colored barriers, which means we must constantly measure it before using it, they ultimately found. Distances are frequently measured with human inaccuracy. This project's main goal is to precisely calculate the minimum width's distance. This project uses ultrasonic sensors to measure distance measurement. The transferors will reply if you talk. The project value determines the precise distance from any obstacle we want to measure. The device can be used for a variety of purposes, such as robotics, car sensors for avoiding obstructions, and distance estimates in construction camps. The process of creating the gadget was based on as many uses as feasible from university courses, including lab equipment and programs for Microprocessor, Basic Electrical Engineering, Multimedia, and Electronics. The results show that the proposed device offers visually impaired people with higher accessibility, comfort, and simplicity of navigation when compared to the white cane. By using voice commands, this recently developed application can lead the blind.",,,,,Obstacle; Computer science; Measure (data warehouse); Process (computing); Artificial intelligence; Human–computer interaction; Computer vision; Visually impaired; Data mining; Political science; Law; Operating system,,,,,,http://dx.doi.org/10.1109/icaccs57279.2023.10112994,,10.1109/icaccs57279.2023.10112994,,,0,023-309-209-821-636; 027-599-559-972-374; 031-481-010-955-930; 038-074-386-389-072; 043-000-902-861-206; 049-639-325-252-974; 051-004-209-460-867; 061-272-073-396-520; 112-566-962-059-235; 129-622-838-722-010,1,false,,
109-822-701-564-796,Towards Outdoor Navigation System for Visually Impaired People using YOLOv5,2022-01-27,2022,conference proceedings article,"2022 12th International Conference on Cloud Computing, Data Science & Engineering (Confluence)",,IEEE,,Swati Chandna; Abhishek Singhal,"Navigation is one of the many difficulties the Blind and Visually Impaired (BVI) face, be it indoor or outdoor. BVI people use their remaining senses such as smell, touch, and memory to navigate in a familiar environment. But the real problem begins when the environment is new. In the new environment, they seek help from closed ones or use assistive tools. This paper proposes a deep learning based outdoor smartphone navigation system approach which will assist the user in crossing the road by detecting the crosswalks from a mobile device. CNN of Deep learning has been proven to give the best results for image classification. Hence we want to analyze and compare different variants of CNN. As mobile devices have computed limitations, we explore two lightweight state-of-the-art neural networks, namely YOLOv5 and MobileNetSSDV2, for this task. The image dataset of crosswalks for work is collected via web scraping and manual effort. To evaluate the performance of the two neural networks: mAP, loss, and training time of the models are compared. As a result, it is determined that YOLOv5 has a five times faster inference speed than MobileNetSSDv2, which makes YOLOv5 more suitable for real-time applications. This approach utilizes the smartphone camera to detect the crosswalks and provide voice feedback to help people with low vision understand their environment.",,,,,Computer science; Task (project management); Artificial intelligence; Computer vision; Mobile device; Inference; Deep learning; Visually impaired; Human–computer interaction; Face (sociological concept); Engineering; World Wide Web; Social science; Systems engineering; Sociology,,,,,,http://dx.doi.org/10.1109/confluence52989.2022.9734204,,10.1109/confluence52989.2022.9734204,,,0,016-988-681-008-984; 018-828-594-955-588; 045-886-754-687-645; 067-583-626-133-824; 075-519-293-886-898; 098-910-337-444-079; 104-208-613-793-423; 142-105-374-847-047; 180-515-374-647-90X,10,false,,
109-848-382-475-539,When universal access is not quite universal enough: case studies and lessons to be learned,2018-10-16,2018,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Simeon Keates,"While the theory of designing for Universal Access is increasingly understood, there remain persistent issues over realising products and systems that meet the goal of being accessible and usable by the broadest possible set of users. Clearly, products or services that are designed without even considering the needs of the wider user base are implicitly going to struggle to be universally accessible. However, even products that have been designed, knowing that they are to be used by broad user bases frequently still struggle to achieve the ambition of being universally accessible. This paper examines a number of such products that did not achieve, at least initially, the desired level of universal accessibility. Principal recommendations from each case study are presented to provide a guide to common issues to be avoided.",19,1,133,144,Principal (computer security); Set (psychology); Data science; USable; Computer communication networks; Computer science; Base (topology); Universal design,,,,,https://dblp.uni-trier.de/db/journals/uais/uais19.html#Keates20 http://eprints.chi.ac.uk/id/eprint/5618/ https://gala.gre.ac.uk/id/eprint/21080/ https://link.springer.com/article/10.1007/s10209-018-0636-2 https://doi.org/10.1007/s10209-018-0636-2 https://link.springer.com/content/pdf/10.1007/s10209-018-0636-2.pdf https://paperity.org/p/162911710/when-universal-access-is-not-quite-universal-enough-case-studies-and-lessons-to-be https://core.ac.uk/download/195265145.pdf,http://dx.doi.org/10.1007/s10209-018-0636-2,,10.1007/s10209-018-0636-2,2896177675,,0,004-683-323-734-641; 008-475-994-285-09X; 009-938-574-961-754; 010-249-165-435-317; 011-595-486-638-113; 015-971-252-114-378; 018-246-662-133-739; 030-097-179-463-293; 036-506-182-512-909; 039-575-614-900-103; 045-570-745-167-575; 047-832-179-773-684; 047-872-407-476-216; 049-706-499-461-87X; 055-798-387-948-307; 055-800-152-687-050; 059-238-292-281-79X; 064-103-252-975-634; 067-619-952-555-741; 068-647-186-072-728; 070-875-178-006-938; 080-433-968-395-063; 089-002-311-968-886; 092-645-402-737-79X; 096-858-774-845-665; 099-030-629-799-534; 102-356-829-446-553; 105-656-423-996-399; 106-587-883-056-779; 108-564-220-688-450; 108-666-321-386-256; 109-649-162-745-005; 112-984-215-427-585; 119-810-262-786-315; 134-133-274-082-655; 135-935-087-426-003; 154-839-654-493-966; 155-810-537-787-428; 156-411-397-691-026; 169-759-711-071-001; 184-305-932-269-481; 199-938-707-131-042,7,true,cc-by,hybrid
110-207-459-703-743,IP&amp;C - SMAS - Stereovision Mobility Aid System for People with a Vision Impairment,,2010,book,Advances in Intelligent and Soft Computing,18675662; 18675670; 18600794; 16153871,Springer Berlin Heidelberg,United States,Rafał Kozik,"New computer vision solutions dedicated for blind and partially sighted people have been recently introduced as a result of significant progress in computer science. Also the growing computation power of mobile and portable devices together with development of information systems allow to adopt and apply new and robust solutions that are able to work in nearly in a real-time and share and use information spread over IP network. Many of currently developed solutions are dedicated to support the user, giving the information about divert obstacles located in the environment. However many of them are using simple detectors (commonly ultrasonic echo-location) for obstacles tracking without its classification and recognition. Therefore the solution presented in this paper engages the stereo camera and image processing algorithms to facilitate its user with object detection and recognition mechanisms. The inference engine combined together with ontology based problem modeling allows to handle the risk, predict possible user’s moves and provide the user with appropriate set of tips that will eliminate or reduce the discovered risk.",,,315,322,Digital image processing; Human–computer interaction; Artificial intelligence; Ontology (information science); Information system; Stereo camera; Visual Word; Object detection; Mobility aid; Computer vision; Computer science; Inference engine,,,,,http://dblp.uni-trier.de/db/conf/ip-c/ip-c2010.html#Kozik10 https://rd.springer.com/chapter/10.1007/978-3-642-16295-4_36 https://dblp.uni-trier.de/db/conf/ip-c/ip-c2010.html#Kozik10 https://link.springer.com/chapter/10.1007/978-3-642-16295-4_36,http://dx.doi.org/10.1007/978-3-642-16295-4_36,,10.1007/978-3-642-16295-4_36,52878307,,0,011-997-033-854-51X; 030-167-497-654-555; 033-009-617-288-613; 045-635-890-933-496; 047-647-472-033-525; 066-617-423-476-399; 068-864-616-604-642; 095-685-247-420-623; 102-344-932-252-986,5,false,,
110-358-551-243-856,Intelligent Blind-User Navigational Device Utilizing Image Processing,,2023,journal article,IRO Journal on Sustainable Wireless Systems,25823167,Inventive Research Organization,,J Manikandan; Aparna S; Swathi R; Sneha S N,"<jats:p>There are many thousands of blind people in the world who frequently require support. Since the white cane has long been known as an essential component of blind people's navigation, efforts have been made to enhance it by adding new sensors. Blind people are relatively comfortable inside their homes because they can pinpoint where things are, but in the outside world, as the objects are in motion, they face difficulties in locating the objects without an assistance. The research put forth strives to develop a sound navigation and range system for blind people. A real-time obstacle identification has been accomplished using ultrasonic sensors. In order to create a safe path for navigation to a destination, image processing in real-time is used for providing the blind person's perception of the size and distance of the obstacle utilizing raspberry pi for object detection. The suggested method makes use of a camera for recording the video sequence of surroundings, which it subsequently processes using a vocal input from a person who is visually impaired. It takes the location data while taking the (GPS) module for outdoor use. The location detail gathered is transported to the cloud.</jats:p>",5,2,161,174,Obstacle; Computer vision; Computer science; Global Positioning System; Artificial intelligence; Identification (biology); Object (grammar); Image processing; Image (mathematics); Geography; Telecommunications; Botany; Archaeology; Biology,,,,,,http://dx.doi.org/10.36548/jsws.2023.2.007,,10.36548/jsws.2023.2.007,,,0,045-321-254-881-478; 066-037-919-140-345; 066-526-177-885-125; 104-496-157-995-791; 121-350-695-153-190,0,false,,
111-047-696-748-861,Personal Area Network (PAN) Smart Guide for the Blind (PSGB),2023-09-28,2023,book chapter,Computer Vision and Robotics,25247565; 25247573,Springer Nature Singapore,,Suleiman Abdulhakeem; Suleiman Zubair; Bala Alhaji Salihu; Chika Innocent,"One of the biggest problems faced by the visually impaired is navigating from one place to another, be it indoors or outdoors. Furthermore, the adverse conditions of the roads make it even more difficult to walk outdoors. They have to be alert at all times to avoid consequences like colliding with stable or moving objects. In this project, a simple but very efficient and affordable obstacle detection system was designed with the functions of detecting and informing the user about the obstacle detected and also instructs the blind to move to either right or left if obstacle is detected in the front direction. The ultrasonic sensor detects obstacle by sensing the surrounding objects. When the ultrasonic sensor detects obstacle, it sends signal to the Arduino Nano microcontroller for further processing. The microcontroller communicates the real-time data generated by the ultrasonic sensor to the buzzer and MicroSD card adapter. The buzzer makes sound when an obstacle is detected within the safe zone that is already declared within the codes and also audio commands will notify the user of the obstacle detected and guide the user to the obstacle-free path.",,,535,544,Buzzer; Obstacle; Microcontroller; Arduino; Computer science; Ultrasonic sensor; Real-time computing; Computer hardware; Embedded system; Computer vision; Engineering; ALARM; Electrical engineering; Geography; Acoustics; Physics; Archaeology,,,,,,http://dx.doi.org/10.1007/978-981-99-4577-1_44,,10.1007/978-981-99-4577-1_44,,,0,003-976-442-054-457; 027-836-322-726-673; 030-251-661-047-944; 084-621-956-547-983; 138-833-826-742-547; 179-654-625-487-90X,0,false,,
111-110-675-731-429,Computer-Aided Diagnosis-Based Grading Classification of Diabetic Retinopathy Using Deep Graph Correlation Network with IRF,2024-01-22,2024,journal article,SN Computer Science,26618907; 2662995x,Springer Science and Business Media LLC,,Venkata Kotam Raju Poranki; B. Srinivasarao,,5,2,,,Artificial intelligence; Computer science; Pattern recognition (psychology); Blindness; Feature selection; Random forest; Correlation; Classifier (UML); CAD; Diabetic retinopathy; Graph; Deep learning; Grading (engineering); Machine learning; Medicine; Mathematics; Civil engineering; Geometry; Theoretical computer science; Engineering drawing; Optometry; Engineering; Diabetes mellitus; Endocrinology,,,,,,http://dx.doi.org/10.1007/s42979-023-02565-8,,10.1007/s42979-023-02565-8,,,0,002-192-454-260-563; 005-686-963-481-881; 008-918-722-113-382; 014-848-809-831-703; 016-020-079-876-688; 021-817-067-753-639; 024-331-928-689-333; 034-215-985-163-500; 035-950-030-134-464; 036-307-408-514-121; 045-788-038-698-539; 046-299-469-677-018; 047-988-752-442-61X; 066-243-459-466-80X; 075-994-163-672-325; 080-278-917-834-92X; 081-800-842-701-342; 082-819-034-379-005; 089-781-196-339-35X; 098-818-042-601-979; 117-512-573-964-002; 126-248-154-713-424; 126-821-698-065-751; 128-010-799-826-648; 142-455-839-543-060; 142-995-017-988-228; 144-703-199-118-957; 149-439-573-829-110; 198-416-444-604-842,1,false,,
111-122-214-350-752,A rich RGBD images captioning for scene understanding,2024-08-14,2024,journal article,"Signal, Image and Video Processing",18631703; 18631711,Springer Science and Business Media LLC,Germany,Khadidja Delloul; Slimane Larabi,,,,,,,,,,,,http://dx.doi.org/10.1007/s11760-024-03449-x,,10.1007/s11760-024-03449-x,,,0,003-640-818-454-186; 013-892-918-718-760; 019-341-588-702-615; 020-233-013-143-936; 028-917-689-751-497; 044-794-424-743-62X; 046-583-027-625-542; 047-777-721-582-281; 049-584-451-742-16X; 053-418-347-085-156; 056-832-017-451-136; 065-575-150-960-505; 069-147-563-736-864; 069-480-623-909-181; 072-832-627-910-440; 073-913-224-311-778; 079-022-391-882-614; 080-642-014-897-952; 092-512-411-641-336; 100-055-707-534-748; 104-191-095-173-903; 109-844-104-326-949; 122-016-983-638-814; 130-708-723-862-702; 143-592-807-654-003; 147-682-704-907-922; 167-093-910-171-645; 168-127-824-466-622; 184-868-564-140-471; 188-899-041-470-436,0,false,,
111-374-087-817-321,Beyond the Cane: Describing Urban Scenes to Blind People for Mobility Tasks.,2022-08-19,2022,journal article,ACM transactions on accessible computing,19367228; 19367236,Association for Computing Machinery (ACM),United States,Karst M P Hoogsteen; Sarit Szpiro; Gabriel Kreiman; Eli Peli,"Blind people face difficulties with independent mobility, impacting employment prospects, social inclusion, and quality of life. Given the advancements in computer vision, with more efficient and effective automated information extraction from visual scenes, it is important to determine what information is worth conveying to blind travelers, especially since people have a limited capacity to receive and process sensory information. We aimed to investigate <i>which</i> objects in a street scene are useful to describe and <i>how</i> those objects should be described. Thirteen cane-using participants, five of whom were early blind, took part in two urban walking experiments. In the first experiment, participants were asked to voice their information needs in the form of questions to the experimenter. In the second experiment, participants were asked to score scene descriptions and navigation instructions, provided by the experimenter, in terms of their usefulness. The descriptions included a variety of objects with various annotations per object. Additionally, we asked participants to rank order the objects and the different descriptions per object in terms of priority and explain why the provided information is or is not useful to them. The results reveal differences between early and late blind participants. Late blind participants requested information more frequently and prioritized information about objects' locations. Our results illustrate how different factors, such as the level of detail, relative position, and what type of information is provided when describing an object, affected the usefulness of scene descriptions. Participants explained how they (indirectly) used information, but they were frequently unable to explain their ratings. The results distinguish between various types of travel information, underscore the importance of featuring these types at multiple levels of abstraction, and highlight gaps in current understanding of travel information needs. Elucidating the information needs of blind travelers is critical for the development of more useful assistive technologies.",15,3,1,29,Orientation and Mobility; Object (grammar); Variety (cybernetics); Computer science; Psychology; Rank (graph theory); Process (computing); Cognitive psychology; Human–computer interaction; Artificial intelligence; Visually impaired; Mathematics; Combinatorics; Operating system,Blindness; assistive technologies; impaired vision; independence; mobility; navigation; outdoor; scene description,,,NEI NIH HHS (P30 EY003790) United States; NEI NIH HHS (R01 EY026025) United States; NEI NIH HHS (R21 EY032180) United States,,http://dx.doi.org/10.1145/3522757,36148267,10.1145/3522757,,PMC9491388,0,000-659-254-096-600; 000-876-701-854-188; 002-391-643-288-41X; 003-517-682-660-226; 008-564-581-653-709; 008-569-263-704-972; 014-423-869-181-577; 015-729-471-527-230; 017-734-135-554-60X; 018-654-826-336-966; 018-782-154-401-919; 020-585-547-019-445; 025-306-351-240-479; 025-448-114-338-062; 030-429-262-401-518; 030-694-949-297-70X; 040-995-936-965-942; 041-958-810-244-078; 044-093-096-610-606; 046-262-856-533-736; 048-301-582-552-287; 049-715-443-644-102; 051-194-122-391-125; 051-289-097-618-069; 053-872-980-515-457; 054-154-174-839-713; 055-466-774-024-744; 059-713-590-199-380; 060-062-058-323-460; 068-163-377-582-711; 068-763-040-241-128; 077-625-440-231-421; 077-828-420-658-663; 079-433-787-480-087; 080-411-167-059-855; 087-272-456-866-28X; 088-470-436-866-921; 089-738-656-495-377; 090-567-303-142-789; 109-525-288-959-67X; 109-583-029-153-984; 110-650-958-929-874; 122-704-318-024-950; 124-235-270-500-041; 129-948-693-450-519; 136-732-042-076-069; 139-267-595-049-632; 140-091-865-029-447; 141-355-454-562-258; 141-489-442-115-69X; 166-405-745-827-631; 167-659-009-486-214; 173-490-682-739-038; 173-658-701-117-40X,6,true,,green
111-896-448-632-436,Attitudes of potential recipients toward emerging visual prosthesis technologies.,2023-07-06,2023,journal article,Scientific reports,20452322,Springer Science and Business Media LLC,United Kingdom,Vicky Karadima; Elizabeth A Pezaris; John S Pezaris,"With the advent of multiple visual prosthesis devices to treat blindness, the question of how potential patients view such interventions becomes important in order to understand the levels of expectation and acceptance, and the perceived risk-reward balance across the different device approaches. Building on previous work on single device approaches done with blind individuals in Chicago and Detroit, USA, Melbourne, Australia, and Bejing, China, we investigated attitudes in blind individuals in Athens, Greece with coverage expanded to three of the contemporary approaches, Retinal, Thalamic, and Cortical. We presented an informational lecture on the approaches, had potential participants fill out a preliminary Questionnaire 1, then organized selected subjects into focus groups for guided discussion on visual prostheses, and finally had these subjects fill out a more detailed Questionnaire 2. We report here the first quantitative data that compares multiple prosthesis approaches. Our primary findings are that for these potential patients, perceived risk continues to outweigh perceived benefits, with the Retinal approach having the least negative overall impression and the Cortical approach the most negative. Concerns about the quality of restored vision were primary. Factors that drove the choice of hypothetical participation in a clinical trial were age and years of blindness. Secondary factors focused on positive clinical outcomes. The focus groups served to swing the impressions of each approach from neutrality toward the extremes of a Likert scale, and shifted the overall willingness to participate in a clinical trial from neutral to negative. These results, coupled with informal assessment of audience questions after the informational lecture, suggest that a substantial improvement in performance over currently available devices will be necessary before visual prostheses gain wide acceptance.",13,1,10963,,Visual prosthesis; Blindness; Likert scale; Psychology; Psychological intervention; Focus group; Medicine; Applied psychology; Optometry; Psychiatry; Developmental psychology; Marketing; Ophthalmology; Retinal; Business,,"Humans; Visual Prosthesis; Vision, Ocular; Blindness/therapy; Prosthesis Implantation; Attitude",,William M. Wood Foundation,https://www.nature.com/articles/s41598-023-36913-8.pdf https://doi.org/10.1038/s41598-023-36913-8,http://dx.doi.org/10.1038/s41598-023-36913-8,37414798,10.1038/s41598-023-36913-8,,PMC10325978,0,000-820-862-296-831; 001-965-322-482-776; 005-167-819-781-818; 005-207-439-987-578; 005-553-361-556-816; 005-770-350-672-206; 005-879-697-557-185; 006-894-570-162-874; 007-499-222-301-387; 007-534-193-447-037; 007-780-369-755-636; 011-553-324-834-730; 013-454-845-759-712; 014-355-665-073-740; 015-051-314-900-103; 016-248-155-042-687; 019-656-881-052-451; 020-276-868-405-911; 022-345-934-517-145; 023-766-657-932-341; 025-606-903-509-267; 026-204-328-399-384; 026-213-053-979-076; 026-695-615-342-224; 027-594-367-109-792; 027-739-438-047-811; 028-305-272-187-475; 029-508-095-621-707; 030-429-262-401-518; 030-457-611-554-191; 030-492-879-869-083; 032-680-937-011-669; 033-525-139-736-901; 036-040-179-719-288; 036-251-058-846-635; 036-328-012-763-677; 036-741-429-791-04X; 037-736-107-056-915; 039-372-348-801-398; 040-586-055-717-909; 041-542-174-952-057; 042-125-442-624-332; 048-474-047-657-437; 049-749-673-971-85X; 050-017-473-526-702; 050-118-932-626-132; 056-607-063-057-083; 059-137-656-628-625; 060-062-058-323-460; 063-048-719-705-828; 065-208-410-473-97X; 068-333-046-011-446; 071-665-101-637-935; 072-392-131-280-899; 075-212-643-012-810; 075-339-259-431-349; 077-451-387-340-188; 079-047-627-150-809; 079-228-979-733-12X; 103-988-093-816-014; 110-376-091-251-888; 123-701-661-025-523; 126-371-351-117-262; 136-679-218-505-278; 140-592-954-090-931; 142-783-797-933-724; 154-728-099-059-156; 160-128-183-468-145; 163-084-139-784-877,3,true,"CC BY, CC BY-NC-ND",gold
112-184-049-947-220,Object Localization Assistive System Based on CV and Vibrotactile Encoding.,2022-07-11,2022,journal article,Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference,26940604; 23757477,,United States,Zhikai Wei; Aiguo Song; Xuhui Hu,"Intelligent assistive systems can navigate blind people, but most of them could only give non-intuitive cues or inefficient guidance. Based on computer vision and vibrotactile encoding, this paper presents an interactive system that provides blind people with intuitive spatial cognition. Different from the traditional auditory feedback strategy based on speech cues, this paper firstly introduces a vibration-encoded feedback method that leverages the haptic neural pathway and enables the users to interact with objects other than manipulating an assistance device. Based on this strategy, a wearable visual module based on an RGB-D camera is adopted for 3D spatial object localization, which contributes to accurate perception and quick object localization in the real environment. The experimental results on target blind individuals indicate that vibrotactile feedback reduces the task completion time by over 25% compared with the mainstream voice prompt feedback scheme. The proposed object localization system provides a more intuitive spatial navigation and comfortable wearability for blindness assistance.",2022,,2882,,Computer science; Encoding (memory); Computer vision; Object (grammar); Wearable computer; Haptic technology; Artificial intelligence; Human–computer interaction; Perception; Auditory feedback; Interface (matter); Scheme (mathematics); Embedded system; Mathematical analysis; Mathematics; Bubble; Neuroscience; Maximum bubble pressure method; Parallel computing; Biology,,"Blindness; Feedback; Feedback, Sensory; Humans; Vision, Ocular; Visually Impaired Persons",,National Natural Science Foundation of China,https://arxiv.org/pdf/2206.09432 https://arxiv.org/abs/2206.09432,http://dx.doi.org/10.1109/embc48229.2022.9871382,36086052,10.1109/embc48229.2022.9871382,,,0,009-756-076-796-541; 041-644-601-112-907; 047-844-248-722-855; 060-758-780-724-275; 065-983-543-415-033; 066-476-237-408-046; 111-772-107-673-112; 122-704-318-024-950,2,true,,green
112-849-097-976-249,Development of a Wearable Computer Orientation System,2002-02-01,2002,journal article,Personal and Ubiquitous Computing,16174909; 16174917,Springer Science and Business Media LLC,Germany,David Ross; Bruce B. Blasch,,6,1,49,63,Human–computer interaction; Variety (cybernetics); Mobile computing; Wearable computer; Flexibility (engineering); Severe visual impairment; Computer science; Orientation (computer vision),,,,,https://dblp.uni-trier.de/db/journals/puc/puc6.html#RossB02 https://dl.acm.org/doi/10.1007/s007790200005 https://link.springer.com/article/10.1007%2Fs007790200005,http://dx.doi.org/10.1007/s007790200005,,10.1007/s007790200005,2020864137,,2,022-686-712-325-83X; 028-357-147-126-416; 030-344-810-220-767; 037-452-515-170-720; 047-926-647-780-322; 077-352-552-120-40X; 111-362-297-156-092; 133-238-762-604-971; 141-212-757-762-718; 142-426-241-123-210; 183-935-933-940-273,58,false,,
112-876-946-309-787,A novel hardware plane fitting implementation and applications for bionic vision,2016-04-26,2016,journal article,Machine Vision and Applications,09328092; 14321769,Springer Science and Business Media LLC,Germany,Horace Josh; Lindsay Kleeman,,27,7,967,982,Gate array; Pattern recognition (psychology); Artificial intelligence; Pixel; Plane (geometry); Object detection; Computer vision; Frame rate; Computer science; Field-programmable gate array; PCI Express; Computer hardware,,,,Australian Research Council,https://link.springer.com/article/10.1007/s00138-016-0764-8/fulltext.html https://research.monash.edu/en/publications/a-novel-hardware-plane-fitting-implementation-and-applications-fo https://dblp.uni-trier.de/db/journals/mva/mva27.html#JoshK16 https://link.springer.com/article/10.1007/s00138-016-0764-8,http://dx.doi.org/10.1007/s00138-016-0764-8,,10.1007/s00138-016-0764-8,2342904406,,0,000-393-021-102-613; 001-471-631-263-788; 002-352-012-178-407; 012-125-195-504-780; 016-771-910-621-508; 016-796-580-055-240; 021-752-297-194-209; 028-303-905-295-179; 031-702-469-587-138; 035-168-465-799-975; 040-466-369-452-628; 047-952-492-806-979; 048-308-759-694-056; 052-104-972-727-451; 053-524-950-079-147; 061-410-176-684-229; 063-048-719-705-828; 063-239-439-088-070; 064-069-666-882-597; 072-603-939-753-887; 083-196-450-895-103; 089-315-089-887-010; 090-534-178-360-136; 093-069-788-989-968; 106-921-167-815-763; 111-606-904-961-447; 113-338-838-168-17X; 115-038-203-045-852; 120-224-001-278-543; 123-043-120-011-485; 133-255-868-175-698; 139-782-944-444-638; 141-152-175-249-066; 144-732-801-553-701; 167-442-601-531-195; 173-788-691-233-436; 187-417-651-610-052; 198-602-864-367-379,3,false,,
112-900-751-936-18X,Oral presentations,2009-08-20,2009,journal article,Cognitive Processing,16124782; 16124790,Springer Science and Business Media LLC,Germany,,,10,S2,148,165,Psychology; Behavioural sciences; Psychotherapist,,,,,,http://dx.doi.org/10.1007/s10339-009-0327-2,,10.1007/s10339-009-0327-2,,,0,,0,false,,
113-018-601-425-331,W4A - Impact of Expertise on Interaction Preferences for Navigation Assistance of Visually Impaired Individuals,2019-05-13,2019,book,Proceedings of the 16th International Web for All Conference,,ACM,,Dragan Ahmetovic; João Guerreiro; Eshed Ohn-Bar; Kris M. Kitani; Chieko Asakawa,"Navigation assistive technologies have been designed to support individuals with visual impairments during independent mobility by providing sensory augmentation and contextual awareness of their surroundings. Such information is habitually provided through predefned audio-haptic interaction paradigms. However, individual capabilities, preferences and behavior of people with visual impairments are heterogeneous, and may change due to experience, context and necessity. Therefore, the circumstances and modalities for providing navigation assistance need to be personalized to different users, and through time for each user. We conduct a study with 13 blind participants to explore how the desirability of messages provided during assisted navigation varies based on users' navigation preferences and expertise. The participants are guided through two different routes, one without prior knowledge and one previously studied and traversed. The guidance is provided through turn-by-turn instructions, enriched with contextual information about the environment. During navigation and follow-up interviews, we uncover that participants have diversifed needs for navigation instructions based on their abilities and preferences. Our study motivates the design of future navigation systems capable of verbosity level personalization in order to keep the users engaged in the current situational context while minimizing distractions.",,,1,9,Human–computer interaction; Personalization; Verbosity; Modalities; Context (language use); Contextual information; Navigation assistance; Visually impaired; Computer science; Turn-by-turn navigation,,,,,https://dl.acm.org/doi/pdf/10.1145/3315002.3317561 https://doi.org/10.1145/3315002.3317561 https://air.unimi.it/handle/2434/697951 https://dblp.uni-trier.de/db/conf/w4a/w4a2019.html#AhmetovicGOKA19 https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_3261906,http://dx.doi.org/10.1145/3315002.3317561,,10.1145/3315002.3317561,2966896132,,0,000-501-534-285-334; 005-497-078-814-142; 005-991-142-819-380; 006-792-447-761-719; 009-142-401-900-87X; 010-719-927-108-990; 012-499-653-654-505; 013-056-435-015-274; 016-209-364-096-807; 019-150-105-799-327; 019-828-744-033-99X; 026-450-734-929-647; 028-041-236-528-221; 030-002-779-817-417; 031-175-485-676-104; 033-940-482-248-436; 033-947-592-266-127; 035-101-435-747-318; 035-395-409-643-765; 040-940-319-245-132; 042-547-247-654-317; 045-673-549-227-819; 058-347-570-370-829; 061-014-638-937-565; 061-978-196-805-368; 063-388-850-988-043; 073-829-360-441-444; 073-938-112-385-378; 074-272-058-786-709; 075-827-600-908-546; 082-060-789-804-822; 089-188-444-759-648; 089-864-554-912-747; 095-008-123-692-063; 100-302-280-724-192; 101-301-179-485-803; 101-969-663-263-663; 102-795-246-835-316; 103-185-330-791-684; 136-680-482-487-214; 139-233-304-780-009; 139-812-238-822-149; 142-261-123-140-527; 151-015-279-636-660; 167-521-103-760-249; 173-490-682-739-038,21,false,,
113-650-376-132-020,Human navigation ability: Tests of the encoding-errormodel of path integration,,1999,journal article,Spatial Cognition and Computation,13875868,Springer Science and Business Media LLC,United States,Roberta L. Klatzky; Andrew C. Beall; Jack M. Loomis; Reginald G. Golledge; John W. Philbeck,,1,1,31,65,Sensory cue; Algorithm; Encoding (memory); Position (vector); Artificial intelligence; Fujita scale; Context (language use); Generality; Computer science; Representation (mathematics); Path integration,,,,,https://dblp.uni-trier.de/db/journals/scc/scc1.html#KlatzkyBLGP99 https://link.springer.com/article/10.1023/A:1010061313300 http://www.geog.ucsb.edu/pgs/papers/basic4.pdf,http://dx.doi.org/10.1023/a:1010061313300,,10.1023/a:1010061313300,2099866819,,0,001-168-395-097-068; 003-055-508-972-538; 004-383-979-528-205; 005-181-094-163-481; 014-015-592-708-563; 016-795-216-845-76X; 017-898-756-199-461; 024-187-710-701-703; 024-559-392-427-44X; 025-771-457-853-883; 031-922-513-835-616; 032-269-204-936-963; 033-591-124-833-00X; 035-864-289-678-662; 037-703-101-538-576; 040-690-288-101-937; 044-494-480-311-659; 048-766-163-674-407; 050-658-840-600-100; 054-356-941-892-685; 056-647-085-401-056; 058-468-795-176-403; 059-084-198-148-336; 061-233-905-431-623; 065-167-056-390-072; 069-312-296-054-776; 084-193-570-203-610; 097-145-406-647-709; 099-213-725-162-623; 104-886-137-101-66X; 112-375-165-695-472; 115-425-430-095-93X; 145-989-140-387-645; 162-527-547-301-284; 169-924-490-744-300; 174-901-049-262-975,60,false,,
113-764-349-033-065,Obstacle-Free Pathway Detection by Means of Depth Maps,2010-11-03,2010,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Nuria Ortigosa; Samuel Morillas; Guillermo Peris-Fajarnés,,63,1,115,129,Image (mathematics); Depth map; Artificial intelligence; Pixel; Linear model; Matching (graph theory); Obstacle; Computer vision; Computer science,,,,,https://dialnet.unirioja.es/servlet/articulo?codigo=3657787 https://link.springer.com/content/pdf/10.1007%2Fs10846-010-9498-4.pdf https://dblp.uni-trier.de/db/journals/jirs/jirs63.html#OrtigosaMP11 https://dl.acm.org/doi/10.1007/s10846-010-9498-4 https://link.springer.com/article/10.1007/s10846-010-9498-4,http://dx.doi.org/10.1007/s10846-010-9498-4,,10.1007/s10846-010-9498-4,2166492906,,5,003-292-570-197-359; 005-517-773-959-947; 011-978-415-308-553; 016-475-171-211-946; 021-534-133-705-636; 024-057-556-391-907; 025-079-654-137-228; 028-436-680-232-191; 029-326-112-192-207; 029-651-909-583-968; 030-167-497-654-555; 031-205-347-767-125; 031-769-951-373-070; 032-140-807-409-68X; 039-110-125-194-56X; 039-429-854-098-345; 040-892-740-649-890; 044-938-649-488-121; 049-478-900-413-179; 052-949-561-819-671; 055-996-671-896-089; 058-350-834-986-051; 059-514-966-557-843; 066-374-042-552-286; 067-185-325-949-041; 070-039-890-294-570; 076-259-207-751-275; 076-853-538-573-049; 078-747-544-702-234; 079-570-504-854-335; 087-272-456-866-28X; 087-624-055-791-527; 089-547-813-246-576; 090-151-530-806-670; 099-358-123-258-633; 101-840-700-415-260; 108-504-953-453-881; 108-948-730-939-814; 109-523-081-131-729; 122-391-692-522-621; 131-715-028-811-075; 134-287-245-306-72X; 143-359-898-565-039; 145-316-996-433-892; 153-697-872-076-389; 185-266-364-086-800; 193-123-800-849-774; 195-210-909-960-60X; 199-425-332-224-176,13,false,,
114-709-618-489-242,Wearable Navigation Assistance - A Tool for the Blind,2005-05-15,2005,journal article,Measurement Science Review,13358871,Slovak Academy of Sciences - Inst. Measurement Science,Slovakia,F. van der Heijden; Paulus P.L. Regtien,"This paper describes the system architecture for a navigation tool for visually impaired persons. The major parts are: a multi-sensory system (comprising stereo vision, acoustic range finding and movement sensors), a mapper, a warning system and a tactile human-machine interface. The sensory parts are described in more detail, and the first experimental results are presented.",5,5,53,56,Navigational instrument; Interface (computing); Engineering; Wearable computer; Artificial intelligence; Warning system; Navigation assistance; Computer vision; Stereopsis; Sensory system; Systems architecture,,,,,http://doc.utwente.nl/54625/ https://www.narcis.nl/publication/RecordID/oai%3Aris.utwente.nl%3Apublications%2F634a5da7-526c-48df-9753-7c9fddd7ee45 https://research.utwente.nl/en/publications/wearable-navigation-assistance-a-tool-for-the-blind https://ris.utwente.nl/ws/files/6541207/Heijden05wearable.pdf http://www.measurement.sk/2005/S2/Heijden.pdf,https://www.narcis.nl/publication/RecordID/oai%3Aris.utwente.nl%3Apublications%2F634a5da7-526c-48df-9753-7c9fddd7ee45,,,2120375713,,0,030-127-369-356-741; 045-321-254-881-478; 065-992-777-809-97X; 107-675-121-223-290,8,true,cc-by-nc-nd,gold
114-916-338-500-357,Bank note recognition for the vision impaired.,,2006,journal article,Australasian physical & engineering sciences in medicine,01589938; 18795447,Springer Science and Business Media LLC,Germany,AL Hinwood; P. Preston; Gregg J. Suaning; Nigel H. Lovell,,29,2,229,233,Human–computer interaction; Statistical classification; Currency; Project commissioning; Usability; Braille; USable; Bank note; Computer science; Multimedia; Reflection (computer programming),,"Algorithms; Artificial Intelligence; Australia; Commerce/instrumentation; Electronic Data Processing; Equipment Design; Equipment Failure Analysis; Pattern Recognition, Automated/methods; Sensory Aids; Vision Disorders/rehabilitation",,,https://www.ncbi.nlm.nih.gov/pubmed/16845929 https://link.springer.com/article/10.1007/BF03178897 https://link.springer.com/content/pdf/10.1007/BF03178897.pdf,http://dx.doi.org/10.1007/bf03178897,16845929,10.1007/bf03178897,2057090793,,0,010-587-737-301-78X; 025-838-847-730-663; 037-762-384-203-775; 094-336-360-386-20X; 110-694-011-140-381; 146-446-682-779-446; 170-623-507-470-34X,24,false,,
115-053-089-097-274,SeeWay: Vision-Language Assistive Navigation for the Visually Impaired,2022-10-09,2022,conference proceedings article,"2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",,IEEE,,Zongming Yang; Liang Yang; Liren Kong; Ailin Wei; Jesse Leaman; Johnell Brooks; Bing Li,"Assistive navigation for blind or visually impaired (BVI) individuals is of significance to extend their mobility and safety in traveling, enhancing their employment opportunities and fostering personal fulfillment. Conventional research is mainly based on robotic navigation approaches through localization, mapping, and path planning frameworks. They require heavy manual annotation of semantic information in maps and its alignment with sensor mapping. Inspired by the fact that we human beings naturally rely on language instruction inquiry and visual scene understanding to navigate in an unfamiliar environment, this paper proposes a novel vision-language model-based approach for BVI navigation. It does not need heavy-labeled indoor maps and provides a Safe and Efficient E-Wayfinding (SeeWay) assistive solution for BVI individuals. The system consists of a scene-graph map construction module, a navigation path generation module for global path inference by vision-language navigation (VLN), and a navigation with obstacle avoidance module for real-time local navigation. The SeeWay system was deployed on portable iPhone devices with cloud computing assistance for the VLN model inference. The field tests show the effectiveness of the VLN global path finding and local path re-planning. Experiments and quantitative results reveal that heuristic-style instruction outperforms direction/detailed-style instructions for VLN success rate (SR), and the SR decreases as the navigation length increases.",,,,,Computer science; Obstacle avoidance; Mobile robot navigation; Computer vision; Navigation system; Motion planning; Human–computer interaction; Artificial intelligence; Path (computing); Heuristic; Language understanding; Obstacle; Inference; Mobile robot; Robot; Law; Political science; Robot control; Programming language,,,,U.S. Department of Transportation,,http://dx.doi.org/10.1109/smc53654.2022.9945087,,10.1109/smc53654.2022.9945087,,,0,001-082-598-947-731; 001-325-059-112-077; 001-360-928-241-406; 001-443-399-861-469; 006-331-277-518-553; 007-223-103-783-187; 007-725-872-437-764; 007-907-758-104-429; 009-896-970-982-095; 013-839-889-491-903; 016-645-836-018-745; 016-988-681-008-984; 017-499-516-520-553; 020-233-013-143-936; 024-163-418-784-234; 031-298-073-564-034; 035-241-773-225-947; 035-482-287-076-98X; 041-099-847-328-950; 053-094-537-530-97X; 061-498-680-791-452; 064-053-186-811-574; 064-964-065-389-714; 073-898-178-314-544; 075-447-719-639-633; 083-915-762-724-168; 084-127-655-567-768; 084-197-601-345-91X; 092-374-505-118-21X; 105-197-645-425-321; 105-278-566-106-933; 109-606-935-509-670; 135-253-843-368-438,3,false,,
115-360-392-890-988,Character detection and recognition system for visually impaired people,,2016,conference proceedings article,"2016 IEEE International Conference on Recent Trends in Electronics, Information & Communication Technology (RTEICT)",,IEEE,,Akhilesh A. Panchal; Shrugal Varde; M. S. Panse,"Nowadays, increasing use of digital technology, availability of economical image capturing devices like mobile phones, digital cameras, etc. and need of powerful technology to aid blind or visually impaired people, attracting researchers to the problem of recognizing text in images. Detecting text from scene image is more difficult as compared to that from printed documents. Lots of research has been done on detecting scene text to overcome certain challenges like perspective distortion, aspect ratio, font size, etc. Speed, complexity, cost and accuracy are important parameters must be taken into consideration while designing such systems. Computer vision is one of the emerging technologies that can be used to aid visually impaired people for navigation (both indoor and outdoor), accessing printed material, etc. This paper describes an approach to extract and recognize text from scene images effectively using computer vision technology and to convert recognized text into speech so that it can be incorporated with hardware to develop Electronic travel aid for visually impaired people in future.",,,1492,1496,Character (computing); Emerging technologies; Perspective distortion; Recognition system; Text recognition; Character recognition; Visually impaired; Computer science; Multimedia,,,,,http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7808080 http://xplorestaging.ieee.org/ielx7/7792522/7807761/07808080.pdf?arnumber=7808080,http://dx.doi.org/10.1109/rteict.2016.7808080,,10.1109/rteict.2016.7808080,2576543438,,0,012-295-501-771-53X; 016-109-180-624-881; 026-736-903-667-021; 046-906-287-427-917; 061-970-343-435-197; 078-896-872-259-98X; 092-391-305-190-481; 162-977-678-711-02X; 187-880-422-425-077; 192-346-188-739-916,15,false,,
115-537-101-926-057,Exploring the role of artificial intelligence and inclusive technologies during navigation-based tasks for individuals who are blind or who have low vision: Future directions and priorities.,2023-12-07,2023,preprint,,,Research Square Platform LLC,,Natalina Martiniello; Maxime Bleau; Nathalie Gingras-Royer; Catherine Tardif-Bernier; Joseph Paul Nemargut,"<jats:title>Abstract</jats:title>;         <jats:p>Background;  Mainstream smartphone applications are increasingly replacing the use of traditional visual aids (such as hand-held telescopes) to facilitate independent travel for individuals who are blind or who have low vision.; Objective;  The goal of this study was to explore the navigation-based apps used by individuals who are blind or low vision, the factors influencing these decisions, and perceptions about gaps to address future needs.; Methods;  An international online survey was conducted with 139 participants who self-identified as blind or low vision (78 women, 52 men) between the ages of 18 and 76.; Results;  Findings indicate that the decision to use an app based on artificial intelligence versus live video assistance is related to whether the task is dynamic or static in nature. Younger participants and those who are congenitally blind are significantly more likely to employ apps during independent travel. Although a majority of participants rely on apps only during unfamiliar routes (60.91%), apps are shown to supplement rather than replace traditional tools such as the white cane and dog guide. Participants underscore the need for future apps to better assist with indoor navigation and to provide more precise information about points of interest.; Conclusions;  These results provide vital insights for rehabilitation professionals who support the growing population of clients with acquired and age-related vision loss, by clarifying the factors to consider when selecting apps for navigation-based needs. As additional technology-based solutions are developed, it is essential that blind and low vision individuals, including rehabilitation professionals, are meaningfully included within design.</jats:p>",,,,,Perception; Psychology; Applied psychology; Mainstream; Population; Task (project management); Internet privacy; Computer science; Medicine; Engineering; Philosophy; Theology; Environmental health; Systems engineering; Neuroscience,,,,,https://www.researchsquare.com/article/rs-3715501/latest.pdf https://doi.org/10.21203/rs.3.rs-3715501/v1,http://dx.doi.org/10.21203/rs.3.rs-3715501/v1,,10.21203/rs.3.rs-3715501/v1,,,0,000-174-432-609-806; 004-756-256-314-376; 010-475-144-749-494; 011-925-783-478-178; 012-157-893-668-99X; 015-008-890-486-381; 021-406-453-523-085; 028-681-663-751-108; 030-090-261-408-54X; 031-842-427-416-191; 045-147-638-207-270; 049-547-774-794-973; 076-432-613-148-718; 077-710-519-038-356; 082-349-890-682-513; 083-915-762-724-168; 110-739-125-146-868; 113-246-601-846-384; 115-580-485-882-770; 141-649-000-612-322; 142-242-828-123-205,0,true,cc-by,green
116-024-910-234-854,Modelling and Forecasting Customer Navigation in Intelligent Retail Environments,2017-10-06,2017,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Marina Paolanti; Daniele Liciotti; Rocco Pietrini; Adriano Mancini; Emanuele Frontoni,,91,2,165,180,Timestamp; Engineering; Test case; Tracking system; Road map; Hidden Markov model; Aisle; Business hours; Mechatronics; Simulation; Database,,,,,https://dblp.uni-trier.de/db/journals/jirs/jirs91.html#PaolantiLPMF18 https://dialnet.unirioja.es/servlet/articulo?codigo=6659496 https://link.springer.com/article/10.1007/s10846-017-0674-7,http://dx.doi.org/10.1007/s10846-017-0674-7,,10.1007/s10846-017-0674-7,2760827267,,2,001-304-173-708-963; 004-181-928-163-087; 004-677-140-746-251; 038-439-322-699-484; 045-719-596-575-147; 049-811-912-748-28X; 049-995-232-191-51X; 050-342-545-630-349; 056-544-429-544-711; 057-799-780-932-261; 059-961-093-293-993; 061-631-962-812-047; 078-143-207-328-395; 080-090-474-120-084; 086-180-902-518-177; 089-860-625-817-272; 092-974-482-031-064; 093-311-354-777-450; 094-304-665-947-702; 095-462-416-595-910; 099-106-692-524-427; 099-126-248-637-917; 099-563-507-375-110; 101-952-091-237-871; 108-690-815-803-494; 115-361-630-219-255; 115-625-294-000-954; 116-654-379-062-707; 127-001-274-907-428; 127-744-944-480-150; 135-842-762-367-375; 145-316-230-145-555; 158-480-921-448-391; 161-777-827-484-092; 173-120-808-721-442; 187-539-865-312-443,45,false,,
116-201-183-198-774,基于Transformer的强化学习方法在智能决策领域的应用: 综述,2024-07-05,2024,journal article,Frontiers of Information Technology & Electronic Engineering,20959184; 20959230,Zhejiang University Press,United States,Weilin Yuan; Jiaxing Chen; Shaofei Chen; Dawei Feng; Zhenzhen Hu; Peng Li; Weiwei Zhao,,25,6,763,790,Materials science,,,,,,http://dx.doi.org/10.1631/fitee.2300548,,10.1631/fitee.2300548,,,0,000-406-256-405-942; 001-587-115-112-072; 001-635-128-099-405; 002-877-141-120-203; 003-395-501-264-433; 003-536-411-571-444; 004-683-780-527-058; 006-331-277-518-553; 006-831-203-475-141; 007-583-154-053-159; 007-612-541-518-745; 010-097-025-559-326; 010-796-783-730-711; 011-997-931-224-237; 013-400-716-063-280; 015-011-888-895-810; 015-532-771-631-962; 016-153-169-123-814; 016-863-159-548-997; 020-233-013-143-936; 021-896-779-935-682; 022-591-994-767-30X; 023-292-645-226-026; 023-796-954-241-149; 023-899-995-937-013; 027-222-309-605-600; 027-243-816-808-325; 027-261-958-641-419; 030-039-123-827-718; 031-132-424-712-255; 037-259-372-042-929; 037-375-653-803-446; 037-570-666-411-496; 037-775-590-264-897; 037-816-721-620-424; 037-962-931-104-650; 038-400-783-502-762; 038-885-452-506-388; 041-685-665-668-876; 043-062-553-419-762; 043-397-496-433-06X; 044-641-867-151-714; 047-292-675-710-792; 048-328-240-405-527; 052-543-951-776-208; 053-094-537-530-97X; 054-703-626-385-935; 057-608-206-274-685; 058-090-904-556-586; 066-305-886-229-375; 067-735-883-427-815; 069-341-547-135-195; 077-002-338-202-500; 077-592-082-075-845; 078-211-985-074-717; 078-602-371-014-70X; 079-623-477-127-55X; 080-455-695-912-500; 080-707-962-091-574; 081-967-438-848-419; 082-178-119-516-788; 088-218-627-274-625; 099-139-258-994-953; 099-300-141-367-188; 101-268-151-629-586; 102-634-720-978-692; 103-636-822-845-730; 105-518-519-430-394; 108-430-388-485-135; 111-642-491-385-477; 113-769-596-056-208; 119-975-477-774-35X; 120-047-640-592-980; 120-080-788-501-680; 120-368-003-425-888; 121-214-643-131-17X; 124-045-183-127-384; 125-121-282-391-007; 126-658-507-697-995; 127-612-751-020-436; 133-322-182-900-514; 139-807-420-872-832; 140-528-432-362-446; 143-661-742-359-228; 146-802-582-361-569; 155-541-515-676-361; 158-651-279-838-117; 161-702-480-894-802; 162-684-138-027-076; 162-774-079-791-419; 170-885-374-418-782; 176-969-507-501-675; 179-451-658-606-047; 180-819-405-560-712; 181-596-794-542-841; 195-706-909-301-830; 197-698-832-049-766,0,false,,
116-220-653-628-913,"A Smart Service System for Spatial Intelligence and Onboard Navigation for Individuals with Visual Impairment (VIS4ION Thailand): study protocol of a randomized controlled trial of visually impaired students at the Ratchasuda College, Thailand.",2022-11-10,2022,preprint,,,Research Square Platform LLC,,Mahya Beheshti; Tahereh Naeimi; Todd Hudson; Chen Feng; Pattanasak Mongkolwat; Wachara Riewpaiboon; William H. Seiple; Rajesh Vedanthan; John-Ross Rizzo,"<jats:title>Abstract</jats:title>;         <jats:p>• <jats:bold>Background</jats:bold>:; ; Blind/low vision (BLV) severely limits information about our three-dimensional world, leading to poor spatial cognition and impaired navigation. BLV engenders mobility losses, debility, illness and premature mortality. These mobility losses have been associated with unemployment and severe compromises in quality of life. VI not only eviscerates mobility and safety but also, creates barriers to inclusive higher education. Although true in almost every high-income country, these startling facts are even more severe in low- and middle-income countries, such as Thailand. We aim to use VIS4ION (Visually Impaired Smart Service System for Spatial Intelligence and Onboard Navigation), an advanced wearable technology, to enable real-time access to microservices, serving as a potential solution to close this gap and provide consistent and reliable access to critical spatial information needed for mobility and orientation during navigation.; ; • <jats:bold>Methods</jats:bold>:; ; We are leveraging 3D reconstruction and semantic segmentation techniques to create a digital twin of the campus that houses Mahidol University’s disability college. We will do cross-over randomization, and two groups of randomized VI students will deploy this augmented platform in two phases: a passive phase, during which the wearable will only record location, and an active phase, in which end users receive orientation cueing during location recording. A group will perform the active phase first, then the passive and the other group will experiment reciprocally. We will assess for acceptability, appropriateness, and feasibility, focusing on experiences with <jats:bold>VIS</jats:bold><jats:sup><jats:bold>4</jats:bold></jats:sup><jats:bold>ION</jats:bold>. In addition, we will test another cohort of students for navigational, health, and wellbeing improvements, comparing weeks 1 to 4. We will also conduct a process evaluation according to the Saunders Framework. Finally, we will extend our computer vision and digital twinning technique to a 12-block spatial grid in Bangkok, providing aid in a more complex environment.; ; • <jats:bold>Discussion</jats:bold>:; ; Although electronic navigation aids seem like an attractive solution, there are several barriers to their use; chief among them is their dependence on either environmental (sensor-based) infrastructure or Wi-Fi/cell 'connectivity' infrastructure or both. These barriers limit their widespread adoption, particularly in low-and-middle-income countries. Here we propose a navigation solution that operates independently of both environmental and Wi-Fi/cell infrastructure. We predict the proposed platform supports spatial cognition in BLV populations, augmenting personal freedom and agency, and promoting health and wellbeing.; ; •  <jats:bold>Trial registration</jats:bold>:; ; ClinicalTrials.gov under the identifier: <jats:bold>NCT03174314</jats:bold>, Registered 2017.06.02; https://clinicaltrials.gov/ct2/show/NCT03174314</jats:p>",,,,,Orientation and Mobility; Quality of life (healthcare); Computer science; Psychology; Applied psychology; Medicine; Human–computer interaction; Nursing; Visually impaired,,,,,https://www.researchsquare.com/article/rs-2085749/latest.pdf https://doi.org/10.21203/rs.3.rs-2085749/v1 https://trialsjournal.biomedcentral.com/counter/pdf/10.1186/s13063-023-07173-8 https://doi.org/10.1186/s13063-023-07173-8,http://dx.doi.org/10.21203/rs.3.rs-2085749/v1,,10.21203/rs.3.rs-2085749/v1,,,0,000-326-447-768-594; 000-765-133-155-52X; 002-073-096-554-999; 003-879-108-915-704; 006-953-356-670-058; 007-290-948-184-733; 011-507-559-759-571; 011-925-783-478-178; 016-327-721-221-926; 018-392-325-202-807; 021-987-891-911-326; 024-118-585-521-180; 027-438-755-330-646; 030-464-942-782-370; 030-617-400-771-313; 031-637-704-228-445; 034-884-619-681-066; 039-981-321-899-739; 048-204-810-509-658; 049-313-676-820-38X; 054-423-459-444-451; 056-220-908-929-116; 062-051-374-404-149; 069-656-898-335-443; 073-756-777-122-700; 080-150-281-543-648; 083-406-127-121-987; 107-614-613-778-779; 116-815-276-926-407; 119-770-768-617-54X; 141-417-261-005-384; 159-232-067-136-703,0,true,cc-by,green
116-690-684-572-824,Towards Human Smart Cities: Internet of Things for sensory impaired individuals,2016-12-28,2016,journal article,Computing,0010485x; 14365057,Springer Science and Business Media LLC,Germany,Alejandro Rafael Garcia Ramirez; Israel González-Carrasco; Gustavo Henrique Jasper; Amarilys Lima Lopez; Jose Luis Lopez-Cuadrado; Ángel García-Crespo,,99,1,107,126,Internet privacy; The Internet; Architecture; Technological change; Business; Haptic technology; Relocation; Context (language use); Process (engineering); Simulation; Autonomy,,,,"Conselho Nacional de Desenvolvimento Científico e Tecnológico CNPq, Brazil",https://link.springer.com/article/10.1007/s00607-016-0529-2 https://dl.acm.org/doi/10.1007/s00607-016-0529-2 https://dblp.uni-trier.de/db/journals/computing/computing99.html#RamirezGJLLG17,http://dx.doi.org/10.1007/s00607-016-0529-2,,10.1007/s00607-016-0529-2,2565223106,,0,000-069-249-354-951; 004-431-445-128-075; 004-579-736-325-985; 008-124-206-643-913; 011-835-483-329-796; 013-056-435-015-274; 013-706-696-180-253; 017-412-769-177-728; 019-704-976-550-81X; 022-712-441-733-562; 023-183-459-720-925; 037-491-194-003-965; 047-716-780-907-104; 057-643-819-952-454; 058-097-824-013-962; 072-257-737-942-357; 074-757-393-964-969; 077-199-892-884-053; 091-650-210-965-122; 092-546-754-501-779; 093-313-084-202-372; 107-426-899-478-115; 108-021-068-224-326; 114-029-678-843-988; 119-036-310-031-196; 122-405-474-116-03X; 137-616-902-479-676; 149-689-843-836-188,33,false,,
117-165-983-063-465,Spatial Models for Wide-Area Visual Surveillance: Computational Approaches and Spatial Building-Blocks,,2005,journal article,Artificial Intelligence Review,02692821; 15737462,Springer Science and Business Media LLC,Netherlands,Richard J. Howarth,,23,2,97,155,Data mining; Spatial intelligence; Applications of artificial intelligence; Range (mathematics); Key (cryptography); Data science; Spatial representation; Visual surveillance; Cellular topology; Wide area; Computer science,,,,,https://dblp.uni-trier.de/db/journals/air/air23.html#Howarth05 https://link.springer.com/article/10.1007/s10462-004-4103-5 https://rd.springer.com/article/10.1007/s10462-004-4103-5 https://link.springer.com/content/pdf/10.1007/s10462-004-4103-5.pdf,http://dx.doi.org/10.1007/s10462-004-4103-5,,10.1007/s10462-004-4103-5,1963570404,,0,000-550-926-907-99X; 000-812-008-890-129; 001-583-715-440-792; 002-523-628-255-910; 002-875-655-105-458; 003-055-508-972-538; 003-832-845-419-710; 004-343-192-195-084; 004-408-085-242-783; 004-437-862-225-666; 004-461-757-867-608; 005-791-575-994-620; 006-502-182-986-606; 007-504-158-856-815; 007-832-800-069-193; 007-926-061-319-060; 007-947-694-212-442; 007-987-009-720-438; 008-321-491-573-039; 009-253-029-032-798; 009-898-754-435-272; 010-162-802-521-356; 010-784-907-758-209; 011-682-629-141-450; 012-215-640-144-330; 012-278-840-599-063; 012-650-397-392-495; 013-725-909-488-520; 015-705-239-143-631; 016-370-704-455-839; 018-111-574-600-175; 018-914-285-931-014; 021-074-819-711-606; 022-066-064-223-597; 022-608-642-410-17X; 023-011-640-239-174; 023-594-066-611-687; 024-263-423-488-497; 024-468-437-188-144; 025-420-015-711-162; 025-950-773-264-961; 026-053-953-784-91X; 026-380-001-884-882; 026-397-051-963-651; 026-532-167-952-056; 026-722-129-912-27X; 027-838-627-002-800; 029-108-663-014-962; 029-868-143-760-664; 030-468-535-303-510; 030-652-226-664-697; 031-718-040-567-077; 031-971-778-472-866; 032-387-699-236-973; 032-878-009-565-785; 033-021-234-906-527; 033-032-802-333-039; 033-106-780-921-073; 033-591-124-833-00X; 034-798-635-033-21X; 035-443-233-823-132; 035-571-892-533-16X; 035-903-136-289-884; 036-326-959-203-075; 036-389-128-885-405; 036-439-762-038-337; 036-616-049-977-19X; 036-925-551-482-159; 037-640-263-522-90X; 037-897-384-277-078; 038-373-503-356-916; 038-502-363-661-887; 039-203-087-961-950; 040-013-339-345-168; 040-299-553-837-960; 040-323-998-851-850; 041-146-339-340-020; 041-266-510-608-346; 041-291-180-071-973; 041-323-504-525-040; 041-339-269-636-045; 042-223-368-755-607; 042-906-482-534-579; 044-479-924-573-444; 044-602-685-095-211; 044-799-382-284-076; 045-770-579-949-593; 046-956-438-132-67X; 047-175-420-949-563; 047-631-113-871-475; 048-062-533-202-285; 049-224-072-622-038; 049-788-891-355-440; 050-867-184-470-029; 051-630-857-057-53X; 052-001-707-908-196; 052-343-129-390-70X; 052-624-668-817-355; 052-669-727-256-316; 053-538-924-636-576; 055-322-358-111-619; 055-857-025-983-17X; 056-296-595-757-026; 057-085-994-846-903; 057-162-013-889-110; 058-140-266-822-853; 058-170-488-069-69X; 060-502-994-888-911; 061-672-836-645-314; 062-022-250-373-640; 062-814-719-841-145; 062-996-830-453-810; 063-624-367-816-753; 063-929-990-237-38X; 065-355-739-644-367; 066-026-494-464-171; 066-282-636-760-009; 066-837-329-008-091; 067-110-166-041-151; 068-062-574-164-874; 068-421-355-773-639; 069-025-449-385-460; 069-623-351-702-587; 070-044-207-817-936; 070-715-006-996-629; 071-077-836-323-872; 071-522-910-771-28X; 072-253-529-023-163; 072-665-935-576-282; 072-940-060-578-683; 073-548-232-761-24X; 075-029-771-945-345; 075-151-677-402-36X; 075-675-715-567-825; 077-335-876-952-865; 077-364-239-279-682; 078-476-312-019-696; 078-624-484-545-993; 078-690-425-372-020; 080-399-454-165-723; 080-413-174-332-978; 080-769-378-027-438; 081-650-746-348-672; 081-712-993-371-897; 081-927-652-906-388; 082-085-690-629-324; 083-401-466-680-502; 084-077-142-983-755; 084-330-008-150-29X; 085-287-599-482-266; 085-674-040-165-681; 085-787-072-299-79X; 087-531-022-028-40X; 088-654-342-193-988; 088-828-955-168-774; 089-009-907-547-355; 089-290-881-574-52X; 089-486-196-748-24X; 089-597-631-404-110; 089-932-101-960-596; 090-001-456-177-20X; 091-051-144-835-121; 091-822-907-823-018; 093-343-457-845-723; 094-413-646-519-780; 094-477-245-144-773; 094-784-542-800-597; 095-097-838-898-142; 095-390-499-430-942; 095-927-187-228-726; 096-697-660-514-687; 098-143-574-632-861; 099-327-271-738-979; 099-599-247-210-76X; 100-129-226-625-904; 100-843-056-060-100; 101-095-442-713-793; 101-361-827-017-181; 102-205-629-623-668; 102-425-447-709-115; 102-555-702-781-751; 103-157-156-595-232; 105-688-997-574-765; 106-188-974-921-766; 106-573-448-218-801; 107-614-030-786-793; 107-871-578-995-467; 108-879-011-214-378; 108-890-978-306-552; 110-372-024-434-782; 111-189-270-060-895; 111-336-987-827-595; 114-134-443-349-98X; 114-560-087-040-426; 115-447-753-855-89X; 115-818-928-027-747; 118-747-234-831-814; 118-793-847-571-266; 120-613-853-176-597; 121-790-401-277-948; 122-396-206-873-016; 123-113-963-109-035; 123-144-071-890-286; 124-039-112-889-006; 124-385-139-492-393; 126-062-776-457-95X; 127-007-419-725-666; 127-115-664-047-553; 127-686-407-880-957; 128-131-452-887-872; 128-238-363-116-427; 130-263-214-374-250; 130-624-770-611-094; 131-431-864-339-824; 131-779-996-044-216; 133-289-426-096-349; 133-410-999-964-811; 133-796-891-237-596; 138-066-897-128-435; 138-789-141-989-074; 138-857-324-950-879; 140-155-497-528-727; 142-297-654-115-325; 147-156-299-805-358; 149-568-402-157-218; 151-748-153-878-574; 152-380-657-152-922; 153-653-244-128-497; 154-566-097-073-721; 155-913-892-846-105; 160-198-269-362-249; 162-290-944-073-243; 162-847-249-229-221; 163-332-308-018-154; 163-986-166-188-416; 164-507-160-190-107; 164-968-769-007-28X; 166-976-530-910-453; 167-532-095-303-067; 167-643-876-733-117; 169-280-871-992-024; 172-153-601-726-259; 172-248-059-827-19X; 174-010-839-412-561; 175-241-695-773-281; 176-995-318-672-706; 180-119-012-919-352; 181-215-377-923-123; 184-449-800-499-915; 184-746-440-822-977; 187-307-065-356-402; 188-197-525-577-356; 189-205-044-796-965; 190-922-438-934-766; 191-554-605-182-603; 191-760-144-620-902; 199-850-851-255-035,10,false,,
117-320-452-579-582,SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI - Robotic assistant for visually impaired using sensor fusion,,2017,conference proceedings article,"2017 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)",,IEEE,,Gaurao Chaudhari; Asmita Deshpande,"Disabled people are in a dire need of electronic assistance. There are a couple of electronic assisting devices that bring their life to an easier turn. In this paper, we describe the design and implementation of a personal assistant robot for blind people. Visually impaired people need such personal assistant devices for they provide a real-time assistance regarding any necessary problem that blind people face. Some of those main problems are navigation in the indoors, identifying objects around unless getting a physical sense of those objects and sensing the surrounding with the distance of multiple objects. Our paper discusses the various application targeting features like using the LIDAR for local mapping, using a 3D camera for understanding the depth of the surrounding so that the person understands the distance and other information of the objects around. This design has been experimentally validated and required observations are posted in this paper.",,,1,4,Human–computer interaction; 3d camera; Robot vision systems; Disabled people; Visually impaired; Sensor fusion; Computer science; Robot,,,,,https://dblp.uni-trier.de/db/conf/uic/uic2017.html#ChaudhariD17 https://par.nsf.gov/biblio/10092492-robotic-assistant-visually-impaired-using-sensor-fusion https://doi.org/10.1109/UIC-ATC.2017.8397579 https://par.nsf.gov/servlets/purl/10092492,http://dx.doi.org/10.1109/uic-atc.2017.8397579,,10.1109/uic-atc.2017.8397579,2811307482,,0,006-500-640-442-683; 008-723-145-538-785; 039-209-657-828-295; 056-208-907-548-256; 056-839-169-895-213; 091-764-827-661-79X; 096-899-090-360-502; 098-966-666-704-92X; 137-964-956-124-484; 158-780-296-035-026; 165-646-371-179-035,5,false,,
117-368-855-666-392,ICARCV - Overview of assistive technologies for the blind: Navigation and shopping,,2014,conference proceedings article,2014 13th International Conference on Control Automation Robotics & Vision (ICARCV),,IEEE,,Karen Duarte; José Cecílio; Pedro Furtado,"One of the major driving forces behind technology development is the establishment of equality among everyone. It is intended that the strategic use of technology complements competencies or diminished capacities, improving self confidence and autonomy. Blind and partially sighted people often deal with this inequality in the performance of tasks or daily activities. The presented work reviews some of the proposed technologies to assist visually impaired people in common activities, specifically navigation and going shopping.",,,1929,1934,Internet privacy; Engineering; Work (electrical); Technology development; Use of technology; Partially sighted; Self-confidence; Multimedia; Global Positioning System; Autonomy; Activities of daily living,,,,,https://ieeexplore.ieee.org/document/7064611/ https://dblp.uni-trier.de/db/conf/icarcv/icarcv2014.html#DuarteCF14,http://dx.doi.org/10.1109/icarcv.2014.7064611,,10.1109/icarcv.2014.7064611,2008781353,,0,019-925-883-027-48X; 023-085-100-690-935; 028-460-767-882-226; 053-968-972-722-364; 061-190-872-413-287; 070-596-255-418-628; 079-306-886-875-058; 090-884-510-770-832; 107-106-634-124-837; 128-897-617-870-753; 143-544-756-515-778; 175-130-600-114-960,8,false,,
117-523-102-689-538,These tools help visually impaired scientists read data and journals.,2023-03-06,2023,journal article,Nature,14764687; 00280836,Springer Science and Business Media LLC,United Kingdom,Alla Katsnelson,Innovative software and modes of presentation are helping to broaden access to the literature. Innovative software and modes of presentation are helping to broaden access to the literature.,615,7951,362,363,Presentation (obstetrics); Software; Computer science; World Wide Web; Data science; Data presentation; Human–computer interaction; Medicine; Operating system; Documentation; Radiology,Careers; Communication; Lab life; Publishing,Periodicals as Topic; Reading; Research Personnel; Visually Impaired Persons; Research Report; Software,,,,http://dx.doi.org/10.1038/d41586-023-00645-6,36879053,10.1038/d41586-023-00645-6,,,0,034-845-993-737-89X; 091-831-370-926-387; 151-049-677-184-810; 161-932-590-674-759,1,true,,bronze
117-677-849-214-196,Visual Assistance for Blind Using Image Processing,,2018,conference proceedings article,2018 International Conference on Communication and Signal Processing (ICCSP),,IEEE,,B Deepthi Jain; Shwetha M Thakur; K. V. Suresh,"Visually impaired people face lot of difficulties in their daily life. Many a times they rely on others for help. Several technologies for assistance of visually impaired people have been developed. Among the various technologies being utilized to assist the blind, Computer Vision based solutions are emerging as one of the most promising options due to their affordability and accessibility. This paper proposes a system for visually impaired people. The proposed system aims to create a wearable visual aid for visually impaired people in which speech commands are accepted from the user. Its functionality addresses identification of objects and sign boards. This will help the visually impaired person to manage day-to-day activities and to navigate through his/her surroundings. Raspberry Pi is used to implement artificial vision using python language on the Open CV platform.",,,0499,0503,Human–computer interaction; Wearable computer; Object detection; Artificial vision; Python language; Computer science; Feature extraction; Cognitive neuroscience of visual object recognition; Identification (information); Image processing,,,,,https://ieeexplore.ieee.org/document/8524251 http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8524251,http://dx.doi.org/10.1109/iccsp.2018.8524251,,10.1109/iccsp.2018.8524251,2901385360,,1,041-644-601-112-907; 106-707-960-881-945; 124-703-344-498-509; 135-442-888-799-760; 161-932-258-966-266,25,false,,
118-168-875-354-767,Iot based intelligent jacket,2020-03-13,2020,journal article,"International Journal of Advance Research, Ideas and Innovations in Technology",2454132x,,,Pratiksha B. Patil; Dhanashree A. Kambli; Rishikesh Vanjare; Siddhi A. Kolekar; Ravina R. Kurane; M. A. Pardesi,"“IoT Based Intelligent Jacket” gives an intelligent jacket that gives security for girls' safety and blind support. In the global scenario, the prime question in every girl’s mind is about her safety and harassment issues. The simplest notion haunting every female is when they may be able to move freely on the streets even in bizarre hours without demanding about their protection. This assignment suggests a new generation to shield a ladies. The 2nd scenario is set the blind people, Blind individuals war when visiting from place to region and depend on predefined and repetitive routes with a minimum impediment to guide them to their destination without an assistant. The task also makes a specialty of navigation of the visually impaired system. “IoT Based Intelligent jacket” gives a sensible jacket that provides navigation for girls protection, visually impaired people. The machine encompasses ultrasonic sensor, GSM, GPS Sensor, Arduino UNO, Node MCU, Buzzer, Power supply, smartphone.",6,1,266,269,Set (psychology); Microcontroller; Buzzer; Task (project management); Computer security; Computer science; Node (networking); GSM; Global Positioning System; Internet of Things,,,,,https://www.ijariit.com/manuscript/iot-based-intelligent-jacket/,https://www.ijariit.com/manuscript/iot-based-intelligent-jacket/,,,3012550857,,0,,0,false,,
118-251-400-045-44X,Atmospheric Transmission and Thermal Inertia Induced Blind Road Segmentation with a Large-Scale Dataset TBRSD,2023-10-01,2023,conference proceedings article,2023 IEEE/CVF International Conference on Computer Vision (ICCV),,IEEE,,Junzhang Chen; Xiangzhi Bai,"Computer vision-based walking assistants are prominent tools for aiding visually impaired people in navigation. Blind road segmentation is a key element in these walking assistant systems. However, most walking assistant systems rely on visual light images, which is dangerous in weak illumination environments such as darkness or fog. To address this issue and enhance the safety of vision-based walking assistant systems, we developed a thermal infrared blind road segmentation neural network (TINN). In contrast to conventional segmentation techniques that primarily concentrate on enhancing feature extraction and perception, our approach is geared towards preserving the inherent radiation characteristics within the thermal imaging process. Initially, we modelled two critical factors in thermal infrared imaging - thermal light atmospheric transmission and thermal inertia effect. Subsequently, we use an encoder-decoder architecture to fuse the feathers extracted by the two modules. Additionally, to train the network and evaluate the effectiveness of the proposed method, we constructed a large-scale thermal infrared blind road segmentation dataset named TBRSD consists 5180 pixel-level manual annotations. The experimental results demonstrate that our method outperforms existing techniques and achieves state-of-the-art performance in thermal blind road segmentation, as validated on benchmark thermal infrared semantic segmentation datasets such as MFNet and SODA. The dataset and our code are both publicly available in https://github.com/chenjzBUAA/TBRSD or http://xzbai.buaa.edu.cn/datasets.html.",,,,,Computer science; Segmentation; Artificial intelligence; Computer vision; Benchmark (surveying); Geography; Cartography,,,,National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities,,http://dx.doi.org/10.1109/iccv51070.2023.00103,,10.1109/iccv51070.2023.00103,,,0,002-649-617-859-886; 002-826-876-497-888; 003-061-447-131-161; 003-561-233-382-310; 004-269-574-716-057; 012-714-204-920-096; 013-687-284-638-685; 018-843-508-574-144; 028-939-404-645-349; 030-540-246-658-722; 034-795-713-250-110; 037-491-194-003-965; 041-314-165-476-904; 046-687-138-501-725; 049-986-867-299-749; 051-263-461-368-647; 064-174-676-899-442; 064-521-070-547-235; 065-983-543-415-033; 066-037-919-140-345; 066-974-781-302-685; 092-795-850-215-331; 100-575-932-125-341; 103-728-042-717-787; 103-974-418-343-087; 114-409-327-634-337; 127-240-128-835-954; 131-046-929-434-814; 134-950-160-470-035; 144-738-058-499-258; 148-070-750-157-885; 167-131-435-704-717; 178-017-051-968-545,1,false,,
118-432-281-415-657,Commute Booster: A Mobile Application for First/Last Mile and Middle Mile Navigation Support for People With Blindness and Low Vision.,2023-07-07,2023,journal article,IEEE journal of translational engineering in health and medicine,21682372,Institute of Electrical and Electronics Engineers (IEEE),United States,Junchi Feng; Mahya Beheshti; Mira Philipson; Yuvraj Ramsaywack; Maurizio Porfiri; John-Ross Rizzo,"<AbstractText Label=""OBJECTIVE"" NlmCategory=""OBJECTIVE"">People with blindness and low vision face substantial challenges when navigating both indoor and outdoor environments. While various solutions are available to facilitate travel to and from public transit hubs, there is a notable absence of solutions for navigating within transit hubs, often referred to as the ""middle mile"". Although research pilots have explored the middle mile journey, no solutions exist at scale, leaving a critical gap for commuters with disabilities. In this paper, we proposed a novel mobile application, Commute Booster, that offers full trip planning and real-time guidance inside the station.</AbstractText>;           <AbstractText Label=""METHODS AND PROCEDURES"" NlmCategory=""METHODS"">Our system consists of two key components: the general transit feed specification (GTFS) and optical character recognition (OCR). The GTFS dataset generates a comprehensive list of wayfinding signage within subway stations that users will encounter during their intended journey. The OCR functionality enables users to identify relevant navigation signs in their immediate surroundings. By seamlessly integrating these two components, Commute Booster provides real-time feedback to users regarding the presence or absence of relevant navigation signs within the field of view of their phone camera during their journey.</AbstractText>;           <AbstractText Label=""RESULTS"" NlmCategory=""RESULTS"">As part of our technical validation process, we conducted tests at three subway stations in New York City. The sign detection achieved an impressive overall accuracy rate of 0.97. Additionally, the system exhibited a maximum detection range of 11 meters and supported an oblique angle of approximately 110 degrees for field of view detection.</AbstractText>;           <AbstractText Label=""CONCLUSION"" NlmCategory=""CONCLUSIONS"">The Commute Booster mobile application relies on computer vision technology and does not require additional sensors or infrastructure. It holds tremendous promise in assisting individuals with blindness and low vision during their daily commutes. Clinical and Translational Impact Statement: Commute Booster translates the combination of OCR and GTFS into an assistive tool, which holds great promise for assisting people with blindness and low vision in their daily commute.</AbstractText>;           <CopyrightInformation>© 2023 The Authors.</CopyrightInformation>",11,,523,535,Signage; Computer science; Mile; Booster (rocketry); Mobile phone; Transport engineering; Multimedia; Telecommunications; Engineering; Geography; Art; Geodesy; Aerospace engineering; Visual arts,General transit feed specification; indoor navigation; low-vision aid; mobile application; optical character recognition,"Humans; Vision, Low; Mobile Applications; Transportation; Blindness; Self-Help Devices",,NEI NIH HHS (R21 EY033689) United States,https://ieeexplore.ieee.org/ielx7/6221039/6563131/10175612.pdf https://doi.org/10.1109/jtehm.2023.3293450,http://dx.doi.org/10.1109/jtehm.2023.3293450,38059065,10.1109/jtehm.2023.3293450,,PMC10697290,0,000-276-479-976-352; 008-535-554-837-892; 010-011-058-528-753; 018-396-597-404-988; 031-545-111-217-873; 033-564-709-739-146; 038-298-556-220-476; 044-758-274-873-42X; 046-934-556-926-609; 055-368-719-911-202; 064-633-691-119-141; 077-953-340-723-43X; 087-498-521-751-289; 089-264-374-221-122; 089-822-552-618-426; 093-311-354-777-450; 093-848-913-060-772; 098-405-398-975-810; 108-902-908-402-050; 149-297-463-971-588; 156-573-992-516-080; 160-184-828-425-836,3,true,"CC BY, CC BY-NC-ND",gold
118-775-292-132-518,Obstacle avoidance using stereo vision and depth maps for visual aid devices,2020-05-28,2020,journal article,SN Applied Sciences,25233963; 25233971,Springer Science and Business Media LLC,,Vaibhav Bansal; Krithika Balasubramanian; P. Natarajan,"Comprehending the environment accurately and proficiently is one of the fundamental undertakings for the visually impaired. Many electronic travel aid devices are utilizing stereo vision techniques. However, there is no existing technology that helps to fulfill the requirements of the visually impaired and be economical at the same time. All the current technologies are implemented only for a specific purpose and have some constraints. In this paper, we propose a new framework that integrates the critical aspects of stereo vision with some additional features for aiding the visually impaired to solve their existing problems efficiently. In this system, both stereo vision and ultrasonic sensors have been integrated to get a precise awareness of the obstacles present in the real-time surroundings. The central concept is to categorize point vertices for the detection of objects in three-dimensional space according to height, width and slope of the adjacent vertices present in the images captured by CCD cameras. The intended assistive system serves to be more cost-effective for the visually impaired with the usage of CCD cameras and low-budget sensors. The assistive system in its recognition phase employs the proposed obstacle detection algorithm using stereo vision to recognize and track impediments that are at a farther distance. While, it employs ultrasonic sensors to detect obstacles present at shorter distances. The system also overcomes the problem of finding holes and descending stairs, faced by devices that use monoscopic vision. Additional features, like the Buzzer, GPS, Voice module, Alert message, are being incorporated. The principle intention of this paper is to contribute our knowledge to assist the visually impaired to become independent and help them to adapt to various daily situations.",2,6,1,17,Artificial intelligence; Point (typography); Obstacle; Buzzer; Computer vision; Computer science; Global Positioning System; Obstacle avoidance; Stereopsis; Ultrasonic sensor; Categorization,,,,,https://link.springer.com/article/10.1007/s42452-020-2815-z https://link.springer.com/content/pdf/10.1007/s42452-020-2815-z.pdf,http://dx.doi.org/10.1007/s42452-020-2815-z,,10.1007/s42452-020-2815-z,3030976695,,0,002-704-194-668-787; 007-690-488-385-915; 017-804-671-829-970; 025-909-397-100-730; 030-440-500-148-114; 045-802-145-074-623; 068-526-647-480-345; 075-519-293-886-898; 078-123-623-816-39X; 078-725-425-518-672; 098-418-441-499-649; 101-588-853-544-889; 119-013-410-049-180; 123-814-577-052-263; 124-319-904-779-469; 124-411-545-803-229; 133-864-872-584-834; 155-696-581-350-219,12,true,cc-by,gold
119-096-517-156-652,Design of an auditory guidance system for the blind with signal transformation from stereo ultrasonic to binaural audio,,2000,journal article,Artificial Life and Robotics,14335298; 16147456,Springer Science and Business Media LLC,Germany,Young Jip Kim; Chong Hui Kim; Byung Kook Kim,"We have designed an auditory guidance system for the blind using ultrasonic-to-audio signal transformation. We first investigated the system requirements, and designed a simple but useful portable guidance system for the blind. The system derives visual information using multiple ultrasonic sensors, and transforms it to binaural auditory information using a suitable technique. The user can recognize the position of obstacles and the surrounding environment. The system is composed of two parts. One is a glasses-type system, and the other is a cane-type system with guide wheels. The former functions as an environment sensor, and the latter functions as a clear-path indicator. Wide-beam-angle ultrasonic sensors are used to detect bojects over a broader range. The system is designed as a battery-supplied portable model. Our design is focused on low power consumption, small size, light weight, and easy manipulation.",4,4,220,226,Position (vector); Artificial intelligence; Guidance system; Binaural recording; Power consumption; Signal transformation; Auditory information; Computer vision; System requirements; Computer science; Ultrasonic sensor,,,,,https://dblp.uni-trier.de/db/journals/alr/alr4.html#KimKK00 https://link.springer.com/article/10.1007%2FBF02481178,http://dx.doi.org/10.1007/bf02481178,,10.1007/bf02481178,1987175205,,1,005-496-094-610-874; 006-956-729-410-739; 009-142-401-900-87X; 032-947-937-472-019; 045-673-862-191-277; 064-309-126-323-022; 065-992-777-809-97X; 068-581-156-196-413; 098-811-589-762-176; 132-287-528-433-371; 135-646-014-794-306,13,true,,green
119-133-119-889-48X,ICCE-TW - A Mobile Application-Based Learning Aid Developer for Teaching Visually Impaired Students,2021-09-15,2021,conference proceedings article,2021 IEEE International Conference on Consumer Electronics-Taiwan (ICCE-TW),,IEEE,,Bien Grenier Sasing; Aaron Raymond See; Welsey Daniel C. Advincula; Yeou-Jiunn Chen,"The process of educating visually impaired and blind (VIB) students is quite complex, from the creation of engaging educational materials to its development as it requires time, skill, and versatility. Current educational materials for the VIB also lack interactivity, which reduces students’ autonomy. This can be solved by providing an unsophisticated mobile APP that can quickly develop tactile learning aids. In this paper, a mobile application is developed to provide an accessible interface that can be used by either the teacher or the student implemented with computer vision, gesture-based inputs, and audio accessibility features with the ability to take pictures and add braille language sent to the database of 3D images for the filtering and printing process as a tactile learning aid. For future development, an additional voice-command system for navigating the user interface and more filter options is considered.",,,1,2,Human–computer interaction; Interface (computing); Interactivity; Gesture; Object detection; Visually impaired; Computer science; Process (engineering); Kinesthetic learning; User interface,,,,,http://dblp.uni-trier.de/db/conf/icce-tw/icce-tw2021.html#SasingSAC21 http://xplorestaging.ieee.org/ielx7/9601161/9602869/09603008.pdf?arnumber=9603008 https://doi.org/10.1109/ICCE-TW52618.2021.9603008 http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=9603008,http://dx.doi.org/10.1109/icce-tw52618.2021.9603008,,10.1109/icce-tw52618.2021.9603008,3217208534,,0,022-971-236-761-173; 045-063-120-299-66X; 046-416-632-791-102; 054-284-851-636-370; 060-675-942-999-842; 061-074-600-422-368; 062-912-697-268-366; 066-555-214-738-814; 067-452-802-406-263; 127-968-649-646-51X,0,false,,
119-273-238-956-635,Diabetic retinopathy detection and severity classification using optimized deep learning with explainable AI technique,2024-04-03,2024,journal article,Multimedia Tools and Applications,15737721; 13807501,Springer Science and Business Media LLC,Netherlands,B. Lalithadevi; S. Krishnaveni,,,,,,Computer science; Diabetic retinopathy; Artificial intelligence; Deep learning; Machine learning; Pattern recognition (psychology); Diabetes mellitus; Medicine; Endocrinology,,,,,,http://dx.doi.org/10.1007/s11042-024-18863-z,,10.1007/s11042-024-18863-z,,,0,002-453-703-535-568; 005-436-963-947-352; 007-634-217-719-307; 010-839-519-424-026; 012-952-891-899-818; 013-818-478-480-000; 014-125-082-299-787; 019-177-857-214-256; 020-233-013-143-936; 022-593-644-415-119; 027-604-370-848-250; 028-805-688-323-793; 034-543-393-935-186; 040-867-702-502-770; 041-300-181-853-274; 042-240-098-526-453; 043-847-091-036-547; 043-930-647-309-334; 046-267-356-766-552; 056-829-504-590-953; 057-392-808-443-01X; 061-071-305-101-138; 066-305-886-229-375; 067-264-139-685-236; 068-534-160-542-92X; 071-398-807-064-477; 073-470-254-263-447; 076-255-225-755-079; 076-959-563-840-44X; 077-108-325-031-795; 077-919-204-626-812; 084-022-310-654-638; 084-990-770-009-857; 089-375-017-895-956; 089-781-196-339-35X; 092-755-911-991-320; 094-227-695-007-23X; 094-307-242-798-964; 096-501-512-271-962; 106-837-261-462-528; 108-860-427-726-756; 111-717-701-604-783; 111-959-906-179-444; 113-611-314-911-019; 114-745-688-699-489; 115-855-944-901-855; 117-453-990-654-411; 119-124-629-312-758; 120-566-047-820-49X; 124-523-510-660-48X; 125-203-871-080-951; 131-087-853-517-533; 134-701-226-038-003; 139-552-118-652-140; 158-990-473-178-26X; 162-300-681-613-979; 164-407-670-250-431; 168-091-160-148-522; 178-423-719-824-570; 180-559-374-049-445; 184-564-692-610-017; 186-715-096-036-088; 195-822-401-859-866; 199-158-948-932-265,2,false,,
119-356-569-027-981,Feasibility and Usability of Augmented Reality Technology in the Orthopaedic Operating Room.,2024-04-12,2024,journal article,Current reviews in musculoskeletal medicine,1935973x; 19359748,Springer Science and Business Media LLC,United States,Stephen P Canton; Confidence Njoku Austin; Fritz Steuer; Srujan Dadi; Nikhil Sharma; Nicolás M Kass; David Fogg; Elizabeth Clayton; Onaje Cunningham; Devon Scott; Dukens LaBaze; Edward G Andrews; Jacob T Biehl; MaCalus V Hogan,"<AbstractText Label=""PURPOSE OF REVIEW"" NlmCategory=""OBJECTIVE"">Augmented reality (AR) has gained popularity in various sectors, including gaming, entertainment, and healthcare. The desire for improved surgical navigation within orthopaedic surgery has led to the evaluation of the feasibility and usability of AR in the operating room (OR). However, the safe and effective use of AR technology in the OR necessitates a proper understanding of its capabilities and limitations. This review aims to describe the fundamental elements of AR, highlight limitations for use within the field of orthopaedic surgery, and discuss potential areas for development.</AbstractText>;           <AbstractText Label=""RECENT FINDINGS"" NlmCategory=""RESULTS"">To date, studies have demonstrated evidence that AR technology can be used to enhance navigation and performance in orthopaedic procedures. General hardware and software limitations of the technology include the registration process, ergonomics, and battery life. Other limitations are related to the human response factors such as inattentional blindness, which may lead to the inability to see complications within the surgical field. Furthermore, the prolonged use of AR can cause eye strain and headache due to phenomena such as the vergence-convergence conflict. AR technology may prove to be a better alternative to current orthopaedic surgery navigation systems. However, the current limitations should be mitigated to further improve the feasibility and usability of AR in the OR setting. It is important for both non-clinicians and clinicians to work in conjunction to guide the development of future iterations of AR technology and its implementation into the OR workflow.</AbstractText>;           <CopyrightInformation>© 2024. The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.</CopyrightInformation>",17,5,117,128,Usability; Augmented reality; Workflow; Popularity; End user; Computer science; Medicine; Human–computer interaction; Psychology; Social psychology; Database; Operating system,Augmented reality; Operating room; Orthopaedic surgery; Virtual reality,,,,,http://dx.doi.org/10.1007/s12178-024-09888-w,38607522,10.1007/s12178-024-09888-w,,PMC11068703,0,001-279-250-742-54X; 002-163-092-429-188; 004-769-015-094-168; 007-086-467-876-888; 007-786-831-110-430; 009-589-933-265-919; 010-152-199-744-23X; 010-908-593-239-984; 012-390-902-172-621; 015-009-685-442-296; 015-556-438-515-921; 016-338-001-022-72X; 017-060-182-151-709; 017-264-681-097-924; 019-589-578-250-327; 020-106-915-018-171; 020-942-176-379-11X; 021-528-948-812-757; 021-901-183-460-683; 023-231-951-420-40X; 023-621-376-783-178; 025-242-655-701-713; 025-336-961-943-03X; 026-465-779-438-915; 028-420-877-163-023; 029-046-072-620-396; 029-672-486-533-784; 030-845-888-788-438; 033-167-709-130-702; 034-196-265-339-546; 034-514-859-067-534; 039-989-217-987-006; 040-660-589-903-725; 040-718-008-088-080; 045-532-532-517-511; 045-785-214-647-455; 045-925-247-746-320; 047-648-201-998-143; 048-503-024-552-157; 048-628-441-502-838; 053-354-145-659-991; 053-704-087-150-86X; 056-759-000-464-434; 057-846-084-148-798; 060-279-091-926-789; 060-345-531-649-520; 061-491-285-169-442; 061-516-909-614-531; 062-123-524-120-927; 066-764-231-345-482; 068-821-520-878-93X; 070-341-519-831-172; 070-736-000-178-492; 074-685-690-263-448; 074-924-379-406-384; 081-801-687-079-227; 082-046-279-169-557; 086-479-735-304-249; 089-974-566-730-18X; 090-597-066-804-458; 091-806-159-957-570; 099-164-263-315-884; 099-586-099-259-074; 128-491-368-446-555; 136-936-727-229-35X; 164-717-808-671-970,1,true,,unknown
119-517-530-171-868,ACIS-ICIS - Wearable Navigation System for the Visually Impaired and Blind People,,2012,conference proceedings article,2012 IEEE/ACIS 11th International Conference on Computer and Information Science,,IEEE,,Esteban Bayro Kaiser; Michael Lawo,"A wearable navigation system for visually impaired and blind people in unknown indoor and outdoor environment is presented. This system will map and track the position of the pedestrian during the exploration of the new environment. In order to build this system the well known Simultaneous Localization and Mapping (SLAM) from mobile robotics will implemented. Once a map is created the user can be guided efficiently through it. The user will be equipped with a short range laser, an inertial measurement unit (IMU), a wearable computer for processing purpose and a bone head phon. This system does not intent to replace the use of the white cane. Its purpose is to gather contextual information to aid the user to navigate",,,230,233,Wearable computer; Artificial intelligence; Mobile robot; Navigation system; Headphones; Computer vision; Robotics; Computer science; Global Positioning System; Inertial measurement unit; Simultaneous localization and mapping,,,,,http://doi.ieeecomputersociety.org/10.1109/ICIS.2012.118 http://ieeexplore.ieee.org/document/6211819/ http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.ieee-000006211819 https://dblp.uni-trier.de/db/conf/ACISicis/ACISicis2012.html#KaiserL12,http://dx.doi.org/10.1109/icis.2012.118,,10.1109/icis.2012.118,1973997762,,3,000-374-345-433-896; 002-580-724-068-489; 006-422-963-628-65X; 107-476-322-788-165; 117-969-812-392-401,43,false,,
119-553-924-738-845,AI Based Pilot System for Visually Impaired People,,2020,conference proceedings article,"2020 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems (icABCD)",,IEEE,,Nkosinathi Emmanuel Shandu; Pius A. Owolawi; Temitope Mapayi; Kehinde O. Odeyemi,"In today's world we live with visually impaired people struggling to do things at their full potential as they lack sight of the environment they live in. Affected individuals are often seen with slips, trips and fall over light obstacles on their walkway. To some extent, these blind people cannot relate to any objects they come across such as cars and people around. As technology evolves, there has been numerous attempts in solving this problems for the affected group of people and the proposed solutions need further improvement on how to effectively assist the affected individuals to navigate from one place to the other using real-time updates of their whereabouts. This paper presents the design of an intelligent walking stick for the blind using Raspberry Pi 3 b+ as a central microcontroller, Ultrasonic sensors and Global Positioning System (GPS). The ultrasonic sensors are used for scanning the environment on walkway and sideways using sound waves at certain defined distances, and GPS module is used for real time directions and navigation. It also contains Bluetooth-headset that is used for the audio navigation through the aid of interpretation from the real time feed of ultrasonic sensors and coordinates from the GPS, thereby giving the user the actual route and possible turns until the destination point. Special emergency messages using keywords of the location coordinates obtained through GPS are sent via SMS subscription account connected to the GPIO pins of the Raspberry Pi to care-givers for tracking purposes when required. The entire setup of the intelligent walking stick makes navigation and tracking possible, thus effectively assisting the visually impaired people.",,,,,Human–computer interaction; Sight; Point (typography); Pilot system; Visually impaired; Computer science; Global Positioning System; Ultrasonic sensor,,,,,http://xplorestaging.ieee.org/ielx7/9169263/9183800/09183857.pdf?arnumber=9183857 http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=9183857,http://dx.doi.org/10.1109/icabcd49160.2020.9183857,,10.1109/icabcd49160.2020.9183857,3082624252,,0,007-168-164-260-495; 027-339-511-595-943; 034-429-981-360-930; 043-764-055-494-133; 063-848-176-030-726; 089-923-728-985-127; 098-734-546-760-457; 106-587-878-038-509; 107-768-951-988-666,6,false,,
119-662-130-013-124,Realtime Object Detection and Disease Prediction of Visually Impaired People,2023-04-27,2023,journal article,"International Journal of Advanced Research in Science, Communication and Technology",25819429,Naksh Solutions,,null Dr. Manohar Chaudhary; null Pratik Gade; null Swapnil Waghmare; null Vaibhav Misal; null Abhishek Waghmare,"<jats:p>In order to detect objects, this research combines real-time object identification with appropriate deep learning techniques. In our study, we describe the creation of a real-time system for item recognition, classification, and position estimation in the open environment. Blind and visually impaired people daily deal with a variety of difficulties. The proposed plan's goal. Computer vision is used to precisely identify indoor items. Those who are blind or visually challenged can use navigational aids, The Technology for navigation of the blind is not sufficiently accessible, without vision it can be challenging for visually impaired persons to navigate through rooms or different road paths .The main aim to develop the project is to help the visually impaired people and to detect the obstacles .The blind persons life become easier and without anyone helps they can walk alone through street they does not need anyone to assist them they can handle their self correctly. The preventing users from dangerous location our aim is to collected from environment (cameras, sensors, scanners, etc.) and transmitted to the users to the audio format. Data in the healthcare industry consists of all the information related to patients. Herea general architecture has been proposed for predicting the disease in the healthcare industry</jats:p>",,,88,102,Computer science; Identification (biology); Variety (cybernetics); Object (grammar); Artificial intelligence; Computer vision; Plan (archaeology); Architecture; Human–computer interaction; Visually impaired; Cognitive neuroscience of visual object recognition; History; Art; Botany; Archaeology; Visual arts; Biology,,,,,,http://dx.doi.org/10.48175/ijarsct-9547,,10.48175/ijarsct-9547,,,0,016-572-318-414-022; 040-941-067-536-586; 063-518-361-601-226; 064-484-968-441-883; 092-452-392-581-46X,0,true,,gold
120-188-559-440-567,Tactile web browsing for blind people,2007-09-12,2007,journal article,Multimedia Tools and Applications,13807501; 15737721,Springer Science and Business Media LLC,Netherlands,Martin Rotard; Christiane Taras; Thomas Ertl,,37,1,53,69,Dynamic web page; Web server; Web modeling; Web 2.0; World Wide Web; HTML; Web-based simulation; Static web page; Web standards; Client-side scripting; Web Accessibility Initiative; Web page; Web mapping; Web service; Computer science; Ajax; Web design; JavaScript; Web navigation; Web development; HTML5; Cascading Style Sheets,,,,,http://dx.doi.org/10.1007/s11042-007-0170-3 https://www.researchgate.net/profile/Christiane_Taras/publication/220663945_Tactile_web_browsing_for_blind_people/links/0912f50d36c6d57c56000000.pdf https://dx.doi.org/10.1007/s11042-007-0170-3 http://www.sfb716.uni-stuttgart.de/uploads/tx_vispublications/MTAP2007-Rotard-TWB.pdf https://dl.acm.org/citation.cfm?id=1343083 https://dl.acm.org/doi/abs/10.1007/s11042-007-0170-3 https://dblp.uni-trier.de/db/journals/mta/mta37.html#RotardTE08 https://link.springer.com/article/10.1007%2Fs11042-007-0170-3 http://www.vis.uni-stuttgart.de/uploads/tx_vispublications/MTAP2007-Rotard-TWB.pdf,http://dx.doi.org/10.1007/s11042-007-0170-3,,10.1007/s11042-007-0170-3,2080017708,,1,000-954-242-078-298; 001-500-232-053-716; 002-201-032-015-180; 028-657-433-450-369; 032-264-229-256-507; 049-401-792-382-532; 051-108-375-891-896; 077-889-798-538-689; 081-482-705-753-890; 085-960-456-264-98X; 092-118-304-240-830; 093-658-019-894-61X; 097-910-302-014-267; 098-572-364-265-333; 114-088-369-760-046; 127-901-083-564-809; 132-036-358-764-648; 140-890-329-875-368; 161-428-668-072-043; 166-653-260-829-13X; 172-279-260-425-40X; 172-859-953-149-170; 174-232-629-300-370; 198-343-496-169-673,33,false,,
120-238-153-114-350,"Design, development and performance analysis of cognitive assisting aid with multi sensor fused navigation for visually impaired people",2023-02-10,2023,journal article,Journal of Big Data,21961115,Springer Science and Business Media LLC,United Kingdom,Myneni Madhu Bala; D. N. Vasundhara; Akkineni Haritha; CH. V. K. N. S. N. Moorthy,"<jats:title>Abstract</jats:title><jats:p>The research and innovations in the field of wearable auxiliary devices for visually impaired and blind people are playing a vital role towards improving their quality of life. However, in spite of the promising research outcomes, the existing wearable aids has several weaknesses such as more weight, limitations in the number of features and cost. The main objective of this manuscript is to provide the detailed design of a novel lightweight wearable aid with higher number of features for visually impaired and blind people. The proposed research aims to design a cognitive assistant that will guide the blind people for walking by detecting the environment around them. The framework include a Multi-Sensor Fused Navigation system comprises of a sensor-based, vision-based, and cognitive (intelligent/smart) application. The visual features for the design include obstacle detection, uneven surface detection, slope and downward steps detection, pothole detection and hallow object detection; location tracking, walking guide, image capturing and video recording. This prototype is named as Blind’s Apron based on its appearance. The invention focusses on parameters like reduction on size (quite handy) and light weight (comfortable to wear), higher number of detection features, and minimum user intervention (high end operations like switching on and off). All user interactions are friendly and affordable to everyone. The results obtained in this research lead to a high end technical intervention with ease of use. Finally, the performance of the proposed cognitive assistant is tested with a user study in real-time. The feedback and corresponding results establish the effective outcome of the proposed invention which is a light weight and feature enhanced device with easily understandable instructions.</jats:p>",10,1,,,Computer science; Wearable computer; Human–computer interaction; Obstacle; Augmented reality; Cognition; Object detection; Computer vision; Wearable technology; Artificial intelligence; Multimedia; Embedded system; Pattern recognition (psychology); Neuroscience; Political science; Law; Biology,,,,,https://journalofbigdata.springeropen.com/counter/pdf/10.1186/s40537-023-00689-5 https://doi.org/10.1186/s40537-023-00689-5,http://dx.doi.org/10.1186/s40537-023-00689-5,,10.1186/s40537-023-00689-5,,,0,003-561-233-382-310; 010-371-675-659-125; 013-056-435-015-274; 016-663-644-145-618; 018-843-508-574-144; 025-390-882-033-641; 030-884-979-386-25X; 037-376-150-587-539; 039-209-657-828-295; 040-351-685-890-777; 040-635-976-918-076; 041-571-183-213-822; 053-826-604-836-17X; 058-799-545-804-872; 073-361-230-697-651; 074-130-193-645-753; 080-148-076-219-168; 088-551-209-391-094,9,true,cc-by,gold
120-307-776-404-078,A Fast and Comprehensive Indoor Scene Understanding Approach: For Blind People,2014-05-27,2014,conference proceedings,,,,,Mohamed Lamine Mekhalfi; Farid Melgani; Mohammed A.-M. Salem; Alaa Khamis,"Scene perception and understanding is regarded as a major prospect for blind and visually impaired people. To date, there is no system than can completely recover the sight to them. However, various designs intended to improve their quality of life have been proposed over the last years, and have proven that assistive technologies can provide a significant help towards achieving such a prospect. The vast majority of existing assistive prototypes tends to tackle the blind rehabilitation issue in two ways. The first one includes all sorts of navigational aids, while the second one refers to object recognition. In this context, we intend in this paper to contribute with novel assistive computer-vision-based designs, to solve the problem of fast and comprehensive indoor scene description for the blind. Proposed strategies have been conducted on real images and have proven to be prominent regarding obtained results.",,,1,5,Human–computer interaction; Bag-of-words model; Sight; Artificial intelligence; Perception; Context (language use); Blind rehabilitation; Visually impaired; Computer science; Cognitive neuroscience of visual object recognition,,,,,http://dl.acm.org/citation.cfm?id=2678832.2678955,http://dl.acm.org/citation.cfm?id=2678832.2678955,,,2465713597,,0,,0,false,,
120-512-675-769-214,Computer Supported Pre-operative Simulation of Neonatus Cranial Bone Bending in Craniosynostosis Surgery Planning,2006-05-19,2006,journal article,International Journal of Computer Assisted Radiology and Surgery,18616410; 18616429,Springer Science and Business Media LLC,Germany,,,1,S1,251,263,,,,,,,http://dx.doi.org/10.1007/s11548-006-0024-x,,10.1007/s11548-006-0024-x,,,0,,2,false,,
121-173-664-145-310,Autonomous UAV Trail Navigation with Obstacle Avoidance Using Deep Neural Networks,2020-09-22,2020,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Seungho Back; Gangik Cho; Jinwoo Oh; Xuan-Toa Tran; Hyondong Oh,,100,3,1195,1211,Deep learning; Artificial intelligence; Optical flow; Deep neural networks; Computer science; Obstacle avoidance; Real-time computing; Convolutional neural network; Path (graph theory),,,,National Research Foundation of Korea; National Research Foundation of Korea,https://scholarworks.unist.ac.kr/handle/201301/48295 https://dblp.uni-trier.de/db/journals/jirs/jirs100.html#BackCOTO20 https://doi.org/10.1007/s10846-020-01254-5 https://link.springer.com/article/10.1007/s10846-020-01254-5 https://www.scilit.net/article/91a268bfd6d7ccf5ce604a65e172c8a4 https://dialnet.unirioja.es/servlet/articulo?codigo=7819176,http://dx.doi.org/10.1007/s10846-020-01254-5,,10.1007/s10846-020-01254-5,3089298066,,0,000-122-946-719-709; 005-016-665-157-706; 005-237-145-655-209; 005-930-832-968-448; 011-969-064-214-381; 019-594-637-274-65X; 020-280-679-328-71X; 021-781-288-480-273; 023-174-131-121-553; 023-740-134-481-507; 023-838-793-812-416; 028-490-522-746-08X; 033-928-519-356-675; 036-548-527-819-398; 045-121-165-058-424; 045-565-041-480-612; 047-988-789-654-448; 048-095-208-536-821; 060-173-817-564-022; 068-762-869-922-684; 069-931-111-932-724; 074-006-536-890-200; 080-502-067-830-343; 082-481-087-393-892; 084-510-831-812-921; 089-196-130-137-694; 090-598-194-848-230; 090-970-238-752-029; 093-308-884-273-105; 093-860-086-109-06X; 096-791-478-246-840; 104-117-298-733-821; 107-780-842-901-455; 113-782-832-717-511; 124-121-064-158-550; 133-748-976-770-051; 141-850-126-151-124; 145-032-090-481-201; 147-187-948-964-386; 155-321-370-788-56X; 163-549-480-680-518; 164-922-759-483-000,33,false,,
121-309-359-844-617,State of the art review on walking support system for visually impaired people,,2011,journal article,International Journal of Biomechatronics and Biomedical Robotics,17576792; 17576806,Inderscience Publishers,,Eklas Hossain; Raisuddin Khan; Riza Muhida; Ahad Ali,"The technology for terrain detection and walking support system for blind people has rapidly been improved the last couple of decades but to assist visually impaired people may have started long ago. Currently, a variety of portable or wearable navigation system is available in the market to help the blind for navigating their way in his local or remote area. The focused category in this work can be subgroups as electronic travel aids (ETAs), electronic orientation aids (EOAs) and position locator devices (PLDs). However, we will focus mainly on electronic travel aids (ETAs). This paper presents a comparative survey among the various portable or wearable walking support systems as well as informative description (a subcategory of ETAs or early stages of ETAs) with its working principal advantages and disadvantages so that the researchers can easily get the current stage of assisting blind technology along with the requirement for optimising the design of walking support system for its users.",1,4,232,,Human–computer interaction; Engineering; Variety (cybernetics); Principal (computer security); Wearable computer; Artificial intelligence; Navigation system; Support system; Visually impaired; Computer vision; Focus (computing); Obstacle avoidance,,,,,https://www.inderscienceonline.com/doi/abs/10.1504/IJBBR.2011.043751 https://www.inderscience.com/link.php?id=43751 http://irep.iium.edu.my/23418/,http://dx.doi.org/10.1504/ijbbr.2011.043751,,10.1504/ijbbr.2011.043751,2094385032,,0,001-076-828-128-846; 012-613-947-324-057; 013-056-435-015-274; 013-551-333-215-556; 013-847-555-426-159; 014-157-934-526-940; 017-312-938-936-230; 020-678-057-737-391; 022-697-627-694-941; 030-376-516-941-332; 031-799-548-880-483; 032-531-242-860-059; 034-350-308-512-672; 036-280-949-293-766; 040-890-766-325-078; 043-899-478-442-384; 046-649-192-050-710; 048-066-235-481-368; 057-879-237-577-755; 064-309-126-323-022; 065-992-777-809-97X; 066-526-177-885-125; 078-013-883-109-627; 084-604-873-305-191; 098-811-589-762-176; 121-644-089-423-334; 125-501-372-179-565; 127-483-973-270-088; 132-287-528-433-371; 146-425-395-790-21X; 146-475-698-003-821; 172-545-025-378-390; 185-266-364-086-800; 185-783-077-691-122; 192-600-440-415-958,10,false,,
121-439-923-704-187,Smartphone-based computer vision travelling aids for blind and visually impaired individuals: A systematic review.,2020-04-17,2020,journal article,Assistive technology : the official journal of RESNA,19493614; 10400435,Taylor and Francis Ltd.,United Kingdom,Andrius Budrionis; Darius Plikynas; Povilas Daniušis; Audrius Indrulionis,"Given the growth in the numbers of visually impaired (VI) people in low-income countries, the development of affordable electronic travel aid (ETA) systems employing devices, sensors, and apps embe...",34,2,1,17,Human–computer interaction; Acquired immunodeficiency syndrome (AIDS); Visually impaired; Computer science; Cognitive neuroscience of visual object recognition,computer vision techniques; electronic travel aid; object recognition; obstacle detection; smartphone device,Blindness; Humans; Smartphone; Visually Impaired Persons,,Lietuvos Mokslo Taryba,https://europepmc.org/article/MED/32207640 https://www.ncbi.nlm.nih.gov/pubmed/32207640 https://www.tandfonline.com/doi/full/10.1080/10400435.2020.1743381,http://dx.doi.org/10.1080/10400435.2020.1743381,32207640,10.1080/10400435.2020.1743381,3012919787,,0,000-973-829-797-949; 004-269-574-716-057; 004-772-554-982-380; 007-619-251-889-048; 009-435-402-661-806; 011-126-524-756-136; 012-344-135-607-729; 016-415-106-921-058; 017-300-737-144-757; 017-804-671-829-970; 019-215-610-025-597; 019-914-067-455-570; 020-998-360-866-973; 022-839-587-252-946; 023-438-345-162-875; 023-688-634-685-180; 023-868-294-867-153; 027-780-924-643-322; 028-628-140-541-774; 030-091-798-618-058; 033-627-073-872-114; 033-940-482-248-436; 034-118-182-410-34X; 034-600-684-728-898; 038-858-720-835-740; 039-078-788-373-393; 039-720-388-801-921; 041-765-592-251-26X; 042-251-157-585-319; 042-625-517-285-285; 042-893-530-917-399; 045-599-200-378-144; 046-262-856-533-736; 053-215-378-840-089; 059-149-073-001-124; 061-205-550-296-347; 062-265-879-417-563; 062-370-727-285-236; 062-630-991-312-529; 064-043-611-038-916; 064-490-466-346-713; 066-305-886-229-375; 066-754-017-141-711; 067-894-235-988-508; 068-446-370-600-681; 068-526-647-480-345; 069-480-623-909-181; 069-810-669-971-158; 075-519-293-886-898; 077-455-969-276-663; 077-472-501-938-240; 082-716-665-111-206; 089-441-470-748-412; 089-822-552-618-426; 093-548-249-489-049; 094-570-327-764-040; 094-746-158-886-704; 097-395-382-637-910; 098-440-015-522-27X; 098-910-337-444-079; 102-952-660-914-374; 103-263-755-004-288; 103-769-217-393-19X; 104-208-613-793-423; 108-796-489-480-460; 113-195-576-127-857; 113-352-010-968-215; 119-516-706-529-460; 126-391-658-917-860; 129-366-417-730-311; 131-613-947-343-041; 134-226-557-899-385; 141-355-454-562-258; 156-959-541-956-206; 174-289-314-292-190; 186-751-156-331-141,40,false,,
121-635-909-620-620,Development of a visual to audio and tactile substitution system for mobility and orientation of visually impaired people: a review,2023-08-01,2023,journal article,Multimedia Tools and Applications,15737721; 13807501,Springer Science and Business Media LLC,Netherlands,Komal Mahadeo Masal; Shripad Bhatlawande; Sachin Dattatraya Shingade,,83,7,20387,20427,Computer science; Sensory substitution; Human–computer interaction; Perspective (graphical); Assistive technology; Orientation (vector space); Visually impaired; Emerging technologies; Population; Visual impairment; Multimedia; Artificial intelligence; Psychology; Cognitive psychology; Medicine; Geometry; Mathematics; Environmental health; Sensory system; Psychiatry,,,,,,http://dx.doi.org/10.1007/s11042-023-16355-0,,10.1007/s11042-023-16355-0,,,0,001-067-522-654-793; 001-433-725-596-623; 002-123-448-882-287; 003-485-284-705-210; 004-702-133-893-992; 005-642-690-871-424; 007-907-758-104-429; 008-139-948-663-237; 009-829-120-033-790; 009-896-970-982-095; 013-501-956-117-041; 014-090-386-279-841; 014-153-330-736-922; 015-672-084-912-775; 016-682-314-529-591; 017-499-516-520-553; 018-031-085-269-362; 018-843-508-574-144; 021-025-067-076-564; 022-622-218-704-364; 022-858-944-348-940; 023-688-956-164-353; 025-415-238-630-208; 027-720-558-803-335; 028-143-078-747-070; 032-102-165-870-302; 035-733-335-128-371; 037-199-326-802-024; 038-037-768-189-508; 038-326-446-864-583; 040-804-518-877-806; 049-120-683-911-297; 049-519-975-039-482; 052-142-044-790-359; 053-794-028-526-128; 055-949-804-362-602; 061-427-166-076-036; 064-228-677-130-580; 065-078-747-021-140; 065-272-316-935-556; 065-416-833-902-63X; 068-526-647-480-345; 070-859-524-188-835; 077-082-781-691-428; 083-194-118-366-374; 083-422-029-296-652; 084-088-294-723-948; 085-961-783-812-300; 086-023-599-511-650; 091-910-369-114-547; 092-630-635-515-571; 096-555-936-749-425; 096-733-124-330-978; 098-910-337-444-079; 099-164-837-383-131; 102-952-660-914-374; 111-903-320-332-699; 114-740-836-402-286; 123-890-691-474-106; 128-975-670-807-332; 135-105-511-909-109; 143-662-839-410-848; 149-376-162-623-147,6,false,,
121-953-987-795-796,Development of Self Speaking Body Weight Scale for Visually Impaired People in Tanzania,,2021,journal article,International Journal of Advances in Scientific Research and Engineering,24548006,Sretechjournal Publication,,Gloriose Nzasangamariya; Ramadhani Sinde; Shubi Kaijage,"Speaking weight scale is an important low vision health aid that measures and announces the measured weight. It is valuable in numerous applications such as Bathroom scale, Kitchen scale, and more. Different talking scales have been developed for the blind community. Many talking scales have language options for English, German, French, or Spanish. However, only limited work exists for Self speaking visually impaired community in EAC given the fact that no talking scale can announce weight in Self which is the common language in EAC. Therefore, this project aims to develop a Self-speaking weighing machine to assist visually impaired people in Tanzania. The developed device is divided into two major parts. On the front end of the design, sensors are used to capture weight parameters. The captured values are mapped onto a sequence of voice patterns. The back-end consists of transferring a sequence of voice patterns to a loudspeaker whereby the voice patterns are stored on an SD card. Finally, the developed device has been evaluated on several objects with known weights. The results show that the developed device accurately measures weight, displays weight, and announces it in the Self language. However, blind people still need assistance from sighted persons to be directed to the scale’s platform. The developed device has great potential as a low vision health aid for Self speakers. Moreover, the features of this device can be further improved by integrating iBeacon technology to increase the autonomy of blind people to use the scale and navigate to the device’s location safely.",07,07,28,33,Human–computer interaction; German; Loudspeaker; Self; iBeacon; Tanzania; Scale (chemistry); Visually impaired; Computer science; Autonomy,,,,,https://www.ijasre.net/index.php/ijasre/article/view/1305,http://dx.doi.org/10.31695/ijasre.2021.34044,,10.31695/ijasre.2021.34044,3192483698,,0,007-223-103-783-187; 008-972-422-228-943; 093-115-137-092-596; 161-766-603-289-518,0,true,,gold
121-978-164-772-471,Image to Audio Conversion to Aid Visually Impaired People by CNN,2023-07-06,2023,conference proceedings article,2023 4th International Conference on Electronics and Sustainable Communication Systems (ICESC),,IEEE,,D Sivaganesan; M Venkateshwaran; S P Dhinesh,"This study suggests an innovative method for helping people who are blind or visually handicapped by turning visuals into sounds. In the proposed system, audio descriptions are produced in real-time together with significant features that are extracted from photos using deep learning algorithms. The proposed work is developed to be user-friendly, which includes a simple interface that aids blind individuals to easily capture and process images using a mobile device. A user research was undertaken to assess the efficiency of the suggested method, and the results were encouraging in terms of precision and usability. This initiative offers a promising technique to give people who are blind or visually impaired an alternate means of perceiving and interacting with their environment, therefore improving their quality of life. The suggested picture to audio converter system aims to overcome the drawbacks of current assistive devices that rely on braille or textual descriptions. Blind people can more easily interpret visual information that is necessary for daily life, such as recognising items, interpreting signs, or navigate unfamiliar situations, through offering audio descriptions of images. The system makes use of recent deep learning developments that have significantly improved picture identification as natural language processing. As a result, the suggested technique has the ability to offer audio descriptions that are more precise and comprehensive than current methods. This technology has the potential to be implemented into a variety of products, from cellphones to intelligent glasses, and could significantly improve the lives of people who are blind or visually impaired.",,,,,Computer science; Usability; Braille; Human–computer interaction; Identification (biology); Process (computing); Multimedia; Interface (matter); Variety (cybernetics); Mobile device; Visually impaired; Natural (archaeology); Artificial intelligence; World Wide Web; History; Botany; Archaeology; Parallel computing; Biology; Operating system; Bubble; Maximum bubble pressure method,,,,,,http://dx.doi.org/10.1109/icesc57686.2023.10193308,,10.1109/icesc57686.2023.10193308,,,0,025-291-742-510-860; 042-715-509-051-963; 045-093-965-031-240; 063-783-140-107-766; 070-874-143-565-153; 134-981-576-377-848; 149-185-703-248-940; 156-274-893-770-382,1,false,,
122-485-641-822-488,Active navigation with a monocular robot,,1994,journal article,Biological Cybernetics,03401200; 14320770,Springer Science and Business Media LLC,Germany,P. J. Sobey,,71,5,433,440,Motion (physics); Artificial intelligence; Monocular vision; Mobile robot navigation; Autonomous robot; Computer vision; Computer science; Displacement (vector); Robot control; Monocular; Robot,,,,,https://dblp.uni-trier.de/db/journals/bc/bc71.html#Sobey94 https://link.springer.com/article/10.1007/BF00198919,http://dx.doi.org/10.1007/bf00198919,,10.1007/bf00198919,2048621813,,0,003-828-997-049-673; 007-545-564-798-885; 007-919-813-542-615; 009-379-873-960-42X; 015-314-443-510-277; 016-871-229-894-829; 020-637-948-609-616; 021-602-380-762-314; 022-782-613-437-190; 026-730-868-058-847; 026-953-090-649-547; 027-414-277-115-434; 027-614-751-078-287; 029-725-405-930-021; 033-728-550-404-501; 036-636-988-611-717; 036-937-804-474-025; 037-208-205-694-76X; 059-649-154-791-210; 067-668-236-111-761; 071-114-869-772-447; 074-465-026-221-579; 081-033-406-033-283; 084-029-329-593-38X; 087-558-627-033-930; 095-461-795-599-318; 096-108-733-130-409; 106-156-052-981-651; 122-029-494-293-58X; 140-200-977-966-646; 147-156-299-805-358; 163-524-925-370-993; 164-634-242-932-292; 169-219-460-630-368,34,false,,
122-631-967-023-822,Robotics-Based Obstacle-Avoidance Systems for the Blind and Visually Impaired,,2003,,,,,,Shraga Shoval; Iwan Ulrich; Johann Borenstein,"This article presents two novel travel aids for blind pedestrians. The two computerized devices are based on advanced mobile robotics obstacle avoidance technologies. The first aid – the NavBelt – is worn by the user like a belt and is equipped with an array of ultrasonic sensors. It provides, via a set of stereo earphones, acoustic signals that guide the user around obstacles, or “displays” a virtual acoustic panoramic image of the traveler’s surroundings. One limitation of the NavBelt is that it is exceedingly difficult for the user to comprehend the guidance signals in time to allow fast walking. A newer device, called GuideCane effectively overcomes this problem. The GuideCane uses the same mobile robotics technology as the NavBelt but it is a wheeled device pushed ahead of the user via an attached cane. When the GuideCane detects an obstacle it steers around it. The user immediately feels this steering action and can follow the GuideCane’s new path easily and without any conscious effort. This article describes the two devices, including the mechanical, electronic, and software components, user-machine interface, and some experimental results.",,,,,Interface (computing); Engineering; Artificial intelligence; Set (psychology); Component-based software engineering; Obstacle; Action (philosophy); PATH (variable); Computer vision; Robotics; Obstacle avoidance,,,,,http://www-personal.umich.edu/~johannb/Papers/paper85.pdf,http://www-personal.umich.edu/~johannb/Papers/paper85.pdf,,,69628867,,0,010-791-941-379-195; 012-220-148-772-614; 045-321-254-881-478; 046-649-192-050-710; 055-224-290-217-847; 058-443-047-291-990; 065-992-777-809-97X; 073-072-747-275-047; 073-722-974-726-137; 078-736-735-476-395; 087-838-253-289-179; 096-803-870-115-728; 098-867-223-476-32X; 127-129-010-898-639; 135-646-014-794-306,10,false,,
122-644-434-677-391,An Indoor Navigation Service Robot System Based on Vibration Tactile Feedback,2017-04-11,2017,journal article,International Journal of Social Robotics,18754791; 18754805,Springer Science and Business Media LLC,Germany,Huang Peng; Guangming Song; Jian You; Ying Zhang; Jie Lian,,9,3,331,341,Trajectory; Wearable computer; Artificial intelligence; Mobile robot navigation; Service robot; Tactile perception; Computer vision; Robotics; Computer science; Simulation; Robot; Robustness (computer science),,,,National Natural Science Foundation of China,https://dblp.uni-trier.de/db/journals/ijsr/ijsr9.html#PengSYZL17 https://link.springer.com/article/10.1007/s12369-017-0403-1,http://dx.doi.org/10.1007/s12369-017-0403-1,,10.1007/s12369-017-0403-1,2606453423,,0,000-391-287-701-673; 003-274-959-651-784; 006-413-645-080-743; 013-736-048-543-391; 014-119-391-926-878; 017-467-901-574-623; 020-100-925-249-005; 024-430-366-548-11X; 027-033-143-017-928; 029-293-172-783-836; 033-940-482-248-436; 047-417-556-221-888; 049-878-204-983-752; 051-861-879-703-203; 057-892-590-045-967; 063-334-343-483-565; 076-887-216-382-077; 091-764-827-661-79X; 097-735-406-546-035; 098-430-297-950-379; 099-113-948-250-462; 101-718-374-273-666; 119-865-444-409-025; 128-724-129-542-026; 165-646-371-179-035; 170-662-239-998-00X,10,false,,
122-799-035-680-952,CityGuide: a seamless indoor–outdoor wayfinding system for people with vision impairments,2023-06-28,2023,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Seyed Ali Cheraghi; Vinod Namboodiri; Güler Arsal,,,,,,Computer science; Global Positioning System; Navigation system; Human–computer interaction; Bluetooth; Visually impaired; Real-time computing; Telecommunications; Wireless,,,,Directorate for Computer and Information Science and Engineering,,http://dx.doi.org/10.1007/s10209-023-01009-7,,10.1007/s10209-023-01009-7,,,0,004-475-836-966-337; 004-611-538-214-341; 011-951-961-574-170; 028-707-243-078-925; 055-660-300-282-556; 057-847-555-860-541; 062-370-727-285-236; 092-391-305-190-481; 115-998-456-191-40X; 139-114-429-196-103,0,false,,
123-202-479-988-060,WAY FINDING ELECTRONIC BRACELET FOR VISUALLY IMPAIRED PEOPLE,2017-04-30,2017,,,,,,Tejaswini Ananda Gavade; Komal Sitaram Dange; Vishal Vikas Mane; Dipali G. Sadekar,This paper represents hardware implementation of finding way using bracelet or cane. Way finding is depending upon movement of the user. God gifted sense of vision to the human being is an important aspect of our life. But there are some unfortunate people who lack the ability of visualizing things. The visually impaired have to face many challenges in their daily life. The problem gets worse when there is an obstacle in front of them. Blind stick is an innovative stick designed for visually disabled people for improved navigation. The project presents a practical system to provide a smart ultrasonic aid for blind people. The system is intended to provide overall measures â€“ Artificial vision and object detection. The aim of the overall system is to provide a low cost and efficient navigation aid for a visually impaired person who gets a sense of artificial vision by providing information about the environmental scenario of static and dynamic objects around them. Ultrasonic sensors are used to calculate distance of the obstacles around the blind person to guide the user towards the available path.,3,04,9,12,Human–computer interaction; Object detection; Obstacle; PATH (variable); Artificial vision; Navigation aid; Way finding; Disabled people; Visually impaired; Computer science,,,,,https://repo.journalnx.com/index.php/nx/article/view/2658,https://repo.journalnx.com/index.php/nx/article/view/2658,,,3152934642,,0,,0,false,,
123-216-370-976-514,Enhancing statistical chart accessibility for people with low vision: insights from a user test,2024-05-08,2024,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Rubén Alcaraz-Martínez; Mireia Ribera; Adrià Adeva-Fillol; Afra Pascual-Almenara,"<jats:title>Abstract</jats:title><jats:p>A remote user test was performed with two versions (one accessible and one non-accessible) of three types of web-based charts (horizontal bar chart, vertical stacked bar chart, and line chart). The objectives of the test were: (a) to validate a set of heuristic indicators for the evaluation of the accessibility of statistical charts presented in a previous work (Fariñas Falcón et al. in Mediocentro Electrónica 21(1):65–68, 2017); (b) to identify new barriers and preferences for users with low vision in the access and use of this content not previously contemplated. 12 users were tested, with a variety of conditions associated with low vision: low visual acuity (6 users), reduced central vision (2 users), reduced peripheral vision (2 users), blurry vision (1 user), sensitivity to light (3 users), Nystagmus (2 users) and color vision deficiency (CVD) (4 users). From a quantitative standpoint, accessible versions of charts were more efficient, effective, and satisfactory. From a qualitative point of view, results verify the relevance of heuristics H2, Legend; H3, Axes; H6, Data source (as data table); H10, Safe colors; H11, Contrast; H12, Legibility; H13, Image quality; H14, Resize; H16, Focus visible; H17, Independent navigation; related to the proposed tasks. As new observations, tooltips were highly valued by all users, but their implementation must be improved to avoid covering up significant parts of the charts when displayed. The data table has also been frequently used by all users, especially in the non-accessible versions, allowing them to carry out tasks more efficiently. The position and size of the legend can be a significant barrier if it is too small or appears in an unusual position. Finally, despite the limitations related to color perception, some users prefer color graphics to black and white, so, to target all profiles, it is necessary to redundantly encode categories with colors and patterns as well.</jats:p>",,,,,Chart; Computer science; Test (biology); Low vision; Human–computer interaction; Information retrieval; Optometry; Statistics; Medicine; Mathematics; Paleontology; Biology,,,,"Agencia Estatal de Investigación (AEI), Spain; Universitat de Barcelona",https://link.springer.com/content/pdf/10.1007/s10209-024-01111-4.pdf https://doi.org/10.1007/s10209-024-01111-4,http://dx.doi.org/10.1007/s10209-024-01111-4,,10.1007/s10209-024-01111-4,,,0,002-195-301-076-590; 003-361-746-002-524; 005-683-548-989-545; 013-154-105-974-754; 013-485-093-061-179; 013-829-872-250-493; 014-925-639-618-702; 015-839-442-082-698; 017-977-871-188-114; 019-533-544-686-031; 019-625-834-542-667; 020-074-292-662-206; 020-671-301-375-600; 020-711-208-605-516; 022-414-146-824-79X; 027-727-618-742-485; 029-420-088-068-863; 030-536-830-085-01X; 034-016-789-400-095; 036-895-827-642-260; 039-261-041-475-960; 040-335-036-012-704; 042-381-074-344-865; 044-356-060-642-288; 047-737-992-293-872; 048-432-642-518-233; 049-784-481-197-736; 052-037-636-497-266; 052-266-909-935-081; 053-628-776-837-139; 053-833-674-032-922; 055-704-268-515-146; 055-730-471-788-800; 056-621-206-285-122; 056-660-526-272-648; 057-054-513-506-464; 058-247-393-278-352; 058-817-323-134-531; 060-230-697-077-382; 060-736-925-658-153; 063-309-598-226-11X; 065-983-543-415-033; 067-983-639-563-762; 071-473-189-198-757; 078-640-373-091-693; 078-875-379-560-055; 081-673-190-031-03X; 087-154-996-019-893; 091-578-168-871-304; 092-477-848-831-529; 092-960-526-879-01X; 093-668-999-939-359; 102-129-387-771-577; 105-790-942-783-392; 109-573-436-202-30X; 116-308-781-027-254; 120-674-325-697-857; 125-789-763-828-601; 136-201-645-570-805; 138-828-116-565-842; 142-902-126-160-74X; 148-152-866-858-148; 159-879-304-340-538; 164-604-233-574-000; 170-157-031-005-497; 179-044-661-384-950; 199-416-944-510-769,0,true,cc-by,hybrid
123-403-530-043-451,Real-time Machine Vision System for the Visually Impaired,2024-04-04,2024,journal article,SN Computer Science,26618907; 2662995x,Springer Science and Business Media LLC,,A. N. Krishna; Y. L. Chaitra; Atul M. Bharadwaj; K. T. Abbas; Allen Abraham; Anirudh S. Prasad,,5,4,,,Visually impaired; Impaired Vision; Computer science; Machine vision; Computer vision; Artificial intelligence; Human–computer interaction; Optometry; Medicine,,,,,,http://dx.doi.org/10.1007/s42979-024-02741-4,,10.1007/s42979-024-02741-4,,,0,007-844-504-285-762; 007-907-758-104-429; 013-653-425-479-927; 029-360-136-216-567; 030-127-369-356-741; 032-156-041-220-133; 044-907-063-003-762; 047-326-510-583-783; 060-524-782-488-948; 069-480-623-909-181; 071-315-108-184-561; 089-606-647-937-546; 092-726-040-947-902; 099-627-678-838-087; 104-900-805-549-848; 124-591-330-711-213; 125-678-921-586-723; 146-035-713-738-545,0,false,,
124-011-309-146-252,Calibration of bi-planar radiography with a minimal-size calibration object,2008-05-27,2008,journal article,International Journal of Computer Assisted Radiology and Surgery,18616410; 18616429,Springer Science and Business Media LLC,Germany,Daniel C. Moura; Jorge G. Barbosa; João Manuel R. S. Tavares; Ana Mafalda Fontes Pinto Dos Reis,"[Abstract] The 22nd International Congress and Exhibition, Barcelona, Spain, June 25-28, 2008",3,S1,330,417,Electrical engineering; Computer graphics (images); Calibration (statistics); Planar; Geography; International congress; Radiography; Exhibition; Object (computer science),,,,,https://paginas.fe.up.pt/~tavares/downloads/publications/artigos/cars08_abstract.pdf https://repositorio-aberto.up.pt/handle/10216/100652 https://paginas.fe.up.pt/%7Etavares/downloads/publications/artigos/cars08_poster.pdf https://e-archivo.uc3m.es/handle/10016/12019 https://web.fe.up.pt/~tavares/downloads/publications/artigos/cars08_abstract.pdf https://core.ac.uk/download/30044435.pdf,http://dx.doi.org/10.1007/s11548-008-0206-9,,10.1007/s11548-008-0206-9,977441193,,0,060-363-354-405-46X,1,true,,green
124-421-562-309-002,Automated angular measurement for puncture angle using a computer-aided method in ultrasound-guided peripheral insertion.,2024-02-15,2024,journal article,Physical and engineering sciences in medicine,26624737; 26624729,Springer Science and Business Media LLC,Switzerland,Haruyuki Watanabe; Hironori Fukuda; Yuina Ezawa; Eri Matsuyama; Yohan Kondo; Norio Hayashi; Toshihiro Ogura; Masayuki Shimosegawa,,47,2,679,689,Hough transform; Artificial intelligence; Computer vision; Segmentation; Biomedical engineering; Image processing; Ultrasound; Computer science; Visibility; Medicine; Image (mathematics); Radiology; Optics; Physics,Angular measurements; Deep learning; Puncture angle; U-Net; Ultrasound,"Humans; Phantoms, Imaging; Punctures; Image Processing, Computer-Assisted; Automation; Deep Learning; Needles; Ultrasonography; Adult; Male",,,,http://dx.doi.org/10.1007/s13246-024-01397-x,38358620,10.1007/s13246-024-01397-x,,,0,001-233-447-583-796; 002-649-617-859-886; 004-368-396-805-615; 005-604-044-453-95X; 008-997-317-290-754; 009-182-304-050-750; 024-797-733-626-396; 025-107-584-803-250; 028-178-630-811-911; 029-818-190-718-068; 034-727-300-634-760; 035-610-636-874-298; 041-314-165-476-904; 051-540-103-423-047; 054-041-970-960-409; 055-179-506-278-615; 057-370-256-331-855; 063-312-554-059-898; 068-426-867-257-585; 071-746-516-195-598; 072-626-661-840-928; 078-820-702-625-026; 079-302-674-161-434; 083-264-478-746-339; 087-318-895-062-33X; 098-589-983-993-779; 099-409-022-260-723; 109-111-937-618-622; 116-506-370-824-248; 127-667-381-043-590; 128-280-302-391-803; 128-511-432-427-902; 130-351-723-185-79X; 135-597-376-660-075; 137-860-922-920-590; 144-046-320-706-463; 147-211-018-624-577; 148-525-819-217-988; 148-981-250-371-997; 158-589-133-063-801; 167-131-435-704-717,0,false,,
124-990-634-641-902,AMCIS - e-Accessibility: Making the Web Accessible to the Visually Impaired Persons,2008-12-15,2008,conference proceedings,,,,,Simone Bacellar Leal Ferreira; Denis Silva da Silveira; Marie Agnes Chauvel; Marcos Gurgel do Amaral Leal Ferreira,"Accessibility is the possibility of any person to make use of all the benefits of society, including the use of the Inte rnet. Graphical are an obstacle for visually impaired persons to access the Internet, so they need a support technology capable of capturing interfaces and making them accessible. Interfaces should be designed so that when accessed by support technologies they continue to be friendly. For a site to be accessible to blind persons it is necessary that the information be reproduced by means of an “equivalent” textual description, capable of transmitting the same information as the visual resources. The present study is aimed at identifying and defining usability guidance compliant with accessibility W3C directives that can facilitate the interaction between visually impaired and the Internet and still guarantee sites with understandable navigation content. Towa rds this end an exploratory study was conducted, comprised of a field study and interviews with various visually disabled people from the Instituto Benjamin Constant , reference center in Brazil for the education and re -education of visually impaired person s, in order to get to know visually disabled users better . Through the understanding acquired, different types of impositions and limits that these users are subject to have been identified, enabling a better perception of their needs and special abilities . The impaired user -machine interaction were observed and analyzed, which enabled the identification of aspects that could contribute to the accessibility of sites, with emphasis on facilitating the access of those visually impaired to the Web.",,,120,,Human–computer interaction; The Internet; World Wide Web; Usability; Perception; Visually Impaired Persons; Field (computer science); Computer science; Identification (information),,,,,https://aisel.aisnet.org/cgi/viewcontent.cgi?article=1129&context=amcis2008 http://aisel.aisnet.org/amcis2008/120/ https://dblp.uni-trier.de/db/conf/amcis/amcis2008.html#FerreiraSCF08,http://aisel.aisnet.org/amcis2008/120/,,,1508732324,,0,008-021-754-708-595; 010-633-147-264-012; 035-501-447-998-846; 041-701-361-906-617; 058-100-144-971-379; 127-367-438-036-427; 163-918-564-862-088; 166-528-896-364-072; 183-617-660-742-453; 194-829-956-963-902,1,false,,
125-129-152-920-421,Object Detection in Computer Vision Using Machine Learning Algorithm For Visually Impaired People,2022-10-16,2022,conference proceedings article,2022 IEEE 2nd Mysore Sub Section International Conference (MysuruCon),,IEEE,,Dileep Reddy Bolla; S Trisheela; P Nagarathana; Sarojadevi H.,"Traditionally, blind individuals go to their destinations with a white cane or a guide dog. They were unable to quickly recognize their surroundings, though. Consequently, The development of a navigation system for visually impaired people in an interior environment is the topic of this study. In order to provide an efficient and user-friendly navigation tool, passive radio frequency identification (RFID) transponders are mounted on the floor, such as on tactile pavement, to establish such RFID networks. the navigational system created includes a digital compass to help persons who are blind or visually handicapped walk correctly and in the right direction, especially while turning. This system makes use of the ideas of localization and placement using a digital compass as well as vocal prompts for directions. Others looked into how to get blind people back onto the right path when they go off course and calibrate the digital compass for accuracy. Additionally, in order to determine the practicality of the developed navigation system, a comparison between two subjects—a mobile robot and a human—is done. The results of the experiment show that a mobile robot and a human move at different speeds. People who are blind or visually impaired will benefit from this project since the voice-activated navigation system will improve their travel experience and make it safer and more comfortable.",,,,,Compass; Computer vision; Computer science; Visually impaired; Artificial intelligence; SAFER; Navigation system; Human–computer interaction; Mobile robot navigation; Identification (biology); Robot; Mobile robot; Computer security; Botany; Cartography; Robot control; Biology; Geography,,,,,,http://dx.doi.org/10.1109/mysurucon55714.2022.9972494,,10.1109/mysurucon55714.2022.9972494,,,0,003-532-678-017-874; 016-572-318-414-022; 024-828-978-723-644; 025-112-431-982-303; 027-158-849-017-398; 027-588-207-376-882; 027-818-804-198-595; 030-767-420-001-184; 033-618-910-829-718; 037-017-970-019-639; 039-724-288-440-151; 041-872-163-351-477; 058-465-600-944-828; 067-573-508-702-134; 076-930-338-217-361; 092-958-099-002-180; 098-566-964-227-081; 098-926-109-358-108; 121-865-935-216-30X; 140-921-274-168-71X; 149-821-637-885-419; 187-810-073-579-41X,2,false,,
125-179-210-676-865,"RGB-D image-based detection of stairs, pedestrian crosswalks and traffic signs",2014-02-01,2014,journal article,Journal of Visual Communication and Image Representation,10473203,Academic Press Inc.,United States,null WangShuihua; null PanHangrong; null ZhangChenyang; null TianYingli,"A computer vision-based wayfinding and navigation aid can improve the mobility of blind and visually impaired people to travel independently. In this paper, we develop a new framework to detect and...",,,,,Artificial intelligence; Pedestrian; Stairs; Navigation aid; Image based; Visually impaired; Computer vision; Computer science; Cognitive neuroscience of visual object recognition,,,,,https://dl.acm.org/citation.cfm?id=2580412,https://dl.acm.org/citation.cfm?id=2580412,,,2997057842,,0,,0,false,,
125-249-465-698-661,Multimodal Interaction Strategies for Walker-Assisted Gait: A Case Study for Rehabilitation in Post-Stroke Patients,2024-01-16,2024,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Mario F. Jimenez; Ricardo C. Mello; Flavia Loterio; Anselmo Frizera-Neto,"<jats:title>Abstract</jats:title><jats:p>Stroke has been considered the main cause of neuromuscular damages worldwide and one of the most common causes of walking disabilities, with approximately 60% of the individuals suffering from persistent problems in walking. These patients generally use technical aids for walking to achieve independent gait, however, when cognitive impairments are also present, conventional assistive devices such as walkers could be difficult to handle. By leveraging multimodal interfaces, smart walkers can offer natural and intuitive human-robot interaction. In this work, we present two multimodal interaction strategies for smart walkers focusing on guiding post-stroke patients through their environment. These strategies leverage different communication channels and provide distinct levels of guidance: one strategy uses haptic feedback and a visual interface to indicate the desired path to the user, while the other strategy uses haptic feedback and a virtual torque to maintain the user on path. We also present two case studies with post-stroke patients to preliminarily validate these interaction strategies with their target population and to collect valuable insight as to how multimodal strategies for smart walkers can be enhanced to deal with the characteristic asymmetries of post-stroke patients. Our results show that both strategies can guide the volunteers, however, the first one demands more effort from the volunteer and is more suited for patients with increased levels of independence. The second interaction strategy allows for higher linear velocity (Volunteer 1, <jats:inline-formula><jats:alternatives><jats:tex-math>$$\varvec{0.18}$$</jats:tex-math><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"">;                   <mml:mrow>;                     <mml:mn>0.18</mml:mn>;                   </mml:mrow>;                 </mml:math></jats:alternatives></jats:inline-formula><jats:inline-formula><jats:alternatives><jats:tex-math>$$\varvec{\pm 0.026}$$</jats:tex-math><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"">;                   <mml:mrow>;                     <mml:mo>±</mml:mo>;                     <mml:mn>0.026</mml:mn>;                   </mml:mrow>;                 </mml:math></jats:alternatives></jats:inline-formula><jats:inline-formula><jats:alternatives><jats:tex-math>$$\varvec{m/s}$$</jats:tex-math><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"">;                   <mml:mrow>;                     <mml:mi>m</mml:mi>;                     <mml:mo>/</mml:mo>;                     <mml:mi>s</mml:mi>;                   </mml:mrow>;                 </mml:math></jats:alternatives></jats:inline-formula>; Volunteer 2, <jats:inline-formula><jats:alternatives><jats:tex-math>$$\varvec{0.22}$$</jats:tex-math><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"">;                   <mml:mrow>;                     <mml:mn>0.22</mml:mn>;                   </mml:mrow>;                 </mml:math></jats:alternatives></jats:inline-formula><jats:inline-formula><jats:alternatives><jats:tex-math>$$\varvec{\pm 0.0283}$$</jats:tex-math><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"">;                   <mml:mrow>;                     <mml:mo>±</mml:mo>;                     <mml:mn>0.0283</mml:mn>;                   </mml:mrow>;                 </mml:math></jats:alternatives></jats:inline-formula><jats:inline-formula><jats:alternatives><jats:tex-math>$$\varvec{m/s}$$</jats:tex-math><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"">;                   <mml:mrow>;                     <mml:mi>m</mml:mi>;                     <mml:mo>/</mml:mo>;                     <mml:mi>s</mml:mi>;                   </mml:mrow>;                 </mml:math></jats:alternatives></jats:inline-formula>) than the first one (Volunteer 1, <jats:inline-formula><jats:alternatives><jats:tex-math>$$\varvec{0.10}$$</jats:tex-math><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"">;                   <mml:mrow>;                     <mml:mn>0.10</mml:mn>;                   </mml:mrow>;                 </mml:math></jats:alternatives></jats:inline-formula><jats:inline-formula><jats:alternatives><jats:tex-math>$$\varvec{\pm 0.031}$$</jats:tex-math><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"">;                   <mml:mrow>;                     <mml:mo>±</mml:mo>;                     <mml:mn>0.031</mml:mn>;                   </mml:mrow>;                 </mml:math></jats:alternatives></jats:inline-formula><jats:inline-formula><jats:alternatives><jats:tex-math>$$\varvec{m/s}$$</jats:tex-math><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"">;                   <mml:mrow>;                     <mml:mi>m</mml:mi>;                     <mml:mo>/</mml:mo>;                     <mml:mi>s</mml:mi>;                   </mml:mrow>;                 </mml:math></jats:alternatives></jats:inline-formula>; Volunteer 2, <jats:inline-formula><jats:alternatives><jats:tex-math>$$\varvec{0.20}$$</jats:tex-math><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"">;                   <mml:mrow>;                     <mml:mn>0.20</mml:mn>;                   </mml:mrow>;                 </mml:math></jats:alternatives></jats:inline-formula><jats:inline-formula><jats:alternatives><jats:tex-math>$$\varvec{\pm 0.012}$$</jats:tex-math><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"">;                   <mml:mrow>;                     <mml:mo>±</mml:mo>;                     <mml:mn>0.012</mml:mn>;                   </mml:mrow>;                 </mml:math></jats:alternatives></jats:inline-formula><jats:inline-formula><jats:alternatives><jats:tex-math>$$\varvec{m/s}$$</jats:tex-math><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"">;                   <mml:mrow>;                     <mml:mi>m</mml:mi>;                     <mml:mo>/</mml:mo>;                     <mml:mi>s</mml:mi>;                   </mml:mrow>;                 </mml:math></jats:alternatives></jats:inline-formula>), suggesting improved guidance.</jats:p>",110,1,,,Haptic technology; Rehabilitation; Multimodal interaction; Human–computer interaction; Physical medicine and rehabilitation; Computer science; Gait; Leverage (statistics); Population; Stroke (engine); Artificial intelligence; Physical therapy; Medicine; Engineering; Mechanical engineering; Environmental health,,,,H2020 European Research Council; FAPES; CNPq; Universidad del Rosario,https://link.springer.com/content/pdf/10.1007/s10846-023-02031-w.pdf https://doi.org/10.1007/s10846-023-02031-w,http://dx.doi.org/10.1007/s10846-023-02031-w,,10.1007/s10846-023-02031-w,,,0,002-690-859-505-76X; 004-297-940-009-335; 006-675-659-208-672; 007-409-223-391-642; 009-415-702-707-118; 011-799-078-023-974; 013-348-228-876-500; 016-503-335-302-381; 016-727-745-179-50X; 018-444-905-320-14X; 019-641-881-998-210; 020-243-038-140-642; 021-915-282-244-393; 024-320-175-256-360; 033-500-637-438-279; 034-631-044-795-533; 035-428-825-402-272; 038-189-158-765-996; 039-510-695-098-396; 044-983-691-260-481; 050-489-372-109-454; 051-041-011-437-728; 060-005-612-382-855; 066-338-849-146-328; 069-523-876-223-951; 076-121-436-407-96X; 077-830-779-795-675; 087-181-732-370-085; 096-375-704-050-625; 105-342-648-744-991; 107-063-472-129-653; 111-853-427-895-974; 112-731-483-510-777; 119-937-626-294-88X; 122-046-771-566-30X; 130-440-149-627-155; 133-143-498-039-296; 141-423-200-657-339; 154-766-342-680-274; 190-950-526-049-490,1,true,cc-by,hybrid
125-444-903-283-620,Image guided craniomaxillofacial procedures,2008-05-16,2008,journal article,International Journal of Computer Assisted Radiology and Surgery,18616410; 18616429,Springer Science and Business Media LLC,Germany,,,3,S1,223,229,Computer science; Health informatics; Computer vision; Artificial intelligence; Medical physics; Medicine; Pathology; Public health,,,,,,http://dx.doi.org/10.1007/s11548-008-0202-0,,10.1007/s11548-008-0202-0,,,0,,0,false,,
125-617-649-438-262,Application of the Fuzzy Logic for the Development of Automnomous Robot with Obstacles Deviation,2018-04-07,2018,journal article,"International Journal of Control, Automation and Systems",15986446; 20054092,Springer Science and Business Media LLC,South Korea,Lucas Alves Dias; Roger William de Oliveira Silva; Paulo César da Silva Emanuel; André Ferrus Filho; Rodrigo T. Bento,,16,2,823,833,Control engineering; Artificial intelligence; Mobile robot; Navigation system; Autonomous robot; Mechatronics; Robotics; Computer science; Fuzzy logic; Control theory; Robot,,,,,https://link.springer.com/article/10.1007/s12555-017-0055-9,http://dx.doi.org/10.1007/s12555-017-0055-9,,10.1007/s12555-017-0055-9,2785535268,,0,010-161-396-437-209; 033-981-139-320-630; 041-015-983-423-345; 047-085-274-934-873; 047-110-451-642-575; 049-754-221-403-495; 053-232-398-466-447; 058-939-326-089-567; 066-692-923-423-453; 068-756-571-356-084; 071-742-600-200-486; 080-182-771-129-331; 084-210-369-797-885; 085-321-591-676-564; 089-322-359-271-431; 098-244-451-849-756; 104-720-020-706-411; 119-427-076-022-088; 121-623-813-202-469; 163-712-641-847-874; 180-388-168-094-17X,14,false,,
126-112-927-352-833,Vision-Based Real-Time Object Detection and Voice Alert for Blind Assistance System,2023-08-25,2023,conference proceedings article,2023 IEEE 17th International Conference on Industrial and Information Systems (ICIIS),,IEEE,,S.A. Nayana Thilanka; G.C.H. Vihanga Dimantha Jinadasa; Prasad M. Bandara; W.M.M. Tharindu Weerakoon,"Blindness is a very common and unendurable disability among many disabilities. About 285 million people are visually impaired in the world, as per the WHO. Visually impaired people often face various challenges in their daily lives, particularly when it comes to navigating unfamiliar locations and recognizing unfamiliar objects in their surroundings. State-of-the-art computer vision technologies can be used for assisting visually impaired people. Real-time object detection is a difficult operation as it requires more computing power to recognize objects in real time. SSD (Single Shot multi-box Detection) is a quicker detection method that is suitable for real-time object detection. A lightweight network model is required as the prototype device is tested with a Raspberry Pi microprocessor. Therefore, MobileNetV2 is incorporated with the SSD model to increase the accuracy level of detecting real-time household objects. The developed portable prototype comprises Raspberry Pi 3 model B, a USB camera, earphones, a distance sensor, and rechargeable batteries. The objects captured by the camera are classified by the object detection model and the audio feedback is given to the user through the earphones.",,,,,Computer science; USB; Object detection; Computer vision; Object (grammar); Artificial intelligence; Real-time computing; Software; Pattern recognition (psychology); Programming language,,,,,,http://dx.doi.org/10.1109/iciis58898.2023.10253546,,10.1109/iciis58898.2023.10253546,,,0,015-672-084-912-775; 068-526-647-480-345; 150-874-452-798-479; 164-332-313-558-557,0,false,,
126-440-575-968-02X,Blind Hexapod Locomotion in Complex Terrain with Gait Adaptation Using Deep Reinforcement Learning and Classification,2020-03-19,2020,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Teymur Azayev; Karel Zimmerman,,99,3,659,671,Hexapod; Recurrent neural network; Artificial intelligence; Terrain; Actuator; Inference; Computer vision; Scalability; Artificial neural network; Reinforcement learning,,,,Grantová Agentura České Republiky,https://link.springer.com/article/10.1007/s10846-020-01162-8 https://dblp.uni-trier.de/db/journals/jirs/jirs99.html#AzayevZ20 https://dialnet.unirioja.es/servlet/articulo?codigo=7815488 https://doi.org/10.1007/s10846-020-01162-8,http://dx.doi.org/10.1007/s10846-020-01162-8,,10.1007/s10846-020-01162-8,3010754899,,0,002-216-542-761-53X; 011-304-727-115-979; 025-400-786-602-169; 028-972-844-706-488; 033-222-179-746-778; 039-855-516-046-229; 041-485-584-093-015; 043-835-045-438-598; 049-483-581-867-363; 052-917-929-460-227; 053-094-537-530-97X; 057-661-109-738-671; 072-111-786-616-054; 074-184-836-409-345; 083-028-165-506-866; 092-916-006-153-735; 100-795-059-852-580; 107-999-899-883-274; 120-033-505-076-310; 122-063-432-254-138; 129-293-051-107-888; 140-689-370-341-638; 181-422-400-221-157; 184-997-053-850-886; 190-888-400-321-773; 199-249-248-866-23X,22,false,,
126-598-610-396-056,Design and Evaluation of a Multi-Sensor Assistive Robot for the Visually Impaired,2023-10-17,2023,book chapter,Lecture Notes in Mechanical Engineering,21954356; 21954364,Springer Nature Singapore,United States,S. Bhaskar Nikhil; Ambuj Sharma; Niranjan S. Nair; C. Sai Srikar; Yatish Wutla; Bhavanasi Rahul; Suyog Jhavar; Pankaj Tambe,"Visual impairment affects approximately 285 million people worldwide, the profound challenge faced by those who are visually impairedVisually impaired or blind lies in the intricate task of maneuvering through different surroundings. The intricate nature of navigating both indoor and outdoor spaces poses significant difficulties for people with visual impairments. This paper presents an innovative undertaking that strives to address this very challenge by developing a cutting-edge robot dedicated to guiding the blind securely through various environments. Employing a sophisticated amalgamation of diverse sensors and a high-resolution camera, the robot adeptly identifies obstacles and furnishes the user with real-time information pertaining to their immediate surroundings. Remarkably, this state-of-the-art robot can be seamlessly operated through voice commandsVoice command, facilitated by a bespoke mobile application, thereby enabling users to effortlessly guide it to their desired destinations. Equipped with comprehensive audio feedback capabilities, the robot effectively communicates crucial details to the user, encompassing obstacles, directions, and more. This robot can help blind people to navigate unfamiliar environments and travel independently.",,,119,131,Bespoke; Human–computer interaction; Robot; Computer science; Visually impaired; Task (project management); Multimedia; Engineering; Artificial intelligence; Systems engineering; Political science; Law,,,,,,http://dx.doi.org/10.1007/978-981-99-5613-5_10,,10.1007/978-981-99-5613-5_10,,,0,002-026-495-923-809; 020-302-288-518-801; 029-313-431-507-058; 031-608-060-688-477; 048-501-565-604-299; 051-082-693-037-81X; 089-874-517-577-758; 091-764-827-661-79X; 096-448-676-660-868; 099-504-789-002-426; 109-266-076-255-458; 119-648-127-943-362; 126-965-272-601-548; 164-061-088-525-756,0,false,,
126-600-447-780-612,CONVERSION OF GESTURES TO VOICE AND TEXT MESSAGE IN REGIONAL LANGUAGE,2021-04-01,2021,journal article,Journal of emerging technologies and innovative research,23495162,,,V. Sathya Preiya; null S.Menaka; null P.Preethi; Sheshadri Roshni Sruthi,"Speech and text is the main medium for human communication. A person needs vision to access the information in a text. However those who have poor vision can gather information from voice. This paper proposes a camera based assistive text reading to help visually impaired person in reading the text present on the captured image. The faces can also be detected when a person enter into the frame by the mode control. The proposed idea involves text extraction from scanned image using Tesseract Optical Character Recognition (OCR) and converting the text to speech by e-Speak tool, a process which makes visually impaired persons to read the text. This is a prototype for blind people to recognize the products in real world by extracting the text on image and converting it into speech. Computer vision is one of the emerging technologies that can be used to aid visually impaired people for navigation (both indoor and outdoor), accessing printed material, etc. This paper describes an approach to extract and recognize text from scene images effectively using computer vision technology and to convert recognized text into speech so that it can be incorporated with hardware to develop Electronic travel aid for visually impaired people in future.",8,4,127-130,127-130,Human–computer interaction; Frame (networking); Speech synthesis; Tesseract; Gesture; Human communication; Computer science; Optical character recognition; Reading (process); Process (computing),,,,,https://www.jetir.org/papers/JETIRES06028.pdf https://www.jetir.org/view?paper=JETIRES06028,https://www.jetir.org/view?paper=JETIRES06028,,,3165411045,,0,069-379-491-028-73X; 144-058-464-802-575,0,false,,
127-059-701-789-529,CARS: computer assisted radiology and surgery,2012-05-17,2012,journal article,International Journal of Computer Assisted Radiology and Surgery,18616410; 18616429,Springer Science and Business Media LLC,Germany,,,7,S1,507,521,,,,,,,http://dx.doi.org/10.1007/s11548-012-0703-8,,10.1007/s11548-012-0703-8,,,0,,2,false,,
127-268-360-568-701,Enhancing Outdoor Mobility and Environment Perception for Visually Impaired Individuals Through a Customized CNN-based System,,2023,journal article,International Journal of Advanced Computer Science and Applications,21565570; 2158107x,The Science and Information Organization,,Athulya N K; Sivakumar Ramachandran; Neetha George; Ambily N; Linu Shine,"Visual impairment indicates any kind of vision loss including blindness. Individuals with visual impairments face significant challenges when trying to perceive their surroundings from a global perspective and navigating unfamiliar environments. Existing assistive technologies predominantly focus on obstacle avoidance, neglecting to provide comprehensive information about the overall environment. To address this gap, the proposed system employs a customized Convolutional Neural Network (CNN) model tailored to accurately predict the type of outdoor ground terrain the user is traversing. This information is then conveyed to the user audibly. It can also detect the presence of puddles on the road and let the user know whether the outside floor is wet (slippery). The proposed deep-learning architecture is trained on images collected from sources including the Stagnant Water dataset, the GTOS-Mobile dataset and a custom dataset. The trained model is then integrated into an Android app, providing visually impaired (VI) people with effective surrounding perception capabilities, leading to better travel and, ultimately, better living.",14,9,,,Computer science; Convolutional neural network; Perception; Human–computer interaction; Terrain; Deep learning; Artificial intelligence; Focus (optics); Mobile device; Android (operating system); Obstacle; Blindness; Architecture; Visual impairment; Computer vision; World Wide Web; Medicine; Art; Ecology; Psychology; Physics; Neuroscience; Psychiatry; Law; Political science; Optometry; Optics; Visual arts; Biology; Operating system,,,,,http://thesai.org/Downloads/Volume14No9/Paper_109-Enhancing_Outdoor_Mobility_and_Environment_Perception.pdf https://doi.org/10.14569/ijacsa.2023.01409109,http://dx.doi.org/10.14569/ijacsa.2023.01409109,,10.14569/ijacsa.2023.01409109,,,0,,0,true,,gold
127-483-973-270-088,A wearable system for mobility improvement of visually impaired people,2006-07-11,2006,journal article,The Visual Computer,01782789; 14322315,Springer Science and Business Media LLC,Germany,Sylvain Cardin; Daniel Thalmann; Frédéric Vexo,Degradation of the visual system can lead to a dramatic reduction of mobility by limiting a person to his sense of touch and hearing. This paper presents the development of an obstacle detection system for visually impaired people. While moving in his environment the user is alerted to close obstacles in range. The system we propose detects an obstacle surrounding the user by using a multi-sonar system and sending appropriate vibrotactile feedback. The system aims at increasing the mobility of visually impaired people by offering new sensing abilities.,23,2,109,118,Human–computer interaction; Wearable computer; Artificial intelligence; Limiting; Obstacle; Visually impaired; Computer vision; Computer science,,,,,https://infoscience.epfl.ch/record/99038 https://dl.acm.org/doi/10.1007/s00371-006-0032-4 https://dblp.uni-trier.de/db/journals/vc/vc23.html#CardinTV07 https://link.springer.com/article/10.1007/s00371-006-0032-4 https://core.ac.uk/download/147924436.pdf,http://dx.doi.org/10.1007/s00371-006-0032-4,,10.1007/s00371-006-0032-4,1985260248,,1,022-697-627-694-941; 027-867-169-362-210; 031-427-837-990-884; 032-032-650-575-04X; 034-500-631-429-204; 049-512-100-716-012; 053-583-192-599-759; 060-037-097-151-505; 060-198-935-469-457; 078-736-735-476-395; 085-207-379-891-276; 093-329-426-780-145; 096-986-231-321-861; 101-532-242-508-763; 104-276-290-811-90X; 104-386-565-018-378; 111-010-979-052-080; 147-904-173-646-823; 164-696-902-160-755,162,true,,green
127-543-606-863-545,A Novel Perceptive Robotic Cane with Haptic Navigation for Enabling Vision-Independent Participation in the Social Dynamics of Seat Choice,2022-10-23,2022,conference proceedings article,2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),,IEEE,,Shivendra Agrawal; Mary Etta West; Bradley Hayes,"Goal-based navigation in public places is critical for independent mobility and for breaking barriers that exist for blind or visually impaired (BVI) people in a sight-centric society. Through this work we present a proof-of-concept system that autonomously leverages goal-based navigation assistance and perception to identify socially preferred seats and safely guide its user towards them in unknown indoor environments. The robotic system includes a camera, an IMU, vibrational motors, and a white cane, powered via a backpack-mounted laptop. The system combines techniques from computer vision, robotics, and motion planning with insights from psychology to perform 1) SLAM and object localization, 2) goal disambiguation and scoring, and 3) path planning and guidance. We introduce a novel 2-motor haptic feedback system on the cane's grip for navigation assistance. Through a pilot user study we show that the system is successful in classifying and providing haptic navigation guidance to socially preferred seats, while optimizing for users' convenience, privacy, and intimacy in addition to increasing their confidence in independent navigation. The implications are encouraging as this technology, with careful design guided by the BVI community, can be adopted and further developed to be used with medical devices enabling the BVI population to better independently engage in socially dynamic situations like seat choice.",,,,,Human–computer interaction; Computer science; Artificial intelligence; Motion planning; Haptic technology; Computer vision; Robotics; Navigation system; Robot,,,,NSF,,http://dx.doi.org/10.1109/iros47612.2022.9981219,,10.1109/iros47612.2022.9981219,,,0,001-757-439-881-117; 002-841-567-358-302; 003-182-252-177-056; 003-976-442-054-457; 005-456-300-241-939; 018-306-238-013-430; 020-373-201-301-684; 021-886-191-776-01X; 029-313-431-507-058; 030-127-369-356-741; 033-233-012-462-718; 035-155-677-944-748; 038-326-446-864-583; 038-734-719-540-991; 045-599-200-378-144; 066-246-145-459-483; 066-339-002-798-982; 068-708-501-300-740; 069-523-876-223-951; 077-102-051-461-044; 081-510-263-955-940; 090-762-140-041-556; 098-910-337-444-079; 102-551-569-903-749; 109-756-730-610-832; 128-975-670-807-332; 139-233-304-780-009; 172-031-658-891-591; 184-116-517-417-325; 187-275-448-283-374,5,false,,
128-106-934-326-205,The Fuzzy Control Approach for a Quadruped Robot Guide Dog,2021-06-20,2021,journal article,International Journal of Fuzzy Systems,15622479; 21993211,Springer Science and Business Media LLC,Taiwan,Kuo-Yi Chen; Chin-Yuan Tsui,,23,6,1789,1796,Computational intelligence; Artificial intelligence; Fuzzy control system; Control (management); Order (business); Computer science; Robot,,,,,https://link.springer.com/article/10.1007/s40815-020-01046-x https://dblp.uni-trier.de/db/journals/ijfs/ijfs23.html#ChenT21 https://doi.org/10.1007/s40815-020-01046-x,http://dx.doi.org/10.1007/s40815-020-01046-x,,10.1007/s40815-020-01046-x,3176649297,,0,000-097-275-634-12X; 000-248-336-315-33X; 005-967-907-774-920; 008-246-416-345-944; 009-780-385-897-207; 012-128-315-521-256; 016-577-930-973-165; 020-628-772-598-890; 027-458-262-555-853; 029-313-431-507-058; 031-578-472-361-820; 032-556-169-730-514; 036-901-069-917-816; 039-487-215-586-022; 041-482-329-692-746; 044-502-719-616-540; 049-361-709-307-125; 053-654-775-005-065; 058-013-370-483-44X; 061-421-925-109-488; 073-527-444-128-822; 084-258-490-609-72X; 086-869-440-167-789; 097-312-123-618-145; 113-093-437-897-694; 161-329-007-936-322; 163-958-138-768-351,6,false,,
128-414-998-276-221,An improved method for blind navigation based on image-to-sound conversion,2023-10-17,2023,conference proceedings article,Fifth International Conference on Artificial Intelligence and Computer Science (AICS 2023),,SPIE,,Jiaqi Miao; Botao Wang; Zihao Liu; Chaoying Yu,"For a long time, blind or visually impaired people have relied on traditional aids such as canes or guide dogs to assist them in walking, but these aids have significant limitations. In order to help visually impaired individuals walk independently, this article improves the method of mapping images to sound in the literature, making it more applicable to blind navigation. By limiting the range and size of captured images, as well as improving the mapping relationship between captured images and transformed sound, the accuracy of the method has been improved from 84.2% to 88.8%, an increase of nearly 5% compared to the method in the literature, which can better assist in guiding the blind.",,,,,Limiting; Computer vision; Computer science; Visually impaired; Artificial intelligence; Sound (geography); Image (mathematics); Range (aeronautics); Human–computer interaction; Acoustics; Engineering; Mechanical engineering; Physics; Aerospace engineering,,,,,,http://dx.doi.org/10.1117/12.3009107,,10.1117/12.3009107,,,0,001-420-206-231-830; 004-179-296-353-268; 034-000-108-324-263; 037-136-436-731-53X; 105-270-726-944-965,0,false,,
128-531-541-463-003,Computer assisted evaluation of the pivot shift phenomenon for post-operative assessment of anterior cruciate ligament reconstruction,2009-04-28,2009,journal article,International Journal of Computer Assisted Radiology and Surgery,18616410; 18616429,Springer Science and Business Media LLC,Germany,David R. Labbe; Jacques A. de Guise; Véronique Godbout; Guy Grimard; Nicola Hagemeister,,4,S1,97,105,Anterior cruciate ligament reconstruction; Orthodontics; Pivot shift; Post operative; Medicine,,,,,https://espace2.etsmtl.ca/id/eprint/1599/,http://dx.doi.org/10.1007/s11548-009-0314-1,,10.1007/s11548-009-0314-1,1031203523,,0,,1,false,,
129-084-216-980-95X,Indoor Topological Localization Based on a Novel Deep Learning Technique,2020-01-02,2020,journal article,Cognitive Computation,18669956; 18669964,Springer Science and Business Media LLC,Germany,Qiang Liu; Ruihao Li; Huosheng Hu; Dongbing Gu,"Millions of people in the world suffer from vision impairment or vision loss. Traditionally, they rely on guide sticks or dogs to move around and avoid potential obstacles. However, both guide sticks and dogs are passive. They are unable to provide conceptual knowledge or semantic contents of an environment. To address this issue, this paper presents a vision-based cognitive system to support the independence of visually impaired people. More specifically, a 3D indoor semantic map is firstly constructed with a hand-held RGB-D sensor. The constructed map is then deployed for indoor topological localization. Convolutional neural networks are used for both semantic information extraction and location inference. Semantic information is used to further verify localization results and eliminate errors. The topological localization performance can be effectively improved despite significant appearance changes within an environment. Experiments have been conducted to demonstrate that the proposed method can increase both localization accuracy and recall rates. The proposed system can be potentially deployed by visually impaired people to move around safely and have independent life.",12,3,528,541,Recall; Deep learning; Topology; Artificial intelligence; Inference; Semantic map; Cognitive systems; Semantic information; Independence (probability theory); Computer science; Convolutional neural network,,,,China Scholarship Council; China Scholarship Council,https://doi.org/10.1007/s12559-019-09693-5 https://link.springer.com/article/10.1007/s12559-019-09693-5 https://dblp.uni-trier.de/db/journals/cogcom/cogcom12.html#LiuLHG20 http://repository.essex.ac.uk/27661/,http://dx.doi.org/10.1007/s12559-019-09693-5,,10.1007/s12559-019-09693-5,2998701356,,0,008-175-250-084-514; 009-819-458-553-268; 011-794-297-109-740; 012-301-551-524-439; 012-642-196-605-159; 014-478-034-924-123; 027-716-746-350-079; 028-303-905-295-179; 030-001-669-846-677; 032-377-688-503-985; 033-031-667-961-467; 033-270-530-004-349; 034-172-281-440-840; 041-056-605-156-906; 042-616-534-018-512; 044-527-309-096-969; 044-792-836-800-521; 048-347-265-024-628; 050-601-374-343-997; 057-567-246-168-045; 057-934-449-231-341; 064-378-074-351-839; 070-531-547-183-691; 074-529-571-267-564; 076-015-870-338-689; 079-310-940-223-758; 079-946-938-890-74X; 087-410-264-581-151; 097-812-242-244-837; 107-458-943-516-753; 128-572-548-507-786; 130-973-342-787-531; 145-722-308-591-082; 167-079-291-125-197; 180-559-374-049-445; 192-697-687-929-866,11,true,,green
130-033-970-363-22X,The virtual lands of Oz: testing an agribot in simulation,2020-02-25,2020,journal article,Empirical Software Engineering,13823256; 15737616,Springer Science and Business Media LLC,Netherlands,Clément Robert; Thierry Sotiropoulos; Hélène Waeselynck; Jérémie Guiochet; Simon Vernhes,"Testing autonomous robots typically requires expensive test campaigns in the field. To alleviate them, a promising approach is to perform intensive tests in virtual environments. This paper presents an industrial case study on the feasibility and effectiveness of such an approach. The subject system is Oz, an agriculture robot for autonomous weeding. Its software was tested with weeding missions in virtual crop fields, using a 3D simulator based on Gazebo. The case study faced several challenges: the randomized generation of complex 3D environments, the automated checking of the robot behavior (test oracle), and the imperfect fidelity of simulation with respect to real-world behavior. We describe the test approach we developed, and compare the results with the ones of the industrial field tests. Despite the low-fidelity physics of the robot, the virtual tests revealed most software issues found in the field, including a major one that caused the majority of failures; they also revealed a new issue missed in the field. On the downside, the simulation could introduce spurious failures that would not occur in the real world.",25,3,2025,2054,Industrial engineering; Behavior-based robotics; Fidelity; Software; Test (assessment); Field (computer science); Oracle; Robot; Spurious relationship,,,,European Union’s H2020 research and innovation Programme,https://hal.archives-ouvertes.fr/hal-02436216/document https://doi.org/10.1007/s10664-020-09800-3 https://jglobal.jst.go.jp/detail?JGLOBAL_ID=202002214811669302 https://link.springer.com/article/10.1007/s10664-020-09800-3 https://hal.laas.fr/hal-02436216v1 https://dblp.uni-trier.de/db/journals/ese/ese25.html#RobertSWGV20,http://dx.doi.org/10.1007/s10664-020-09800-3,,10.1007/s10664-020-09800-3,3003066549,,0,001-520-497-283-670; 021-543-973-196-963; 026-510-541-811-986; 030-105-785-883-295; 039-345-205-921-516; 044-798-403-075-739; 046-556-736-298-616; 055-066-018-523-852; 061-774-262-249-298; 063-951-969-024-938; 068-357-812-571-300; 069-847-178-617-786; 073-190-218-119-61X; 078-383-060-566-77X; 081-964-850-370-96X; 088-373-720-174-055; 089-910-124-525-493; 094-321-674-103-741; 101-445-262-679-740; 103-230-383-583-096; 106-554-440-312-39X; 110-608-825-485-377; 113-370-772-475-395; 116-340-324-989-124; 126-235-537-218-081; 131-731-817-529-056; 138-706-344-629-240; 140-399-012-631-787; 188-632-044-450-198; 189-437-923-359-692,16,true,,green
130-200-481-403-062,Smart and Guide Hat for Blind Persons in Smart Cities Using Deep Learning,,2023,journal article,Journal of Advances in Information Technology,17982340,Engineering and Technology Publishing,,Tariq S Almurayziq; Naif Alotibi; Gharbi Alshammari; Abdullah Alshammari; Mohammad Alsaffar,"In recent years, Artificial Intelligence (AI) technology has evolved significantly and is used in various fields including banking, email management, surgery, etc.The primary objective of the current study is to assist visually impaired (blind) individuals using AI technology.Blindness is a natural occurrence but it need not prevent blind people from experiencing the world similarly to sighted people.They must execute daily tasks such as walking with such precision so that various obstacles do not impede their progress.Typically, a blind individual uses a white cane as a guiding aid to navigate around obstacles.It is difficult, however, to design smart headgear that is both supportive and intelligent in order to provide numerous services that accurately and promptly anticipate obstacles at ground level and in traffic.Technological advances present an opportunity to develop intelligent headwear which helps its wearer to anticipate potential dangers.This paper presents a work-in-progress and analysis of the challenges blind people face when attempting to identify traffic signals, objects, plant types and QR codes on city maps.Through this endeavour, we hope to gain a better understanding of how blind individuals will be able to navigate smart cities.",14,6,1214,1220,Computer science; Artificial intelligence,,,,,https://www.jait.us/uploadfile/2023/JAIT-V14N6-1214.pdf https://doi.org/10.12720/jait.14.6.1214-1220,http://dx.doi.org/10.12720/jait.14.6.1214-1220,,10.12720/jait.14.6.1214-1220,,,0,001-863-881-838-751; 004-882-502-549-603; 019-827-712-091-154; 021-736-241-758-765; 022-705-697-906-740; 030-427-556-505-92X; 031-993-775-824-042; 047-960-215-580-73X; 047-984-140-662-023; 048-379-892-594-458; 054-303-996-576-55X; 065-312-963-366-437; 071-475-460-546-691; 077-455-969-276-663; 079-462-485-357-887; 101-637-582-441-287; 103-774-457-019-603; 144-857-572-745-37X; 169-332-720-457-720,0,true,,gold
130-639-281-399-338,A Deep Reinforcement Learning Approach with Visual Semantic Navigation with Memory for Mobile Robots in Indoor Home Context,2022-02-24,2022,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Iury B. de A. Santos; Roseli A. F. Romero,,104,3,,,Computer science; Reinforcement learning; Artificial intelligence; Mobile robot; Convolutional neural network; Context (archaeology); Mobile robot navigation; Recurrent neural network; Robot; Graph; Task (project management); Artificial neural network; Machine learning; Robot control; Engineering; Theoretical computer science; Paleontology; Systems engineering; Biology,,,,coordenação de aperfeiçoamento de pessoal de nível superior; fundação de amparo à pesquisa do estado de são paulo; conselho nacional de desenvolvimento científico e tecnológico,,http://dx.doi.org/10.1007/s10846-021-01566-0,,10.1007/s10846-021-01566-0,,,0,005-097-213-595-841; 006-331-277-518-553; 014-682-823-339-013; 018-193-483-515-989; 020-233-013-143-936; 027-736-275-841-725; 037-143-885-059-991; 044-692-024-531-380; 045-140-157-764-525; 046-583-027-625-542; 051-774-614-041-51X; 053-094-537-530-97X; 053-600-802-130-58X; 057-567-246-168-045; 058-028-260-452-046; 059-930-393-216-111; 068-286-242-197-322; 070-480-053-996-812; 075-347-104-835-218; 084-197-601-345-91X; 084-310-938-773-516; 087-838-253-289-179; 088-715-316-335-598; 088-860-567-973-720; 101-672-083-713-691; 115-818-167-448-183,4,false,,
130-764-189-898-284,An optimized deep-learning algorithm for the automated detection of diabetic retinopathy,2023-07-14,2023,journal article,Soft Computing,14327643; 14337479,Springer Science and Business Media LLC,Germany,A. Rafega Beham; V. Thanikaiselvan,,,,,,Convolutional neural network; Computer science; Benchmark (surveying); Artificial intelligence; Algorithm; Diabetic retinopathy; Deep learning; Machine learning; Population; Evolutionary algorithm; Diabetes mellitus; Medicine; Environmental health; Geodesy; Endocrinology; Geography,,,,,,http://dx.doi.org/10.1007/s00500-023-08930-2,,10.1007/s00500-023-08930-2,,,0,001-129-462-110-639; 007-133-373-399-928; 007-948-548-074-699; 014-125-082-299-787; 017-641-347-246-139; 032-258-763-957-812; 042-171-256-839-361; 045-140-728-164-241; 046-934-964-789-287; 092-509-168-433-205; 120-566-047-820-49X; 135-529-811-849-581; 136-600-082-584-418; 138-688-762-874-47X; 142-981-089-703-976; 180-029-268-924-818,3,false,,
131-128-266-877-284,A New Improved Obstacle Detection Framework Using IDCT and CNN to Assist Visually Impaired Persons in an Outdoor Environment,2022-02-18,2022,journal article,Wireless Personal Communications,09296212; 1572834x,Springer Science and Business Media LLC,Netherlands,Yadwinder Singh; Lakhwinder Kaur; Nirvair Neeru,,124,4,3685,3702,Computer science; Discrete cosine transform; Convolutional neural network; Artificial intelligence; Gaussian; Obstacle; Gaussian filter; Computer vision; Classifier (UML); Pattern recognition (psychology); Image (mathematics); Physics; Quantum mechanics; Political science; Law,,,,,,http://dx.doi.org/10.1007/s11277-022-09533-0,,10.1007/s11277-022-09533-0,,,0,004-772-554-982-380; 012-344-135-607-729; 014-166-020-318-269; 015-040-823-132-436; 018-068-926-359-821; 021-909-628-200-486; 024-225-776-970-302; 026-994-702-158-53X; 039-274-384-165-289; 039-881-031-638-831; 040-580-321-731-731; 043-268-160-528-021; 058-268-022-053-157; 060-674-898-321-422; 061-205-550-296-347; 068-526-647-480-345; 077-101-929-493-550; 082-437-177-177-722; 125-897-318-346-362; 149-961-061-562-856,2,false,,
131-171-411-950-817,A comprehensive review of artificial intelligence models for screening major retinal diseases,2024-04-05,2024,journal article,Artificial Intelligence Review,15737462; 02692821,Springer Science and Business Media LLC,Netherlands,Bilal Hassan; Hina Raja; Taimur Hassan; Muhammad Usman Akram; Hira Raja; Alaa A. Abd-alrazaq; Siamak Yousefi; Naoufel Werghi,"<jats:title>Abstract</jats:title><jats:p>This paper provides a systematic survey of artificial intelligence (AI) models that have been proposed over the past decade to screen retinal diseases, which can cause severe visual impairments or even blindness. The paper covers both the clinical and technical perspectives of using AI models in hosipitals to aid ophthalmologists in promptly identifying retinal diseases in their early stages. Moreover, this paper also evaluates various methods for identifying structural abnormalities and diagnosing retinal diseases, and it identifies future research directions based on a critical analysis of the existing literature. This comprehensive study, which reviews both the conventional and state-of-the-art methods to screen retinopathy across different modalities, is unique in its scope. Additionally, this paper serves as a helpful guide for researchers who want to work in the field of retinal image analysis in the future.</jats:p>",57,5,,,Computer science; Retinal; Artificial intelligence; Medicine; Ophthalmology,,,,,https://link.springer.com/content/pdf/10.1007/s10462-024-10736-z.pdf https://doi.org/10.1007/s10462-024-10736-z,http://dx.doi.org/10.1007/s10462-024-10736-z,,10.1007/s10462-024-10736-z,,,0,000-180-182-660-799; 000-693-907-118-58X; 000-836-768-633-965; 001-418-482-300-676; 001-629-640-030-30X; 001-667-703-992-15X; 001-813-712-545-020; 001-995-715-626-07X; 002-249-715-253-500; 002-468-777-594-001; 002-542-881-281-243; 002-546-669-950-196; 002-951-559-723-135; 003-453-803-156-334; 003-460-980-096-622; 003-576-862-870-191; 003-928-107-374-799; 004-961-373-389-601; 005-244-813-591-673; 005-343-882-454-510; 005-362-949-086-278; 005-378-532-295-660; 005-638-813-289-812; 006-026-365-975-921; 006-039-882-006-005; 006-240-262-372-949; 006-398-896-375-586; 006-454-362-651-180; 006-600-916-843-954; 006-610-138-164-478; 006-689-599-091-94X; 006-984-767-834-813; 007-350-866-848-77X; 007-353-611-646-862; 007-584-260-041-981; 007-622-635-729-876; 007-973-907-298-34X; 008-254-987-396-93X; 008-937-623-918-971; 009-240-276-674-429; 009-917-722-606-867; 010-254-716-884-386; 010-371-813-007-388; 010-563-690-126-411; 010-721-845-495-395; 011-004-485-076-297; 011-358-105-065-077; 011-660-519-543-187; 011-728-884-565-534; 011-763-809-769-557; 011-876-540-689-087; 012-247-264-752-733; 012-450-779-246-69X; 012-648-780-378-853; 013-049-393-541-43X; 013-195-996-775-724; 013-283-082-967-287; 013-344-717-628-18X; 013-533-616-574-592; 013-600-343-336-552; 013-686-147-030-716; 014-284-354-392-845; 014-558-364-612-739; 014-562-213-660-305; 014-692-040-114-335; 014-848-809-831-703; 015-326-339-326-38X; 015-367-433-567-666; 015-632-725-919-787; 015-655-531-848-476; 015-813-601-387-97X; 016-267-914-825-759; 016-682-793-328-067; 016-960-499-969-135; 017-094-892-337-853; 017-890-277-377-463; 017-941-012-773-505; 017-971-020-083-069; 019-283-179-547-21X; 020-131-600-587-817; 020-163-010-020-032; 020-763-482-956-997; 020-770-100-135-583; 020-994-383-708-822; 021-008-706-247-661; 021-061-706-209-641; 021-326-158-692-61X; 021-542-776-052-720; 021-835-327-757-548; 022-087-373-382-105; 022-388-813-310-375; 022-612-316-002-72X; 022-749-842-529-713; 022-846-944-954-117; 023-376-349-335-779; 024-150-477-030-27X; 025-303-084-594-198; 025-973-134-848-671; 026-561-074-640-566; 026-975-210-087-495; 026-988-933-984-686; 027-512-837-579-405; 027-893-167-603-286; 028-895-355-641-39X; 029-336-246-744-256; 029-351-316-821-946; 029-829-088-842-912; 029-893-141-129-048; 030-333-675-932-652; 030-440-500-148-114; 031-957-258-335-488; 032-615-826-079-168; 032-699-771-822-700; 033-331-355-256-86X; 033-422-030-030-866; 033-904-989-751-745; 034-010-872-036-83X; 034-726-462-079-021; 034-745-175-201-753; 034-850-037-226-64X; 035-961-741-965-700; 036-316-118-231-38X; 037-174-037-273-356; 037-258-042-625-596; 037-468-945-649-623; 037-509-952-850-243; 039-311-527-745-337; 039-378-325-610-668; 039-551-287-454-081; 040-040-343-915-622; 040-163-117-334-600; 040-280-111-260-797; 040-767-493-942-930; 041-686-233-762-572; 041-792-515-733-250; 041-834-715-979-215; 042-115-515-568-898; 042-127-241-412-973; 042-171-256-839-361; 042-321-764-946-818; 042-529-960-228-285; 043-127-102-868-686; 043-405-255-469-405; 043-520-561-522-837; 043-861-556-794-817; 044-097-165-509-551; 044-320-609-571-647; 044-487-417-777-776; 046-722-813-785-620; 046-876-772-405-119; 046-911-115-509-007; 046-999-732-352-323; 047-095-207-629-632; 047-762-192-313-024; 047-879-381-292-014; 048-521-420-234-280; 048-689-473-798-652; 049-119-087-213-443; 049-302-482-439-288; 050-105-054-262-802; 050-112-916-335-952; 050-114-941-084-365; 050-252-028-244-491; 050-606-900-199-490; 051-211-318-743-227; 051-231-616-968-430; 051-608-571-504-225; 051-619-357-428-050; 051-784-316-592-291; 051-844-670-123-14X; 052-706-415-835-183; 052-796-073-542-648; 053-355-479-229-101; 054-600-556-766-801; 055-502-988-908-057; 055-591-411-758-91X; 056-321-170-018-481; 056-698-431-141-403; 056-764-514-715-41X; 056-873-468-482-59X; 057-042-136-409-707; 057-725-109-988-785; 057-792-765-525-148; 058-009-890-465-059; 058-877-098-777-673; 060-169-424-470-285; 060-351-837-701-556; 060-446-661-002-891; 061-347-046-951-983; 061-348-785-547-663; 062-061-855-411-234; 062-345-307-328-290; 062-521-807-679-556; 062-992-394-191-325; 063-156-493-892-314; 065-264-729-660-95X; 065-566-392-081-541; 065-769-583-667-211; 066-024-224-245-64X; 066-392-573-076-388; 066-416-854-686-186; 066-681-165-387-671; 067-203-508-238-96X; 067-632-178-226-223; 068-597-853-538-761; 069-418-948-217-565; 069-722-232-905-430; 069-952-905-085-021; 070-652-914-369-043; 070-859-163-060-974; 071-398-807-064-477; 072-128-493-031-926; 073-107-516-193-922; 074-154-416-865-064; 074-834-406-052-835; 075-844-204-384-482; 076-519-002-473-29X; 076-594-707-466-145; 077-043-844-750-885; 080-229-851-829-106; 080-617-029-750-799; 080-924-463-158-958; 081-098-836-317-996; 081-568-699-062-360; 081-992-535-768-36X; 082-245-319-116-348; 082-429-144-115-189; 082-453-012-648-901; 082-484-119-833-893; 082-571-297-605-469; 083-026-198-627-585; 083-701-521-825-01X; 083-871-787-233-064; 083-991-785-318-810; 084-066-030-166-888; 084-577-962-582-94X; 084-602-963-840-99X; 084-628-739-448-294; 084-630-488-291-458; 084-775-829-713-602; 085-073-078-525-845; 086-374-841-199-381; 087-302-774-085-563; 087-524-269-634-211; 087-731-468-990-552; 087-745-805-393-212; 088-209-578-388-33X; 088-289-294-367-388; 088-317-175-439-786; 088-444-372-616-990; 089-005-696-227-264; 089-252-011-056-684; 089-510-339-119-764; 090-423-257-113-764; 090-817-641-868-08X; 090-878-051-824-422; 090-950-445-675-38X; 092-009-232-440-944; 092-594-439-260-756; 093-449-478-311-048; 096-681-772-388-498; 097-118-137-242-653; 097-636-463-769-872; 098-232-752-217-80X; 098-468-134-539-239; 098-958-003-074-920; 099-734-660-130-594; 100-099-443-565-640; 100-337-474-262-897; 100-459-013-160-463; 100-896-000-585-281; 101-121-170-444-222; 101-486-008-618-521; 102-468-813-552-104; 102-519-050-314-796; 103-083-688-482-256; 103-084-186-707-19X; 103-562-816-780-230; 104-418-115-334-369; 104-731-372-890-738; 104-810-517-818-987; 105-410-590-956-28X; 106-377-107-359-27X; 106-494-888-664-762; 106-692-138-247-780; 106-839-480-849-297; 106-935-798-896-122; 107-418-234-277-386; 108-610-561-475-634; 108-687-546-202-465; 109-198-581-967-120; 109-819-884-915-685; 110-314-447-088-126; 110-699-025-352-118; 111-038-308-927-186; 111-380-579-295-874; 111-617-522-838-471; 112-016-758-906-218; 112-810-704-915-969; 113-001-897-471-984; 113-687-257-714-094; 114-750-125-981-516; 114-889-769-590-699; 115-150-119-793-081; 115-178-018-965-424; 115-880-402-166-950; 116-739-111-791-899; 117-080-804-777-145; 117-585-131-101-231; 117-623-932-861-948; 117-644-947-866-111; 118-134-025-551-865; 120-261-375-111-679; 121-452-020-475-113; 122-246-308-889-234; 124-086-311-837-921; 124-957-331-241-288; 125-744-670-781-892; 126-070-542-050-233; 126-248-154-713-424; 126-751-740-021-488; 128-402-551-603-574; 128-654-340-592-351; 129-234-562-669-209; 129-700-077-059-971; 130-615-895-955-451; 131-280-097-665-323; 131-859-574-178-783; 133-678-752-180-350; 135-221-025-367-605; 136-791-621-102-567; 136-988-221-426-721; 137-156-021-982-736; 137-225-654-983-663; 139-445-216-945-461; 140-535-612-451-652; 141-556-800-904-814; 143-935-700-462-126; 144-522-480-783-147; 146-158-408-430-00X; 148-040-291-176-86X; 148-493-164-240-592; 148-520-283-248-627; 149-256-620-057-335; 149-463-549-256-674; 151-689-877-006-096; 152-146-364-499-193; 155-298-779-849-476; 157-594-461-894-68X; 157-737-128-946-760; 160-178-377-523-005; 161-237-664-288-910; 162-017-086-367-554; 163-288-220-136-716; 163-449-686-448-935; 164-418-726-392-513; 166-671-131-628-728; 167-212-116-289-899; 167-798-510-488-301; 168-508-123-031-481; 168-570-239-187-661; 169-001-196-268-678; 178-304-049-953-370; 178-342-287-904-301; 178-488-423-003-80X; 179-133-601-557-615; 179-648-357-547-96X; 181-782-359-538-767; 183-387-841-186-037; 185-731-613-759-211; 186-403-763-247-995; 188-083-425-456-893; 189-098-956-160-029; 193-148-709-247-976; 196-681-027-688-621; 196-761-738-916-627; 197-699-680-352-831; 197-705-436-598-352; 199-628-184-511-615,1,true,cc-by,hybrid
131-239-391-680-28X,Investigating socially assistive systems from system design and evaluation: a systematic review,2021-11-15,2021,journal article,Universal access in the information society,16155297; 16155289,Springer Science and Business Media LLC,Germany,Shi Qiu; Pengcheng An; Kai Kang; Jun Hu; Ting Han; Matthias Rauterberg,"Purpose The development of assistive technologies that support people in social interactions has attracted increased attention in HCI. This paper presents a systematic review of studies of Socially Assistive Systems targeted at older adults and people with disabilities. The purpose is threefold: (1) Characterizing related assistive systems with a special focus on the system design, primarily including HCI technologies used and user-involvement approach taken; (2) Examining their ways of system evaluation; (3) Reflecting on insights for future design research. Methods A systematic literature search was conducted using the keywords “social interactions” and “assistive technologies” within the following databases: Scopus, Web of Science, ACM, Science Direct, PubMed, and IEEE Xplore. Results Sixty-five papers met the inclusion criteria and were further analyzed. Our results showed that there were 11 types of HCI technologies that supported social interactions for target users. The most common was cognitive and meaning understanding technologies, often applied with wearable devices for compensating users’ sensory loss; 33.85% of studies involved end-users and stakeholders in the design phase; Four types of evaluation methods were identified. The majority of studies adopted laboratory experiments to measure user-system interaction and system validation. Proxy users were used in system evaluation, especially in initial experiments; 42.46% of evaluations were conducted in field settings, primarily including the participants’ own homes and institutions. Conclusion We contribute an overview of Socially Assistive Systems that support social interactions for older adults and people with disabilities, as well as illustrate emerging technologies and research opportunities for future work.",22,2,1,25,Social relation; Inclusion (education); Psychology; Cognition; Emerging technologies; Research design; Systems design; Meaning (linguistics); Wearable technology; Knowledge management,Assistive technology; Older adults; People with disabilities; Social interaction; Socially assistive system,,,Shanghai Pujiang Program; Shanghai Jiao Tong University,https://link.springer.com/content/pdf/10.1007/s10209-021-00852-w.pdf https://link.springer.com/article/10.1007/s10209-021-00852-w,http://dx.doi.org/10.1007/s10209-021-00852-w,34803565,10.1007/s10209-021-00852-w,3211780852,PMC8591319,0,000-543-167-751-107; 001-294-844-894-183; 001-349-623-632-441; 003-605-078-192-286; 006-103-001-451-744; 006-662-263-066-919; 006-765-190-707-210; 009-105-187-473-965; 010-099-297-277-858; 010-477-914-043-121; 011-267-216-204-427; 011-451-845-613-346; 011-535-569-726-245; 012-408-749-472-312; 012-530-013-957-260; 013-069-781-816-544; 013-290-146-403-33X; 013-541-969-076-695; 013-991-523-028-347; 014-984-968-440-511; 015-000-333-331-786; 015-058-340-255-429; 015-861-937-792-65X; 016-044-892-981-356; 016-640-044-754-418; 016-693-899-322-679; 018-909-301-790-72X; 020-665-144-403-223; 021-020-725-483-696; 021-036-852-459-724; 021-500-548-835-840; 022-406-395-711-411; 022-501-468-015-608; 022-705-697-906-740; 025-328-912-318-780; 026-161-587-191-601; 027-953-643-973-387; 028-016-231-114-834; 028-220-333-652-092; 028-675-176-841-973; 028-727-838-832-956; 029-406-097-089-825; 030-573-943-508-181; 030-778-403-727-323; 036-401-139-083-677; 037-467-971-254-452; 041-627-493-235-20X; 042-373-971-999-482; 043-667-531-590-501; 044-066-214-886-715; 048-311-943-094-349; 048-524-984-854-593; 048-534-769-496-550; 051-793-960-818-890; 051-900-266-338-249; 053-111-215-335-348; 053-457-628-504-291; 053-640-732-806-789; 055-106-205-470-569; 056-207-689-684-49X; 057-086-605-710-688; 058-929-110-630-036; 059-398-290-385-221; 059-847-276-395-563; 060-147-659-950-016; 062-787-182-296-106; 062-793-775-725-464; 063-332-100-575-992; 065-078-747-021-140; 066-140-242-167-794; 066-338-849-146-328; 068-491-440-345-413; 069-993-137-495-503; 071-438-835-926-699; 073-521-963-832-232; 073-988-861-948-524; 074-145-266-598-225; 075-784-044-642-881; 080-217-976-553-803; 083-275-795-865-394; 085-076-156-444-420; 086-463-493-325-656; 089-835-896-282-922; 091-939-768-732-392; 093-351-911-740-199; 095-370-382-872-28X; 096-097-411-837-478; 098-000-911-902-28X; 099-179-831-271-851; 100-940-893-056-795; 100-950-617-368-126; 102-637-970-903-742; 104-823-048-240-547; 105-551-808-223-054; 105-790-942-783-392; 110-361-274-848-132; 120-366-175-754-152; 124-528-201-210-648; 127-124-774-047-532; 127-729-219-024-309; 128-403-459-202-721; 129-893-190-826-574; 130-956-207-085-081; 133-785-878-806-893; 133-787-783-024-011; 135-908-853-701-649; 137-044-795-689-019; 138-284-300-055-137; 140-231-311-718-912; 145-051-572-342-371; 147-158-164-302-760; 148-147-612-350-364; 151-357-381-498-196; 159-008-393-918-894; 166-943-289-907-205; 177-217-283-200-762; 188-258-270-148-088; 189-357-714-386-010,9,true,cc-by,hybrid
131-503-960-053-375,Obstacle detectors for visually impaired people,,2014,conference proceedings article,2014 International Conference on Optimization of Electrical and Electronic Equipment (OPTIM),,IEEE,,Larisa Dunai; Ismael Lengua Lengua; Ignacio Tortajada; Fernando Brusola Simón,"This paper carries out a review on Electronic Travel Aid Systems (ETAS) for visually impaired people and describes a new wearable Cognitive Aid System for Blind People (CASBliP) developed within the frame of European CASBliP project, in which the authors are taking part. Information on the environment enables humans and vertebrates to know about sources that are in many different directions, particularly signals that are outside the detection range of other senses. Sound source localization is inherently important for safety-survival and navigation. In addition to the acoustical cues, the visual cues such as object detection, tracking and distance measurement play an important role in the navigation not only for robots, but also for blind people, since they are often dependent on artificial intelligence. Due to the fact that blind people make maximum use of sound not only to know the obstacle presence, but also how dangerous it is, in order to avoid it effectively, the CASBliP devices use acoustical sounds in order to represent the visual information detected by the sensors and artificial vision systems.",,,809,816,Sensory cue; Frame (networking); Engineering; Wearable computer; Artificial intelligence; Cognition; Object detection; Obstacle; Computer vision; Robot; Acoustic source localization,,,,,http://ieeexplore.ieee.org/document/6850903 https://ieeexplore.ieee.org/document/6850903 https://www.researchgate.net/profile/Larisa_Dunai/publication/269297377_Obstacle_detectors_for_visually_impaired_people/links/54f5c80b0cf27d8ed71cbcd0.pdf,http://dx.doi.org/10.1109/optim.2014.6850903,,10.1109/optim.2014.6850903,2045941706,,1,002-391-643-288-41X; 007-527-228-704-510; 007-960-664-017-028; 009-142-401-900-87X; 010-864-608-924-297; 012-073-295-714-214; 013-847-555-426-159; 014-157-934-526-940; 015-476-928-897-846; 016-068-376-695-077; 018-586-141-570-854; 019-550-995-456-408; 024-469-706-851-368; 024-546-991-078-402; 025-093-676-267-243; 026-048-436-398-74X; 034-350-308-512-672; 034-937-831-827-866; 036-742-969-097-379; 047-920-181-585-025; 050-130-070-252-430; 058-443-047-291-990; 059-647-236-855-012; 062-819-743-168-475; 072-192-178-588-464; 075-842-653-595-091; 087-272-456-866-28X; 089-487-966-698-248; 090-090-091-154-917; 099-086-994-780-682; 104-031-700-543-974; 105-584-440-275-77X; 125-501-372-179-565; 132-342-571-633-36X; 169-421-672-075-385,28,false,,
131-599-278-416-881,Real-Time Multi-Car Localization and See-Through System,2022-01-04,2022,journal article,International Journal of Computer Vision,09205691; 15731405,Springer Science and Business Media LLC,Netherlands,Francois Rameau; Oleksandr Bailo; Jinsun Park; Kyungdon Joo; In So Kweon,,130,2,384,404,Computer science; Graph; Real-time computing; The Internet; Consistency (knowledge bases); Bandwidth (computing); Distributed computing; Architecture; Internet access; Artificial intelligence; Computer network; Theoretical computer science; World Wide Web; Visual arts; Art,,,,Bosch (China) Investment Ltd.; National Research Foundation of Korea,,http://dx.doi.org/10.1007/s11263-021-01558-5,,10.1007/s11263-021-01558-5,,,0,000-090-590-274-564; 000-357-505-293-809; 004-522-657-548-956; 006-263-516-420-704; 016-177-613-019-527; 018-303-097-489-166; 025-393-995-674-65X; 026-861-335-262-140; 029-759-808-378-556; 030-091-798-618-058; 031-371-216-833-827; 032-998-902-099-55X; 034-948-396-631-706; 037-077-626-994-093; 040-214-281-474-370; 040-451-416-204-853; 042-056-346-349-581; 042-396-386-799-596; 043-264-172-620-732; 045-036-980-455-569; 048-692-462-751-899; 049-921-334-736-611; 055-150-189-287-833; 055-629-665-241-217; 055-676-274-953-454; 056-332-008-726-803; 058-561-555-460-321; 059-840-507-417-116; 061-199-231-534-098; 064-329-735-133-185; 069-441-346-566-472; 073-555-335-101-098; 077-642-711-505-004; 078-730-677-959-85X; 083-419-288-690-497; 084-417-011-202-574; 086-290-111-320-583; 086-749-251-515-944; 087-577-313-400-228; 088-921-387-907-319; 089-779-057-821-290; 093-320-551-844-274; 097-505-182-877-375; 098-888-323-553-276; 107-909-359-121-920; 112-205-627-541-848; 120-682-021-924-619; 130-904-505-470-768; 153-697-872-076-389; 180-088-747-638-245; 192-219-275-226-64X; 192-751-999-674-131,2,false,,
131-752-337-934-186,Hardware Implementation of Obstacle Detection for Assisting Visually Impaired People in an Unfamiliar Environment by Using Raspberry Pi,2016-12-27,2016,book chapter,Communications in Computer and Information Science,18650929; 18650937,Springer Nature Singapore,Germany,Sanket Khade; Yogesh H. Dandawate,"For assisting blind or visually impaired persons, many computer vision technology has been developed. Some camera based systems were developed to help those people in way finding, navigation and finding daily necessities. The motion of the observer causes all scene object stationary or non-stationary in motion. And hence it is very much important to detect moving object with the moving observer. In this context we have proposed a camera based prototype system for assisting blind person in detection of obstacles by using motion vectors. We have collected dataset of their indoor and outdoor environment and estimated the optical flow to perform object detection. Furthermore we have detected the objects in the region of interest without using costly Depth cameras and sensors. The hardware used in the proposed work is ‘Raspberry Pi 2-B’ and the algorithms used for object detection is performed using MATLAB (for simulation purpose) and Python language.",,,889,895,Engineering; Artificial intelligence; Optical flow; Region of interest; Object detection; Obstacle; Observer (special relativity); MATLAB; Unfamiliar environment; Raspberry pi; Computer vision; Computer hardware,,,,,https://rd.springer.com/chapter/10.1007/978-981-10-3433-6_106 https://link.springer.com/chapter/10.1007/978-981-10-3433-6_106/fulltext.html https://link.springer.com/chapter/10.1007/978-981-10-3433-6_106,http://dx.doi.org/10.1007/978-981-10-3433-6_106,,10.1007/978-981-10-3433-6_106,2570983950,,0,008-809-888-338-271; 013-686-517-172-456; 021-909-628-200-486; 022-489-327-390-755; 024-095-146-218-164; 028-628-140-541-774; 037-039-198-474-114; 070-528-905-441-092; 072-421-218-196-905; 096-014-228-878-398; 109-996-890-834-037,8,false,,
131-938-117-397-513,How to Train Your Guide Dog: Wayfinding and Safe Navigation with Human-Robot Modeling,2023-03-13,2023,conference proceedings article,Companion of the 2023 ACM/IEEE International Conference on Human-Robot Interaction,,ACM,,J. Taery Kim; Wenhao Yu; Jie Tan; Greg Turk; Sehoon Ha,"A robot guide dog has the potential to enhance the independence and quality of life of individuals who are blind or visually impaired by providing accessible, automated, and intelligent guidance. However, developing effective robot guide dogs requires researchers not only to solve robotic perception and planning problems but also to understand complicated two-way interactions of the human-robot team. This work presents the formal definition of the wayfinding task of the robotic guide dog that is grounded by common practices in the real world. Given such a task, we train an effective policy for the robot guide dog while investigating two different human models, a rotating rod model and a rigid harness model. We show that our robot can safely guide a human user to avoid several obstacles in the real world. We also demonstrate that a proper human model is necessary to achieve collision-free navigation for both the human and the robot.",,,,,Robot; Human–computer interaction; Task (project management); Computer science; Human–robot interaction; Perception; Artificial intelligence; Mobile robot navigation; Collision avoidance; Simulation; Mobile robot; Robot control; Engineering; Collision; Systems engineering; Computer security; Psychology; Neuroscience,,,,Google Research Collabs; NSF (National Science Foundation),https://dl.acm.org/doi/pdf/10.1145/3568294.3580076 https://doi.org/10.1145/3568294.3580076,http://dx.doi.org/10.1145/3568294.3580076,,10.1145/3568294.3580076,,,0,020-373-201-301-684; 032-137-454-249-408; 164-482-392-929-70X,3,true,,bronze
132-516-504-714-351,Image captioning to aid blind and visually impaired outdoor navigation,2023-09-01,2023,journal article,IAES International Journal of Artificial Intelligence (IJ-AI),22528938; 20894872,Institute of Advanced Engineering and Science,,Ruvita Faurina; Anisa Jelita; Arie Vatresia; Indra Agustian,"<jats:p>Artificial intelligence technology has dramatically improved the quality of services for human needs, one of which is technology to improve the quality of services for the blind and visually impaired, particularly technology that can help them understand visual sights to facilitate navigation in their daily lives. This study developed an image captioning model to aid the blind and visually impaired in outdoor navigation. The image captioning model employs the encoder-decoder method, with the convolutional neural network (CNN) feature extraction and attention layer as encoders and the long short-term memory (LSTM) as decoders. ResNet101 and ResNet152 are used in the encoder to extract image features. The results of the extraction and caption are forwarded to the attention layer and the LSTM network. The attention layer uses the Bahdanau attention mechanism. The accuracy of the model is calculated using the bilingual evaluation understudy score (BLEU), metric for evaluation of translation with explicit ordering (METEOR) and recall-oriented understudy for gisting evaluation-longest common subsequence (ROUGE-L). ResNet101 performed the best on BLEU-4, scoring 91.811% and 94.0337% in the METEOR evaluation. The captioning results show that the model is quite successful in displaying a simple caption that is suitable for each image.</jats:p>",12,3,1104,1104,Closed captioning; Computer science; Artificial intelligence; Encoder; Convolutional neural network; Computer vision; Layer (electronics); Metric (unit); Feature extraction; Feature (linguistics); Speech recognition; Image (mathematics); Linguistics; Chemistry; Operations management; Philosophy; Organic chemistry; Economics; Operating system,,,,,https://ijai.iaescore.com/index.php/IJAI/article/download/22587/13681 https://doi.org/10.11591/ijai.v12.i3.pp1104-1117 https://zenodo.org/records/7793767/files/22587%205_%203nov22%2021dec22%20(Edit%20FR).pdf https://zenodo.org/record/7793767,http://dx.doi.org/10.11591/ijai.v12.i3.pp1104-1117,,10.11591/ijai.v12.i3.pp1104-1117,,,0,,0,true,,gold
133-107-831-230-660,Landing Site Detection for Autonomous Rotor Wing UAVs Using Visual and Structural Information,2022-01-22,2022,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Evangelos Chatzikalymnios; Konstantinos Moustakas,"<jats:title>Abstract</jats:title><jats:p>The technology of unmanned aerial vehicles (UAVs) has increasingly become part of many civil and research applications in recent years. UAVs offer high-quality aerial imaging and the ability to perform quick, flexible and in-depth data acquisition over an area of interest. While navigating in remote environments, UAVs need to be capable of autonomously landing on complex terrains for security, safety and delivery reasons. This is extremely challenging as the structure of these terrains is often unknown, and no prior knowledge can be leveraged. In this study, we present a vision-based autonomous landing system for rotor wing UAVs equipped with a stereo camera and an inertial measurement unit (IMU). The landing site detection algorithm introduces and evaluates several factors including terrain’s flatness, inclination and steepness. Considering these features we compute map metrics that are used to obtain a landing-score map, based on which we detect candidate landing sites. The 3D reconstruction of the scene is acquired by stereo processing and the pose of the UAV at any given time is estimated by fusing raw data from the inertial sensors with the pose obtained from stereo ORB-SLAM2. Real-world trials demonstrate successful landing in unknown and complex terrains such as suburban and forest areas.</jats:p>",104,2,,,Inertial measurement unit; Terrain; Computer vision; Artificial intelligence; Computer science; Orientation (vector space); Simultaneous localization and mapping; Engineering; Robot; Mobile robot; Geography; Cartography; Geometry; Mathematics,,,,,https://link.springer.com/content/pdf/10.1007/s10846-021-01544-6.pdf https://doi.org/10.1007/s10846-021-01544-6,http://dx.doi.org/10.1007/s10846-021-01544-6,,10.1007/s10846-021-01544-6,,,0,003-297-695-878-679; 020-255-897-548-054; 027-510-260-551-280; 029-889-561-083-355; 037-185-576-536-833; 050-546-770-351-471; 052-349-014-525-798; 055-805-158-969-473; 056-332-008-726-803; 059-930-393-216-111; 074-181-158-977-98X; 075-940-103-658-472; 079-914-305-236-151; 100-501-550-145-361; 104-322-057-565-743; 107-762-967-502-002; 112-134-515-689-922; 113-774-383-120-161; 127-272-840-723-928; 131-566-248-546-211; 149-232-553-501-507; 152-557-532-980-397; 171-722-748-049-054,6,true,cc-by,hybrid
133-317-652-903-976,Tailoring assistive smart glasses according to pathologies of visually impaired individuals: an exploratory investigation on social needs and difficulties experienced by visually impaired individuals,2021-12-22,2021,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Simon Ruffieux; Chiwoong Hwang; Vincent Junod; Roberto Caldara; Denis Lalanne; Nicolas Ruffieux,"<jats:title>Abstract</jats:title><jats:p>Recent advances in the field of assistive devices technology represent a great opportunity for improving the quality of life of people with moderate to severe visual impairment. However, it is still unclear what are the precise daily difficulties, needs and expectations of the smart glasses technology for visually impaired individuals. To this aim, we conducted a survey based on three questionnaires to provide qualitative and quantitative insights on those questions across five groups suffering from various visual pathologies (<jats:inline-formula><jats:alternatives><jats:tex-math>$$N=50$$</jats:tex-math><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"">;                   <mml:mrow>;                     <mml:mi>N</mml:mi>;                     <mml:mo>=</mml:mo>;                     <mml:mn>50</mml:mn>;                   </mml:mrow>;                 </mml:math></jats:alternatives></jats:inline-formula>). The results clearly showed the importance of developing tailored solutions to fulfill the heterogeneous daily difficulties and needs identified across pathologies. Overall, groups shared similar expectations regarding the assistive smart glasses functionalities in order to improve social interactions.</jats:p>",22,2,463,475,Visually impaired; Assistive technology; Computer science; Visual impairment; Exploratory research; Field (mathematics); Quality of life (healthcare); Psychology; Applied psychology; Human–computer interaction; Mathematics; Psychotherapist; Psychiatry; Sociology; Anthropology; Pure mathematics,,,,Fondation Gelbert,https://link.springer.com/content/pdf/10.1007/s10209-021-00857-5.pdf https://doi.org/10.1007/s10209-021-00857-5,http://dx.doi.org/10.1007/s10209-021-00857-5,,10.1007/s10209-021-00857-5,,,0,000-394-661-764-185; 001-432-915-664-207; 007-530-851-453-706; 010-004-081-294-240; 010-359-743-819-214; 013-041-559-281-791; 013-056-435-015-274; 016-645-836-018-745; 018-031-085-269-362; 022-644-507-748-088; 025-968-486-135-433; 028-628-140-541-774; 034-239-782-467-201; 036-011-916-817-310; 036-401-139-083-677; 040-419-618-479-011; 041-155-921-602-190; 045-505-911-987-050; 068-526-647-480-345; 070-603-201-368-250; 073-180-689-038-913; 075-519-293-886-898; 090-230-614-158-223; 093-548-249-489-049; 097-221-905-887-186; 100-911-014-973-499; 100-950-617-368-126; 105-790-942-783-392; 115-270-079-299-842; 120-987-209-359-719; 122-704-318-024-950; 132-757-474-504-253; 135-583-248-281-524; 140-674-406-966-620; 144-657-387-189-280; 173-093-712-875-446,4,true,cc-by,hybrid
133-560-124-683-260,QAOVDetect: A Novel Syllogistic Model with Quantized and Anchor Optimized Approach to Assist Visually Impaired for Animal Detection using 3D Vision,2022-06-08,2022,journal article,Cognitive Computation,18669956; 18669964,Springer Science and Business Media LLC,Germany,Kanak Manjari; Madhushi Verma; Gaurav Singal; Neeraj Kumar,,14,4,1269,1286,Quantization (signal processing); Computer science; Inference; Detector; Artificial intelligence; Software; Computer vision; Telecommunications; Programming language,,,,,,http://dx.doi.org/10.1007/s12559-022-10020-8,,10.1007/s12559-022-10020-8,,,0,003-456-545-749-041; 005-005-103-531-133; 009-864-381-071-748; 012-861-264-126-337; 014-800-243-761-036; 016-663-644-145-618; 016-714-895-336-942; 018-843-508-574-144; 023-309-209-821-636; 026-471-523-202-707; 031-991-132-972-341; 033-899-954-183-203; 035-758-548-044-607; 037-133-652-496-868; 040-351-685-890-777; 042-432-325-709-281; 048-210-535-578-229; 050-066-462-007-107; 052-046-438-827-353; 052-721-542-960-61X; 053-160-212-697-23X; 054-151-572-436-553; 056-311-013-170-616; 056-851-388-613-863; 061-294-782-051-637; 061-573-667-596-545; 067-780-696-895-659; 069-480-623-909-181; 077-216-334-371-79X; 077-675-041-270-883; 081-187-105-109-848; 083-671-887-012-721; 106-858-884-742-641; 115-704-248-560-278; 116-131-244-760-687; 121-350-695-153-190; 124-432-468-869-576; 138-729-760-737-154; 151-039-447-100-109; 182-301-959-368-729,3,false,,
133-639-727-636-932,Obs-tackle: an obstacle detection system to assist navigation of visually impaired using smartphones,2024-01-19,2024,journal article,Machine Vision and Applications,09328092; 14321769,Springer Science and Business Media LLC,Germany,U. Vijetha; V. Geetha,,35,2,,,Obstacle; Computer science; Computer vision; Artificial intelligence; Identification (biology); Segmentation; Obstacle avoidance; Deep learning; Key (lock); Enhanced Data Rates for GSM Evolution; Path (computing); Mobile robot; Robot; Computer security; Geography; Botany; Archaeology; Programming language; Biology,,,,,,http://dx.doi.org/10.1007/s00138-023-01499-8,,10.1007/s00138-023-01499-8,,,0,000-416-032-829-85X; 008-783-981-093-093; 017-387-981-396-440; 020-581-037-678-426; 020-745-838-621-241; 025-044-700-570-429; 026-333-865-330-463; 029-275-026-992-155; 030-540-246-658-722; 038-805-841-160-840; 041-314-165-476-904; 041-997-100-015-950; 043-183-236-993-50X; 043-196-700-803-427; 049-551-659-753-473; 052-046-438-827-353; 052-287-464-667-493; 052-731-143-530-822; 055-811-056-010-238; 067-936-687-916-954; 068-246-254-541-665; 069-706-555-672-053; 070-606-003-984-347; 071-556-023-985-658; 073-648-791-606-711; 079-433-787-480-087; 089-019-312-017-506; 091-973-674-139-589; 097-161-848-218-610; 097-179-179-603-700; 100-817-282-240-389; 106-897-180-380-082; 112-566-962-059-235; 115-953-715-917-123; 127-070-672-662-621; 141-649-000-612-322; 142-968-657-337-219; 152-127-960-081-400; 153-967-311-458-704; 165-422-564-417-041; 185-616-751-096-539; 186-002-073-406-20X,1,false,,
133-723-110-042-570,Seeing Through Sound: Object Detection and Distance Analyzer for the Visually Impaired Using Audio Feedback,2023-11-16,2023,conference proceedings article,2023 Annual International Conference on Emerging Research Areas: International Conference on Intelligent Systems (AICERA/ICIS),,IEEE,,Anishamol Abraham; Bini M Issac; Allen Roy; Abhijith J Thoppil; Aswin Raj; Daan Jacob; Christy K Mathews,"Blind Aid is a technology designed specifically for visually impaired individuals, aiming to assist them in navigating their environment. Through a combination of hardware and software, users are provided with real-time auditory information about their surroundings, including details about nearby obstacles. The hardware component of the system consists of a small and portable device equipped with sensors that track and assess the user's surroundings, along with a speaker for delivering audio output. The software component utilizes machine learning algorithms to interpret and process the sensor data, enabling users to receive concise and accurate audio descriptions of their environment. By enabling simpler and more confident navigation, Blind Aid significantly contributes to enhancing the independence and overall quality of life for people with visual impairments.",,,,,Computer science; Spectrum analyzer; Audio feedback; Visually impaired; Computer vision; Sound (geography); Audio analyzer; Audio frequency; Object (grammar); Audio signal processing; Speech recognition; Artificial intelligence; Audio signal; Acoustics; Human–computer interaction; Speech coding; Telecommunications; Physics; Sound pressure,,,,,,http://dx.doi.org/10.1109/aicera/icis59538.2023.10419988,,10.1109/aicera/icis59538.2023.10419988,,,0,016-555-255-894-377; 054-187-640-302-472; 062-279-197-826-869,0,false,,
134-161-971-157-104,RF Communication Based Assistive Device for Visually impaired person,2013-01-12,2013,,,,,,Akella Subrahmanya Narasimha Raju; G Satya Mohan Chowdary; Ch. Karunakar,"The project presents a design to develop an ETA (Electronic Travel Aid) the visually impaired people to navigate independently within an enclosed environment. It is linked to the other components of this design through the RF wireless communication.              The blind person can perceive obstacles in front by means of an ETA from head height to foot level. The vision sensor, which is mounted on the head of the user, captures the image that is in front of him. The server processes the captured image and enhances the significant vision data by employing a set of image processing procedures. The processed information is presented as a structured form of acoustic signal and is conveyed to the user through a set of earphones.  The blind rod (guide cane) is used for foot level obstacle detection. The obstacle sensor senses the obstacle that lies ahead of the user and informs him.          This system also incorporates position and direction information. The remote server does optimal path planning. The user issues the commands to ETA and receives direction response through audio signals. The directions are given to the user based on the environmental map that is stored in the predetermined database through RF communication.",2,11,1236,1242,Signal; Engineering; Artificial intelligence; Set (psychology); Obstacle; Rf communication; Computer vision; Wireless; Audio signal; Motion planning; Image processing,,,,,,,,,1885250972,,0,,0,false,,
134-658-451-919-279,Application of stereovision in a navigation aid for blind people,,2003,conference proceedings article,"Fourth International Conference on Information, Communications and Signal Processing, 2003 and the Fourth Pacific Rim Conference on Multimedia. Proceedings of the 2003 Joint",,IEEE,,Farrah Wong; R. Nagarajan; S. Yaacob,"A ""Navigation Aid for Visually Impaired"" (NAVI) system has been developed in UMS in 2001 and improved in 2002. The NAVI system comprises of 3-part functioning units namely a digital video camera, a single board computer and a headphone. The single camera limitation in providing the depth information, which is critical for navigation purposes, has prompted the extension into a stereovision system by using two cameras. This stereovision system also uses the fuzzy-based segmentation procedure as an image preprocessing package that was developed for the single camera NAVI but with an additional adaptive loop. The segmented images undergo a rule-based stereo matching procedure. From the matching features, the disparity is computed. The disparity in combination with information of focal length as well as the space between two cameras provides the information on the distance between cameras and object. This distance information is incorporated into the final processed image as four gray levels such as white, light gray, dark gray and black. The size and location of object in the visual plane is then conveyed to the blind individual by means of a structured coded sound. The distance information is represented by means of verbal sound. Preliminary experimental analysis reveals a promising new approach for developing a navigational aid for blinds through the transformation of stereo image to stereo sound.",2,,734,737,Computer graphics (images); Image segmentation; Stereophonic sound; Artificial intelligence; Navigational aid; Single-board computer; Headphones; Computer vision; Computer science; Object (computer science); Focal length; Segmentation,,,,,http://eprints.ums.edu.my/582/ http://ieeexplore.ieee.org/document/1292553/ https://ieeexplore.ieee.org/document/1292553/,http://dx.doi.org/10.1109/icics.2003.1292553,,10.1109/icics.2003.1292553,2160707421,,2,013-729-854-652-837; 020-533-837-608-729; 053-266-220-901-486; 129-400-209-376-57X; 192-600-440-415-958,30,false,,
134-759-398-139-320,SightAid: empowering the visually impaired in the Kingdom of Saudi Arabia (KSA) with deep learning-based intelligent wearable vision system,2024-03-29,2024,journal article,Neural Computing and Applications,09410643; 14333058,Springer Science and Business Media LLC,Germany,Fatma M. Talaat; Mohammed Farsi; Mahmoud Badawy; Mostafa Elhosseini,,36,19,11075,11095,Visually impaired; Wearable computer; Computational Science and Engineering; Computer science; Artificial intelligence; Deep learning; Human–computer interaction; Computer vision; Machine learning; Embedded system,,,,,,http://dx.doi.org/10.1007/s00521-024-09619-9,,10.1007/s00521-024-09619-9,,,0,019-535-516-864-012; 031-218-334-653-826; 048-254-076-918-785; 049-317-239-158-314; 055-771-464-165-542; 063-625-314-832-762; 064-140-744-806-266; 064-657-343-403-492; 066-133-135-091-645; 071-017-280-102-93X; 085-102-500-348-494; 094-444-440-758-681; 100-354-403-007-64X; 125-112-083-666-544; 147-328-469-754-521; 147-702-618-323-643; 176-227-441-484-899; 189-761-821-536-39X,2,false,,
135-076-511-985-283,A novel finetuned YOLOv6 transfer learning model for real-time object detection,2023-04-10,2023,journal article,Journal of Real-Time Image Processing,18618200; 18618219,Springer Science and Business Media LLC,Germany,Chhaya Gupta; Nasib Singh Gill; Preeti Gulia; Jyotir Moy Chatterjee,,20,3,,,Computer science; Pruning; Object detection; Artificial intelligence; Transfer of learning; Inference; Object (grammar); Baseline (sea); Deep learning; Machine learning; Task (project management); Computation; Computer vision; Pattern recognition (psychology); Algorithm; Oceanography; Management; Agronomy; Economics; Biology; Geology,,,,,,http://dx.doi.org/10.1007/s11554-023-01299-3,,10.1007/s11554-023-01299-3,,,0,001-077-526-034-347; 016-328-182-364-198; 017-279-113-166-153; 018-276-572-177-049; 021-835-494-883-736; 028-939-404-645-349; 029-185-305-219-553; 033-247-143-544-171; 038-672-734-718-734; 049-317-239-158-314; 059-624-024-015-132; 066-604-838-471-239; 070-654-838-974-725; 074-908-977-307-95X; 089-539-914-726-480; 090-743-883-871-143; 091-172-609-933-41X; 093-651-476-799-835; 105-115-586-060-112; 125-678-921-586-723; 142-690-783-266-614; 149-119-341-850-866; 170-838-967-044-286; 171-189-370-546-346,17,false,,
135-409-936-810-860,Low-latency automotive vision with event cameras.,2024-05-29,2024,journal article,Nature,14764687; 00280836,Springer Science and Business Media LLC,United Kingdom,Daniel Gehrig; Davide Scaramuzza,"The computer vision algorithms used currently in advanced driver assistance systems rely on image-based RGB cameras, leading to a critical bandwidth-latency trade-off for delivering safe driving experiences. To address this, event cameras have emerged as alternative vision sensors. Event cameras measure the changes in intensity asynchronously, offering high temporal resolution and sparsity, markedly reducing bandwidth and latency requirements<sup>1</sup>. Despite these advantages, event-camera-based algorithms are either highly efficient but lag behind image-based ones in terms of accuracy or sacrifice the sparsity and efficiency of events to achieve comparable results. To overcome this, here we propose a hybrid event- and frame-based object detector that preserves the advantages of each modality and thus does not suffer from this trade-off. Our method exploits the high temporal resolution and sparsity of events and the rich but low temporal resolution information in standard images to generate efficient, high-rate object detections, reducing perceptual and computational latency. We show that the use of a 20 frames per second (fps) RGB camera plus an event camera can achieve the same latency as a 5,000-fps camera with the bandwidth of a 45-fps camera without compromising accuracy. Our approach paves the way for efficient and robust perception in edge-case scenarios by uncovering the potential of event cameras<sup>2</sup>.",629,8014,1034,1040,Computer science; Computer vision; Artificial intelligence; Latency (audio); RGB color model; Frame rate; Smart camera; Low latency (capital markets); Image sensor; Exploit; Event (particle physics); Real-time computing; Telecommunications; Physics; Quantum mechanics; Computer network; Computer security,,,,,https://www.nature.com/articles/s41586-024-07409-w.pdf https://doi.org/10.1038/s41586-024-07409-w,http://dx.doi.org/10.1038/s41586-024-07409-w,38811712,10.1038/s41586-024-07409-w,,PMC11136662,0,001-489-694-893-512; 005-089-205-369-844; 005-958-952-312-616; 006-694-991-787-948; 008-051-449-355-129; 008-373-827-005-268; 008-696-225-225-956; 009-863-259-184-066; 011-318-711-711-044; 011-692-917-361-860; 014-730-436-204-438; 016-136-542-319-205; 016-470-907-085-135; 016-632-911-328-354; 020-233-013-143-936; 021-338-475-215-321; 024-131-470-445-452; 024-503-233-209-637; 025-727-694-133-822; 026-871-473-695-832; 028-234-452-244-582; 029-795-959-508-838; 029-947-097-076-954; 029-965-606-424-03X; 030-218-469-222-63X; 031-218-334-653-826; 043-354-275-632-68X; 044-829-310-904-621; 049-150-136-772-723; 049-317-239-158-314; 049-443-290-587-218; 057-567-246-168-045; 059-374-878-002-264; 063-070-499-751-486; 066-133-135-091-645; 069-480-623-909-181; 078-352-631-206-603; 082-475-038-514-52X; 090-302-495-120-336; 092-605-597-810-486; 100-446-628-501-874; 100-797-793-405-899; 105-103-863-266-736; 107-685-136-664-063; 108-967-686-897-269; 111-081-410-478-472; 111-169-393-863-876; 113-550-621-177-831; 117-350-008-538-976; 118-075-109-045-225; 118-340-408-984-497; 120-212-773-213-100; 122-129-330-816-127; 125-112-083-666-544; 125-294-924-950-049; 125-616-021-020-077; 126-090-210-000-465; 131-945-042-519-955; 140-468-813-411-457; 148-041-848-484-937; 158-094-641-048-136; 161-801-541-319-339; 164-010-073-268-339; 164-878-069-648-183; 192-138-231-587-676,2,true,cc-by,hybrid
135-442-888-799-760,A Stereo Image Processing System for Visually Impaired,,,journal article,"World Academy of Science, Engineering and Technology, International Journal of Computer, Electrical, Automation, Control and Information Engineering",,,,G. Balakrishnan; G. Sainarayanan; R. Nagarajan; Sazali Yaacob,"This paper presents a review on vision aided systems; and proposes an approach for visual rehabilitation using stereo vision; technology. The proposed system utilizes stereo vision, image; processing methodology and a sonification procedure to support; blind navigation. The developed system includes a wearable; computer, stereo cameras as vision sensor and stereo earphones, all; moulded in a helmet. The image of the scene infront of visually; handicapped is captured by the vision sensors. The captured images; are processed to enhance the important features in the scene in front,; for navigation assistance. The image processing is designed as model; of human vision by identifying the obstacles and their depth; information. The processed image is mapped on to musical stereo; sound for the blind-s understanding of the scene infront. The; developed method has been tested in the indoor and outdoor; environments and the proposed image processing methodology is; found to be effective for object identification.",2,8,2794,2803,Stereophonic sound; Wearable computer; Artificial intelligence; Computer vision; Stereo cameras; Computer science; Object (computer science); Stereopsis; Identification (information); Sonification; Image processing,,,,,https://search.datacite.org/works/10.5281/zenodo.1075125 https://publications.waset.org/10379/a-stereo-image-processing-system-for-visually-impaired https://publications.waset.org/10379/pdf,http://dx.doi.org/10.5281/zenodo.1075125,,10.5281/zenodo.1075125,2226899078,,0,013-847-555-426-159; 045-673-862-191-277; 054-553-316-982-805; 058-942-174-334-646; 063-161-240-623-745; 065-992-777-809-97X; 068-262-811-656-335; 074-324-981-337-749; 077-975-988-687-715; 078-725-425-518-672; 098-418-441-499-649; 105-584-440-275-77X; 123-273-858-461-486; 136-878-651-956-733; 192-600-440-415-958,33,false,,
135-967-297-423-210,Active Guide System for The Blind Based on The Internet of Things and Collaborative Perception,,2022,conference proceedings article,2022 11th International Conference of Information and Communication Technology (ICTech)),,IEEE,,GuangYi Wang; Lu Li; JingJuan Fan; SongYun Shi; YiPeng Xu; Yuan Wang,"Nowadays, the number of visually impaired people is increasing day by day. On average, one in every 100 people is blind. At the same time, due to the imperfect establishment of social barrier-free facilities, it is very difficult for the blind travelling, which seriously affects the degree of social participation of the blind. This paper introduces a blind guide method based on the Internet of Things and collaborative perception. Coordinate through the long-distance of digital map (eg. Google Maps) and the local navigation mode of the blind guide device to realize an active guide system for the blind. This method can effectively assist the blind to cope with the complex traffic environment and thus perceive the world.",,,,,Perception; Computer science; The Internet; Imperfect; Mode (computer interface); Digital mapping; Double blind; Human–computer interaction; Multimedia; World Wide Web; Psychology; Geography; Cartography; Medicine; Placebo; Linguistics; Philosophy; Alternative medicine; Pathology; Neuroscience,,,,,,http://dx.doi.org/10.1109/ictech55460.2022.00012,,10.1109/ictech55460.2022.00012,,,0,007-725-872-437-764; 020-233-013-143-936; 034-000-108-324-263; 042-251-157-585-319; 056-745-340-837-164; 092-700-562-934-337,2,false,,
136-433-502-169-611,An embedded system for aiding navigation of visually impaired persons,,2013,,,,,,Amit Kumar; Rusha Patra; Manjunatha Mahadevappa; Jayanta Mukhopadhyay; Amlan Majumdar,"Visually impaired individuals find navigation difficult as they often lack the much needed information for bypassing obstacles and hazards in their path. In order to help blind people navigate safely and quickly, an obstacle detection system using ultrasonic sensors and USB camera-based visual navigation has been considered. The proposed system detects obstacles up to 300 cm via sonar and sends audio feedback to inform the person about their location. In addition, a USB webcam is connected with eBox 2300TM Embedded System for capturing the field-of-view of the user, for finding the properties of the obstacle in particular, in the context of this work, locating a human being. Identification of human presence is based on face detection and cloth texture analysis. The major constraints for these algorithms to run on the Embedded System are small image frame (160 × 120) having reduced faces, limited memory and very less processing time available to achieve real-time image-processing requirements. Prototype of an electronic travel aid device has been developed and experimentally verified on blind-folded persons to analyse the device performance in a laboratory set-up.",,,,,Embedded system; Artificial intelligence; Obstacle; Context (language use); PATH (variable); Computer vision; Audio feedback; Computer science; Face detection; USB; Sonar; Identification (information),,,,,,,,,2309548451,,0,,9,false,,
136-515-547-049-524,Universal life: the use of virtual worlds among people with disabilities,2011-09-22,2011,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Kel Smith,,11,4,387,398,Human–computer interaction; Input device; Haptic technology; Augmented reality; Interface (Java); Avatar; Computer science; Metaverse; 3D computer graphics; Universal design,,,,,https://link.springer.com/content/pdf/10.1007%2Fs10209-011-0254-8.pdf https://link.springer.com/article/10.1007%2Fs10209-011-0254-8,http://dx.doi.org/10.1007/s10209-011-0254-8,,10.1007/s10209-011-0254-8,2133867049,,0,002-371-034-662-148; 024-503-535-718-971; 027-049-361-368-415; 033-260-137-295-304; 039-089-657-331-904; 061-288-691-194-957; 063-334-026-777-906; 076-117-129-231-78X; 127-254-760-129-567; 162-963-457-779-998; 187-833-690-993-415,11,false,,
136-732-042-076-069,ICCV Workshops - Using Technology Developed for Autonomous Cars to Help Navigate Blind People,,2017,conference proceedings article,2017 IEEE International Conference on Computer Vision Workshops (ICCVW),,IEEE,,Manuel Martinez; Alina Roitberg; Daniel Koester; Rainer Stiefelhagen; Boris Schauerte,"Autonomous driving is currently a very active research area with virtually all automotive manufacturers competing to bring the first autonomous car to the market. This race leads to billions of dollars being invested in the development of novel sensors, processing platforms, and algorithms. In this paper, we explore the synergies between the challenges in self-driving technology and development of navigation aids for blind people. We aim to leverage the recently emerged methods for self-driving cars, and use it to develop assistive technology for the visually impaired. In particular we focus on the task of perceiving the environment in realtime from cameras. First, we review current developments in embedded platforms for real-time computation as well as current algorithms for image processing, obstacle segmentation and classification. Then, as a proof-of-concept, we build an obstacle avoidance system for blind people that is based on a hardware platform used in the automotive industry. To perceive the environment, we adapt an implementation of the stixels algorithm, designed for self-driving cars. We discuss the challenges and modifications required for such an application domain transfer. Finally, to show its usability in practice, we conduct and evaluate a user study with six blindfolded people.",,,1424,1432,Human–computer interaction; Artificial intelligence; Usability; Obstacle; Computer vision; Computer science; Obstacle avoidance; Image processing,,,,,https://openaccess.thecvf.com/content_ICCV_2017_workshops/w22/html/Martinez_Using_Technology_Developed_ICCV_2017_paper.html https://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w22/Martinez_Using_Technology_Developed_ICCV_2017_paper.pdf https://dblp.uni-trier.de/db/conf/iccvw/iccvw2017.html#MartinezRKSS17 https://doi.org/10.1109/ICCVW.2017.169,http://dx.doi.org/10.1109/iccvw.2017.169,,10.1109/iccvw.2017.169,2768577426,,0,006-342-835-630-050; 011-797-116-120-363; 016-344-423-220-225; 019-925-883-027-48X; 021-804-922-051-855; 033-626-844-682-323; 037-163-533-301-156; 039-078-199-889-738; 041-138-366-939-178; 043-932-373-674-113; 048-485-218-690-578; 049-592-217-207-783; 057-890-353-263-937; 061-190-872-413-287; 064-043-611-038-916; 071-473-658-566-47X; 078-515-769-030-114; 081-141-746-741-399; 088-058-968-629-359; 109-616-902-181-284,42,false,,
136-801-217-094-143,ViCLEVR: a visual reasoning dataset and hybrid multimodal fusion model for visual question answering in Vietnamese,2024-07-06,2024,journal article,Multimedia Systems,09424962; 14321882,Springer Science and Business Media LLC,Germany,Khiem Vinh Tran; Hao Phu Phan; Kiet Van Nguyen; Ngan Luu Thuy Nguyen,,30,4,,,Vietnamese; Computer science; Question answering; Visual reasoning; Artificial intelligence; Natural language processing; Information retrieval; Linguistics; Philosophy,,,,Quỹ Đổi mới sáng tạo Vingroup,,http://dx.doi.org/10.1007/s00530-024-01394-w,,10.1007/s00530-024-01394-w,,,0,005-213-744-754-675; 013-663-537-135-079; 015-553-729-889-671; 016-454-117-547-083; 019-391-735-237-449; 020-233-013-143-936; 023-933-129-049-825; 024-295-930-710-511; 028-722-935-581-64X; 035-426-193-510-953; 038-155-539-599-340; 039-069-822-544-826; 041-080-824-883-601; 046-583-027-625-542; 049-479-686-882-285; 053-094-537-530-97X; 053-259-352-615-54X; 058-232-745-039-485; 058-768-795-191-405; 069-480-623-909-181; 071-216-857-527-206; 079-286-837-510-84X; 083-304-714-010-862; 093-358-467-278-162; 095-320-806-977-857; 100-340-275-038-662; 105-357-625-311-511; 111-128-589-009-160; 111-474-942-045-376; 111-932-402-388-824; 115-863-165-141-746; 118-357-955-476-257; 123-748-051-094-933; 129-830-077-704-497; 136-063-653-293-526; 136-094-376-462-759; 138-374-909-833-284; 143-335-299-942-414; 147-245-813-198-496; 156-218-647-497-345; 157-583-130-607-902; 157-665-356-848-847; 162-410-301-313-745; 167-470-403-114-612; 175-744-291-783-162; 182-851-433-775-991; 195-706-909-301-830,0,false,,
136-876-209-245-307,Odour motion sensing enhances navigation of complex plumes.,2022-11-09,2022,journal article,Nature,14764687; 00280836,Springer Science and Business Media LLC,United Kingdom,Nirag Kadakia; Mahmut Demir; Brenden T Michaelis; Brian D DeAngelis; Matthew A Reidenbach; Damon A Clark; Thierry Emonet,"Odour plumes in the wild are spatially complex and rapidly fluctuating structures carried by turbulent airflows<sup>1-4</sup>. To successfully navigate plumes in search of food and mates, insects must extract and integrate multiple features of the odour signal, including odour identity<sup>5</sup>, intensity<sup>6</sup> and timing<sup>6-12</sup>. Effective navigation requires balancing these multiple streams of olfactory information and integrating them with other sensory inputs, including mechanosensory and visual cues<sup>9,12,13</sup>. Studies dating back a century have indicated that, of these many sensory inputs, the wind provides the main directional cue in turbulent plumes, leading to the longstanding model of insect odour navigation as odour-elicited upwind motion<sup>6,8-12,14,15</sup>. Here we show that Drosophila melanogaster shape their navigational decisions using an additional directional cue-the direction of motion of odours-which they detect using temporal correlations in the odour signal between their two antennae. Using a high-resolution virtual-reality paradigm to deliver spatiotemporally complex fictive odours to freely walking flies, we demonstrate that such odour-direction sensing involves algorithms analogous to those in visual-direction sensing<sup>16</sup>. Combining simulations, theory and experiments, we show that odour motion contains valuable directional information that is absent from the airflow alone, and that both Drosophila and virtual agents are aided by that information in navigating naturalistic plumes. The generality of our findings suggests that odour-direction sensing may exist throughout the animal kingdom and could improve olfactory robot navigation in uncertain environments.",611,7937,754,761,Computer science; Sensory cue; Computer vision; Artificial intelligence; Sensory system; SIGNAL (programming language); Motion (physics); Airflow; Drosophila (subgenus); Communication; Biology; Neuroscience; Physics; Sociology; Programming language; Biochemistry; Gene; Thermodynamics,,Animals; Drosophila melanogaster/anatomy & histology; Odorants/analysis; Wind; Spatial Navigation/physiology; Motion Perception/physiology; Time Factors; Olfactory Perception/physiology; Arthropod Antennae/physiology; Cues; Walking/physiology,,NIMH NIH HHS (F32 MH118700) United States; NIDCD NIH HHS (K99 DC019397) United States; NEI NIH HHS (R01 EY026555) United States,,http://dx.doi.org/10.1038/s41586-022-05423-4,36352224,10.1038/s41586-022-05423-4,,PMC10039482,0,003-217-816-317-948; 003-944-348-778-655; 005-110-240-501-752; 006-032-943-303-168; 006-162-748-619-832; 008-851-924-928-287; 009-803-104-835-130; 011-874-380-886-970; 018-634-541-502-619; 019-902-193-951-585; 020-216-276-729-846; 020-565-106-078-468; 020-932-364-111-613; 022-810-652-518-247; 024-147-557-786-520; 025-530-682-980-793; 026-492-475-056-495; 027-064-408-286-894; 028-499-295-035-122; 030-456-764-448-651; 030-553-390-246-207; 034-019-236-294-943; 035-329-441-696-212; 036-602-957-532-535; 036-655-725-254-053; 037-149-440-551-905; 038-256-207-695-902; 038-873-063-298-097; 039-982-540-232-453; 040-512-881-397-976; 043-569-596-734-909; 044-162-872-525-057; 044-616-287-828-333; 047-038-880-481-171; 049-268-819-027-785; 050-175-269-179-081; 056-106-665-562-938; 062-591-462-530-10X; 065-186-568-916-027; 066-431-111-481-177; 071-362-723-226-665; 080-808-020-310-107; 083-429-909-161-934; 085-756-245-346-055; 090-489-598-566-800; 093-727-261-058-608; 094-116-774-952-422; 095-454-588-210-044; 096-992-977-357-749; 107-392-209-590-757; 108-898-893-586-600; 123-883-173-821-440; 133-546-675-923-194; 138-273-848-108-886; 142-978-863-688-932,33,true,,green
137-032-010-877-936,IEEE Conf. on Intelligent Systems - Computer vision guidance system for indoor navigation of visually impaired people,,2016,conference proceedings article,2016 IEEE 8th International Conference on Intelligent Systems (IS),,IEEE,,Kabalan Chaccour; Georges Badr,"Visually Impaired (VI) and blind people suffer from reduced mobility, as they cannot detect the terrain and their environment. They always need assistance and walking support systems in their daily life. Solutions have been proposed many decades ago and are rapidly improving nowadays due to the technology evolution and integration. A large number of assistance aids have been deployed in real life situations whereas other concepts remained as research ideas. This paper describes a new approach of an ambient navigation system that would help the visually impaired or blind person to move freely indoor (house, office, etc.) without the assistance of anyone. The system is composed of IP cameras attached to the ceiling of each room and the smart phone of the subject is used as human machine interface (HMI). Frames are sent to a computer that analyzes the environment, detects and recognizes objects. A computer vision guidance algorithm is designed to help the user reach his destination (or his personal item) with obstacle detection. The system is commanded by voice messages via a simple mobile application. Feedbacks (alerts, route) are voice messages returns by the application to the user. This system provides a reliable solution to assist those users in their indoor navigation providing them a correct route with obstacle avoidance.",,,449,454,Artificial intelligence; Terrain; Guidance system; Obstacle; SIMPLE (military communications protocol); Ceiling (cloud); Navigation system; Visually impaired; Computer vision; Computer science; Multimedia; Obstacle avoidance,,,,,http://ieeexplore.ieee.org/document/7737460/ https://ieeexplore.ieee.org/abstract/document/7737460 https://dblp.uni-trier.de/db/conf/is/is2016.html#ChaccourB16,http://dx.doi.org/10.1109/is.2016.7737460,,10.1109/is.2016.7737460,2555734982,,1,000-631-028-807-828; 001-076-828-128-846; 013-056-435-015-274; 013-551-333-215-556; 013-678-295-106-102; 013-847-555-426-159; 014-157-934-526-940; 015-998-086-943-276; 022-697-627-694-941; 030-127-369-356-741; 031-799-548-880-483; 034-350-308-512-672; 064-309-126-323-022; 066-374-042-552-286; 066-526-177-885-125; 098-811-589-762-176,38,false,,
137-395-677-516-752,Indoor way-finding method using IMU and magnetic tensor sensor measurements for visually impaired users,2021-03-26,2021,journal article,International Journal of Intelligent Robotics and Applications,23665971; 2366598x,Springer Science and Business Media LLC,,Min Li; Jayanth Ammanabrolu,,5,2,264,282,Artificial intelligence; Inertial frame of reference; Earth's magnetic field; Sensitivity (control systems); Systems design; Computer vision; Sensor fusion; Computer science; Obstacle avoidance; Inertial measurement unit; Orientation (computer vision),,,,Faculty Research Grant (FRG) of Minnesota State University Mankato,https://dblp.uni-trier.de/db/journals/ijira/ijira5.html#LiA21 https://link.springer.com/content/pdf/10.1007/s41315-021-00163-6.pdf https://link.springer.com/article/10.1007/s41315-021-00163-6,http://dx.doi.org/10.1007/s41315-021-00163-6,,10.1007/s41315-021-00163-6,3149954652,,0,002-990-910-547-637; 008-361-673-204-978; 008-966-381-948-096; 011-552-293-977-610; 017-716-405-510-692; 019-942-660-196-512; 022-148-597-191-574; 025-831-738-362-468; 033-245-290-160-646; 041-925-549-498-232; 046-042-227-012-511; 054-738-209-625-176; 056-900-525-975-376; 061-190-872-413-287; 064-521-070-547-235; 066-246-145-459-483; 066-442-197-994-494; 075-519-293-886-898; 099-334-901-486-872; 106-961-906-968-672; 127-070-672-662-621; 131-458-939-109-530; 135-027-664-661-019; 161-978-347-744-853; 186-432-344-459-742,5,false,,
137-474-199-524-71X,Computer Vision-Based Aid for the VisuallyImpaired Persons- A Survey And ProposingNew Framework,2015-05-07,2015,journal article,International Journal of Innovative Research in Computer and Communication Engineering,23209798,,,Sujith B; null Safeeda,"Developing a tool for the visually impaired people is not a recently emerged problem. But developing a computer aided tool is a still developing area. The aim of all these systems is to help the user in navigation without the help of a second person. There are several works using computer vision techniques. But there is no existing method that help to solve the all basic needs of blind person .All existing systems are designed only for a specific purpose. In this paper, we propose new theoretical framework which combines the key aspects of some use full methods and added some extra capabilities for assisting the blind person. This new system may solve some of major problems of blind persons that are still existing. Also, we give a comparison and analysis of the current use full navigation methods, and identify some challenges which require further research and development.",2014,5,365,370,Artificial intelligence; Basic needs; Key (cryptography); Computer vision; Computer science; Stereopsis; Image processing,,,,,https://www.rroij.com/open-access/computer-visionbased-aid-for-the-visuallyimpaired-persons-a-survey-and-proposingnew-framework.pdf http://www.rroij.com/open-access/computer-visionbased-aid-for-the-visuallyimpaired-persons-a-survey-and-proposingnew-framework.pdf,http://www.rroij.com/open-access/computer-visionbased-aid-for-the-visuallyimpaired-persons-a-survey-and-proposingnew-framework.pdf,,,2991703942,,0,,0,false,,
137-524-098-820-256,Single image super-resolution: a comprehensive review and recent insight,2023-09-04,2023,journal article,Frontiers of Computer Science,20952228; 20952236,Springer Science and Business Media LLC,Germany,Hanadi Al-Mekhlafi; Shiguang Liu,,18,1,,,Computer science; Artificial intelligence; Convolutional neural network; Deep learning; Image resolution; Computer vision; Resolution (logic); Image processing; Superresolution; Process (computing); Image (mathematics); Operating system,,,,,,http://dx.doi.org/10.1007/s11704-023-2588-9,,10.1007/s11704-023-2588-9,,,0,000-305-333-271-309; 000-918-726-324-194; 001-616-097-646-387; 002-959-474-934-407; 003-042-893-865-815; 003-604-780-499-224; 003-618-371-434-603; 004-116-572-740-291; 004-144-513-503-673; 004-644-808-094-695; 004-708-828-275-303; 005-366-610-168-985; 006-584-220-607-331; 006-615-367-614-906; 010-037-102-410-599; 010-642-772-928-923; 010-881-186-573-195; 013-377-528-123-271; 014-079-636-746-135; 014-475-132-977-145; 017-618-273-850-054; 017-772-165-516-10X; 020-233-013-143-936; 021-032-749-031-916; 021-496-988-088-093; 022-319-224-717-282; 023-756-287-001-179; 025-118-679-843-603; 025-384-948-751-89X; 026-252-287-049-279; 027-434-353-991-101; 027-440-785-116-395; 028-980-610-605-692; 029-067-260-246-672; 032-179-334-393-419; 032-352-877-661-332; 033-852-349-612-614; 034-126-597-030-330; 034-447-344-046-42X; 034-728-681-073-868; 034-779-649-086-386; 037-987-340-615-695; 038-305-768-185-361; 038-707-501-624-750; 038-764-645-124-334; 038-868-027-929-932; 039-209-940-377-59X; 039-585-851-536-842; 039-956-592-137-125; 040-398-735-882-088; 041-330-482-983-520; 042-157-190-170-026; 043-268-213-480-632; 044-095-303-626-459; 044-679-743-934-972; 045-515-972-363-953; 045-650-312-094-219; 045-761-936-639-653; 050-253-004-092-002; 051-613-116-312-172; 052-087-419-293-860; 052-995-329-967-335; 053-227-301-320-470; 053-776-684-452-391; 055-916-692-009-597; 056-908-647-110-895; 057-698-530-994-360; 060-033-860-973-361; 060-681-237-092-98X; 061-258-914-974-399; 062-989-803-211-976; 063-291-451-385-519; 063-924-178-886-933; 065-617-932-379-099; 066-083-258-814-126; 066-186-989-634-85X; 068-094-892-554-399; 069-213-515-569-806; 070-188-075-068-912; 071-493-090-172-846; 074-493-492-867-316; 075-877-161-037-739; 077-855-287-282-401; 078-284-386-986-797; 081-428-717-107-020; 083-925-529-399-556; 088-805-181-481-49X; 090-215-200-905-318; 092-529-772-491-450; 093-189-524-323-528; 093-217-705-237-966; 100-762-348-571-223; 103-335-181-612-822; 105-272-232-973-335; 105-605-845-513-991; 107-626-393-547-079; 107-853-098-496-387; 108-590-922-667-717; 108-789-459-085-683; 109-978-438-768-177; 110-688-067-285-394; 116-030-405-883-678; 119-360-885-848-329; 119-680-185-618-592; 119-911-058-883-057; 120-404-421-228-17X; 122-984-136-154-008; 124-100-079-858-944; 125-825-323-285-207; 126-165-604-213-862; 127-554-297-667-578; 134-640-105-432-93X; 136-521-567-138-71X; 137-000-182-768-350; 138-681-108-753-065; 142-003-099-152-55X; 143-562-655-070-609; 146-879-061-600-398; 151-700-736-594-749; 151-934-493-978-634; 155-694-348-957-533; 186-490-790-115-339,7,false,,
137-573-262-455-432,A Low-Cost Vision System Using a Retrofitted Robot for Locating Parts for Welding Process,2021-11-10,2021,journal article,Arabian Journal for Science and Engineering,2193567x; 21914281; 13198025,Springer Science and Business Media LLC,Saudi Arabia,Fagner Guilherme Ferreira Coelho; Alexandre Queiroz Bracarense; Eduardo José Lima,,47,7,8457,8467,,,,,,,http://dx.doi.org/10.1007/s13369-021-06301-3,,10.1007/s13369-021-06301-3,,,0,009-791-113-585-684; 010-240-796-808-852; 015-628-407-550-097; 019-430-786-483-139; 024-065-249-777-578; 031-687-173-065-709; 081-141-746-741-399; 157-369-722-821-999,1,false,,
138-084-071-561-450,2ndVision: A Smart Navigation Suits to Ensure Safe Movement for Visually Impaired using Machine Learning and Internet of Things (IoT),2024-02-24,2024,conference proceedings article,Artificial Intelligence and Big Data,,Academy & Industry Research Collaboration Center,,Zhongxuan Xu; Ziqi Zeng; Ang Li,"<jats:p>In response to the challenges faced by the blind and visually impaired, exacerbated by the increasing prevalence of screen-related visual impairments, this proposal aims to address the shortcomings of existing solutions. The proposed solution consists of a cost-effective and secure approach, featuring a suit equipped with sensors and motors, complemented by a mobile app. This integrated system enhances the navigational experience for the visually impaired, offering key features such as safe traversal in any environment, map-guided navigation, and timely warnings of high-speed dangers. The suit utilizes time-of-flight sensors and vibration motors to convey crucial information to users, including distance, direction, and warnings, through distinct vibration signals. The mobile app serves as a valuable supplement to the suit, providing features such as traffic sign detection and advanced map navigation [8]. This cohesive blend of innovative hardware and software components establishes a comprehensive solution, addressing the urgent need for effective, affordable, and technologically advanced tools for the blind and visually impaired amidst the era of widespread screen usage.</jats:p>",,,,,Internet of Things; Visually impaired; Computer science; Movement (music); Human–computer interaction; The Internet; Artificial intelligence; Multimedia; Computer security; World Wide Web; Aesthetics; Philosophy,,,,,,http://dx.doi.org/10.5121/csit.2024.140427,,10.5121/csit.2024.140427,,,0,,0,true,,bronze
138-205-762-662-283,Design of a braille writing tutor to combat illiteracy,2009-05-01,2009,journal article,Information Systems Frontiers,13873326; 15729419,Springer Science and Business Media LLC,Netherlands,Nidhi Kalra; T. Lauwers; Daniel Dewey; T. Stepleton; M. B. Dias,,11,2,117,128,Mathematics education; User-centered design; Stylus; Functional illiteracy; Citizen journalism; TUTOR; Dignity; Braille; Prejudice; Computer science; Multimedia,,,,,https://dblp.uni-trier.de/db/journals/isf/isf11.html#KalraLDSD09 https://doi.org/10.1007/s10796-009-9171-2 https://link.springer.com/article/10.1007/s10796-009-9171-2 http://dx.doi.org/10.1007/s10796-009-9171-2 https://jglobal.jst.go.jp/en/detail?JGLOBAL_ID=200902201942281339 https://dl.acm.org/doi/10.1007/s10796-009-9171-2,http://dx.doi.org/10.1007/s10796-009-9171-2,,10.1007/s10796-009-9171-2,2079975350,,0,028-208-095-693-670; 030-295-985-858-57X; 051-008-241-042-874; 051-256-865-327-424; 065-065-938-708-41X; 065-294-146-256-434; 070-147-192-266-773; 081-549-613-117-153; 091-576-026-128-76X; 093-632-677-768-660; 110-087-579-161-48X; 139-426-302-892-251; 149-522-618-881-332; 172-589-890-077-132; 174-349-524-534-460,16,false,,
138-247-498-850-191,United Arab Emirates (UAE) talking map,2018-06-28,2018,journal article,GeoJournal,03432521; 15729893,Springer Science and Business Media LLC,Netherlands,M. M. Yagoub,,84,4,889,899,,,,,UAE University,,http://dx.doi.org/10.1007/s10708-018-9896-x,,10.1007/s10708-018-9896-x,,,0,003-519-813-028-415; 005-536-511-459-815; 010-646-641-896-202; 012-321-586-547-620; 019-849-313-543-747; 030-867-595-754-370; 045-600-105-648-172; 046-985-639-772-772; 051-797-952-190-659; 055-810-261-520-46X; 062-098-260-961-797; 082-359-476-112-16X; 083-196-571-690-630; 093-902-354-133-596; 105-663-985-230-825; 125-161-764-897-074; 125-729-634-680-968; 129-417-828-086-089; 134-002-772-819-234; 144-486-224-235-082,0,false,,
138-439-975-871-099,Cardiac and Thoracic-Abdominal Surgery,2006-05-19,2006,journal article,International Journal of Computer Assisted Radiology and Surgery,18616410; 18616429,Springer Science and Business Media LLC,Germany,,,1,S1,265,292,,,,,,,http://dx.doi.org/10.1007/s11548-006-0021-0,,10.1007/s11548-006-0021-0,,,0,,3,false,,
138-528-673-324-553,A smartphone-based virtual white cane,2013-04-02,2013,journal article,Pattern Analysis and Applications,14337541; 1433755x,Springer Science and Business Media LLC,Germany,Pablo Vera; Daniel Zenteno; Joaquin Salas,,17,3,623,632,Human–computer interaction; Pattern recognition (psychology); Artificial intelligence; Laser pointer; Travel time; White cane; Assistive technology; Visually impaired; Computer vision; Triangulation (geometry); Computer science; Reflection (computer programming),,,,,https://dblp.uni-trier.de/db/journals/paa/paa17.html#AlfaroZS14 https://link.springer.com/article/10.1007/s10044-013-0328-8/fulltext.html https://dl.acm.org/doi/10.1007/s10044-013-0328-8 https://link.springer.com/article/10.1007/s10044-013-0328-8 https://link.springer.com/content/pdf/10.1007%2Fs10044-013-0328-8.pdf,http://dx.doi.org/10.1007/s10044-013-0328-8,,10.1007/s10044-013-0328-8,1977151663,,0,001-076-828-128-846; 005-319-706-263-504; 009-612-733-037-965; 013-056-435-015-274; 013-551-333-215-556; 013-847-555-426-159; 024-131-470-445-452; 030-127-369-356-741; 031-799-548-880-483; 034-350-308-512-672; 036-280-949-293-766; 039-946-476-241-022; 043-818-840-401-642; 044-949-263-620-170; 050-483-037-308-846; 057-879-237-577-755; 058-802-736-623-77X; 059-407-478-677-340; 060-257-784-475-928; 063-948-491-636-830; 064-309-126-323-022; 064-683-024-880-936; 066-526-177-885-125; 072-033-955-060-341; 073-173-715-600-055; 074-017-562-218-104; 084-679-921-430-947; 086-404-814-970-85X; 098-811-589-762-176; 112-121-560-424-373; 125-501-372-179-565; 127-483-973-270-088; 155-330-779-796-500; 176-619-480-167-936,34,false,,
138-915-262-210-711,Step Simple – Guiding the Visually Challenged,2024-04-30,2024,journal article,IJARCCE,23195940; 22781021,Tejass Publishers,,Karthik Ganesh; Advaith Prasad; Mohammed Mohammed; Mrs. Ashwitha Shetty,"This project presents the design and implementation of a smart blind stick prototype aimed at enhancing the mobility and safety of visually impaired individuals.The Blind Stick integrates an ultrasonic sensor, USB web camera, speakers, to provide object detection, The prototype leverages an Raspberry pi microcontroller to efficiently manage the sensor data and interactions.The ultrasonic sensor is employed to detect obstacles in the user's path, triggering a speak out to warn the user of potential collisions.The integration of a switch allows the user to initiate an emergency alert.Upon pressing the switch, Additionally, the Blind Stick prototype capitalizes on computer vision techniques through the utilization of the YOLO (You Only Look Once) framework.Connected to a PC, the Blind Stick leverages a USB web camera to capture images.Detected objects are then identified using YOLO, and corresponding audio alerts are relayed to the user through the earphones, enabling the user to understand their surroundings more comprehensively.",13,4,,,Computer science; Simple (philosophy); Human–computer interaction; Artificial intelligence; Epistemology; Philosophy,,,,,https://zenodo.org/records/11073925/files/IJRESM_V7_I4_29.pdf https://doi.org/10.5281/zenodo.11073925,http://dx.doi.org/10.17148/ijarcce.2024.134175,,10.17148/ijarcce.2024.134175,,,0,,0,true,,gold
139-628-445-487-371,A multimodal dense convolution network for blind image quality assessment,2023-12-07,2023,journal article,Frontiers of Information Technology & Electronic Engineering,20959184; 20959230,Zhejiang University Press,United States,Nandhini Chockalingam; Brindha Murugan,,24,11,1601,1615,Computer science; Overfitting; Convolutional neural network; Artificial intelligence; Deep learning; Image quality; Benchmark (surveying); Convolution (computer science); Pattern recognition (psychology); Machine learning; Image (mathematics); Artificial neural network; Geodesy; Geography,,,,,,http://dx.doi.org/10.1631/fitee.2200534,,10.1631/fitee.2200534,,,0,001-243-747-198-071; 001-484-504-895-566; 004-269-574-716-057; 010-123-413-227-012; 011-744-647-117-180; 011-927-414-739-394; 015-571-444-508-782; 015-679-822-738-134; 016-048-256-870-530; 020-233-013-143-936; 025-997-108-960-906; 027-573-410-159-97X; 029-942-674-356-399; 030-351-539-838-943; 030-852-680-745-370; 036-668-984-674-717; 039-135-942-929-563; 040-484-775-336-912; 041-805-724-562-03X; 048-974-100-355-309; 049-880-214-943-175; 051-726-934-370-252; 052-312-496-861-524; 052-832-454-147-524; 056-760-111-976-860; 060-141-480-164-106; 064-401-628-721-647; 066-146-679-480-57X; 066-152-116-887-991; 073-196-568-275-126; 074-978-572-209-802; 078-263-191-848-80X; 079-247-399-160-293; 081-573-769-901-753; 095-047-484-202-826; 096-439-410-985-767; 104-018-015-345-340; 110-449-562-069-308; 124-434-973-275-620; 125-641-102-255-972; 127-744-666-647-810; 130-507-563-382-935; 136-099-343-498-117; 139-552-118-652-140; 157-435-584-392-315; 172-655-619-392-269; 193-848-405-591-652,1,false,,
139-856-465-147-133,Assistive Technology for Navigation of Visually Impaired People,2024-05-04,2024,journal article,Journal of Electrical Systems,11125209,Science Research Society,France,null Mohammed Fayiz Ferosh,"<jats:p>In this paper, an assistive navigational technology was proposed for those who are blind. People who are visually impaired are more likely to be physically inactive, move more slowly, and dread falling. As a result, the majority of them lack the confidence necessary to travel on their own in strange places. Despite the fact that some technology advancements can help them, not all of them are affordable to the common man. Through our efforts, we hope to help visually impaired persons feel secure and confident while navigating new surroundings. Assistive tech- nology for navigation of visually impaired people is an affordable and efficient computer vision based blind guidance system that helps the user detect the distance to obstacles well in advance and avoid them. The suggested system also includes an emergency distress alert feature that enables users to communicate their locations in an emergency.</jats:p>",20,7s,989,996,,,,,,,http://dx.doi.org/10.52783/jes.3479,,10.52783/jes.3479,,,0,,0,false,,
139-920-245-591-148,Sensors for the Visually Impaired,,1996,journal article,Sensors Update,14322404; 16168984,Wiley,,Klaus-P Dr Koch,"In this chapter a survey of the background on visual impairment and blindness has been given. Common causes of visual impairment and statistical data have been presented in a short form. Of the wide variety of sensory aids, ultrasonic, optical, and other electronic sensors for mobility, orientation, and navigation have been discussed, in addition to sensory aids for communication and information. Sensors for mobility and orientation are considered as secondary travel aids and are used as a supplement to the traditional long cane or the guide dog. Among the sensory aids for communication and information, mostly closed circuit television systems and reading machines are in use today. A quick look into the relevant patent literature showed that almost all aspects of present sensory aids are covered by patents and patent applications. It is expected that in the near future further progress will be made with more sophisticated and miniaturized sensory aids for the visually impaired. A real technological challenge will be the realization of retina implants. If this approach works successfully, there is a good chance of restoring vision for blind people.",1,1,197,222,Human–computer interaction; Engineering; Variety (cybernetics); Realization (linguistics); Orientation (mental); Visual impairment; Sensory Aid; Closed circuit; Visually impaired; Reading (process); Telecommunications,,,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/1616-8984%28199607%291%3A1%3C197%3A%3AAID-SEUP197%3E3.0.CO%3B2-G,http://dx.doi.org/10.1002/1616-8984(199607)1:1<197::aid-seup197>3.0.co;2-g,,10.1002/1616-8984(199607)1:1<197::aid-seup197>3.0.co;2-g,2135709576,,0,002-380-803-849-092; 012-014-676-390-022; 012-243-002-594-984; 017-427-521-109-432; 019-866-317-188-986; 021-838-739-528-796; 041-890-437-632-903; 057-879-237-577-755; 064-309-126-323-022; 068-138-060-660-975; 070-924-196-021-283; 077-840-003-302-473; 084-845-709-576-122; 088-864-752-719-271; 090-064-651-645-089; 118-223-407-423-492; 129-590-623-384-866,0,false,,
140-150-662-022-402,A review on speech emotion recognition for late deafened educators in online education,2024-01-24,2024,journal article,International Journal of Speech Technology,13812416; 15728110,Springer Science and Business Media LLC,Netherlands,Aparna Vyakaranam; Tomas Maul; Bavani Ramayah,,27,1,29,52,Computer science; Speech recognition; Emotion recognition,,,,,,http://dx.doi.org/10.1007/s10772-023-10064-7,,10.1007/s10772-023-10064-7,,,0,002-364-854-081-634; 008-844-095-646-622; 010-265-177-811-834; 011-300-349-088-441; 012-879-624-111-645; 012-920-801-301-338; 014-620-748-809-287; 015-700-457-525-010; 019-597-374-565-140; 019-660-252-049-059; 019-957-027-520-569; 026-959-266-386-881; 028-130-959-116-84X; 030-279-672-890-069; 031-380-587-002-496; 031-802-622-588-814; 034-596-711-746-655; 034-842-799-196-336; 036-256-559-986-380; 040-777-602-324-592; 041-817-916-670-48X; 042-405-429-818-278; 042-642-351-654-478; 042-914-683-309-132; 044-294-488-959-576; 046-226-946-139-611; 048-569-183-606-054; 050-218-647-151-64X; 051-256-200-282-006; 055-463-266-761-373; 055-542-397-117-313; 056-862-062-445-883; 056-915-597-567-250; 058-805-751-062-417; 060-634-674-203-236; 062-639-290-428-28X; 062-664-118-195-810; 064-316-939-503-894; 065-161-528-162-397; 067-657-052-076-240; 073-013-415-762-041; 075-955-691-426-556; 078-317-590-361-514; 079-277-496-914-365; 080-299-188-383-366; 086-072-108-146-267; 089-951-800-240-399; 092-342-288-073-433; 095-240-140-583-623; 098-588-704-799-443; 099-246-948-839-743; 104-023-254-487-377; 105-057-420-005-006; 108-335-506-348-92X; 110-010-970-346-342; 113-257-567-619-754; 114-111-225-479-821; 121-125-581-318-005; 124-377-862-747-842; 127-484-247-424-593; 130-414-439-331-951; 132-286-054-915-234; 139-121-680-312-667; 142-079-357-802-975; 149-794-159-028-365; 153-428-663-970-709; 181-363-908-490-289,0,false,,
140-711-550-102-237,"Eye Machines: Robot Eye, Vision and Gaze",2021-06-04,2021,journal article,International Journal of Social Robotics,18754791; 18754805,Springer Science and Business Media LLC,Germany,Chris Chesher; Fiona Andreallo,,14,10,2071,2081,,,,,,,http://dx.doi.org/10.1007/s12369-021-00777-7,,10.1007/s12369-021-00777-7,,,0,011-081-501-666-668; 020-948-358-524-709; 023-142-373-084-429; 023-252-391-340-286; 026-767-514-826-430; 027-014-914-501-248; 028-659-954-037-202; 028-969-290-316-818; 029-560-751-132-169; 036-830-834-574-473; 038-377-250-225-712; 038-397-803-771-413; 045-590-635-030-322; 049-491-250-968-063; 054-643-541-113-613; 061-228-888-585-148; 070-519-366-247-906; 072-693-002-839-050; 080-976-624-782-490; 084-566-704-738-91X; 100-255-720-614-79X; 103-313-623-552-728; 122-508-063-348-76X; 122-867-087-141-039; 142-732-526-434-704; 152-675-762-572-118; 153-894-222-959-238; 172-654-167-854-690,6,false,,
140-878-264-335-501,CoRL - Personalized Dynamics Models for Adaptive Assistive Navigation Systems,2018-10-23,2018,book,,,,,Eshed Ohn-Bar; Kris M. Kitani; Chieko Asakawa,"Consider an assistive system that guides visually impaired users through speech and haptic feedback to their destination. Existing robotic and ubiquitous navigation technologies (e.g., portable, ground, or wearable systems) often operate in a generic, user-agnostic manner. However, to minimize confusion and navigation errors, our real-world analysis reveals a crucial need to adapt the instructional guidance across different end-users with diverse mobility skills. To address this practical issue in scalable system design, we propose a novel model-based reinforcement learning framework for personalizing the system-user interaction experience. When incrementally adapting the system to new users, we propose to use a weighted experts model for addressing data-efficiency limitations in transfer learning with deep models. A real-world dataset of navigation by blind users is used to show that the proposed approach allows for (1) more accurate long-term human behavior prediction (up to 20 seconds into the future) through improved reasoning over personal mobility characteristics, interaction with surrounding obstacles, and the current navigation goal, and (2) quick adaptation at the onset of learning, when data is limited.",,,16,39,Human–computer interaction; Dynamics (music); Transfer of learning; Haptic technology; Personal mobility; Wearable systems; Visually impaired; Computer science; Adaptation (computer science); Reinforcement learning,,,,,https://dblp.uni-trier.de/db/conf/corl/corl2018.html#Ohn-BarKA18 http://proceedings.mlr.press/v87/ohnbar18a.html http://proceedings.mlr.press/v87/ohnbar18a/ohnbar18a.pdf,https://dblp.uni-trier.de/db/conf/corl/corl2018.html#Ohn-BarKA18,,,2964338736,,0,,2,false,,
141-047-839-723-337,Vision Enhanced: Empowering Visually Impaired with Smart Sense Technology,2024-05-11,2024,journal article,INTERANTIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ENGINEERING AND MANAGEMENT,25823930,Indospace Publications,,"Preet Dixit,","<jats:p>People who are blind or visually impaired often encounter challenges when navigating streets and might not be aware of surrounding landmarks. To move from one location to another, they may need assistance such as a cane, guide dog, or human aid. This paper is based on the designing of integrated assistive devices for visually impaired people. It consists of the integration of two devices smart navigating helmet and a stick based on object detection. The novelty of this paper is that we are processing the data of both devices and then providing a better navigation solution. (Abstract)     Keywords- YOLO, MQTT, Ultrasonic Sensor, Raspberry Pi 4, Image Processing (key words)</jats:p>",8,5,1,5,Novelty; Visually impaired; MQTT; Computer science; Key (lock); Raspberry pi; Computer vision; Human–computer interaction; Artificial intelligence; Internet of Things; Embedded system; Psychology; Computer security; Social psychology,,,,,https://ijsrem.com/download/vision-enhanced-empowering-visually-impaired-with-smart-sense-technology/?wpdmdl=33552&refresh=66597eb99588d1717141177 https://doi.org/10.55041/ijsrem33785,http://dx.doi.org/10.55041/ijsrem33785,,10.55041/ijsrem33785,,,0,,0,true,,bronze
141-084-316-319-231,"Transforming a Quadruped into a Guide Robot for the Visually Impaired: Formalizing Wayfinding, Interaction Modeling, and Safety Mechanism",2023-01-01,2023,preprint,arXiv (Cornell University),,,,J. Taery Kim; Wenhao Yu; Yash Kothari; Jie Tan; Greg Turk; Sehoon Ha,"This paper explores the principles for transforming a quadrupedal robot into a guide robot for individuals with visual impairments. A guide robot has great potential to resolve the limited availability of guide animals that are accessible to only two to three percent of the potential blind or visually impaired (BVI) users. To build a successful guide robot, our paper explores three key topics: (1) formalizing the navigation mechanism of a guide dog and a human, (2) developing a data-driven model of their interaction, and (3) improving user safety. First, we formalize the wayfinding task of the human-guide robot team using Markov Decision Processes based on the literature and interviews. Then we collect real human-robot interaction data from three visually impaired and six sighted people and develop an interaction model called the ``Delayed Harness'' to effectively simulate the navigation behaviors of the team. Additionally, we introduce an action shielding mechanism to enhance user safety by predicting and filtering out dangerous actions. We evaluate the developed interaction model and the safety mechanism in simulation, which greatly reduce the prediction errors and the number of collisions, respectively. We also demonstrate the integrated system on a quadrupedal robot with a rigid harness, by guiding users over $100+$~m trajectories.",,,,,Robot; Human–computer interaction; Mechanism (biology); Human–robot interaction; Computer science; Task (project management); Simulation; Artificial intelligence; Engineering; Systems engineering; Philosophy; Epistemology,,,,,https://arxiv.org/abs/2306.14055,http://dx.doi.org/10.48550/arxiv.2306.14055,,10.48550/arxiv.2306.14055,,,0,,0,true,other-oa,green
141-236-664-832-39X,Static Vision Support: Leveraging YOLOv3 and Tesseract OCR for the Visually Impaired,2024-04-21,2024,journal article,International Research Journal of Modernization in Engineering Technology and Science,25825208,International Research Journal of Modernization in Engineering Technology and Science,,V Supriya; Rakhi Patil; Anagha Rajput; Dhanashree Nimse; Aishwarya Motghare,"People with visual impairments encounter various difficulties in navigating their environments autonomously.Traditional aids like canes and guide dogs offer only partial assistance, especially in detecting obstacles and reading text.The progress in computer vision technology presents hopeful remedies to tackle these issues by facilitating the identification of objects and text from images obtained through cameras.This paper introduces the creation of a system to assist the blind, leveraging the YOLOv3 object detection algorithm and the Tesseract OCR engine for text identification for static objects.The system incorporates these algorithms with multithreading functionalities to deliver real-time support to visually impaired individuals.",,,,,Visually impaired; Artificial intelligence; Computer science; Computer vision; Human–computer interaction,,,,,,http://dx.doi.org/10.56726/irjmets53276,,10.56726/irjmets53276,,,0,,0,true,,bronze
141-271-998-410-713,AR assistance for efficient dynamic target search,2022-10-18,2022,journal article,Computational Visual Media,20960433; 20960662,Springer Science and Business Media LLC,,Zixiang Zhao; Jian Wu; Lili Wang,"<jats:title>Abstract</jats:title><jats:p>When searching for a dynamic target in an unknown real world scene, search efficiency is greatly reduced if users lack information about the spatial structure of the scene. Most target search studies, especially in robotics, focus on determining either the shortest path when the target’s position is known, or a strategy to find the target as quickly as possible when the target’s position is unknown. However, the target’s position is often known intermittently in the real world, e.g., in the case of using surveillance cameras. Our goal is to help user find a dynamic target efficiently in the real world when the target’s position is intermittently known. In order to achieve this purpose, we have designed an AR guidance assistance system to provide optimal current directional guidance to users, based on searching a prediction graph. We assume that a certain number of depth cameras are fixed in a real scene to obtain dynamic target’s position. The system automatically analyzes all possible meetings between the user and the target, and generates optimal directional guidance to help the user catch up with the target. A user study was used to evaluate our method, and its results showed that compared to free search and a top-view method, our method significantly improves target search efficiency.</jats:p>",9,1,177,194,Computer science; Position (finance); Computer vision; Artificial intelligence; Graph; Focus (optics); Computer graphics; Dynamic programming; Robotics; Robot; Algorithm; Theoretical computer science; Physics; Finance; Optics; Economics,,,,,https://link.springer.com/content/pdf/10.1007/s41095-021-0266-0.pdf https://doi.org/10.1007/s41095-021-0266-0,http://dx.doi.org/10.1007/s41095-021-0266-0,,10.1007/s41095-021-0266-0,,,0,000-729-047-591-837; 001-550-838-337-663; 004-877-171-081-463; 006-017-316-278-658; 007-005-016-217-835; 008-070-391-060-718; 010-436-631-849-060; 010-645-838-342-399; 014-718-606-982-282; 017-677-868-683-507; 017-736-794-401-024; 018-764-309-436-499; 023-858-106-556-210; 030-272-880-448-396; 031-294-750-698-550; 031-724-978-801-224; 032-476-772-428-934; 032-563-691-084-653; 045-985-889-073-484; 047-755-992-533-185; 049-763-167-883-727; 051-072-324-838-09X; 058-752-197-949-347; 062-052-598-535-353; 062-393-763-632-938; 065-049-423-481-180; 069-231-188-624-028; 076-421-353-526-028; 077-413-337-734-544; 087-690-960-188-686; 094-432-027-307-045; 101-336-987-835-285; 103-899-692-238-098; 114-456-904-074-15X; 114-924-392-830-154; 115-421-450-136-251; 121-886-023-092-837; 150-331-249-783-940; 152-357-061-233-937; 154-853-217-067-820; 155-970-129-445-931; 156-558-904-590-921; 173-635-438-239-490,0,true,cc-by,gold
141-592-622-683-366,Intelligent face recognition and navigation system using neural learning for smart security in Internet of Things,2017-11-20,2017,journal article,Cluster Computing,13867857; 15737543,Springer Science and Business Media LLC,Netherlands,Priyan Malarvizhi Kumar; Ushadevi Gandhi; R. Varatharajan; Gunasekaran Manogaran; R Jidhesh; Thanjai Vadivel,,22,4,7733,7744,Human–computer interaction; Artificial intelligence; Navigation system; Facial recognition system; Computer vision; Computer science; Process (engineering); Artificial neural network; Feature extraction; Global Positioning System; Identification (information); Internet of Things,,,,,https://dblp.uni-trier.de/db/journals/cluster/cluster22.html#KumarGVMRV19 https://research.vit.ac.in/publication/intelligent-face-recognition-and-navigation https://doi.org/10.1007/s10586-017-1323-4 https://link.springer.com/article/10.1007/s10586-017-1323-4,http://dx.doi.org/10.1007/s10586-017-1323-4,,10.1007/s10586-017-1323-4,2769892801,,0,000-424-243-390-605; 004-101-552-393-825; 007-341-498-331-253; 009-360-703-125-907; 013-921-304-447-556; 019-047-043-924-640; 023-110-562-992-518; 024-842-481-370-08X; 037-992-209-565-238; 038-208-649-693-158; 040-220-230-984-080; 049-575-510-227-033; 055-035-875-400-361; 059-325-958-345-418; 060-734-384-362-229; 062-593-567-944-092; 064-338-002-306-207; 065-482-797-451-610; 071-014-774-035-77X; 074-582-786-844-990; 079-349-286-645-136; 081-166-460-834-152; 095-345-501-461-885; 100-276-614-214-795; 113-503-848-706-476; 114-969-187-153-544; 115-217-343-776-669; 123-743-979-650-295; 130-767-419-494-116; 131-037-604-050-850; 134-201-678-817-699; 136-117-530-086-456; 136-614-844-556-358; 142-528-534-500-945; 151-932-389-123-08X; 158-446-940-304-255; 159-694-950-776-300; 169-404-359-583-67X; 175-517-888-149-914; 183-935-933-940-273,146,false,,
141-885-771-209-384,Initial phantom studies for an office-based low-field MR system for prostate biopsy,2021-04-23,2021,journal article,International journal of computer assisted radiology and surgery,18616429; 18616410,Springer Science and Business Media LLC,Germany,Selin Chiragzada; Eva Hellman; Duncan Michael; Ramakrishnan Narayanan; Aleksandar Nacev; Dinesh Kumar,"Prostate cancer is the second most prevalent cancer in US men, with about 192,000 new cases and 33,000 deaths predicted for 2020. With only a 31% 5-year survival rate for patients with an initial diagnosis of stage-four prostate cancer, the necessity for early screening and diagnosis is clear. In this paper, we present navigation accuracy results for Promaxo’s MR system intended to be used in a physician’s office for image-guided transperineal prostate biopsy. The office-based low-field MR system was used to acquire images of prostate phantoms with needles inserted through a transperineal template. Coordinates of the estimated sample core locations in the office-based MR system were compared to ground truth needle coordinates identified in a 1.5T external reference scan. The error was measured as the distance between the planned target and the ground truth core center and as the shortest perpendicular distance between the planned target and the ground truth trajectory of the whole core. The average error between the planned target and the ground truth core center was 2.57 ± 1.02 mm, [1.93–3.21] 95% CI. The average error between the planned target to the actual core segment was 2.05 ± 1.24 mm, [1.53–2.56] 95% CI. The average navigation errors were below the clinically significant threshold of 5 mm. The initial phantom results demonstrate the feasibility of the office-based system for prostate biopsy.",16,5,741,748,Imaging phantom; Ground truth; Prostate; Survival rate; Prostate cancer; Prostate biopsy; Nuclear medicine; Office based; Phantom studies; Medicine,Low-field MRI; MR-guided biopsy; Navigation accuracy; Office-based MRI; Prostate cancer; Targeted prostate biopsy,"Biopsy/methods; Humans; Image Processing, Computer-Assisted; Image-Guided Biopsy/methods; Magnetic Resonance Imaging/methods; Male; Needles; Phantoms, Imaging; Prostate/diagnostic imaging; Prostatic Neoplasms/diagnostic imaging; Reproducibility of Results; Treatment Outcome",,"Promaxo, Inc",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8134310 https://pubmed.ncbi.nlm.nih.gov/33891253/ https://link.springer.com/article/10.1007/s11548-021-02364-7 https://www.ncbi.nlm.nih.gov/pubmed/33891253 https://doi.org/10.1007/s11548-021-02364-7 http://www.ncbi.nlm.nih.gov/pubmed/33891253 https://link.springer.com/content/pdf/10.1007/s11548-021-02364-7.pdf https://dblp.uni-trier.de/db/journals/cars/cars16.html#ChiragzadaHMNNK21 https://europepmc.org/article/MED/33891253,http://dx.doi.org/10.1007/s11548-021-02364-7,33891253,10.1007/s11548-021-02364-7,3153486360,PMC8134310,1,006-362-811-218-502; 008-297-269-258-655; 008-378-273-406-82X; 013-497-988-711-855; 013-637-869-814-490; 015-766-608-735-608; 018-999-452-675-157; 019-904-978-515-037; 026-569-413-763-568; 029-910-535-416-172; 031-723-870-324-45X; 038-055-229-803-205; 041-823-688-955-758; 043-479-799-497-078; 044-907-508-299-43X; 047-361-900-138-312; 052-111-924-809-132; 053-337-415-072-230; 054-295-974-425-403; 076-206-214-120-114; 098-377-951-021-006; 103-379-855-175-302; 106-292-642-099-729; 110-104-410-576-534; 129-307-363-281-118; 131-501-872-209-705,5,true,cc-by,hybrid
142-105-374-847-047,MIWAI - Smartphone Based Outdoor Navigation and Obstacle Avoidance System for the Visually Impaired,2019-10-21,2019,book chapter,Lecture Notes in Computer Science,03029743; 16113349,Springer International Publishing,Germany,Chen Qiaoyu; Lijun Wu; Zhicong Chen; Peijie Lin; Shuying Cheng; Zhenhui Wu,"Interlaced roads and unexpected obstacles restrict the blind from traveling. Existing outdoor blind auxiliary systems are bulky or costly, and some of them cannot even feedback the type or distance of obstacles. It is important for auxiliary blind systems to provide navigation, obstacle detection and ranging functions with affordable price and portable size. This paper presents an outdoor navigation system based on smartphone for the visually impaired, which can also help them avoid multi-type dangerous obstacles. Geographic information obtained from GPS receiving module is processed by professional navigation API to provide directional guidance. In order to help the visually impaired avoid obstacle, SSD-MobileNetV2 is retrained by a self-collected dataset with 4500 images, for better detecting the typical obstacles on the road, i.e. car, motorcycle, electric bicycle, bicycle, and pedestrian. Then, a light-weight monocular ranging method is employed to estimate the obstacle’s distance. Based on category and distance, the risk level of obstacle is evaluated, which is timely conveyed to the blind via different tunes. Field tests show that the retrained SSD-MobileNetV2 model can detect obstacles with considerable precision, and the vision-based ranging method can effectively estimate distance.",,,26,37,Ranging; Artificial intelligence; Obstacle; Pedestrian; Navigation system; restrict; Computer vision; Computer science; Global Positioning System; Monocular; Obstacle avoidance,,,,,https://doi.org/10.1007/978-3-030-33709-4_3 https://link.springer.com/chapter/10.1007%2F978-3-030-33709-4_3 https://rd.springer.com/chapter/10.1007/978-3-030-33709-4_3,http://dx.doi.org/10.1007/978-3-030-33709-4_3,,10.1007/978-3-030-33709-4_3,2983730795,,0,010-759-689-155-412; 016-988-681-008-984; 018-828-594-955-588; 021-326-533-295-625; 031-218-334-653-826; 045-886-754-687-645; 048-254-076-918-785; 049-317-239-158-314; 066-133-135-091-645; 070-978-717-326-853; 098-926-109-358-108; 104-208-613-793-423; 106-264-944-278-433; 109-806-014-448-128; 125-112-083-666-544; 146-093-154-688-224; 178-606-451-314-116,10,false,,
142-498-774-227-947,Review of sensor-driven assistive device technologies for enhancing navigation for the visually impaired,2023-11-09,2023,journal article,Multimedia Tools and Applications,15737721; 13807501,Springer Science and Business Media LLC,Netherlands,Iftekar Patel; Makarand Kulkarni; Ninad Mehendale,,83,17,52171,52195,Computer science; Human–computer interaction; Visually impaired; Perception; Artificial intelligence; Computer vision; Psychology; Neuroscience,,,,Somaiya Vidyavihar Unviersity,,http://dx.doi.org/10.1007/s11042-023-17552-7,,10.1007/s11042-023-17552-7,,,0,002-855-185-767-183; 003-245-163-059-395; 006-409-052-293-645; 010-371-675-659-125; 010-653-842-426-00X; 013-501-956-117-041; 014-900-548-112-30X; 019-984-401-850-976; 022-053-334-359-361; 024-417-401-364-679; 025-151-529-700-089; 032-075-502-829-992; 034-936-259-439-98X; 034-993-530-118-464; 035-363-747-813-775; 035-462-419-995-817; 038-529-429-962-974; 039-400-012-166-664; 039-749-829-926-551; 040-770-922-118-251; 041-149-943-965-071; 045-840-950-755-015; 054-242-712-225-541; 054-249-128-447-200; 056-459-675-784-407; 065-553-420-193-637; 067-650-435-280-941; 070-096-820-256-609; 074-128-090-023-289; 077-220-320-708-621; 090-183-338-625-200; 098-910-337-444-079; 108-186-632-977-341; 110-831-712-365-194; 111-772-107-673-112; 118-540-529-844-833; 119-978-971-564-976; 145-145-257-002-323; 147-576-426-453-940; 147-898-042-304-243; 149-119-341-850-866; 155-762-880-845-951; 158-527-319-852-178; 194-331-117-851-821,5,false,,
142-499-243-410-237,,,1998,journal article,Autonomous Robots,09295593; 15737527,Springer Science and Business Media LLC,Netherlands,Gordon Wyeth,,5,3/4,381,394,Computer science; Robot; Artificial intelligence; Microprocessor; Computer vision; Task (project management); Mobile robot; Process (computing); Actuator; Computation; Fault tolerance; Real-time computing; Embedded system; Distributed computing; Management; Algorithm; Economics; Operating system,,,,,,http://dx.doi.org/10.1023/a:1008870625003,,10.1023/a:1008870625003,,,0,,1,false,,
142-822-514-298-708,EyeSee: Camera to Caption with Attention Mechanism,2020-12-16,2020,conference proceedings article,2020 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE),,IEEE,,Tarini Prsti Ramsewak; Dave Appadoo; Zahra Mungloo-Dilmohamud,"According to the WHO, there are currently around 2.2 billion people who are either visually impaired or blind in the world. Previously, these people had to rely only on classic aids such as the white cane and the guide dog for mobility and magnifiers and screen readers amongst others for reading. The massive use of smartphones has opened many new possibilities for the visually impaired and blind. They can now use their smartphones to help them navigate around cities and other places. In this project it is proposed to have an app for smartphones which automatically tells the blind user the objects around him. However, automatically identifying and describing the content of an image is not such a simple task. It involves tasks from 2 complex fields namely computer vision and natural language processing. The proposed application, EyeSee, takes images from a real-time environment, processes these frame by frame and tells the user what the image represents. The app also annotates the images with text. The app uses Deep Learning, more specifically, Show, Attend and Tell and GRU.",,,,,Human–computer interaction; Deep learning; Frame (networking); Artificial intelligence; Task (project management); White cane; Visually impaired; Computer science; Reading (process); Mechanism (biology),,,,,http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=9411571 http://xplorestaging.ieee.org/ielx7/9411519/9411520/09411571.pdf?arnumber=9411571,http://dx.doi.org/10.1109/csde50874.2020.9411571,,10.1109/csde50874.2020.9411571,3158881407,,0,003-215-340-330-223; 005-609-733-141-209; 006-757-908-435-833; 011-581-507-141-206; 018-501-247-620-279; 033-226-351-329-326; 059-485-619-431-787; 069-480-623-909-181; 084-510-831-812-921; 096-918-336-989-978; 099-451-642-714-334; 104-208-613-793-423; 109-260-833-365-957; 143-294-819-806-400; 194-481-922-219-128; 197-989-903-407-891,0,false,,
142-986-171-231-405,ICARCV - A vision-based system supports mapping services for visually impaired people in indoor environments,,2014,conference proceedings article,2014 13th International Conference on Control Automation Robotics & Vision (ICARCV),,IEEE,,Quoc-Hung Nguyen; Hai Vu; Thanh-Hai Tran; Quang-Hoan Nguyen,"This paper describes and extensively evaluates a visual-based system that autonomously operators for both building a map and localization tasks. The proposed system is to assist mapping services to the visually impaired/blind people in small or mid-scale environments such as inside a building or campus of school, hospital. Toward this end, the proposed approaches solely rely on visual data thanks to a self-designed image acquisition system. On one hand, a robust visual odometry method is utilized to create a map of the environments. On the other hand, the proposed approaches utilize FAB-MAP algorithm that is maybe the most successful for learning places in the environments. Map building and learning places in an environment are processed in an off-line phase. Through a matching place procedure, online captured images are continuously positioned on the map. Furthermore, we utilize a Kaiman Filter that combines the matching results of current observation and the estimation of robot states based on its kinematic model. We evaluate performances of the proposed system through experimental schemes. The results show that the constructed map coincides with ground truth, and matching image-to-map is high confidence. The evaluations also contain scenarios which the blind pupils move following Robot. The experimental results confirmed that proposed system feasibly navigating blind pupils in indoor environments.",,,1518,1523,Ground truth; Artificial intelligence; Kalman filter; Matching (statistics); Visually impaired; Computer vision; Visualization; Kinematics; Computer science; Visual odometry; Robot,,,,,https://dblp.uni-trier.de/db/conf/icarcv/icarcv2014.html#NguyenVTN14 https://ieeexplore.ieee.org/document/7064541/ http://ieeexplore.ieee.org/document/7064541/ https://doi.org/10.1109/ICARCV.2014.7064541,http://dx.doi.org/10.1109/icarcv.2014.7064541,,10.1109/icarcv.2014.7064541,2003243585,,0,002-580-724-068-489; 012-967-237-747-061; 013-056-435-015-274; 014-279-588-505-48X; 023-960-131-541-476; 031-763-353-100-817; 051-766-223-654-722; 061-190-872-413-287; 065-664-501-877-944; 076-518-832-389-690; 085-095-942-302-490; 085-507-633-373-250; 109-930-243-130-269; 115-246-676-960-424; 123-863-288-894-65X; 160-234-441-041-937; 169-421-672-075-385; 170-387-846-090-147,3,false,,
143-038-799-416-022,"Assistive technology-based solutions in learning mathematics for visually-impaired people: exploring issues, challenges and opportunities.",2023-10-25,2023,journal article,Multimedia tools and applications,13807501; 15737721,Springer Science and Business Media LLC,Netherlands,Muhammad Shoaib; Donal Fitzpatrick; Ian Pitt,"In the absence of vision, visually impaired and blind people rely upon the tactile sense and hearing to obtain information about their surrounding environment. These senses cannot fully compensate for the absence of vision, so visually impaired and blind people experience difficulty with many tasks, including learning. This is particularly true of mathematical learning. Nowadays, technology provides many effective and affordable solutions to help visually impaired and blind people acquire mathematical skills. This paper is based upon a systematic review of technology-based mathematical learning solutions for visually impaired people and discusses the findings and objectives for technological improvements. It analyses the issues, challenges and limitations of existing techniques. We note that audio feedback, tactile displays, a supportive academic environment, digital textbooks and other forms of accessible math applications improve the quality of learning mathematics in visually impaired and blind people. Based on these findings, it is suggested that smartphone-based solutions could be more convenient and affordable than desktop/laptop-based solutions as a means to enhance mathematical learning. Additionally, future research directions are discussed, which may assist researchers to propose further solutions that will improve the quality of life for visually impaired and blind people.",82,29,46153,46184,Visually impaired; Laptop; Computer science; Quality (philosophy); Human–computer interaction; Hearing impaired; Assistive technology; Multimedia; Artificial intelligence; Audiology; Operating system; Medicine; Philosophy; Epistemology,Accessibility; Assistive Technology; Education; Learning; Mathematics; Visually Impaired,,,Science Foundation Ireland,https://link.springer.com/content/pdf/10.1007/s11042-023-17409-z.pdf https://doi.org/10.1007/s11042-023-17409-z,http://dx.doi.org/10.1007/s11042-023-17409-z,38037570,10.1007/s11042-023-17409-z,,PMC10684398,0,001-500-232-053-716; 002-391-643-288-41X; 002-682-563-503-44X; 003-179-052-061-924; 005-096-053-827-249; 005-461-695-502-801; 005-490-013-035-381; 006-672-922-263-393; 008-915-556-943-300; 010-566-337-889-630; 014-727-945-688-998; 015-101-983-521-584; 015-890-463-931-998; 017-666-338-446-287; 018-442-894-487-788; 023-301-723-911-590; 027-652-211-846-887; 028-011-973-323-309; 029-105-781-915-872; 030-090-261-408-54X; 032-156-951-550-625; 035-227-123-340-790; 035-733-335-128-371; 039-180-277-011-72X; 039-983-114-810-657; 047-429-720-895-760; 057-885-707-226-089; 058-394-325-763-583; 061-321-160-752-499; 061-715-593-814-992; 081-132-952-338-846; 081-810-550-806-897; 084-934-084-521-74X; 089-307-395-706-521; 091-214-707-212-379; 096-457-034-087-048; 098-488-586-093-176; 099-503-531-539-969; 108-184-865-345-936; 119-967-251-516-956; 135-245-985-390-403; 137-943-403-445-787; 141-489-442-115-69X; 147-317-681-938-156; 153-684-502-640-875; 156-196-356-928-569; 165-431-480-577-376; 179-220-735-699-921; 180-177-638-535-006,1,true,cc-by,hybrid
143-676-420-241-712,IJCNN - Assistive System for Navigating Complex Realistic Simulated World Using Reinforcement Learning,,2020,conference proceedings article,2020 International Joint Conference on Neural Networks (IJCNN),,IEEE,,Faruk Ahmed;  Mahmud; Mohammed Yeasin,Finding a free path without obstacles or situation that pose minimal risk is critical for safe navigation. People who are sighted and people who are blind or visually impaired require navigation safety while walking on a sidewalk. In this paper we develop assistive navigation on a sidewalk by integrating sensory inputs using reinforcement learning. We train the reinforcement model in a simulated robotic environment which is used to avoid sidewalk obstacles. A conversational agent is built by training with real conversation data. The reinforcement learning model along with a conversational agent improved the obstacle avoidance experience about 2.5% from the base case which is 78.75%.,,,1,8,Human–computer interaction; Dialog system; Conversation; Collision avoidance; Computer science; Obstacle avoidance; Reinforcement; Reinforcement learning,,,,,https://dblp.uni-trier.de/db/conf/ijcnn/ijcnn2020.html#AhmedMY20 https://doi.org/10.1109/IJCNN48605.2020.9207716,http://dx.doi.org/10.1109/ijcnn48605.2020.9207716,,10.1109/ijcnn48605.2020.9207716,3090152226,,0,004-269-574-716-057; 004-869-396-896-866; 007-024-005-789-989; 007-722-596-235-918; 010-905-536-422-252; 021-415-875-273-324; 024-057-556-391-907; 024-916-195-779-836; 028-115-943-729-849; 030-127-369-356-741; 032-538-345-541-781; 033-421-314-782-061; 036-779-046-415-85X; 046-083-595-644-433; 049-739-043-640-133; 051-766-223-654-722; 054-131-104-654-367; 057-608-858-853-826; 061-294-318-497-272; 072-064-377-282-154; 073-928-284-197-323; 089-595-295-429-087; 092-150-716-488-939; 108-234-112-296-175; 109-627-172-037-435; 115-519-719-811-818; 115-535-299-135-963; 125-318-712-417-262; 131-731-817-529-056; 139-114-429-196-103; 154-758-907-139-558; 174-411-075-984-590,1,false,,
143-677-242-498-818,Editorial,2023-08-23,2023,journal article,International Journal of Information Technology,25112104; 25112112,Springer Science and Business Media LLC,,M. N. Hoda,,15,6,2859,2862,Computer science,,,,,https://link.springer.com/content/pdf/10.1007/s41870-023-01413-5.pdf https://doi.org/10.1007/s41870-023-01413-5,http://dx.doi.org/10.1007/s41870-023-01413-5,,10.1007/s41870-023-01413-5,,,0,,0,true,,bronze
144-123-956-963-592,Deep Learning and Fuzzy Decision Support System for visually impaired persons,2022-03-25,2022,conference proceedings article,2022 8th International Conference on Advanced Computing and Communication Systems (ICACCS),,IEEE,,Malini. P; Ushus.S. Kumar; Vijayakumari. G; Thirukkumaran. R; Lakshmi S Hanne; Karpakam. S,"This paper presents the concept of wearable guidance device using deep learning to support blind or visually impaired people. Using suggested wearable technology, this effort seeks to provide supplemental support to them with more flexible movements. The large volume of device data also assures that its users are safer. The suggested device employs an RGB camera to transform RGB photos and compute a level surface for recognizing obstacles and safe walking paths, deep learning is used. The proposed navigation system takes sensor data as input and gives the BVIP the necessary safety orientation. To assist BVIP in choosing a safe course, a fuzzy logic-based decision support system was built. The method has been tested on blinded and visually challenged individuals. Both categories of users thought the framework was capable and saw it as having the possibility as a useful navigation tool in future.",,,,,SAFER; Computer science; Wearable computer; Visually impaired; Fuzzy logic; Artificial intelligence; Human–computer interaction; Orientation (vector space); RGB color model; Wearable technology; Computer vision; Deep learning; Global Positioning System; Embedded system; Computer security; Geometry; Mathematics; Telecommunications,,,,,,http://dx.doi.org/10.1109/icaccs54159.2022.9785322,,10.1109/icaccs54159.2022.9785322,,,0,004-841-247-614-218; 013-501-956-117-041; 017-499-516-520-553; 040-057-279-881-544; 040-351-685-890-777; 044-469-609-982-070; 062-822-666-607-303; 069-013-773-466-651; 077-378-872-339-772; 087-496-050-387-679; 097-758-575-667-809; 114-941-093-799-753,2,false,,
144-532-453-832-301,"Assistive Technology and Artificial Intelligence: Applications in Robotics, User Interfaces and Natural Language Processing",1998-07-15,1998,book,,,,,Vibhu O. Mittal; Holly A. Yanco; John M. Aronis; Richard C. Simpson,Interface and language issues in intelligent systems for people with disabilities.- Iconic language design for people with significant speech and multiple impairments.- Lexicon for computer translation of American sign language.- On Building Intelligence into EagleEyes.- Providing intelligent language feedback for augmentative communication users.- Saliency in human-computer interaction.- A wearable computer based American sign language recognizer.- Towards automatic translation from Japanese into Japanese sign language.- An augmentative communication interface based on conversational schemata.- Assistive robotics: An overview.- Progress on the deictically controlled wheelchair.- Developing intelligent wheelchairs for the handicapped.- Integrating vision and spatial reasoning for assistive navigation.- Speech and gesture mediated intelligent teleoperation.- Personal adaptive mobility aid for the infirm and elderly blind.- HITOMI: Design and development of a Robotic Travel Aid.- NavChair: An assistive wheelchair navigation system with automatic adaptation.- Wheelesley: A robotic wheelchair system: Indoor navigation and user interface.,,,,,Natural language user interface; Gesture; American Sign Language; Japanese Sign Language; Mobility aid; Computer science; Multimedia; Intelligent decision support system; Adaptation (computer science); User interface,,,,,https://www.amazon.com/Assistive-Technology-Artificial-Intelligence-Applications/dp/3540647902,https://www.amazon.com/Assistive-Technology-Artificial-Intelligence-Applications/dp/3540647902,,,1553990100,,0,,11,false,,
144-732-801-553-701,EMBC - Smart image processing system for retinal prosthesis,,2012,clinical trial,Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference,26940604; 23757477,,United States,James D. Weiland; Neha Jagdish Parikh; Vivek Pradeep; Gerard Medioni,"Retinal prostheses for the blind have demonstrated the ability to provide the sensation of light in otherwise blind individuals. However, visual task performance in these patients remains poor relative to someone with normal vision. Computer vision algorithms for navigation and object detection were evaluated for their ability to improve task performance. Blind subjects navigating a mobility course had fewer collisions when using a wearable camera system that guided them on a safe path. Subjects using a retinal prosthesis simulator could locate objects more quickly when an object detection algorithm assisted them. Computer vision algorithms can assist retinal prosthesis patients and low-vision patients in general.",2012,,300,303,Sensation; Wearable computer; Artificial intelligence; Object detection; Task (project management); Retinal; Retinal Prosthesis; Computer vision; Computer science; Image processing,,"Algorithms; Blindness; Eye, Artificial; Female; Humans; Image Processing, Computer-Assisted/instrumentation; Male; Retina; Task Performance and Analysis; Visual Prosthesis",,,https://ieeexplore.ieee.org/document/6345928/ https://www.ncbi.nlm.nih.gov/pubmed/23365889 https://dblp.uni-trier.de/db/conf/embc/embc2012.html#WeilandPPM12 http://ieeexplore.ieee.org/document/6345928/ http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.ieee-000006345928,http://dx.doi.org/10.1109/embc.2012.6345928,23365889,10.1109/embc.2012.6345928,2084737289,,0,011-576-484-103-373; 012-647-276-763-408; 021-724-360-774-137; 047-137-534-686-864; 051-472-067-465-255; 059-407-478-677-340; 060-614-968-414-313; 115-315-649-469-222,15,false,,
144-888-824-530-300,Empowering Individuals with Visual Impairments: A Deep Learning-Based Smartphone Navigation Assistant,2023-09-18,2023,book chapter,Proceedings of the 9th International Conference on Advanced Intelligent Systems and Informatics 2023,23674512; 23674520,Springer Nature Switzerland,,Fatema A. Shawki; Mariem Mahfouz; Mohamed A. Abdelrazek; Gehad Ismail Sayed,"Long white canes and other technological aids are frequently used by visually impaired people to identify and avoid obstacles ahead and hazards. Due to their general knowledge of everything’s location, visually impaired individuals can move freely in their homes without much assistance. However, they encounter more challenges and risk harm when they roam the streets. To help visually impaired individuals navigate the streets independently and safely, this paper proposes a deep learning-based smartphone navigation assistant system. The backend and frontend are the two main components. On the front end, the images are captured by utilizing the mobile camera. The backend is fed with these captured images. A You Only Look Once (YOLOv8) deep learning architecture is used in the backend, followed by a rule-based model. Finally, a set of pre-recorded audio messages that contain navigational guidance is returned to the user. The deep-learning architecture is trained and fine-tuned on a dataset gathered from five different sources. The experimental results showed that the proposed model can be effectively used to help people who are blind. Additionally, the outcomes demonstrated that YOLOv8 achieved the best outcomes when compared to other deep-learning architectures. The proposed system achieved a 97% overall accuracy.",,,19,30,Deep learning; Artificial intelligence; Computer science; Architecture; Set (abstract data type); Harm; Human–computer interaction; Mobile device; Visually impaired; Computer vision; Multimedia; World Wide Web; Psychology; Geography; Social psychology; Archaeology; Programming language,,,,,,http://dx.doi.org/10.1007/978-3-031-43247-7_2,,10.1007/978-3-031-43247-7_2,,,0,004-216-069-855-831; 009-864-381-071-748; 015-040-823-132-436; 016-572-318-414-022; 023-309-209-821-636; 025-522-385-395-133; 030-673-322-355-563; 033-995-544-517-274; 038-167-227-974-410; 048-939-303-075-071; 049-892-685-028-772; 055-858-990-745-91X; 064-140-744-806-266; 078-265-637-126-668; 120-973-521-906-206; 134-048-572-801-690; 137-973-182-180-54X; 141-649-000-612-322,0,false,,
145-055-846-543-807,A Hybrid Technique to Re-establish Squat Contrast Underwater Images Centred on Principal Component Analysis,2023-09-23,2023,journal article,SN Computer Science,26618907; 2662995x,Springer Science and Business Media LLC,,Aman Mittal,,4,6,,,Artificial intelligence; Principal component analysis; Computer science; Contrast (vision); Computer vision; Pattern recognition (psychology); Underwater; Convolutional neural network; Histogram; Image (mathematics); Geography; Archaeology,,,,,,http://dx.doi.org/10.1007/s42979-023-02191-4,,10.1007/s42979-023-02191-4,,,0,001-821-224-138-901; 003-059-615-532-077; 005-639-880-163-00X; 007-392-942-838-545; 021-575-361-340-510; 022-221-319-934-698; 026-758-878-149-325; 034-863-345-818-375; 047-114-997-317-81X; 065-573-446-201-023; 088-300-199-987-444; 096-258-566-517-123; 097-008-761-929-056; 119-930-946-990-073; 139-509-042-364-471; 160-505-007-090-009; 172-564-363-978-229,0,false,,
146-608-577-252-748,Meta smart glasses-large language models and the future for assistive glasses for individuals with vision impairments.,2023-12-04,2023,journal article,"Eye (London, England)",14765454; 0950222x,Nature Publishing Group,United Kingdom,Ethan Waisberg; Joshua Ong; Mouayad Masalkhi; Nasif Zaman; Prithul Sarker; Andrew G Lee; Alireza Tavakkoli,,38,6,1036,1038,Neuro-ophthalmology; Assistive technology; Optometry; Computer science; Medicine; Physical medicine and rehabilitation; Psychology; Human–computer interaction; Glaucoma; Ophthalmology,,Humans; Smart Glasses; Eyeglasses,,,https://www.nature.com/articles/s41433-023-02842-z.pdf https://doi.org/10.1038/s41433-023-02842-z,http://dx.doi.org/10.1038/s41433-023-02842-z,38049627,10.1038/s41433-023-02842-z,,PMC11009354,0,044-095-408-216-835; 080-139-047-616-629; 133-483-746-255-125; 140-054-836-346-846; 159-301-608-210-846; 167-252-793-890-442; 187-126-033-783-335; 188-889-374-702-179,7,true,cc-by,hybrid
147-361-488-227-422,Walk Assistance For Outwardly Challenged People,2021-08-01,2021,journal article,Journal of Physics: Conference Series,17426588; 17426596,IOP Publishing,United Kingdom,K. Rajkumar; K. Thejaswini; S. Subarna; P. Yuvashri,"<jats:title>Abstract</jats:title>;                <jats:p>The blind’s competency is to navigate to a particular place and to commence their daily activities is of decisive importance for their prosperity. It is estimated about one billion people are blind out of 285 million people visually impaired of all ages, according to the statistics of World Health Organization. This work includes an affordable and more efficient navigation aid for the blind which provides artificial vision by providing knowledge about an environmental scenario of static and dynamic characteristics of objects around them. This system induces a smart cane with ultrasonic sensors placed to intimate the intermediaries to their acknowledgment through Bluetooth. ZIGBEE used in this project so as to met the sensor and control device communication standards for navigation for the blind with the smart cane via Google maps their destination.</jats:p>",1979,1,012065,,Arduino microcontroller; Navigation aid; Computer science; Computer hardware,,,,,https://ui.adsabs.harvard.edu/abs/2021JPhCS1979a2065R/abstract,http://dx.doi.org/10.1088/1742-6596/1979/1/012065,,10.1088/1742-6596/1979/1/012065,3189372410,,0,073-593-742-123-291,0,true,cc-by,gold
147-854-960-761-078,Multiclass multilabel ophthalmological fundus image classification based on optimised deep feature space evolutionary model,2023-11-02,2023,journal article,Multimedia Tools and Applications,15737721; 13807501,Springer Science and Business Media LLC,Netherlands,Akanksha Bali; Vibhakar Mansotra,,83,16,49813,49843,Computer science; Artificial intelligence; Feature selection; Deep learning; Machine learning; Heuristic; Beehive; Fitness function; Pattern recognition (psychology); Genetic algorithm; Botany; Biology,,,,,,http://dx.doi.org/10.1007/s11042-023-17530-z,,10.1007/s11042-023-17530-z,,,0,002-130-520-878-692; 003-997-816-725-036; 005-350-127-982-518; 005-871-483-899-036; 009-400-516-237-94X; 012-750-003-896-788; 018-950-030-320-551; 023-395-229-984-604; 023-703-365-890-372; 023-806-178-022-408; 033-299-961-041-933; 037-331-362-792-316; 037-580-647-855-728; 037-745-221-573-451; 048-677-709-813-751; 048-847-665-048-582; 050-320-887-456-697; 052-165-685-044-454; 056-929-916-831-811; 057-238-334-333-453; 058-260-985-486-864; 067-037-422-630-257; 071-396-811-405-327; 075-710-804-025-72X; 077-647-308-290-528; 080-651-026-571-92X; 084-386-909-175-710; 090-161-441-117-266; 093-190-397-270-766; 093-311-354-777-450; 098-631-679-043-88X; 100-137-471-286-934; 104-880-760-654-066; 107-441-124-567-660; 107-526-528-684-292; 108-603-812-108-826; 110-925-375-462-387; 119-111-107-306-067; 119-294-929-096-913; 119-640-311-749-360; 134-546-485-378-400; 139-751-505-746-667; 147-450-256-726-163; 151-881-050-394-705; 156-208-293-104-479; 158-622-733-499-264; 188-426-330-073-429,1,false,,
147-898-042-304-243,An automated navigation system for blind people,2022-02-01,2022,journal article,Bulletin of Electrical Engineering and Informatics,23029285; 20893191,Institute of Advanced Engineering and Science,,Md. Atiqur Rahman; Sadia Siddika; Md. Abdullah Al-Baky; Md. Jueal Mia,"<jats:p>Proper navigation and detailed perception in familiar or unfamiliar environments are the main roles for human life. Eyesight sense helps humans to abstain from all kinds of dangers and navigate to indoor and outdoor environments. These are challenging activities for blind people in all environments. Many assistive tools have been developed by the blessing of technology like braille compasses and white canes that help them to navigate around in the environment. A vision and cloud-based navigation system for the visually impaired or blind person was developed. Our aim was not only to navigate them but also to perceive the environment in as much detail as a normal person. The proposed system includes ultrasonic sensors detecting obstacles, stereo camera to capture videos to perceive the environment using deep learning algorithms. Face recognition approach identified known faces in front of him. Blind people interacted with the whole system through a speech recognition module and all the information was stored in the cloud. Web and android applications were developed to track blinds so that guardians were monitoring them while visiting and reached them in an emergency. The experimental results showed the proposed system could provide more plenty information and user-friendly interaction.</jats:p>",11,1,201,212,Human–computer interaction; Computer science; Android (operating system); Cloud computing; Artificial intelligence; Perception; Computer vision; Multimedia; Operating system; Biology; Neuroscience,,,,,https://beei.org/index.php/EEI/article/download/3452/2488 https://doi.org/10.11591/eei.v11i1.3452 https://zenodo.org/records/6650828/files/23%203452.pdf https://zenodo.org/record/6650828,http://dx.doi.org/10.11591/eei.v11i1.3452,,10.11591/eei.v11i1.3452,,,0,,4,true,cc-by-sa,gold
148-359-676-287-872,Accessibility landmarks identification in web applications based on DOM elements classification,2022-12-13,2022,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Willian M. Watanabe; Guilherme de Lemos; Rene Willian Nascimento,,23,2,765,777,Computer science; Landmark; Classifier (UML); The Internet; Identification (biology); Web mapping; Artificial intelligence; Web page; Information retrieval; World Wide Web; Web mining; Web application; Web navigation; Data mining; Machine learning; Botany; Biology,,,,,,http://dx.doi.org/10.1007/s10209-022-00959-8,,10.1007/s10209-022-00959-8,,,0,000-324-616-345-738; 004-930-618-968-061; 006-987-944-559-327; 013-055-034-563-398; 016-369-149-812-148; 017-345-885-993-100; 020-277-180-682-202; 020-787-788-259-242; 030-619-689-003-057; 045-817-155-398-869; 046-763-615-566-68X; 050-702-474-319-647; 055-609-488-326-982; 062-791-837-732-485; 071-666-521-667-895; 077-666-882-501-127; 078-728-519-543-522; 083-160-952-876-311; 084-379-745-989-041; 087-186-690-000-838; 098-628-415-652-65X; 101-088-622-417-54X; 101-487-221-034-343; 123-498-859-171-509; 131-231-561-402-850; 132-443-608-621-366; 140-021-566-419-070; 141-763-172-737-77X; 149-556-582-870-978; 149-585-331-830-739,1,false,,
148-798-784-250-879,Hexapod Adaptive Gait Inspired by Human Behavior for Six-Legged Robot Without Force Sensor.,2017-04-02,2017,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Yilin Xu; Feng Gao; Yang Pan; Xun Chai,,88,1,19,35,Engineering; Hexapod; Gait; Payload; Artificial intelligence; Control system; Obstacle; Legged robot; Computer vision; Simulation; Robot control; Robot,,,,National Basic Research Program of China,https://dblp.uni-trier.de/db/journals/jirs/jirs88.html#XuGPC17 https://dialnet.unirioja.es/servlet/articulo?codigo=6146262 https://link.springer.com/article/10.1007/s10846-017-0532-7,http://dx.doi.org/10.1007/s10846-017-0532-7,,10.1007/s10846-017-0532-7,2599296684,,0,002-034-848-921-164; 003-229-482-095-780; 005-157-667-550-025; 006-652-401-088-069; 007-438-228-476-146; 012-993-966-635-878; 015-522-551-499-219; 020-977-059-201-101; 021-648-093-046-829; 021-758-613-738-771; 024-501-559-313-873; 026-431-264-445-416; 026-457-133-931-428; 044-241-936-575-48X; 051-621-112-761-165; 065-372-038-329-190; 071-047-602-279-947; 072-213-315-255-296; 075-314-846-743-450; 084-861-597-737-496; 091-572-771-116-330; 092-916-006-153-735; 097-164-564-068-829; 112-752-735-498-835; 150-545-752-417-757; 154-196-279-377-851; 160-396-861-281-146; 184-891-137-016-105; 186-027-171-531-62X,13,false,,
148-804-350-664-466,Electronic Travel Aid for Crosswalk Detection for Visually Challenged People,2023-01-01,2023,book chapter,Lecture Notes in Electrical Engineering,18761100; 18761119,Springer Nature Singapore,Germany,Shripad Bhatlawande; Neel Gokhale; Dewang V. Mehta; Parag Gaikwad; Swati Shilaskar; Jyoti Madake,"Vision is one of the most important sensory systems. It helps us to sense and gauge the surroundings around us. It also helps us to see colors as well as to assess any dangers in our path. Therefore, the proposed system is aimed to aid people who are visually impaired or blind. These people face the most difficulties when they are commuting alone. Independent movement in an unknown environment is strenuous for these people. They need to be always accompanied by someone, especially when crossing roads. The proposed system can be used when crossing the roads to detect a zebra crossing. Although there are a few solutions available today, those are not easy to use. Achieving ease of use with a cheaper cost is the author’s aim in building this system. Computer vision and machine learning models have been used to find a solution to this problem. The authors have prepared a dataset of 5510 images. The system uses SIFT descriptors for feature extraction. K-means and PCA for dimensionality reduction are used. Five ML classifiers are trained, and the performance is assessed. The highest accuracy was achieved by the Random Forest Classifier, which was 89%.",,,249,259,Schema crosswalk; Computer science; Random forest; Artificial intelligence; Computer vision; Classifier (UML); Feature extraction; Scale-invariant feature transform; Dimensionality reduction; Machine learning; Human–computer interaction; Engineering; Transport engineering; Pedestrian,,,,,,http://dx.doi.org/10.1007/978-981-19-6581-4_20,,10.1007/978-981-19-6581-4_20,,,0,000-937-332-306-827; 001-315-674-035-879; 004-306-253-497-690; 017-571-875-650-919; 020-638-834-068-203; 023-901-299-485-228; 027-893-571-523-130; 048-846-148-755-225; 049-592-217-207-783; 052-736-396-283-869; 053-489-205-336-851; 055-495-282-581-758; 066-570-259-227-917; 075-519-293-886-898; 107-400-877-992-302; 112-241-302-532-386; 123-863-288-894-65X,0,false,,
148-978-504-138-62X,Computer Vision based Tools and Technologies for Navigational Assistance for the Visually Impaired,2022-11-24,2022,conference proceedings article,2022 International Conference on Augmented Intelligence and Sustainable Systems (ICAISS),,IEEE,,Shravya Dasu; Manali M. Ranade; Shravasti Sarkar; Anala M R,"One of the most pressing challenges concerning visual impairment is navigation in complex real-life environments. Blind people often use guide dogs which are expensive, canes, or other people to find their way around. Over the years, various tools have been developed to make the process of way-finding easier for the visually impaired, based on numerous technologies, including computer vision, SLAM algorithms and embedded systems. In the following paper, a number of the most effective tools and their corresponding mechanisms are discussed. These have been divided into software and hardware-oriented tools to provide the reader with better clarity. Furthermore, an analysis of the effectiveness of these alternative methods has been done, along with possible solutions to any shortcomings posed by these tools. The review concludes with a discussion about the different aspects to keep in mind while building navigational tools to assist the visually impaired.",,,,,Visually impaired; CLARITY; Computer science; Process (computing); Human–computer interaction; Software; Visual impairment; Artificial intelligence; Multimedia; Computer vision; Psychology; Biochemistry; Chemistry; Psychiatry; Programming language; Operating system,,,,,,http://dx.doi.org/10.1109/icaiss55157.2022.10010946,,10.1109/icaiss55157.2022.10010946,,,0,003-086-871-090-793; 006-409-052-293-645; 006-917-922-806-186; 013-672-230-759-113; 017-739-144-960-987; 030-127-369-356-741; 032-661-597-680-28X; 037-491-194-003-965; 041-142-363-026-244; 048-510-019-754-753; 049-166-030-940-238; 053-567-501-093-090; 055-495-282-581-758; 056-332-008-726-803; 060-135-893-855-211; 064-521-070-547-235; 065-719-616-117-529; 066-037-919-140-345; 066-246-145-459-483; 066-622-591-843-254; 067-936-687-916-954; 069-231-188-624-028; 080-148-076-219-168; 096-803-870-115-728; 109-525-288-959-67X; 117-188-991-441-413; 132-715-593-157-754; 142-554-745-154-754; 163-631-115-293-424,0,false,,
149-121-921-007-157,LookTell: A Deep Learning Based Smart Assistant for Visually Impaired,2023-10-06,2023,conference proceedings article,2023 International Conference on Advanced Computing Technologies and Applications (ICACTA),,IEEE,,Rucha Pingale; Prachi Sharma; Jyoti Kundale; Shreyash Padge; Samruddhi Patil,"Around 14 million people in our country are visually impaired, facing difficulties when travelling and requiring assistance. Family and friends often worry about their whereabouts, while emergencies can be challenging. Identifying currency and using online banking can also be problematic, leaving them vulnerable to fraud. To address this, we have developed a Currency Recognition System (CRS) using a Teachable Machine with smart Image-to-Text conversion. This enables blind individuals to recognize bank notes, while an android-based location tracking system helps loved ones monitor their real-time location. Our system uses transfer learning to provide fast and accurate results, making life easier for the visually impaired. The accuracy of proposed system is 97.33%.",,,,,Visually impaired; Computer science; Android (operating system); Location tracking; Transfer of learning; Currency; Android application; Artificial intelligence; Deep learning; Human–computer interaction; Computer security; Real-time computing; Operating system; Monetary economics; Economics,,,,,,http://dx.doi.org/10.1109/icacta58201.2023.10392849,,10.1109/icacta58201.2023.10392849,,,0,016-995-808-167-399; 019-128-498-646-397; 025-259-830-428-648; 035-418-097-282-604; 038-252-894-265-753; 048-497-739-974-304; 069-592-523-557-330; 081-224-380-937-728; 092-060-815-275-796; 127-115-170-544-365; 128-310-497-581-520; 149-273-136-335-633,0,false,,
149-162-653-144-382,Object Recognition and Classification System for Visually Impaired,,2020,conference proceedings article,2020 International Conference on Communication and Signal Processing (ICCSP),,IEEE,,Rashika Joshi; Meenakshi Tripathi; Amit Kumar; Manoj Singh Gaur,"There have been diverse accounts of research relating to navigation assistance for the visually impaired. Using the object recognition approach, we aim to assist the blind to travel independently with the ability to identify any obstacles the the path. The subject now is to identify a solution that is low powered, easily portable and still effective. The system consists of Jetson Nano ported with the trained deep learning model and is interfaced with camera, that acts as a easy-to-use platform for object recognition, speech processing and image classification. Tests indicate that system is accurate and functions as navigation assistant.",,,1568,1572,Porting; Deep learning; Artificial intelligence; PATH (variable); Navigation assistance; Visually impaired; Computer vision; Computer science; Speech processing; Cognitive neuroscience of visual object recognition; Contextual image classification,,,,,https://ieeexplore.ieee.org/abstract/document/9182077/metrics,http://dx.doi.org/10.1109/iccsp48568.2020.9182077,,10.1109/iccsp48568.2020.9182077,3082682760,,0,031-218-334-653-826; 049-317-239-158-314; 051-843-465-255-165; 052-575-832-686-740; 066-133-135-091-645; 071-858-943-169-534; 092-726-040-947-902; 100-616-859-032-349; 108-107-790-423-506; 125-112-083-666-544; 146-093-154-688-224; 171-091-935-415-285,12,false,,
149-269-127-172-025,17th Annual Conference of the International Society for Computer Aided Surgery,2013-05-15,2013,journal article,International Journal of Computer Assisted Radiology and Surgery,18616410; 18616429,Springer Science and Business Media LLC,Germany,,,8,S1,337,395,,,,,,https://riunet.upv.es/bitstream/10251/38359/1/PO-13-000226_Monserrat_Carlos_inicial.pdf,http://dx.doi.org/10.1007/s11548-013-0881-z,,10.1007/s11548-013-0881-z,,,0,,0,true,cc-by-nc-nd,green
149-448-951-837-674,Systematic Review of Retinal Blood Vessels Segmentation Based on AI-driven Technique.,2024-03-04,2024,journal article,Journal of imaging informatics in medicine,29482933; 29482925,Springer Science and Business Media LLC,Switzerland,Prem Kumari Verma; Jagdeep Kaur,"Image segmentation is a crucial task in computer vision and image processing, with numerous segmentation algorithms being found in the literature. It has important applications in scene understanding, medical image analysis, robotic perception, video surveillance, augmented reality, image compression, among others. In light of this, the widespread popularity of deep learning (DL) and machine learning has inspired the creation of fresh methods for segmenting images using DL and ML models respectively. We offer a thorough analysis of this recent literature, encompassing the range of ground-breaking initiatives in semantic and instance segmentation, including convolutional pixel-labeling networks, encoder-decoder architectures, multi-scale and pyramid-based methods, recurrent networks, visual attention models, and generative models in adversarial settings. We study the connections, benefits, and importance of various DL- and ML-based segmentation models; look at the most popular datasets; and evaluate results in this Literature.",37,4,1783,1799,,Deep learning; Machine learning; Retinal image segmentation,"Humans; Retinal Vessels/diagnostic imaging; Deep Learning; Image Processing, Computer-Assisted/methods; Algorithms; Machine Learning; Neural Networks, Computer",,,,http://dx.doi.org/10.1007/s10278-024-01010-3,38438695,10.1007/s10278-024-01010-3,,PMC11300804,0,000-062-771-782-201; 001-641-685-480-55X; 001-657-125-236-23X; 001-798-129-739-394; 002-611-673-379-162; 004-115-000-019-103; 005-378-532-295-660; 005-445-442-095-945; 006-180-155-589-024; 007-074-026-760-953; 008-812-068-625-790; 010-575-443-285-659; 011-158-238-413-702; 013-575-368-789-61X; 018-496-571-143-757; 018-848-743-751-780; 022-060-759-734-716; 022-638-730-347-657; 022-746-640-867-518; 025-303-084-594-198; 025-480-719-982-686; 025-539-484-164-26X; 026-447-339-088-262; 026-889-381-684-912; 027-144-128-030-595; 030-341-724-954-269; 033-521-340-686-637; 033-836-521-985-472; 033-950-689-189-970; 034-678-348-744-063; 036-011-140-293-897; 038-094-347-842-733; 038-376-152-323-082; 043-429-931-469-598; 043-781-914-033-55X; 044-148-413-170-137; 047-903-575-818-413; 049-439-011-342-487; 050-747-879-358-977; 051-608-571-504-225; 051-619-357-428-050; 054-444-394-443-441; 060-351-837-701-556; 060-402-942-224-955; 060-755-509-845-340; 061-347-046-951-983; 061-368-870-585-750; 064-552-548-741-144; 065-591-745-183-848; 065-893-572-505-899; 068-215-810-729-510; 069-454-798-784-430; 070-631-182-495-77X; 071-421-884-863-267; 073-000-326-858-68X; 074-097-650-866-235; 077-216-949-221-923; 080-491-095-050-01X; 084-102-246-282-904; 085-234-714-036-01X; 085-528-681-071-110; 086-906-472-434-239; 092-067-274-425-959; 093-050-509-043-003; 093-809-801-025-268; 094-862-330-121-139; 095-057-380-777-755; 095-708-166-219-361; 095-967-835-216-018; 098-666-413-641-700; 099-637-993-492-513; 101-247-028-208-361; 108-289-105-093-44X; 110-353-919-860-05X; 112-784-856-351-182; 119-957-206-553-273; 123-245-905-138-203; 125-313-332-270-788; 130-745-014-036-435; 135-792-036-216-836; 139-003-334-332-14X; 139-220-829-272-274; 147-592-292-133-959; 148-131-494-013-634; 152-411-478-590-90X; 158-582-193-795-463; 168-773-357-592-465; 177-752-873-450-379; 181-257-775-065-836; 184-446-291-593-247; 187-408-515-382-791; 189-116-152-572-200; 190-154-813-544-403,0,true,,unknown
149-679-295-863-636,Detecting Signage and Doors for Blind Navigation and Wayfinding.,2013-02-06,2013,journal article,Network modeling and analysis in health informatics and bioinformatics,21926662; 21926670,Springer Nature,United States,Shuihua Wang; Xiaodong Yang; Yingli Tian,"Signage plays a very important role to find destinations in applications of navigation and wayfinding. In this paper, we propose a novel framework to detect doors and signage to help blind people accessing unfamiliar indoor environments. In order to eliminate the interference information and improve the accuracy of signage detection, we first extract the attended areas using a saliency map. Then the signage is detected in the attended areas using a bipartite graph matching. The proposed method can handle multiple signage detection. Furthermore, in order to provide more information for blind users to access the area associated with the detected signage, we develop a robust method to detect doors based on a geometric door frame model which is independent to door appearances. Experimental results on our collected datasets of indoor signage and doors demonstrate the effectiveness and efficiency of our proposed method.",2,2,81,93,Human–computer interaction; Signage; Doors; Frame (networking); Saliency map; Bipartite graph matching; Computer science; Multimedia,,,,NEI NIH HHS (R21 EY020990) United States,https://paperity.org/p/8732659/detecting-signage-and-doors-for-blind-navigation-and-wayfinding https://link.springer.com/article/10.1007/s13721-013-0027-9 https://link.springer.com/article/10.1007/s13721-013-0027-9/fulltext.html https://dblp.uni-trier.de/db/journals/netmahib/netmahib2.html#WangYT13 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3728285/ https://link.springer.com/content/pdf/10.1007%2Fs13721-013-0027-9.pdf https://europepmc.org/article/MED/23914345,http://dx.doi.org/10.1007/s13721-013-0027-9,23914345,10.1007/s13721-013-0027-9,2016092425,PMC3728285,0,002-409-197-796-23X; 002-469-564-380-629; 005-861-492-610-995; 009-814-791-287-431; 012-016-836-079-639; 016-408-213-077-520; 017-299-300-756-579; 017-994-776-387-929; 020-422-635-395-84X; 020-897-417-219-820; 021-272-916-231-150; 024-267-008-954-380; 027-893-571-523-130; 034-119-578-405-701; 034-810-491-982-094; 039-123-654-734-384; 044-017-034-949-319; 045-188-716-294-113; 046-262-856-533-736; 046-684-991-081-664; 048-275-265-866-075; 051-970-317-843-151; 053-056-851-209-925; 054-374-371-905-358; 057-952-060-739-242; 058-074-142-866-309; 063-161-240-623-745; 069-468-812-769-147; 071-806-151-845-245; 072-128-239-615-811; 072-995-846-715-537; 089-550-333-226-805; 096-014-228-878-398; 098-418-441-499-649; 100-260-517-243-811; 100-824-461-562-135; 105-969-976-015-690; 106-677-330-222-49X; 108-396-749-438-718; 110-943-849-810-718; 111-164-185-434-767; 122-729-034-369-253; 154-664-177-679-925; 182-488-737-721-091; 189-691-486-505-033,11,true,,green
149-869-689-749-737,Advancing Accessibility: An Artificial Intelligence Framework for Obstacle Detection and Navigation Assistance for the Visually Impaired,2023-11-17,2023,journal article,E3S Web of Conferences,22671242; 25550403,EDP Sciences,,Liliek Triyono; Rahmat Gernowo; null Prayitno; Saifur Rohman Cholil; Idhawati Hestiningsih; null Wiktasari; Sirli Fahriah,"<jats:p>The white cane has long been a fundamental tool for individuals with visual impairments, aiding in surface detection and obstacle identification. However, its limitations in detecting moving objects and distant obstacles pose significant safety risks, particularly in congested areas and busy streets. While service animals offer an alternative, they come with training challenges and high costs. To address these limitations and enhance safety, this paper proposes a comprehensive collision detection and prevention system. The proposed system integrates cutting-edge technologies, including image processing, deep learning, Internet of Things (IoT), cloud computing, and audio production devices. By combining these technologies with the white cane, the system offers a sophisticated navigation option for the visually impaired, effectively detecting and preventing potential collisions. In busy environtment scenarios, the system proves its effectiveness by complementing the white cane's use, overcoming its inherent limitations, and significantly improving navigation capabilities. Through this innovative approach, blind individuals gain enhanced situational awareness, empowering them to navigate diverse environments with increased confidence and safety. By mitigating the drawbacks of the white cane, the proposed system provides a comprehensive and cost-effective solution to enhance the mobility and safety of the visually impaired. This research contributes to the advancement of assistive technologies, offering a valuable resource for researchers, policymakers, and practitioners in the field of accessibility and inclusive design.</jats:p>",448,,2042,02042,Obstacle; Situation awareness; Computer science; Cloud computing; Collision avoidance; Risk analysis (engineering); Human–computer interaction; Identification (biology); Computer security; Artificial intelligence; Engineering; Collision; Medicine; Botany; Aerospace engineering; Political science; Law; Biology; Operating system,,,,,,http://dx.doi.org/10.1051/e3sconf/202344802042,,10.1051/e3sconf/202344802042,,,0,002-973-601-349-78X; 003-203-548-870-60X; 003-659-774-237-701; 004-702-133-893-992; 004-772-554-982-380; 006-106-790-585-940; 007-530-851-453-706; 007-767-773-632-893; 014-153-330-736-922; 015-040-823-132-436; 016-988-681-008-984; 017-413-759-318-536; 020-260-934-897-437; 021-742-723-530-553; 022-622-218-704-364; 025-306-351-240-479; 026-994-702-158-53X; 027-862-565-254-999; 028-628-140-541-774; 034-266-206-428-718; 037-566-006-652-399; 039-274-384-165-289; 039-420-403-957-058; 045-309-399-228-849; 046-703-334-644-632; 048-254-076-918-785; 048-929-432-459-187; 049-317-239-158-314; 050-494-915-265-725; 056-198-211-304-564; 063-417-520-342-514; 068-728-469-039-615; 069-835-625-349-747; 070-606-003-984-347; 073-938-112-385-378; 075-156-018-690-305; 083-671-887-012-721; 112-566-962-059-235; 119-418-345-916-131; 129-110-449-573-518; 147-855-184-779-455; 160-234-441-041-937; 198-708-533-050-065,0,true,cc-by,gold
150-769-795-848-912,How do drivers mitigate the effects of naturalistic visual complexity? : On attentional strategies and their implications under a change blindness protocol.,2023-08-09,2023,journal article,Cognitive research: principles and implications,23657464,Springer Science and Business Media LLC,England,Vasiliki Kondyli; Mehul Bhatt; Daniel Levin; Jakob Suchan,"How do the limits of high-level visual processing affect human performance in naturalistic, dynamic settings of (multimodal) interaction where observers can draw on experience to strategically adapt attention to familiar forms of complexity? In this backdrop, we investigate change detection in a driving context to study attentional allocation aimed at overcoming environmental complexity and temporal load. Results indicate that visuospatial complexity substantially increases change blindness but also that participants effectively respond to this load by increasing their focus on safety-relevant events, by adjusting their driving, and by avoiding non-productive forms of attentional elaboration, thereby also controlling ""looked-but-failed-to-see"" errors. Furthermore, analyses of gaze patterns reveal that drivers occasionally, but effectively, limit attentional monitoring and lingering for irrelevant changes. Overall, the experimental outcomes reveal how drivers exhibit effective attentional compensation in highly complex situations. Our findings uncover implications for driving education and development of driving skill-testing methods, as well as for human-factors guided development of AI-based driving assistance systems.",8,1,54,,Cognitive psychology; Gaze; Affect (linguistics); Change blindness; Psychology; Context (archaeology); Change detection; Computer science; Artificial intelligence; Communication; Paleontology; Biology,Attentional strategies; Change blindness; Everyday driving; Naturalistic observation; Visual perception; Visuospatial complexity,Humans; Automobile Driving; Visual Perception; Educational Status,,Örebro University,https://cognitiveresearchjournal.springeropen.com/counter/pdf/10.1186/s41235-023-00501-1 https://doi.org/10.1186/s41235-023-00501-1 https://elib.dlr.de/196869/1/s41235-023-00501-1.pdf,http://dx.doi.org/10.1186/s41235-023-00501-1,37556047,10.1186/s41235-023-00501-1,,PMC10412523,0,000-146-825-886-531; 000-978-118-957-144; 002-047-094-889-366; 002-947-353-095-506; 003-675-351-155-368; 004-397-009-033-559; 005-151-854-755-651; 005-251-846-684-329; 005-276-361-899-349; 005-645-289-010-912; 005-825-587-976-06X; 007-112-517-128-068; 008-883-108-739-142; 009-604-068-586-795; 009-777-509-728-695; 009-962-238-587-011; 010-306-825-919-949; 010-899-114-514-896; 011-637-839-179-817; 011-784-199-720-838; 014-597-652-900-913; 014-812-903-443-673; 017-797-063-115-816; 017-882-058-058-622; 017-965-918-088-48X; 018-717-264-256-870; 019-103-611-757-814; 019-763-636-112-725; 019-897-703-729-295; 020-367-096-580-595; 021-170-778-285-785; 021-812-772-254-234; 022-610-082-964-619; 024-118-836-880-971; 025-419-559-554-945; 025-603-733-694-83X; 025-848-662-230-08X; 026-703-838-677-539; 026-775-807-907-525; 027-103-917-264-123; 028-764-985-552-738; 028-873-142-677-906; 030-695-928-218-042; 030-998-283-776-087; 032-192-840-428-14X; 032-451-469-982-51X; 032-483-888-480-705; 033-837-531-624-106; 036-158-211-062-559; 037-629-963-327-323; 037-822-751-563-804; 038-292-034-951-223; 038-535-835-270-78X; 040-935-145-873-641; 042-312-504-405-842; 045-475-002-814-342; 053-214-260-013-024; 054-951-556-649-079; 054-989-590-449-576; 056-165-658-654-762; 057-682-460-223-643; 059-284-776-202-274; 059-901-179-931-998; 060-617-283-468-064; 061-457-008-942-880; 062-553-055-423-115; 063-256-378-903-776; 064-225-452-895-052; 069-074-874-788-608; 069-463-187-969-393; 070-821-459-707-630; 075-437-057-464-811; 076-789-172-721-292; 076-908-526-122-051; 080-363-139-340-697; 082-198-530-973-223; 087-424-479-266-930; 087-684-140-057-399; 088-474-769-330-250; 090-947-026-738-238; 092-668-757-146-739; 092-954-758-387-152; 094-191-619-830-740; 097-194-379-619-059; 098-300-090-397-46X; 099-257-558-904-849; 101-751-943-609-217; 103-333-893-626-246; 104-561-800-676-473; 105-340-582-072-459; 105-815-422-763-695; 107-187-822-402-387; 109-887-967-308-447; 110-560-406-063-161; 111-050-799-585-708; 114-170-412-980-734; 118-074-659-437-771; 119-319-453-203-450; 125-967-461-778-003; 126-221-566-718-950; 133-347-210-328-553; 139-855-442-331-11X; 158-658-227-005-887; 159-668-646-545-277; 171-301-085-605-64X; 178-956-860-326-927; 184-801-662-188-966; 185-770-922-242-488,1,true,cc-by,gold
151-240-810-896-266,Dark Vision Assistive Application for Deaf-Blind People,2023-09-13,2023,conference proceedings article,2023 3rd International Conference on Computing and Information Technology (ICCIT),,IEEE,,Prithi Samuel; Rajesh Kumar Dhanaraj; Balamurugan Balusamy; Narmatha C,"For visually impaired, deaf-blind people, both communication and navigation are difficult activities and indoor navigation is becoming increasingly harder for them. It is even more difficult for deaf-blind people than it is for non-visually impaired people. People with visual impairments or deaf- blindness frequently rely on external support systems to make judgments, such as trained canines, humans, or specialized equipment. As a result, deaf-blind people are in desperate need of an assistive application that allows blind people to navigate and communicate freely without relying on others. The goal of the application is to create a more adaptable and cost-effective system for deaf-blind and visually impaired people. Through both outdoor and interior applications, as well as walking in even unfamiliar locations without relying on others.",,,,,Deaf blind; Visually impaired; Blindness; Assistive technology; Visual impairment; Disabled people; Computer science; Human–computer interaction; Psychology; Audiology; Applied psychology; Optometry; Medicine; Life style; Psychiatry,,,,,,http://dx.doi.org/10.1109/iccit58132.2023.10273935,,10.1109/iccit58132.2023.10273935,,,0,017-893-832-693-54X; 023-284-802-263-42X; 026-385-480-827-49X; 030-236-716-068-705; 040-138-275-174-097; 041-919-867-236-405; 063-138-022-994-913; 071-012-445-386-158; 089-249-926-805-148; 097-992-529-973-608; 114-280-765-917-623; 115-714-323-963-034; 137-973-182-180-54X; 143-724-370-377-091,0,false,,
151-295-136-828-154,ISSPA - On stereo processing procedure applied towards blind navigation aid - SVETA,,,conference proceedings article,"Proceedings of the Eighth International Symposium on Signal Processing and Its Applications, 2005.",,IEEE,,G. Balakrishnan; G. Sainarayanan; R. Nagarajan; S. Yaccob,"This paper presents a portable traveling support system using stereo image processing for the visually impaired people’s navigation. This system named as SVETA consists of a helmet molded with stereo cameras in the front, wearable computer over the top and stereo earphones. The two cameras capture the visual information infront of the blind user. The captured images are then processed using the proposed methodology in wearable computer. The methodology includes stereo image processing module to calculate distance through disparity. The information is conveyed to the blind through the set of earphones in terms of musical tones. Voices are also used to inform blind user when the obstacle is very close to him. Experimentations were conducted in both indoor and outdoor environment, and the results of this experiment verified the practicability of the newly developed system.",2,,567,570,Computer graphics (images); Wearable computer; Artificial intelligence; Set (abstract data type); Obstacle; Headphones; Computer vision; Stereo cameras; Computer science; Stereopsis; Musical tone; Image processing,,,,,https://dblp.uni-trier.de/db/conf/isspa/isspa2005.html#BalakrishnanSNY05 http://eprints.ums.edu.my/973/ http://dspace.unimap.edu.my:80/xmlui/handle/123456789/6901,http://dx.doi.org/10.1109/isspa.2005.1581001,,10.1109/isspa.2005.1581001,2131325927,,0,001-689-696-637-789; 009-142-401-900-87X; 013-847-555-426-159; 024-202-993-739-041; 058-942-174-334-646; 068-262-811-656-335; 077-975-988-687-715; 078-725-425-518-672; 086-671-517-817-417; 105-584-440-275-77X; 179-896-096-479-358; 192-600-440-415-958,4,false,,
151-392-932-734-76X,Review Paper on Smart Guidance Belt For Blinds,2023-05-31,2023,journal article,International Journal for Research in Applied Science and Engineering Technology,23219653,International Journal for Research in Applied Science and Engineering Technology (IJRASET),,Anant Parashar; Arnav Mishra; Ashish Chaudhary; Rajeshwari Bhatt,"<jats:p>Abstract: This project seeks to create a simplified version of a smart wearable belt that can assist blind people with navigation, using a sensible and compact design for the electronic equipment. The design includes an embedded system, and could potentially incorporate machine learning algorithms to detect potholes and other irregularities on the road. The goal is to develop a smart belt that can help blind people travel independently without the use of a cane. The Electronic Blind Mobility Aid is a technology that uses sensors put on a belt worn around the waist to allow blind persons to travel independently. This advanced technology can help many people with visual impairments to travel independently. Blind people often face obstacles that can make it difficult for them to travel alone, but a blind belt can extend their range of perception by serving as an obstacle detector. Visually impaired people often use their hearing to compensate for their reduced eyesight, and can recognize sound sources. This technology enables blind persons to travel independently and receive obstacle alerts via headphones and speakers using sensors installed on a belt that is worn around the waist. Blind people can utilise this approach every day to find potential hazards, roadblocks, and pathways. In the end, this technology will serve to increase the independence of blind individuals and give them more confidence when they travel. To notify users of potential dangers, the device has a buzzer that produces vibration signals and emits a warning sound. Both the sound frequency and the vibration frequency rise as the space between the elastic gloves and the barriers gets smaller. The design incorporates a buzzer and motor in addition to the capability to notify the proper person in case of an emergency.. The goal of this study is to provide a low-cost, effective means of helping blind individuals travel more easily, quickly, and confidently. This design provides a quick-response, cost-effective, and portable solution. This device can let blind persons traverse their environment more safely and confidently by employing sound and vibration cues to notify the user of impediments.</jats:p>",11,5,3723,3726,Buzzer; Obstacle; Computer science; Wearable computer; Schema crosswalk; Human–computer interaction; Audio signal; Simulation; Computer security; Engineering; Telecommunications; Pedestrian; ALARM; Transport engineering; Embedded system; Electrical engineering; Speech coding; Political science; Law,,,,,,http://dx.doi.org/10.22214/ijraset.2023.52136,,10.22214/ijraset.2023.52136,,,0,,0,true,,gold
151-729-719-938-22X,Development of a Mobile Robot: Robotic Guide Dog for Aid of Visual Disabilities in Urban Environments,,2019,conference proceedings article,"2019 Latin American Robotics Symposium (LARS), 2019 Brazilian Symposium on Robotics (SBR) and 2019 Workshop on Robotics in Education (WRE)",,IEEE,,Diego Renan Bruno; Marcelo Henrique de Assis; Fernando Santos Osório,"The general objective of this work is to develop a mobile robotic platform that is able to avoid obstacles for the benefit of visually impaired people. The idea is that this platform is a robot ""guide dog"" ~for visually impaired persons, and animal guide dogs like these have very high costs (on average 40 thousand dollars - Our current robotic prototype has a cost of 400 dollars), also need a long time for training (more than 1 year), being therefore inaccessible to many visually impaired persons. Our work then turns to social robotics, where we have a big problem with disabled people and who do not get freedom to move around in urban spaces without the help of a caregiver. By collecting data from ultrasound sensors, managing a possible path to the target point in an autonomous manner, using a computer vision system to recognize and treat the obstacles encountered, representing these in the form of audio to the user. The prototype was developed to serve as a platform for research in the development of navigational algorithms and computer vision systems of the automation laboratory of the (omitted for blind review). The robot presented good results in tests with navigation algorithms and computer vision that were applied to validate this platform in tests with visually impaired people in IDVC - (Catanduva Institute for the Visually Impaired).",,,,,Human–computer interaction; Deep learning; Automation; Artificial intelligence; Social robot; Point (typography); Mobile robot; PATH (variable); Visually Impaired Persons; Computer science; Robot,,,,,http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=9018597 http://xplorestaging.ieee.org/ielx7/8995608/9018459/09018597.pdf?arnumber=9018597,http://dx.doi.org/10.1109/lars-sbr-wre48964.2019.00026,,10.1109/lars-sbr-wre48964.2019.00026,3010490505,,0,011-254-131-533-683; 042-460-208-394-397; 061-677-211-881-790; 071-551-805-832-08X; 096-105-268-532-555; 109-806-014-448-128; 139-715-944-221-346; 164-157-088-859-349,11,false,,
151-927-850-041-765,A mobile e-learning application for enhancement of basic mathematical skills in visually impaired children,2023-03-17,2023,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Muhammad Shoaib; Shakeel Khan; Donal Fitzpatrick; Ian Pitt,"<jats:title>Abstract</jats:title><jats:p>Although smartphones are equipped with accessibility functions, they still pose significant problems for visually impaired people. Sometimes these functions cannot fulfil the expectations of users. Early mobile devices had physical buttons and a keypad, and visually impaired users could navigate around the keypad using the tactile markers on the buttons. However, the lack of tactile markers makes it much more difficult to operate a touchscreen device. This paper describes an e-learning platform that is designed to improve the accessibility of smartphone applications for students who are visually impaired but have some useful vision. A User-Centered Design approach was used to develop an effective solution for visually impaired students. A study was conducted during the development of the described platform, and the results showed that our suggested design improves task completion time as compared to the initial version. Participants also expressed higher levels of satisfaction when using the improved design of this platform. The modified design was also assessed using the Mobile Application Rating Scale (MARS), and the results indicate that it is quite reliable and rated well among visually impaired children. Furthermore, developers can use our suggested design guidelines such as clear navigation, color contrast, immediate feedback, icon arrangements, button and text size in the development of new applications.</jats:p>",23,3,1091,1101,Touchscreen; Keypad; Visually impaired; Human–computer interaction; Computer science; Task (project management); Multimedia; Mobile device; Mobile apps; World Wide Web; Engineering; Computer hardware; Systems engineering,,,,advance crt; University College Cork,https://link.springer.com/content/pdf/10.1007/s10209-023-00990-3.pdf https://doi.org/10.1007/s10209-023-00990-3,http://dx.doi.org/10.1007/s10209-023-00990-3,,10.1007/s10209-023-00990-3,,,0,000-082-081-025-981; 001-219-373-623-263; 006-047-427-634-149; 009-300-003-821-819; 016-530-635-379-165; 021-901-794-405-261; 033-827-462-556-277; 038-114-596-461-304; 039-901-769-379-747; 039-983-114-810-657; 046-748-628-888-922; 049-454-414-306-996; 054-081-592-044-054; 056-547-713-371-217; 058-624-608-916-130; 061-162-317-464-407; 076-455-738-292-176; 077-647-694-348-440; 077-907-290-656-920; 089-486-948-113-74X; 112-946-565-888-51X; 118-540-529-844-833; 132-411-633-627-827; 142-575-622-508-48X; 150-753-294-369-298,10,true,cc-by,hybrid
152-183-517-423-628,NAVIG: augmented reality guidance system for the visually impaired,2012-06-12,2012,journal article,Virtual Reality,13594338; 14349957,Springer Science and Business Media LLC,United Kingdom,Brian F. G. Katz; Slim Kammoun; Gaëtan Parseihian; Olivier Gutierrez; Adrien Brilhault; Malika Auvray; Philippe Truillet; Michel Denis; Simon J. Thorpe; Christophe Jouffrais,"Navigating complex routes and finding objects of interest are challenging tasks for the visually impaired. The project NAVIG (Navigation Assisted by artificial VIsion and GNSS) is directed toward increasing personal autonomy via a virtual augmented reality system. The system integrates an adapted geographic information system with different classes of objects useful for improving route selection and guidance. The database also includes models of important geolocated objects that may be detected by real-time embedded vision algorithms. Object localization (relative to the user) may serve both global positioning and sensorimotor actions such as heading, grasping, or piloting. The user is guided to his desired destination through spatialized semantic audio rendering, always maintained in the head-centered reference frame. This paper presents the overall project design and architecture of the NAVIG system. In addition, details of a new type of detection and localization device are presented. This approach combines a bio-inspired vision system that can recognize and locate objects very quickly and a 3D sound rendering system that is able to perceptually position a sound at the location of the recognized object. This system was developed in relation to guidance directives developed through participative design with potential users and educators for the visually impaired.",16,4,253,269,Augmented reality; Artificial intelligence; Virtual reality; Guidance system; Rendering (computer graphics); Computer vision; Computer science; Geographic information system; Machine vision; Global Positioning System; Computer graphics,,,,,https://dx.doi.org/10.1007/s10055-012-0213-6 https://link.springer.com/article/10.1007/s10055-012-0213-6/fulltext.html https://hal.archives-ouvertes.fr/hal-00756714 https://dblp.uni-trier.de/db/journals/vr/vr16.html#KatzKPGBATDTJ12 http://dx.doi.org/10.1007/s10055-012-0213-6 https://link.springer.com/article/10.1007/s10055-012-0213-6,http://dx.doi.org/10.1007/s10055-012-0213-6,,10.1007/s10055-012-0213-6,1212821315,,24,002-391-643-288-41X; 002-849-848-957-025; 002-868-168-418-515; 003-634-369-751-95X; 005-675-328-369-74X; 005-861-492-610-995; 006-256-120-472-553; 006-860-841-078-960; 007-603-719-410-690; 009-069-676-420-498; 009-128-554-117-48X; 009-682-527-269-03X; 010-844-738-340-048; 012-073-295-714-214; 014-666-342-464-665; 015-978-126-687-61X; 016-068-376-695-077; 019-042-214-524-834; 020-601-408-107-469; 024-093-010-731-859; 033-364-231-006-368; 034-350-308-512-672; 038-656-358-670-676; 040-030-169-396-345; 041-339-353-658-714; 044-223-170-614-211; 046-983-906-128-801; 049-106-304-885-412; 051-181-204-576-198; 051-612-966-213-045; 051-766-223-654-722; 058-468-795-176-403; 061-686-108-928-655; 064-014-343-490-355; 067-352-909-875-184; 069-116-650-623-84X; 072-483-684-272-243; 073-407-269-569-770; 079-090-335-218-377; 082-069-548-410-540; 084-581-504-702-117; 086-931-956-789-305; 087-272-456-866-28X; 089-156-568-698-078; 089-553-366-289-654; 095-869-599-243-25X; 098-577-419-119-097; 109-714-673-673-590; 110-650-958-929-874; 134-448-474-253-195; 135-182-333-638-842; 140-091-865-029-447; 140-613-507-034-188; 141-489-442-115-69X; 145-523-738-709-309; 150-849-825-921-982; 152-980-690-273-83X; 156-145-001-387-96X; 160-836-764-758-498; 166-771-600-379-049; 172-904-967-594-662; 183-996-310-924-532; 186-721-874-014-362; 190-621-332-208-185; 196-835-161-415-519,127,true,"CC BY, CC BY-NC-ND",gold
152-315-511-243-142,Assistance For Visually Impaired People Using Deep Learning,2023-05-05,2023,conference proceedings article,2023 2nd International Conference on Vision Towards Emerging Trends in Communication and Networking Technologies (ViTECoN),,IEEE,,Logesh K; Karthikeyan P; Abishek T; Jayamani S,"A disability that disturbs the structure and operations of the vision system is known as a visual impairment. Many people believed that blindness meant being unable to see anything at all or, at most, being able to distinguish light from darkness. With regard to vision impairment, most of the visual impairment Individuals struggle to navigate on their own in an unfamiliar setting. The suggested study aims to provide blind persons the confidence to walk and the awareness to be attentive if their path is blocked by other objects, people, or related odds. In this study, an assistive system for blind persons is proposed, to help them to know what is surround, through deep learning along with visualization methods to identify an objects on walking route. The proposed model benefits the blind users to move around in unfamiliar indoor and outdoor environment. From this device the blind people can easily understand the surrounding through voice feedback. The suggested technique can improve people's protection and safety while going outside or in a strange environment.",,,,,Visual impairment; Blindness; Visually impaired; Impaired Vision; Change blindness; Visualization; Computer science; Low vision; Psychology; Perception; Odds; Human–computer interaction; Cognitive psychology; Artificial intelligence; Optometry; Medicine; Machine learning; Neuroscience; Psychiatry; Logistic regression,,,,,,http://dx.doi.org/10.1109/vitecon58111.2023.10157205,,10.1109/vitecon58111.2023.10157205,,,0,004-071-983-923-934; 004-216-069-855-831; 017-499-516-520-553; 020-581-037-678-426; 035-277-967-833-996; 038-326-446-864-583; 041-058-139-703-974; 045-802-145-074-623; 060-135-893-855-211; 069-608-818-240-967; 092-952-784-829-197; 179-357-837-631-431,0,false,,
152-587-267-213-492,"Deployable, Data-Driven Unmanned Vehicle Navigation System in GPS-Denied, Feature-Deficient Environments",2022-06-11,2022,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Sohum Misra; Kaarthik Sundar; Rajnikant Sharma; Kevin Brink,,105,2,,,,,,,Air Force Research Laboratory; Los Alamos National Laboratory,,http://dx.doi.org/10.1007/s10846-022-01647-8,,10.1007/s10846-022-01647-8,,,0,000-368-290-876-858; 001-374-742-076-817; 002-580-724-068-489; 014-478-034-924-123; 015-497-955-912-506; 027-699-196-867-724; 029-829-259-690-491; 030-674-908-789-312; 033-045-353-396-706; 034-336-527-078-705; 038-622-637-284-14X; 041-333-506-109-883; 042-588-436-277-363; 043-464-336-960-511; 043-945-994-553-114; 045-609-346-472-607; 050-084-334-570-308; 058-705-256-851-736; 060-029-581-945-26X; 062-218-357-107-259; 066-914-348-292-717; 067-634-554-938-646; 082-160-017-941-657; 086-720-900-772-539; 089-202-168-373-412; 100-512-329-956-257; 108-314-090-539-569; 111-203-150-077-711; 115-221-957-896-835; 120-518-342-481-519; 122-904-997-469-092; 124-679-264-672-207; 134-566-071-563-495; 161-285-449-168-277,1,false,,
152-933-832-064-290,Obstacle Detection for Assisting Navigation of Visually Impaired People Based on Segmentation Process,2019-07-26,2019,conference proceedings article,"Proceedings of the 2019 4th International Conference on Robotics, Control and Automation",,ACM,,Fitri Utaminingrum; Yuita Arum Sari; I Komang Somawirata,"Blindness is a condition when a person's sense of sight experiences a disturbance. Hence generally, it requires a tool for help him. One of the assistive devices for people with blind disabilities in carrying out their mobility is a stick or known as ""The White Cane. In general, the blind stick has a permanent shape or cannot be folded. This stick works by tapping or sliding in all directions around the blind person standing, so it is very possible for the surrounding environment to feel disturbed. Based on these problems, we propose a device that can help the blind in walking or doing mobility based on computer vision analysis without having to disturb the surrounding environment. Connected Component Labeling is used to get a blob from an image. The blob that has been detected, then analyzed using segmentation based on the threshold process. The experimental results show that our proposed method using Min-max threshold is able to detect obstacles with an accuracy rate of 82.89 %.",,,,,Artificial intelligence; Visual perception; Obstacle; Tapping; White cane; Visually impaired; Computer vision; Computer science; Connected-component labeling; Segmentation; Process (computing),,,,,https://dl.acm.org/doi/10.1145/3351180.3351202 https://dl.acm.org/doi/pdf/10.1145/3351180.3351202,http://dx.doi.org/10.1145/3351180.3351202,,10.1145/3351180.3351202,2973672271,,0,000-945-592-521-38X; 003-758-450-039-163; 015-714-235-272-975; 030-440-500-148-114; 030-924-411-668-987; 045-878-567-215-685; 092-028-495-040-452; 178-253-687-685-879; 179-654-625-487-90X; 184-562-602-504-167,3,false,,
153-111-007-440-51X,Fuzzy Free Path Detection from Disparity Maps by Using Least-Squares Fitting to a Plane,2013-11-24,2013,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Nuria Ortigosa; Samuel Morillas,"A method to detect obstacle-free paths in real-time which works as part of a cognitive navigation aid system for visually impaired people is proposed. It is based on the analysis of disparity maps obtained from a stereo vision system which is carried by the blind user. The presented detection method consists of a fuzzy logic system that assigns a certainty to be part of a free path to each group of pixels, depending on the parameters of a planar-model fitting. We also present experimental results on different real outdoor scenarios showing that our method is the most reliable in the sense that it minimizes the false positives rate.",75,2,313,330,Artificial intelligence; Pixel; Plane (geometry); Fuzzy logic system; Navigation aid; Computer vision; Computer science; Fuzzy logic; False positive paradox; Stereopsis; Path (graph theory),,,,,https://dblp.uni-trier.de/db/journals/jirs/jirs75.html#OrtigosaM14 https://dialnet.unirioja.es/servlet/articulo?codigo=4750540 https://doi.org/10.1007/s10846-013-9997-1 https://riunet.upv.es/bitstream/10251/50976/3/FreePathDetection_plane.pdf https://link.springer.com/article/10.1007/s10846-013-9997-1 https://riunet.upv.es/handle/10251/50976 https://rd.springer.com/article/10.1007/s10846-013-9997-1,http://dx.doi.org/10.1007/s10846-013-9997-1,,10.1007/s10846-013-9997-1,2039695069,,0,001-812-856-772-682; 005-608-280-761-987; 005-642-018-373-267; 014-810-176-668-753; 015-546-691-802-108; 016-475-171-211-946; 024-131-470-445-452; 024-823-672-991-029; 025-086-085-104-287; 028-436-680-232-191; 029-326-112-192-207; 029-916-098-117-963; 030-167-497-654-555; 031-769-951-373-070; 033-626-844-682-323; 035-982-695-586-701; 036-439-871-218-724; 039-110-125-194-56X; 039-429-854-098-345; 040-892-740-649-890; 042-806-172-235-41X; 045-673-862-191-277; 049-208-047-473-886; 049-478-900-413-179; 052-949-561-819-671; 053-146-979-475-914; 055-057-970-551-712; 058-350-834-986-051; 058-840-546-726-281; 062-563-049-910-151; 067-009-123-253-596; 078-747-544-702-234; 080-115-189-397-779; 089-547-813-246-576; 091-866-870-875-600; 093-598-398-223-688; 098-474-278-615-756; 098-983-371-288-142; 099-086-994-780-682; 099-358-123-258-633; 100-667-285-274-110; 101-840-700-415-260; 104-357-806-589-807; 107-275-338-312-283; 108-504-953-453-881; 109-523-081-131-729; 113-764-349-033-065; 130-535-669-586-408; 131-715-028-811-075; 137-147-049-675-485; 153-697-872-076-389; 161-779-833-939-291; 169-004-645-115-567; 169-104-207-320-534; 182-012-918-939-627; 187-543-116-936-998; 193-123-800-849-774; 195-210-909-960-60X; 198-415-864-822-606; 199-425-332-224-176,3,true,cc-by-nc-nd,green
153-444-529-401-273,A Dynamic Tactile Map as a Tool for Space Organization Perception: Application to the Design of an Electronic Travel Aid for Visually Impaired and Blind People,,2005,journal article,Conference proceedings : ... Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual Conference,1557170x,,United States,Flavien Maingreaud; Edwige Pissaloux; Ramiro Velazquez; F. Gaunet; M. Hafez; J.-M. Alexandre,"This paper addresses a new concept of visuo-tactile human-machine interface for a certain representation of the peripersonnal space, especially useful for visually impaired and blind people navigation. The proposed space representation have been successfully implemented on a tactile device and validated via series of experiments involving naive blindfolded sighted people. Results show that is possible to interact with the space via the proposed tactile representation",7,,6912,6915,Human–computer interaction; Interface (computing); Engineering; User interface design; Artificial intelligence; User-centered design; Display device; Perception; Representation (systemics); Space (commercial competition); Visually impaired; Computer vision,,,,,https://www.researchgate.net/profile/Ramiro_Velazquez2/publication/6522601_A_dynamic_tactile_map_as_a_tool_for_space_organization_perception_Application_to_the_design_of_an_electronic_travel_aid_for_visually_impaired_and_blind_people/links/53d12e940cf228d363e5abd4.pdf https://ieeexplore.ieee.org/document/1616095/ http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.ieee-000001616095 https://www.ncbi.nlm.nih.gov/pubmed/17281864,http://dx.doi.org/10.1109/iembs.2005.1616095,17281864,10.1109/iembs.2005.1616095,2169992327,,1,007-797-333-936-671; 021-167-153-233-857; 047-094-422-923-848; 053-291-126-590-292; 117-949-584-838-863; 158-222-438-791-69X; 176-638-722-195-117,12,false,,
153-796-815-778-724,IoT‐Assisted Smart Device for Blind People,2022-01-16,2022,other,Intelligent Systems for Rehabilitation Engineering,,Wiley,,Roshani Raut; Anuja Jadhav; Swati Jaiswal; Pranav Pathak,"Those who are blind have a lot of trouble going through their daily lives. A lot of effort has gone into making it easier for blind people to complete tasks on their own rather than relying on others. With this inspiration in mind, we proposed and created an intelligent blind stick. The smart walking stick assists visually impaired people in identifying obstacles and getting to their destination. There are a variety of walking sticks and devices that assist users in moving around both indoor and outdoor environments, but none of them include run-time autonomous navigation, object detection and identification warnings, or face and voice recognition. The stick uses IoT, echolocation, image processing, artificial intelligence, and navigation system technology to identify close and far obstacles for the user. If the blind person falls or has some other problem, then the system will send a warning to the designated person. The system uses voice recognition to recognise loved ones.",,,129,150,Computer science; Identification (biology); Human–computer interaction; Variety (cybernetics); Internet of Things; Sight; Voice command device; Object (grammar); Human echolocation; Artificial intelligence; Computer vision; Computer security; Speech recognition; Psychology; Botany; Physics; Astronomy; Neuroscience; Biology,,,,,,http://dx.doi.org/10.1002/9781119785651.ch6,,10.1002/9781119785651.ch6,,,0,003-976-442-054-457; 010-305-498-226-998; 022-898-401-268-246; 025-135-140-535-433; 027-836-322-726-673; 033-432-877-545-952; 039-209-657-828-295; 044-126-162-559-06X; 052-151-486-241-976; 068-153-893-957-613; 073-259-164-809-537; 077-907-866-114-914,1,false,,
153-837-267-715-56X,Enhancing Accessibility: Assistive Devices Pairing Mobile Apps for Visually Impaired People,2023-09-30,2023,journal article,Deleted Journal,,,,Aisyah Ibrahim; Siti Fauziah Toha; N. H. Diyana Nordin; M. O. Tokhi,"Vision impairment and blindness are significant health conditions that affect a considerable portion of the global population and have significant consequences, particularly in developed nations. Its harmful consequences can have a negative influence on a person's personal quality of life, as well as limit their ability to live independently. Navigation plays a crucial role in the activities that are impacted by vision loss. This is because it is almost impossible for the individual to move safely and independently. The goal of this project is to improve mobility for visually impaired people by employing Light Detection and Ranging (LiDAR) sensors to identify obstacles and alert users via haptic feedback if there are potential obstacles in their way. In addition, this study compares two sensors, LiDAR and ultrasonic sensors, besides the implementation of the Internet of Things (IoT), which was integrated into the small, portable robot by pairing a mobile phone. This is to ensure the user may be securely guided to their destination. I-Walk is a mobile app with a voice user interface (VUI) for GPS navigation systems and emergency calling. This app has been developed to allow users safe and independent travel. The simulation result shows that even though ultrasonic sensors are cheaper and serve the same function as LiDAR, their huge standard deviation and percentage error shown in this study cannot equal LiDAR's accuracy in which LiDAR has an average error of 0.63%. In contrast, ultrasonic sensors have an average error of 2.39%. Overall, this obstacle detection and navigation system comprises robust hardware and software to assist visually impaired people effectively.Keywords: Assistive technology, blind, mini guider robot, obstacle detection, navigation, IoT",7,3,32,32,Visually impaired; Assistive technology; Mobile apps; Pairing; Mobile device; Human–computer interaction; Assistive device; Computer science; Internet privacy; Physical medicine and rehabilitation; Medicine; World Wide Web; Physics; Superconductivity; Quantum mechanics,,,,,,,,,,,0,,0,true,cc-by,hybrid
154-049-797-632-445,A Deep Learning Based Model to Assist Blind People in Their Navigation,,2022,journal article,Journal of Information Technology Education: Innovations in Practice,21653151; 2165316x,Informing Science Institute,United States,Nitin Kumar; Anuj jain,"<jats:p>Aim/Purpose: This paper proposes a new approach to developing a deep learning-based prototyping wearable model which can assist blind and visually disabled people to recognize their environments and navigate through them. As a result, visually impaired people will be able to manage day-to-day activities and navigate through the world around them more easily.; ; Background: In recent decades, the development of navigational devices has posed challenges for researchers to design smart guidance systems for visually impaired and blind individuals in navigating through known or unknown environments. Efforts need to be made to analyze the existing research from a historical perspective. Early studies of electronic travel aids should be integrated with the use of assistive technology-based artificial vision models for visually impaired persons.; ; Methodology: This paper is an advancement of our previous research work, where we performed a sensor-based navigation system. In this research, the navigation of the visually disabled person is carried out with a vision-based 3D-designed wearable model and a vision-based smart stick. The wearable model used a neural network-based You Only Look Once (YOLO) algorithm to detect the course of the navigational path which is augmented by a GPS-based smart Stick. Over 100 images of each of the three classes, namely straight path, left path and right path, are being trained using supervised learning. The model accurately predicts a straight path with 79% mean average precision (mAP), the right path with 83% mAP, and the left path with 85% mAP. The average accuracy of the wearable model is 82.33% and that of the smart stick is 96.14% which combined gives an overall accuracy of 89.24%. ; ; Contribution: This research contributes to the design of a low-cost navigational standalone system that will be handy to use and help people to navigate safely in real-time scenarios. The challenging self-built dataset of various paths is generated and transfer learning is performed on the YOLO-v5 model after augmentation and manual annotation. To analyze and evaluate the model, various metrics, such as model losses, recall value, precision, and maP, are used.; ; Findings: These were the main findings of the study:; •	To detect objects, the deep learning model uses a higher version of YOLO, i.e., a YOLOv5 detector, that may help those with visual im-pairments to improve their quality of navigational mobilities in known or unknown environments.; •	The developed standalone model has an option to be integrated into any other assistive applications like Electronic Travel Aids (ETAs); •	It is the single neural network technology that allows the model to achieve high levels of detection accuracy of around 0.823 mAP with a custom dataset as compared to 0.895 with the COCO dataset. Due to its lightning-speed of 45 FPS object detection technology, it has become popular.; ; ; Recommendations for Practitioners: Practitioners can help the model’s efficiency by increasing the sample size and classes used in training the model.; ; Recommendation for Researchers: To detect objects in an image or live cam, there are various algorithms, e.g., R-CNN, Retina Net, Single Shot Detector (SSD), YOLO. Researchers can choose to use the YOLO version owing to its superior performance. Moreover, one of the YOLO versions, YOLOv5, outperforms its other versions such as YOLOv3 and YOLOv4 in terms of speed and accuracy.; ; Impact on Society: We discuss new low-cost technologies that enable visually impaired people to navigate effectively in indoor environments.; ; Future Research: The future of deep learning could incorporate recurrent neural networks on a larger set of data with special AI-based processors to avoid latency.; ; </jats:p>",21,,95,114,Computer science; Wearable computer; Path (computing); Artificial intelligence; Global Positioning System; Human–computer interaction; Deep learning; Computer vision; Perspective (graphical); Wearable technology; Visually impaired; Embedded system; Telecommunications; Programming language,,,,,http://www.jite.org/documents/Vol21/JITE-IIPv21p095-114Kumar8367.pdf https://doi.org/10.28945/5006,http://dx.doi.org/10.28945/5006,,10.28945/5006,,,0,,4,true,,gold
154-448-732-085-549,Surgical robotics and instrumentation,2012-05-17,2012,journal article,International Journal of Computer Assisted Radiology and Surgery,18616410; 18616429,Springer Science and Business Media LLC,Germany,,,7,S1,169,176,Robotics; Instrumentation (computer programming); Artificial intelligence; Computer science; Medical physics; Health informatics; Medical robotics; Medicine; Data science; General surgery; Robot; Pathology; Public health; Programming language,,,,,,http://dx.doi.org/10.1007/s11548-012-0716-3,,10.1007/s11548-012-0716-3,,,0,,2,false,,
154-558-005-032-27X,WIRE 3 : Driving Around the Information Super-Highway,2002-05-01,2002,journal article,Personal and Ubiquitous Computing,16174909; 16174917,Springer Science and Business Media LLC,Germany,Stuart Goose; Safia Djennane,,6,3,164,175,Human–computer interaction; Mobile computing; Speech synthesis; Conceptual model (computer science); Voice browser; Factor (programming language); Computer science; Multimedia; Document Structure Description; Digital audio,,,,,https://core.ac.uk/display/102226223 https://link.springer.com/article/10.1007/s007790200017 https://dblp.uni-trier.de/db/journals/puc/puc6.html#GooseD02 https://link.springer.com/content/pdf/10.1007/s007790200017.pdf,http://dx.doi.org/10.1007/s007790200017,,10.1007/s007790200017,2002893733,,0,009-938-574-961-754; 013-062-678-284-55X; 015-978-126-687-61X; 019-845-340-237-851; 021-129-656-798-756; 024-093-010-731-859; 027-385-647-276-818; 029-228-499-654-12X; 043-927-509-042-048; 050-211-139-330-850; 053-028-684-402-972; 060-222-820-505-064; 068-695-609-164-285; 069-361-197-968-856; 080-562-426-873-623; 083-471-148-449-253; 085-960-456-264-98X; 087-819-550-516-151; 150-732-792-153-93X; 161-428-668-072-043; 162-099-738-889-545,9,false,,
154-764-681-989-082,ICRA - Incorporating information from trusted sources to enhance urban navigation for blind travelers,,2015,conference proceedings article,2015 IEEE International Conference on Robotics and Automation (ICRA),,IEEE,,Byung-Cheol Min; Suryansh Saxena; Aaron Steinfeld; M. Bernardine Dias,"Dynamic changes can present significant challenges for visually impaired travelers to safely and independently navigate urban environments. To address these challenges, we are developing the NavPal suite of technology tools [1]. NavPal includes a dynamic guidance tool [2] in the form of a smartphone app that can provide real-time instructions based on available map information to guide navigation in indoor environments. In this paper we enhance our past work by introducing a framework for blind travelers to add map/navigation information to the tool, and to invite trusted sources to do the same. The user input is realized through audio breadcrumb annotations that could be useful for future trips. The trusted sources mechanism provides invited trusted individuals or organizations an interface to contribute real-time information about the surrounding environment. We demonstrate the feasibility of our solution through a prototype Android smartphone-based outdoor navigation aid for blind travelers. An initial usability study with visually impaired adults informed the design and implementation of this prototype.",2015,,4511,4518,Human–computer interaction; Engineering; TRIPS architecture; Usability; User input; Smartphone app; Multimedia; Android (operating system); Global Positioning System; Suite; Server,,,,,http://dblp.uni-trier.de/db/conf/icra/icra2015.html#MinSSD15 https://dblp.uni-trier.de/db/conf/icra/icra2015.html#MinSSD15 https://ieeexplore.ieee.org/document/7139824/,http://dx.doi.org/10.1109/icra.2015.7139824,,10.1109/icra.2015.7139824,1505721457,,0,000-424-243-390-605; 007-429-644-566-621; 008-264-080-460-955; 020-374-980-887-665; 020-601-408-107-469; 030-127-369-356-741; 036-816-443-268-187; 037-431-429-780-095; 051-893-961-323-409; 056-785-601-003-691; 058-148-305-244-619; 075-402-037-032-739; 087-510-774-676-36X; 093-925-865-515-918; 098-430-297-950-379; 099-829-783-436-886; 103-774-457-019-603; 104-694-342-259-021; 125-351-519-179-754,10,true,cc-by,green
155-503-280-718-987,What blindness can tell us about seeing again: merging neuroplasticity and neuroprostheses,,2005,journal article,Nature reviews. Neuroscience,1471003x; 14710048,Springer Science and Business Media LLC,United Kingdom,Lotfi B. Merabet; Joseph F. Rizzo; Amir Amedi; David C. Somers; Alvaro Pascual-Leone,,6,1,71,77,Psychology; Neuroscience; Cognitive science; Merge (version control); Blindness; Neuroplasticity,,"Animals; Blindness/physiopathology; Humans; Neuronal Plasticity/physiology; Prostheses and Implants; Vision, Ocular/physiology",,,https://www.nature.com/articles/nrn1586.pdf https://nature.com/articles/nrn1586 https://www.ncbi.nlm.nih.gov/pubmed/15611728,http://dx.doi.org/10.1038/nrn1586,15611728,10.1038/nrn1586,2035742535,,0,001-767-840-740-256; 002-367-324-166-044; 002-970-338-382-617; 005-507-088-342-544; 007-254-708-016-804; 008-586-462-228-686; 008-791-556-816-035; 010-779-934-269-391; 011-080-377-899-354; 011-243-944-882-502; 012-062-704-842-88X; 012-624-746-344-493; 013-890-491-623-930; 014-659-392-706-153; 014-716-988-585-987; 014-894-562-306-137; 015-392-965-743-506; 016-553-489-463-642; 016-558-453-816-277; 016-579-899-521-900; 018-094-489-878-074; 019-039-096-998-678; 019-925-299-246-691; 020-192-164-080-604; 020-454-225-952-937; 021-408-707-817-630; 023-017-319-561-404; 026-203-424-274-042; 026-359-780-978-143; 031-181-587-843-197; 034-185-076-627-502; 036-227-020-033-217; 036-953-554-634-89X; 040-329-458-875-250; 043-269-569-868-43X; 044-203-969-445-473; 044-668-316-416-149; 045-783-474-697-29X; 046-849-699-516-053; 047-957-267-743-900; 049-863-453-837-500; 050-859-041-353-124; 054-269-288-800-462; 055-556-342-757-003; 056-532-649-853-016; 056-589-972-506-345; 057-432-148-324-456; 060-451-470-080-797; 063-048-719-705-828; 063-078-813-509-417; 064-069-666-882-597; 066-443-377-470-967; 067-621-069-925-283; 068-829-447-415-205; 072-795-604-053-060; 087-738-561-221-661; 089-008-816-656-13X; 091-390-031-835-850; 098-912-491-443-559; 117-483-878-304-451; 119-604-996-084-411; 127-817-632-406-052; 128-288-848-632-871; 144-122-065-965-338; 150-859-309-801-617; 150-902-081-203-733,172,false,,
155-752-736-182-381,Transaction models for Web accessibility,2011-06-17,2011,journal article,World Wide Web,1386145x; 15731413,Springer Science and Business Media LLC,Netherlands,Jalal Mahmud; I. V. Ramakrishnan,"The Web has evolved into a dominant digital medium for conducting many types of online transactions such as shopping, paying bills, making travel plans, etc. Such transactions typically involve a number of steps spanning several Web pages. For sighted users these steps are relatively straightforward to do with graphical Web browsers. But they pose tremendous challenges for visually impaired individuals. This is because screen readers, the dominant assistive technology used by visually impaired users, function by speaking out the screen’s content serially. Consequently, using them for conducting transactions can cause considerable information overload. But usually one needs to browse only a small fragment of a Web page to do a step of a transaction (e.g., choosing an item from a search results list). Based on this observation this paper presents a model-directed transaction framework to identify, extract and aurally render only the “relevant” page fragments in each step of a transaction. The framework uses a process model to encode the state of the transaction and a concept model to identify the page fragments relevant for the transaction in that state. We also present algorithms to mine such models from click stream data generated by transactions and experimental evidence of the practical effectiveness of our models in improving user experience when conducting online transactions with non-visual modalities.",15,4,383,408,,,,,,,http://dx.doi.org/10.1007/s11280-011-0135-3,,10.1007/s11280-011-0135-3,,,0,017-800-207-958-439; 026-777-687-626-25X; 032-183-833-051-896; 051-196-794-892-116; 065-028-506-334-941; 082-888-714-276-260; 084-879-779-402-625; 101-524-071-004-471; 113-611-307-855-274; 137-168-531-785-087; 143-533-991-276-064; 158-359-687-483-222; 161-428-668-072-043,4,true,,green
155-790-087-638-509,Optimisation of modality based diagnostics processes using computational tools,2012-05-24,2012,journal article,International Journal of Computer Assisted Radiology and Surgery,18616410; 18616429,Springer Science and Business Media LLC,Germany,Vappu Reijonen; Sauli Savolainen,,7,S1,307,365,Machine learning; Artificial intelligence; Modality (human–computer interaction); Computer science,,,,,https://researchportal.helsinki.fi/en/publications/optimisation-of-modality-based-diagnostics-processes-using-comput,http://dx.doi.org/10.1007/s11548-012-0736-z,,10.1007/s11548-012-0736-z,3047787732,,0,,1,false,,
156-988-399-478-444,Advancement in navigation technologies and their potential for the visually impaired: a comprehensive review,2023-04-21,2023,journal article,Spatial Information Research,23663286; 23663294,Springer Science and Business Media LLC,,Vahid Isazade,"Improvements in technology and navigation tools are leading to more affordable and effective solutions to assist individuals with visual impairments. The progress made in navigation technology has the potential to increase inclusivity for the visually impaired in education, social, and workforce settings. After conducting a thorough review of the literature, we have identified key issues and concluded that collaboration among healthcare professionals, caregivers, programmers, engineers, and policymakers is essential for successfully developing navigation projects for the visually impaired. This study highlights different advances and relevant topics in the development of location-based applications for individuals with visual impairments. Our paper involved an extensive search of eight journal databases spanning from 1993 to 2021. We screened 4550 titles, analyzed 560 abstracts, and ultimately reviewed 35 full-text papers, resulting in the examination of 20 papers. Our findings indicate that the advancement of navigation technology can positively affect the quality of life of visually impaired individuals, particularly through assistive technology, mobile applications, and web services. Dot Waker, Nearby Explorer, Get There, and Google Maps are the most commonly used navigation systems by visually impaired individuals. Overall, our research suggests that continued development in navigation-assisted applications can significantly benefit the visually impaired community.",31,5,547,558,Visually impaired; Assistive technology; Workforce; Visual impairment; Multimedia; Computer science; World Wide Web; Psychology; Human–computer interaction; Political science; Psychiatry; Law,,,,,https://link.springer.com/content/pdf/10.1007/s41324-023-00522-4.pdf https://doi.org/10.1007/s41324-023-00522-4,http://dx.doi.org/10.1007/s41324-023-00522-4,,10.1007/s41324-023-00522-4,,,0,000-200-534-867-044; 000-631-028-807-828; 002-617-877-307-635; 007-907-736-129-297; 011-414-630-627-246; 012-779-338-000-233; 013-196-453-981-278; 026-922-910-952-305; 027-476-512-122-719; 043-196-700-803-427; 045-234-299-844-122; 045-599-200-378-144; 045-802-145-074-623; 050-661-149-356-403; 050-813-649-265-217; 053-794-028-526-128; 056-745-340-837-164; 058-268-022-053-157; 064-657-343-403-492; 068-125-995-624-395; 071-281-258-323-085; 075-519-293-886-898; 082-036-797-529-285; 083-345-653-191-276; 084-581-504-702-117; 086-308-757-277-927; 097-390-623-922-438; 108-706-050-311-896; 113-071-971-798-760; 116-020-724-005-291; 116-715-441-276-690; 117-519-162-175-617; 126-083-611-062-426; 143-544-756-515-778; 153-471-712-074-630; 162-116-944-559-725; 162-252-648-127-078; 163-862-448-479-727; 169-478-549-790-022,5,true,,bronze
157-127-483-813-323,Echo Guidance: Voice-Activated Application for Blind with Smart Assistive Stick Using Machine Learning and IoT,2024-04-18,2024,conference proceedings article,2024 International Conference on Advances in Data Engineering and Intelligent Computing Systems (ADICS),,IEEE,,Abishek Kumar G; Surya G; Sathyadurga V,"Navigating the world without sight presents profound challenges for millions of visually impaired individuals. Echo Guidance includes a pioneering application, addresses these obstacles by facilitating independence and mobility through voice-driven interaction and an IoT smart assistive device for obstacle detection by utilizing Ultrasonic, GPS and GSM modules powered by Arduino. This study explores the design and evaluation of Echo Guidance, highlighting its utilization of advanced algorithms like RGB888 conversion, matrix transformation, and Optical Character Recognition (OCR) for real-time object recognition. The application boasts a voice-centric interface tailored to the needs of the visually impaired, seamlessly integrating day-to-day functionalities such as weather forecasting, reminders, and a calculator accessible through voice commands. Central to Echo Guidance is its innovative object detection and recognition approach, employing the MobileNetObjDetector class powered by TensorFlow Lite for swift and accurate object detection. The OverlayView class visually represents detected objects, enhancing spatial awareness. Additionally, the integration of IoT devices, such as the smart stick with ultrasonic sensors for obstacle detection and head-level obstacle detection, coupled with SOS functionality in case of any emergency using GPS and GSM modules, further enhances Echo Guidance's capabilities. This research underscores Echo Guidance's transformative potential in improving the quality of life for visually impaired individuals, representing a significant advancement in assistive technology. Prioritizing accessibility and user experience, this paper empowers individuals worldwide with newfound independence and confidence in navigating their surroundings.",,,,,Echo (communications protocol); Computer science; Internet of Things; Human–computer interaction; Voice command device; Assisted living; Speech recognition; Artificial intelligence; Embedded system; Computer network; Medicine; Nursing,,,,,,http://dx.doi.org/10.1109/adics58448.2024.10533517,,10.1109/adics58448.2024.10533517,,,0,000-130-488-383-514; 006-310-937-905-362; 015-601-294-792-734; 023-735-000-187-111; 031-127-219-937-467; 036-403-057-461-733; 060-909-379-783-207; 071-875-360-614-784; 073-881-199-914-417; 102-854-427-171-288; 108-186-632-977-341; 117-417-062-852-126; 155-096-675-018-832; 171-189-370-546-346; 185-290-273-755-036; 194-279-593-191-11X,0,false,,
157-662-455-064-075,Blind Assistance System using Image Processing,2022-03-31,2022,journal article,International Journal for Research in Applied Science and Engineering Technology,23219653,International Journal for Research in Applied Science and Engineering Technology (IJRASET),,P. Rama Devi; K. Sahaja; S. Santrupth; M. P. Tony Harsha; K. Balasubramanyam Reddy,"<jats:p>Abstract: Eye diseases usually cause blindness and visual impairment. According to the World Health Organization, around 40 million people are blind, while another 250 million have some form of visual impairment. They come across many troubles in their daily life, especially while navigating from one place to another on their own. They often depend on others for help to satisfy their day-to-day needs. So, it is quite a challenging task to implement a technological solution to assist them. Several technologies have been developed for the assistance of visually impaired people. One such attempt is that we would wish to make an Integrated Machine Learning System that allows the blind victims to identify and classify real-time objects generating voice feedback and distance. Which also produces warnings whether they are very close or far away from the thing. Keywords: Blindness, Visual impairment, Machine Learning, Real-time objects</jats:p>",10,3,2232,2240,Blindness; Visual impairment; Task (project management); Computer science; Partially sighted; Change blindness; Human–computer interaction; Artificial intelligence; Computer vision; Psychology; Optometry; Visually impaired; Medicine; Engineering; Systems engineering; Psychiatry; Change detection,,,,,,http://dx.doi.org/10.22214/ijraset.2022.41102,,10.22214/ijraset.2022.41102,,,0,,0,true,,gold
157-943-994-598-763,Comparing sighted and blind users task performance in responsive and non-responsive web design,2018-04-05,2018,journal article,Knowledge and Information Systems,02191377; 02193116,Springer Science and Business Media LLC,Germany,Tiago do Carmo Nogueira; Deller James Ferreira; Sergio T. Carvalho; Luciana de Oliveira Berretta; Mycke R. Guntijo,,58,2,319,339,Human–computer interaction; Design strategy; Usability; Work (electrical); Task (project management); Computer science; Web design; Responsive web design,,,,,https://link.springer.com/article/10.1007/s10115-018-1188-8 https://doi.org/10.1007/s10115-018-1188-8 https://dblp.uni-trier.de/db/journals/kais/kais58.html#NogueiraFCBG19,http://dx.doi.org/10.1007/s10115-018-1188-8,,10.1007/s10115-018-1188-8,2795925643,,0,004-017-242-451-24X; 005-652-272-117-938; 008-134-590-920-899; 011-659-143-396-837; 021-170-778-285-785; 028-522-581-284-632; 032-685-199-294-363; 038-634-214-371-151; 045-142-554-226-244; 056-621-206-285-122; 058-147-809-399-863; 058-805-786-827-264; 060-740-183-488-23X; 062-344-155-654-323; 063-878-054-748-825; 069-843-183-560-675; 070-875-178-006-938; 081-785-716-833-405; 090-327-900-896-834; 104-811-942-245-512; 115-034-200-632-551; 162-343-200-909-593,13,false,,
158-284-395-482-240,Virtual Eye for Blind People Using Deep Learning,2022-07-31,2022,journal article,International Journal for Research in Applied Science and Engineering Technology,23219653,International Journal for Research in Applied Science and Engineering Technology (IJRASET),,Mamatha C,"<jats:p>Abstract: The most crucial sense for any living thing is vision. But due to a few unlucky circumstances, some people become blind as they age, while others are born blind. Extremely disabled people who are visually impaired are unable to move straight away or perform any fundamental tasks like a normal person. This artwork suggests a basic electronic guided incorporated vision device that is functional and configurable to facilitate the mobility of blind and visually impaired people both indoors and outside. The suggested device aims to develop a wearable visual resource for blind people in which receiving instructions verbally from the user is standard. Its capability includes identification of technology and people. This will benefit those who are blind. man or woman to control daily activities and move around their environment. The device enablesthe disabled people to be fairly self-organized and offers affordable, reliable solutions. To put it briefly, the ""VIRTUAL EYE FOR BLIND USING DEEP LEARNING"" assignment is intended to enable visually impaired people who have lost their sight and support them with basic mobility using an effective economic system</jats:p>",10,7,4101,4106,Sight; Visually impaired; Disabled people; Identification (biology); Human–computer interaction; Computer science; Wearable computer; Blindness; Screen reader; Resource (disambiguation); Normal vision; Psychology; Optometry; Applied psychology; Medicine; Embedded system; Computer network; Life style; Physics; Botany; Astronomy; Biology,,,,,,http://dx.doi.org/10.22214/ijraset.2022.45931,,10.22214/ijraset.2022.45931,,,0,,0,true,,gold
158-304-929-176-789,Artificial intelligence and sensors based assistive system for the visually impaired people,,2017,conference proceedings article,2017 International Conference on Intelligent Sustainable Systems (ICISS),,IEEE,,Sandesh Chinchole; Samir Patel,"The need for developing a low-cost assistive system for the visually impaired and blind people has increased with steady increase in their population worldwide. The stick system presented in the paper uses artificial intelligence along with various sensors in real time to help the visually disabled people to navigate their environment independently. Image recognition, collision detection and obstacle detection are the three tasks performed by the system. The image recognition task was performed using a smartphone application powered by artificial intelligence. The tasks of collision detection and obstacle detection utilized ultrasonic sensors to alert the user of the obstacles appearing in his route. The stick system also managed to demonstrate the important characteristics of affordability, high efficiency, mobility and ease of use.",,,,,Artificial intelligence; Usability; Task analysis; Intelligent sensor; Collision detection; Obstacle; Task (project management); Collision avoidance; Population; Computer science,,,,,http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=8389401 http://xplorestaging.ieee.org/ielx7/8376163/8389225/08389401.pdf?arnumber=8389401,http://dx.doi.org/10.1109/iss1.2017.8389401,,10.1109/iss1.2017.8389401,2808810063,,0,001-076-828-128-846; 092-726-040-947-902; 098-811-589-762-176; 119-892-455-283-484,12,false,,
158-632-220-272-530,IoT-Based Smart Stick for the Blind,2022-02-11,2022,book chapter,Advanced Sensing in Image Processing and IoT,,CRC Press,,Varsha Vimal Sood; Kartik Bansal; Nitish Agarwal,"The measure to gauge technological advancements is not just inventions and innovations but the uplifting of the quality of life in general. The right to live with dignity calls for less dependency on others, which is not so obvious with physically and mentally challenged people. Visually challenged people feel that the basic human activity of navigation is their biggest challenge and therefore remain dependent on guides, dogs, or the traditional white canes/sticks. The guiding solution preferred by the blind is the walking cane. The Internet of Things (IOT) is a communication network wherein different physical devices or ""things"" enabled by sensors and Internet connectivity interact amongst each other as well as with the external environment. IoT is playing a crucial role in the improvement of the traditional white cane for the visually impaired so as to make them less dependent on others and able to live a dignified life. This chapter reviews various IoT-enabled smart sticks equipped with various sensors such as infrared, ultrasonic, rain, etc., Global System for Mobile communications (GSM), Global Positioning System (GPS) modules, and other enhanced features. These smart sticks make economic, versatile, and effective aids to navigation for visually challenged people.",,,353,364,Global Positioning System; GSM; Internet of Things; Computer science; The Internet; Internet privacy; Human–computer interaction; Computer security; Telecommunications; Engineering; World Wide Web,,,,,,http://dx.doi.org/10.1201/9781003221333-19,,10.1201/9781003221333-19,,,0,,2,false,,
158-643-842-195-799,OBJECT DETECTION FOR BLIND PEOPLE USING CNN,2023-05-01,2023,journal article,International Research Journal of Modernization in Engineering Technology and Science,25825208,International Research Journal of Modernization in Engineering Technology and Science,,Mathew Sam; Ravi Sharma; Rudresh Chinchur; Milind Bhingardive,"The proposed work aims to use technology to help blind people navigate the world more effectively.By using image processing and machine learning techniques, a real-time object recognition system can identify objects in a blind person's path and inform them of the object and its location through voice output.This can help blind people move independently and safely without assistance.However, existing approaches have limitations in accurately distinguishing between objects, leading to reduced accuracy and poor performance.The main goals of the proposed work are to provide good accuracy, best performance results, and viable options for blind people to enhance their independence and improve their quality of life.Also, it aims to develop a real-time text detection system with multi-language text-to-speech support to assist visually impaired individuals in reading printed text in over 100 languages.The system will use advanced image processing and machine learning algorithms to detect and recognize text in real-time, and convert it to speech output for the user in their preferred language.The accuracy and performance of the system will be evaluated using standard benchmark datasets, and improvements will be made to enhance its efficiency and effectiveness.This system has the potential to significantly enhance the independence and autonomy of visually impaired individuals, enabling them to read signs, labels, and other forms of printed text without the need for assistance in their native language.This could lead to significant improvements in their overall quality of life, as well as their ability to navigate and interact with the world around them, irrespective of the language.",,,,,Computer science; Artificial intelligence; Computer vision; Object (grammar); Object detection; Pattern recognition (psychology),,,,,,http://dx.doi.org/10.56726/irjmets37671,,10.56726/irjmets37671,,,0,,1,true,,bronze
158-927-338-831-148,Collaborative route map and navigation of the guide dog robot based on optimum energy consumption,2024-03-21,2024,journal article,AI & SOCIETY,09515666; 14355655,Springer Science and Business Media LLC,Germany,Bin Hong; Yihang Guo; Meimei Chen; Yahui Nie; Changyuan Feng; Fugeng Li,,,,,,Energy consumption; Robot; Computer science; Human–computer interaction; Energy (signal processing); Road map; Artificial intelligence; Engineering; Geography; Cartography; Mathematics; Statistics; Electrical engineering,,,,,,http://dx.doi.org/10.1007/s00146-024-01879-2,,10.1007/s00146-024-01879-2,,,0,006-968-718-686-762; 090-718-197-956-927; 106-960-708-626-415; 109-923-649-103-118; 125-977-591-700-940; 186-965-620-392-268,0,false,,
159-005-316-598-678,Development of swahili speaking body weight scale for visually impaired people in Tanzania,2023-07-11,2023,dissertation,,,The Nelson Mandela African Institution of Science and Technology,,Gloriose Nzasangamariya,"<jats:p>Speaking weight scale is an important low vision health aid which measures and announces out the measured weight. It is valuable in numerous applications such as Bathroom scale, Kitchen scale and more. Different talking scales have been developed for blind community. Many talking scales have language option for English, German, French or Spanish. However, only limited work exists for Swahili speaking visually impaired community in East African Community (EAC) given the fact that no talking scale can announce weight in Swahili, which is the common language in EAC. Therefore, this project aims to develop a Swahili speaking weighing machine to assist visually impaired people in Tanzania. The proposed speaking scale is divided into two major parts. On the front-end of the design, sensors are used to capture weight parameters. The captured values are mapped onto sequence of voice patterns. The back-end consists of transferring a sequence of voice patterns to a loudspeaker whereby the voice patterns are stored on an SD card. Finally, the developed device has been evaluated on several objects (certified scale calibration weights) with known weights. Each object was reweighed two times. Placed certified calibration weights on the scale and note the output. Took the measured object off the scale and let the scale return to zero. Placed the same object on the scale again. Noted the output again. The results then showed that the scale displayed the same weights on each object. The expected weight of given objects was then compared with the recorded ones to assess the performance of the scale. The results then showed that the scale is able to measure objects, displays digital output of measured weight and announce it in Swahili language within the accuracy of 1% error range of the actual weight. The proposed device has a great potential as a low vision health aid for Swahili speaking. The features of this device can be further improved to increase the autonomy of blind people to use the device and navigate to the device’s location safely.</jats:p>",,,,,Swahili; Scale (ratio); Tanzania; Object (grammar); Computer science; Artificial intelligence; Geography; Linguistics; Cartography; Environmental planning; Philosophy,,,,,https://dspace.nm-aist.ac.tz/bitstream/20.500.12479/1589/1/MSc_EMoS_Gloriose_Nzasangamariya_2022.pdf https://doi.org/10.58694/20.500.12479/1589,http://dx.doi.org/10.58694/20.500.12479/1589,,10.58694/20.500.12479/1589,,,0,,0,true,,bronze
159-706-022-923-81X,DRISHTI: Visual Navigation Assistant for Visually Impaired,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Malay Joshi; Aditi Shukla; Jayesh Srivastava; Manya Rastogi,"In today's society, where independent living is becoming increasingly important, it can be extremely constricting for those who are blind. Blind and visually impaired (BVI) people face challenges because they need manual support to prompt information about their environment. In this work, we took our first step towards developing an affordable and high-performing eye wearable assistive device, DRISHTI, to provide visual navigation assistance for BVI people. This system comprises a camera module, ESP32 processor, Bluetooth module, smartphone and speakers. Using artificial intelligence, this system is proposed to detect and understand the nature of the users' path and obstacles ahead of the user in that path and then inform BVI users about it via audio output to enable them to acquire directions by themselves on their journey. This first step discussed in this paper involves establishing a proof-of-concept of achieving the right balance of affordability and performance by testing an initial software integration of a currency detection algorithm on a low-cost embedded arrangement. This work will lay the foundation for our upcoming works toward achieving the goal of assisting the maximum of BVI people around the globe in moving independently.",,,,,Computer science; Human–computer interaction; Wearable computer; Bluetooth; Sight; Visually impaired; Software; Globe; Path (computing); Visual impairment; Currency; Work (physics); Artificial intelligence; Embedded system; Engineering; Telecommunications; Operating system; Wireless; Psychology; Mechanical engineering; Physics; Astronomy; Neuroscience; Psychiatry; Monetary economics; Economics,,,,,https://arxiv.org/abs/2303.07451,http://dx.doi.org/10.48550/arxiv.2303.07451,,10.48550/arxiv.2303.07451,,,0,,0,true,other-oa,green
159-782-584-038-294,Reading Assistant for Visually Challenged Peoples with Advance Image Capturing Technique Using Machine Learning,2023-12-20,2023,journal article,Migration Letters,17418992; 17418984,Dewspublication,United Kingdom,Dr.R. Sudhakar; Dr.S. Elango; Arya Surendran; Gopika Venu,"<jats:p>Since blindness prevents a person from learning about their surroundings, it is difficult for them to independently navigate, recognise items, avoid hazards, and read. In this essay, we provide a ground-breaking system for visually impaired people who use assistive technology. The concept incorporates a camera, sensors, and effective image processing algorithms that use Raspberry Pi for object detection and obstacle avoidance. Ultrasonic sensors and the camera both measure the user's distance from the obstruction. The system consists of integrated reading help that first generates an audio response before converting images to text. The complete apparatus is small and light, and it can be easily and inexpensively mounted on a regular pair of eyeglasses. The entire system is affordable, easy to use, and can be attached to a regular pair of eyeglasses. It is also portable and lightweight. Ten people who are completely blind will be used to compare the performance of the suggested device to the traditional white cane. The evaluations are conducted in controlled environments intended to mimic day-to-day activities for blind persons. The findings show that the proposed device provides more accessibility, comfort, and simplicity of navigation for the blind when compared to the white cane.</jats:p>",20,S13,361,365,Computer science; Reading (process); Obstacle; Computer vision; Artificial intelligence; Blindness; Raspberry pi; Human–computer interaction; Visually impaired; Object (grammar); Simplicity; Embedded system; Internet of Things; Medicine; Philosophy; Epistemology; Political science; Optometry; Law,,,,,https://migrationletters.com/index.php/ml/article/download/6467/4349 https://doi.org/10.59670/ml.v20is13.6467,http://dx.doi.org/10.59670/ml.v20is13.6467,,10.59670/ml.v20is13.6467,,,0,,0,true,cc-by-nc-nd,hybrid
160-128-183-468-145,Restoring vision.,2018-05-16,2018,journal article,Nature,14764687; 00280836,Springer Science and Business Media LLC,United Kingdom,Botond Roska; José-Alain Sahel,,557,7705,359,367,Blindness; Optometry; Computer science; Medicine,,"Animals; Blindness/pathology; Disease Models, Animal; Humans; Regeneration/physiology; Retina/cytology; Translational Research, Biomedical; Treatment Outcome; Vision, Ocular/physiology",,,,http://dx.doi.org/10.1038/s41586-018-0076-4,29769667,10.1038/s41586-018-0076-4,,,1,001-346-905-665-504; 001-595-689-329-393; 001-638-932-707-33X; 002-072-794-825-368; 002-632-807-995-941; 002-880-259-818-719; 002-922-172-438-708; 004-168-072-582-295; 004-570-879-485-037; 005-285-474-210-004; 006-710-917-654-992; 006-811-072-085-62X; 007-396-934-628-36X; 007-815-718-902-735; 009-662-082-428-860; 009-807-141-493-502; 010-362-590-569-468; 011-777-429-007-734; 011-839-831-460-896; 013-196-453-981-278; 014-189-218-554-007; 015-218-495-442-595; 015-506-231-442-918; 016-135-358-104-840; 017-280-856-105-229; 018-544-699-943-562; 019-349-297-317-529; 020-493-593-138-404; 020-516-411-330-665; 021-412-306-676-55X; 021-599-486-950-482; 024-144-206-344-655; 024-492-697-450-989; 024-582-890-063-891; 024-658-940-471-788; 025-035-300-942-68X; 025-404-893-071-330; 025-568-297-405-059; 026-809-471-215-754; 026-930-420-667-43X; 026-994-157-921-308; 028-363-911-613-502; 028-496-651-531-410; 029-984-566-278-800; 030-420-019-647-007; 030-522-836-283-853; 030-982-959-356-79X; 034-196-646-262-503; 034-735-189-483-211; 034-941-471-749-471; 035-404-935-241-792; 035-572-739-885-286; 036-173-992-179-068; 036-455-191-026-374; 037-768-391-309-823; 038-015-354-675-998; 038-416-950-012-004; 038-870-420-258-576; 038-976-769-904-206; 039-004-640-624-027; 039-630-409-221-039; 039-879-667-350-03X; 040-305-542-511-701; 040-685-881-149-02X; 040-761-480-872-42X; 041-386-092-195-791; 041-542-174-952-057; 041-843-678-236-213; 043-739-533-348-567; 044-182-276-716-461; 045-370-657-517-321; 045-703-897-198-959; 045-784-109-929-691; 046-185-806-718-656; 047-601-659-074-761; 048-315-375-209-870; 048-409-388-308-824; 048-689-757-056-896; 048-815-239-786-420; 050-017-473-526-702; 050-309-456-035-316; 050-670-466-441-411; 051-755-683-440-377; 052-193-376-465-288; 053-716-884-729-661; 054-497-092-452-624; 055-077-524-438-953; 056-014-326-450-730; 057-856-366-326-440; 059-910-708-580-797; 060-480-982-083-628; 060-690-581-527-078; 062-829-382-336-363; 063-487-203-064-120; 063-552-392-937-311; 063-744-820-770-826; 064-069-666-882-597; 064-260-480-828-600; 065-578-675-565-501; 068-475-337-218-774; 070-227-961-882-758; 077-838-265-920-228; 081-388-236-484-286; 081-810-550-806-897; 082-967-915-150-15X; 091-080-067-970-284; 091-738-319-434-284; 097-609-001-652-182; 097-796-033-749-313; 100-780-754-972-566; 103-190-696-002-332; 103-628-483-546-890; 104-796-951-098-38X; 109-599-495-465-391; 112-569-956-570-735; 126-915-118-932-247; 130-428-676-855-799; 130-706-457-218-471; 135-672-762-106-299; 136-010-112-867-030; 144-326-941-755-467; 150-892-440-224-709; 155-705-623-584-577; 161-357-298-090-176; 182-651-281-482-180,115,false,,
160-192-105-496-235,A vision system to assist visually challenged people for face recognition using multi-task cascaded convolutional neural network (MTCNN) and local binary pattern (LBP),2023-02-12,2023,journal article,Journal of Ambient Intelligence and Humanized Computing,18685137; 18685145,Springer Science and Business Media LLC,Germany,A. Baskar; T. Gireesh Kumar; Sathishkumar Samiappan,,14,4,4329,4341,Computer science; Artificial intelligence; Convolutional neural network; Computer vision; Local binary patterns; Process (computing); Facial recognition system; Face (sociological concept); Face detection; Preprocessor; Pattern recognition (psychology); Histogram; Social science; Sociology; Image (mathematics); Operating system,,,,,,http://dx.doi.org/10.1007/s12652-023-04542-8,,10.1007/s12652-023-04542-8,,,0,001-449-115-071-308; 005-003-503-205-434; 009-496-095-565-142; 009-778-180-388-49X; 018-559-495-168-762; 020-505-230-186-621; 037-796-622-599-273; 047-608-581-819-808; 053-319-263-331-694; 053-846-346-961-071; 058-651-101-518-639; 065-983-543-415-033; 067-617-411-083-156; 075-519-293-886-898; 093-548-249-489-049; 095-630-816-420-503; 099-877-635-147-130; 120-658-134-176-530; 143-768-252-527-954; 167-218-101-442-181; 176-803-211-282-063,5,false,,
160-249-952-605-441,Robot Navigation in Domestic Environments: Experiences Using RGB-D Sensors in Real Homes,2018-06-29,2018,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Paloma de la Puente; Markus Bajones; Christian Reuther; Daniel Wolf; David Fischinger; Markus Vincze,,94,2,455,470,Human–computer interaction; Mobile robot; Perception; Test (assessment); Service (systems architecture); Computer science; Open set; RGB color model; Robot,,,,FP7-ICT,https://digital.csic.es/handle/10261/216179 https://dialnet.unirioja.es/servlet/articulo?codigo=7054059 https://link.springer.com/article/10.1007/s10846-018-0885-6 https://dl.acm.org/doi/10.1007/s10846-018-0885-6 https://dblp.uni-trier.de/db/journals/jirs/jirs94.html#PuenteBRWFV19 https://rd.springer.com/article/10.1007/s10846-018-0885-6,http://dx.doi.org/10.1007/s10846-018-0885-6,,10.1007/s10846-018-0885-6,2810241597,,0,001-838-021-445-136; 004-337-183-941-849; 005-090-163-926-89X; 005-220-058-793-936; 007-968-291-845-600; 012-207-758-150-234; 017-098-491-427-717; 018-869-018-748-151; 022-066-005-521-746; 024-956-424-290-45X; 026-672-312-718-982; 034-074-789-524-675; 034-409-889-859-538; 037-204-752-644-405; 043-531-725-692-714; 051-509-614-563-652; 054-572-147-459-565; 055-782-412-923-528; 056-885-863-492-083; 057-755-137-781-869; 058-611-943-553-311; 062-829-256-424-944; 066-909-359-243-113; 072-417-814-960-20X; 072-885-519-314-501; 078-775-735-773-375; 082-295-500-983-169; 089-935-860-888-664; 090-606-765-643-069; 097-396-549-562-354; 097-679-565-642-063; 109-428-732-260-458; 116-174-976-269-690; 118-739-795-272-987; 122-182-347-273-58X; 123-092-217-386-479; 134-331-555-903-613; 141-077-134-230-230; 151-967-570-127-958; 154-254-654-725-794; 157-112-636-960-622; 164-039-316-314-58X; 173-328-057-430-064; 176-621-411-593-715,6,false,,
160-323-700-920-958,Edge Cloud Collaboration Intelligent Assistive Cane for Visually Impaired People,,2023,conference proceedings article,2023 3rd International Conference on Smart Data Intelligence (ICSMDI),,IEEE,,B. Veerasamy; A.Sai Kumar Reddy; Animgi Chandu; K.Siva Sankar Reddy; K. Venkata Naga Gopi Manikanta,"The market for assistive technology for the blind and visually impaired is plagued by high prices and a lack of usefulness. These people face numerous challenges in their daily lives. This study has developed and deployed a low-cost smart assistive cane based on computer vision, sensors, and a local cloud cooperation system, specifically for people with visual impairments. Obstacle detection, fall detection, and visitor light detection features have also been developed and included to make moving easier for those with visual impairments. As part of a part-cloud cooperation strategy to improve the user experience, a photo captioning tool and an object recognition function with high-speed processing power were also developed. It shows the characteristics of low energy consumption, potent real-time performance, flexibility to a few circumstances, and convenience, which can protect the safety of visually impaired individuals while travelling and may enable them to more effectively perceive and interpret their environment.",,,,,Flexibility (engineering); Obstacle; Computer science; Cloud computing; Human–computer interaction; Visitor pattern; Visually impaired; Enhanced Data Rates for GSM Evolution; Assistive technology; Closed captioning; Object detection; Doors; Artificial intelligence; Computer vision; Statistics; Mathematics; Pattern recognition (psychology); Political science; Law; Image (mathematics); Programming language; Operating system,,,,,,http://dx.doi.org/10.1109/icsmdi57622.2023.00031,,10.1109/icsmdi57622.2023.00031,,,0,071-281-258-323-085; 098-888-570-180-630; 126-341-450-210-449; 158-632-220-272-530; 174-741-930-721-25X; 195-169-185-418-265,1,false,,
160-373-203-958-598,Assistive Device Art: aiding audio spatial location through the Echolocation Headphones,2017-10-24,2017,journal article,AI & SOCIETY,09515666; 14355655,Springer Science and Business Media LLC,Germany,Aisen Caro Chacin; Hiroo Iwata; Victoria Vesna,,33,4,583,597,Wearable computer; Psychophysics; Artificial intelligence; Human echolocation; Virtual reality; Perception; Headphones; Computer vision; Computer science; Spatial memory; Just-noticeable difference,,,,,https://link.springer.com/article/10.1007/s00146-017-0766-8 https://dblp.uni-trier.de/db/journals/ais/ais33.html#ChacinIV18 https://doi.org/10.1007/s00146-017-0766-8 https://philpapers.org/rec/CHAADA-9,http://dx.doi.org/10.1007/s00146-017-0766-8,,10.1007/s00146-017-0766-8,2766777616,,0,000-966-299-144-813; 001-702-126-426-203; 005-037-831-719-288; 009-801-631-035-144; 013-965-299-507-482; 015-867-988-453-399; 020-212-478-797-208; 053-152-853-877-062; 097-034-272-665-570; 103-480-102-601-57X; 126-701-564-004-615; 146-818-241-205-847; 175-968-900-133-396; 185-747-423-900-786,0,false,,
160-892-456-078-411,Development of a Modular Real-time Shared-control System for a Smart Wheelchair,2023-02-20,2023,journal article,Journal of Signal Processing Systems,19398018; 19398115,Springer Science and Business Media LLC,Germany,Vaishanth Ramaraj; Atharva Paralikar; Eung Joo Lee; Syed Muhammad Anwar; Reza Monfaredi,"In this paper, we propose a modular navigation system that can be mounted on a regular powered wheelchair to assist disabled children and the elderly with autonomous mobility and shared-control features. The lack of independent mobility drastically affects an individual's mental and physical health making them feel less self-reliant, especially children with Cerebral Palsy and limited cognitive skills. To address this problem, we propose a comparatively inexpensive and modular system that uses a stereo camera to perform tasks such as path planning, obstacle avoidance, and collision detection in environments with narrow corridors. We avoid any major changes to the hardware of the wheelchair for an easy installation by replacing wheel encoders with a stereo camera for visual odometry. An open source software package, the Real-Time Appearance Based Mapping package, running on top of the Robot Operating System (ROS) allows us to perform visual SLAM that allows mapping and localizing itself in the environment. The path planning is performed by the move base package provided by ROS, which quickly and efficiently computes the path trajectory for the wheelchair. In this work, we present the design and development of the system along with its significant functionalities. Further, we report experimental results from a Gazebo simulation and real-world scenarios to prove the effectiveness of our proposed system with a compact form factor and a single stereo camera.",96,3,203,214,Wheelchair; Modular design; Computer science; Stereo camera; Motion planning; Path (computing); Stereopsis; Visual odometry; Odometry; Computer vision; Software; Robot; Trajectory; Obstacle; Global Positioning System; Obstacle avoidance; Real-time computing; Artificial intelligence; Human–computer interaction; Mobile robot; Telecommunications; Physics; Astronomy; World Wide Web; Political science; Law; Programming language; Operating system,,,,,https://arxiv.org/pdf/2211.14711 https://arxiv.org/abs/2211.14711,http://dx.doi.org/10.1007/s11265-022-01828-6,,10.1007/s11265-022-01828-6,,,0,029-117-688-304-266; 053-772-382-028-941; 057-651-901-388-67X; 059-529-518-137-855; 106-162-614-011-739; 108-121-406-794-961,2,true,,green
161-375-825-142-177,An indoor scene recognition system based on deep learning evolutionary algorithms,2023-09-05,2023,journal article,Soft Computing,14327643; 14337479,Springer Science and Business Media LLC,Germany,Mouna Afif; Riadh Ayachi; Yahia Said; Mohamed Atri,,27,21,15581,15594,Computer science; Artificial intelligence; Evolutionary algorithm; Benchmark (surveying); Machine learning; Crossover; Deep learning; Genetic algorithm; Process (computing); Set (abstract data type); Population; Evolutionary computation; Facial recognition system; Face (sociological concept); Feature extraction; Social science; Demography; Geodesy; Sociology; Programming language; Geography; Operating system,,,,,,http://dx.doi.org/10.1007/s00500-023-09177-7,,10.1007/s00500-023-09177-7,,,0,000-589-329-683-885; 002-508-484-172-239; 004-269-574-716-057; 013-422-017-213-208; 015-586-423-151-288; 015-684-934-763-023; 026-400-078-913-130; 039-756-435-021-042; 044-396-615-675-996; 046-230-802-525-283; 046-512-784-872-497; 057-574-302-224-650; 057-600-100-739-337; 061-579-383-511-500; 075-013-055-683-704; 085-906-497-866-752; 094-497-075-237-835; 094-936-883-199-250; 100-334-898-943-764; 115-725-112-822-171; 142-496-868-258-083; 185-705-955-062-271; 192-486-333-277-813,1,false,,
161-459-721-523-133,Computerized spatial language generation for object location,2016-06-22,2016,journal article,Virtual Reality,13594338; 14349957,Springer Science and Business Media LLC,United Kingdom,Graciela Lara; Angélica de Antonio; Adriana Peña,"Spatial language is the syntax used for object or place locations. Because an object location is inherently relative, it implies a frame of reference, which in turn may be aided by a reference object, other than the one to be located. This reference object is commonly selected based on its perceptual salience, that is, its more prominent features. Computer systems linked to various research areas have been developed to facilitate the communication and/or interpretation of spatial language for localization tasks. In this paper is presented a literature review of computer systems that adopt spatial language and perceptual salience for object location.",20,3,183,192,Object-based spatial database; Artificial intelligence; Virtual reality; Natural language processing; Virtual machine; Syntax (programming languages); Computer science; Interpretation (logic); Object (computer science); Frame of reference; Computer graphics,,,,,https://dblp.uni-trier.de/db/journals/vr/vr20.html#LaraJP16 https://link.springer.com/content/pdf/10.1007%2Fs10055-016-0289-5.pdf https://link.springer.com/article/10.1007%2Fs10055-016-0289-5,http://dx.doi.org/10.1007/s10055-016-0289-5,,10.1007/s10055-016-0289-5,2474959782,,0,001-653-377-699-603; 002-149-629-004-382; 008-438-998-168-277; 009-441-058-590-623; 010-663-619-879-187; 014-268-266-061-165; 014-340-775-857-616; 014-970-209-578-694; 018-951-029-041-840; 026-236-800-621-927; 029-266-911-607-542; 031-233-350-877-294; 033-407-587-718-988; 033-861-920-027-874; 039-085-443-560-797; 043-573-792-286-826; 047-079-650-826-219; 054-197-497-519-888; 058-140-266-822-853; 059-784-374-408-801; 059-792-413-502-324; 065-138-611-018-534; 065-186-884-087-849; 068-062-574-164-874; 069-037-250-783-97X; 075-440-778-124-566; 077-190-855-171-803; 078-838-621-525-272; 080-524-661-732-479; 082-615-536-566-551; 084-330-008-150-29X; 089-192-527-108-965; 092-442-127-557-849; 096-943-846-117-656; 098-019-619-369-780; 098-537-836-269-874; 106-053-251-929-774; 107-051-150-853-181; 120-421-815-388-920; 124-516-202-459-359; 124-594-981-031-071; 126-711-854-196-319; 135-220-802-828-150; 152-055-521-634-261; 154-278-144-911-964; 169-933-379-397-536; 173-896-417-612-175; 187-234-939-940-497; 189-606-338-853-774; 197-215-414-869-180,2,true,"CC BY, CC BY-NC-ND",gold
161-740-823-890-676,Applications of a 3D Range Camera Towards Healthcare Mobility Aids,,,conference proceedings article,"2006 IEEE International Conference on Networking, Sensing and Control",,IEEE,,Roger V. Bostelman; Paola Russo; James S. Albus; Tsai Hong; Raj Madhavan,"The National Institute of Standards and Technology (NIST) has recently studied a new 3D range camera for use on mobile robots. These robots have potential applications in manufacturing, healthcare and perhaps several other service related areas beyond the scope of this paper. In manufacturing, the 3D range camera shows promise for standard size obstacle detection possibly augmenting existing safety systems on automated guided vehicles. We studied the use of this new 3D range imaging camera for advancing safety standards for automated guided vehicles. In healthcare, these cameras show promise for guiding the blind and assisting the disabled who are wheelchair dependent. Further development beyond standards efforts allowed NIST to combine the 3D camera with stereo audio feedback to help the blind or visually impaired to stereophonically hear where a clear path is from room to room as objects were detected with the camera. This paper describes the 3D range camera and the control algorithm that combines the camera with stereo audio to help guide people around objects, including the detection of low hanging objects typically undetected by a white cane.",,,416,421,NIST; Engineering; System safety; Stereophonic sound; Artificial intelligence; Smart camera; Mobile robot; Object detection; Wheelchair; Computer vision; Robot,,,,,https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=823581 https://ieeexplore.ieee.org/document/1673182/ https://www.nist.gov/publications/applications-3d-range-camera-towards-healthcare-mobility-aids http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.ieee-000001673182 http://ieeexplore.ieee.org/document/1673182/,http://dx.doi.org/10.1109/icnsc.2006.1673182,,10.1109/icnsc.2006.1673182,1501515607,,0,007-907-693-094-599; 048-298-689-305-353; 050-801-739-300-831; 057-842-726-241-671; 098-418-441-499-649; 101-315-973-280-60X; 104-251-292-619-144; 106-733-069-431-565; 136-055-407-942-906; 167-278-684-701-142,32,false,,
161-764-375-784-701,Smart Tramway Systems for Smart Cities: A Deep Learning Application in ADAS Systems,2022-08-27,2022,journal article,International Journal of Intelligent Transportation Systems Research,13488503; 18688659,Springer Science and Business Media LLC,Germany,Marco Guerrieri; Giuseppe Parla,"<jats:title>Abstract</jats:title><jats:p>Artificial intelligence and deep learning-based techniques undoubtedly are the future of Advanced Driver-Assistance Systems (ADAS) technologies. In this article is presented a technique for detecting, recognizing and tracking pedestrians, vehicles and cyclists along a tramway infrastructure in a complex urban environment by Computer Vision, Deep Learning approaches and YOLOv3 algorithm. Experiments have been conducted in the tramway Line 2 “Borgonuovo –Notarbartolo” (Palermo, Italy) in correspondence of the tramway segments crossing a roundabout having an external diameter of 24 m. A survey vehicle equipped with a video camera was used in the study. The results of the research show that the proposed method is able to search and detect the position and the speed of road users near and over the rails in front of the tram in a very precise way as demonstrate by the estimated values of the Accuracy, Loss and Precision obtained during the neural networks training process. The implementation of this advanced detection method in ADAS systems may increase the safety of novel autonomous trams and autonomous rapid trams (ARTs).</jats:p>",20,3,745,758,Computer science; Advanced driver assistance systems; Artificial intelligence; Artificial neural network; Deep learning; Roundabout; Process (computing); Intersection (aeronautics); Intelligent transportation system; Real-time computing; Computer vision; Simulation; Transport engineering; Engineering; Operating system,,,,Università degli Studi di Trento,https://link.springer.com/content/pdf/10.1007/s13177-022-00322-4.pdf https://doi.org/10.1007/s13177-022-00322-4 https://iris.unitn.it/bitstream/11572/352620/3/s13177-022-00322-4.pdf https://hdl.handle.net/11572/352620,http://dx.doi.org/10.1007/s13177-022-00322-4,,10.1007/s13177-022-00322-4,,,0,004-070-135-899-180; 006-575-114-759-898; 007-855-668-586-872; 013-191-910-506-390; 014-356-721-290-470; 020-563-221-715-320; 024-131-470-445-452; 024-455-760-804-83X; 028-885-576-322-190; 031-413-569-391-908; 031-693-401-176-939; 033-389-805-520-798; 034-557-046-590-15X; 039-187-088-310-888; 041-584-837-549-708; 042-591-505-734-992; 044-425-404-619-409; 048-254-076-918-785; 049-317-239-158-314; 055-814-010-570-17X; 064-683-024-880-936; 065-368-943-890-584; 066-133-135-091-645; 068-782-060-108-790; 069-823-929-409-114; 070-378-214-085-446; 073-990-207-962-674; 075-314-482-896-586; 078-982-399-265-161; 079-648-430-994-577; 089-888-855-554-169; 098-566-016-480-837; 098-926-109-358-108; 102-066-935-584-436; 115-954-501-432-918; 121-921-875-311-455; 125-112-083-666-544; 152-904-938-249-207; 174-929-281-395-312,6,true,cc-by,hybrid
162-153-535-514-940,SceneRecog: A Deep Learning Scene Recognition Model for Assisting Blind and Visually Impaired Navigate using Smartphones,2021-10-17,2021,conference proceedings article,"2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",,IEEE,,Bineeth Kuriakose; Raju Shrestha; Frode Eika Sandnes,"Deep learning models have recently gained popularity in the research community due to their high classification success rates. In this paper, we proposed an EfficientNet-Lite based scene recognition model for scene recognition as a part of the smartphone-based navigation support application for the blind and visually impaired. We created a custom dataset with both indoor and outdoor scenes for training and testing of the model. The main objective of this work is to support people with visual impairments navigate by providing information about the scene via a smartphone application. The results from the experiment show encouraging performance from the proposed model. As a proof of concept, a prototype app was developed on the Android platform. However, the model can be implemented and deployed in any modern smartphone with good processing power.",,,,,Computer science; Android (operating system); Visually impaired; Popularity; Deep learning; Artificial intelligence; Mobile device; Smartphone app; Android app; Human–computer interaction; Computer vision; Multimedia; World Wide Web; Psychology; Social psychology; Operating system,,,,,https://oda.oslomet.no/oda-xmlui/bitstream/11250/2988634/5/IEEE_SMC_SceneRecog__the_Original_Copy_submitted_.pdf https://hdl.handle.net/11250/2988634,http://dx.doi.org/10.1109/smc52423.2021.9658913,,10.1109/smc52423.2021.9658913,,,0,003-456-452-725-171; 005-224-533-356-769; 012-344-135-607-729; 015-586-423-151-288; 023-438-345-162-875; 032-284-166-524-962; 034-073-120-160-844; 057-847-555-860-541; 061-071-256-460-520; 067-894-235-988-508; 078-426-780-912-340; 079-310-940-223-758; 079-433-787-480-087; 088-822-320-203-25X; 098-440-015-522-27X; 101-291-246-644-071; 106-644-239-684-326; 121-707-574-802-675; 141-649-000-612-322; 180-559-374-049-445,7,true,,green
162-876-435-356-737,Real-time Object Detection for Visually Impaired with Optimal Combination of Scores,2019-03-13,2019,conference proceedings,,,,,Siddharth Sagar Nijhawan; Aditi Kumar; Shubham Bhardwaj; Geeta Nijhawan,"In this paper, we propose a computer vision based object detection mechanism for visually impaired individuals by optimally combining the detection scores to aid their indoor navigation. Proposed framework comprises of three stages, viz. image acquisition, object detection and class estimation followed by score combination to generate the final classes of objects. The detection is performed on the image frames captured through wireless webcam and by applying two powerful deep learning algorithms, SSD and YOLO, separately. After generating the scores for each class, they are optimally combined using Proportional Conflict Resolving principle to generate fused set of scores. Based on these score values, object category is chosen with the category having the highest probabilistic score. The particular audio file narrating the object name is selected and the audio signal is sent to the blind user through the attached wearable headphone. The proposed framework achieved a high Mean Average Precision score of 86.15. Model performs well in an indoor environment and successfully aids a blind individual in mobility and mapping surrounding environment effectively.",,,307,311,Deep learning; Wearable computer; Artificial intelligence; Set (abstract data type); Class (computer programming); Object detection; Computer vision; Computer science; Probabilistic logic; Object (computer science); Audio signal,,,,,https://ieeexplore.ieee.org/abstract/document/8991199,https://ieeexplore.ieee.org/abstract/document/8991199,,,3006548563,,0,004-193-609-931-550; 004-213-610-677-073; 004-766-408-593-108; 037-697-337-909-486; 045-298-655-695-753; 069-480-623-909-181; 071-895-492-147-282; 139-149-263-291-329,0,false,,
163-071-105-957-992,Vision Maker: An Audio Visual And Navigation Aid For Visually Impaired Person,,2020,conference proceedings article,2020 International Conference on Intelligent Engineering and Management (ICIEM),,IEEE,,Sagor Saha; Farhan Hossain Shakal; Ahmed Mortuza Saleque; Jerin Jahan Trisha,"People with low vision or complete loss of vision face challenging task to meet their daily demand. The barrier low vision hinders them participating in the society. The evolution of computer vision, artificial intelligence and machine learning proved to effective tool in revitalizing the situation of blind. The propounded design represents implementation of an assistive device that can aid them in recognizing the object. The low-cost device runs a pre-trained model (ssdlite_mobilenet_v2_coco) and can identify up to 80 classes. The user can read any text (English) from images. Ultrasonic sensors have been used in the custom-made device to continuously alert the user of obstacles from all directions. Additionally, it can help navigate them in outdoor using Google map API. The user can read news, listen to music and mail to the member of choice. The command is passed and received through earphone, which is connected in the audio jack of raspberry pi.",,,266,271,Human–computer interaction; Task (project management); Low vision; Navigation aid; Audio visual; Assistive device; Visually impaired; Computer science; Object (computer science); Cognitive neuroscience of visual object recognition,,,,,https://ieeexplore.ieee.org/document/9160169,http://dx.doi.org/10.1109/iciem48762.2020.9160169,,10.1109/iciem48762.2020.9160169,3047705270,,0,000-259-572-898-34X; 017-952-244-691-65X; 030-822-547-527-000; 032-661-597-680-28X; 052-154-945-556-605; 062-265-879-417-563; 088-038-718-945-73X,4,false,,
163-837-935-935-772,An effective obstacle detection system using deep learning advantages to aid blind and visually impaired navigation,,2024,journal article,Ain Shams Engineering Journal,20904479; 20904495,Elsevier BV,Egypt,Ahmed Ben Atitallah; Yahia Said; Mohamed Amin Ben Atitallah; Mohammed Albekairi; Khaled Kaaniche; Sahbi Boubaker,"Blind and visually impaired people face different challenges when navigating indoors and outdoors. In this context, we suggest developing an obstacle detection system based on a modified YOLO v5 neural network architecture. The suggested system is capable of recognizing and locating a set of landmark indoor and outdoor objects that are extremely useful for Blind and Visually Impaired (BVI) navigation aids. Training and evaluation experiments were conducted using two datasets: the IODR dataset for indoor object detection and the MS COCO dataset for outdoor object detection. We used several optimization strategies, such as model width scaling, quantization, and channel pruning, to guarantee that the suggested work is implemented in embedded devices in a lightweight manner. The proposed system was successful in achieving results that were extremely competitive in terms of processing time as well as the precision of obstacle detection.",15,2,102387,102387,Obstacle; Landmark; Computer science; Artificial intelligence; Object detection; Computer vision; Classifier (UML); Context (archaeology); Pattern recognition (psychology); Paleontology; Political science; Law; Biology,,,,"Deanship of Scientific Research, University of Jordan; Jouf University",,http://dx.doi.org/10.1016/j.asej.2023.102387,,10.1016/j.asej.2023.102387,,,0,005-135-800-876-298; 008-535-352-606-703; 020-581-037-678-426; 026-400-078-913-130; 026-994-702-158-53X; 034-904-777-487-838; 035-155-677-944-748; 037-182-418-585-695; 038-326-446-864-583; 045-309-399-228-849; 053-826-604-836-17X; 055-495-282-581-758; 057-442-070-408-80X; 061-579-383-511-500; 066-560-690-089-073; 066-622-591-843-254; 075-465-285-579-483; 086-099-430-510-039; 086-731-195-129-866; 099-334-901-486-872; 104-982-807-825-22X; 109-525-288-959-67X; 164-321-188-343-982,5,true,"CC BY, CC BY-NC-ND",gold
163-882-574-876-14X,Computer Assisted ENT Surgery,2013-05-11,2013,journal article,International Journal of Computer Assisted Radiology and Surgery,18616410; 18616429,Springer Science and Business Media LLC,Germany,,,8,S1,101,107,Computer science; Medicine; Medical physics; General surgery,,,,,,http://dx.doi.org/10.1007/s11548-013-0857-z,,10.1007/s11548-013-0857-z,,,0,,0,false,,
164-000-850-944-540,Integrated AI Based Smart Wearable Assistive Device for Visually and Hearing-Impaired People,2023-02-10,2023,conference proceedings article,2023 International Conference on Recent Trends in Electronics and Communication (ICRTEC),,IEEE,,Rajesh Kannan S; Ezhilarasi P; Rajagopalan VG; Sushanth Krishnamithran; Ramakrishnan H; Harish Kumar Balaji,"An Intelligent Robotics Device (IRD) is developed in order help the disabled people and elderly people with day-to-day activities. The system offers five main functions: obstacle detection and avoidance through bone conduction, live tracking, GPS navigation, GSM SOS alert system and AI based image and face recognition System. It works with a combination of ultrasonic detection and bone conduction, detecting obstacles and letting the user know about them. To assure safe mobility, the product offers some core features namely, GPS live tracking and GPS navigation. GPS live tracking feature is useful in case if any of the user's relative wants to continuously monitor the movements of the blind user. The GSM - SOS alert system is included in the product, which comes into handy when the user finds any exigencies while travelling with just a push of button, the SOS alert system will send customized help request SMS along with location link to their career. GPS navigation system sends alert vibrations regarding the directions to be taken for navigating from one place to another. Those alerts are conveyed to the user through bone conduction phones. Finally, AI-facial and image recognition feature is also included which helps the blind to distinguish between known person and unknown person. All the electronic components which support these features are assembled and embedded on a wearable vest for ease of access.",,,,,Global Positioning System; Computer science; GSM; Wearable computer; Feature (linguistics); Human–computer interaction; Navigation system; Tracking system; Computer vision; Artificial intelligence; Embedded system; Telecommunications; Philosophy; Linguistics; Filter (signal processing),,,,,,http://dx.doi.org/10.1109/icrtec56977.2023.10111863,,10.1109/icrtec56977.2023.10111863,,,0,003-092-190-766-189; 013-501-956-117-041; 019-813-021-570-132; 023-930-680-674-290; 032-793-149-773-524; 037-199-326-802-024; 038-514-510-027-113; 054-821-376-983-912; 061-565-103-534-998; 071-609-193-513-123; 077-082-781-691-428; 093-548-249-489-049; 146-774-499-684-441,0,false,,
164-918-173-421-003,Computer Vision Extended Perception System for Blind People,2022-11-09,2022,conference proceedings article,2022 IEEE 40th Central America and Panama Convention (CONCAPAN),,IEEE,,Federico Machado; Alberto Marroquin; Jose Antonio Fuentes,"There is a considerable number of visually impaired people in the world, who live their daily lives with limitations in their mobility due to the little information they have about their environment. This project proposes to use embedded systems to develop an assistance system for the blind, which, through machine learning models, describes the objects closest to the user and if they are known or unknown. To do this, two trained Machine Learning models will be used, the first to detect common objects and the second to identify particular objects. The intention of using both networks is to improve the accuracy of the system, by first detecting objects or people in an image and then classifying them with known labels (names or some identifier). The results obtained show the benefit of using these neural networks for the recognition of objects in the environment from a general database and then with a personalized one. Finally, to indicate the identified objects to blind people, the text of their labels is translated into speech.",,,,,Computer science; Identifier; Perception; Artificial intelligence; Artificial neural network; Computer vision; Human–computer interaction; Machine learning; Neuroscience; Biology; Programming language,,,,,,http://dx.doi.org/10.1109/concapan48024.2022.9997641,,10.1109/concapan48024.2022.9997641,,,0,015-040-823-132-436; 018-843-508-574-144; 051-032-426-226-397; 062-678-263-012-184; 068-526-647-480-345; 095-211-223-584-688; 098-910-337-444-079; 164-332-313-558-557; 189-574-166-154-171,0,false,,
165-330-207-554-488,Research on GDR Obstacle Detection Method Based on Stereo Vision,2024-03-07,2024,journal article,Automatic Control and Computer Sciences,01464116; 1558108x,Allerton Press,United States,null Jing Hou; Meimei Chen; Yihang Guo; Zhangxi Lin; Bin Hong,,58,1,90,100,Computer science; Obstacle; Computer vision; Stereopsis; Artificial intelligence; Law; Political science,,,,,,http://dx.doi.org/10.3103/s0146411624010061,,10.3103/s0146411624010061,,,0,000-394-661-764-185; 009-612-733-037-965; 015-417-609-922-215; 023-348-887-030-923; 042-125-442-624-332; 044-973-901-680-240; 047-326-510-583-783; 071-458-225-992-021; 075-519-293-886-898; 077-674-235-714-556; 098-811-589-762-176; 189-536-420-158-286; 192-751-999-674-131,0,false,,
165-646-371-179-035,HRI - Robotic Assistance in Indoor Navigation for People who are Blind,2016-03-07,2016,conference proceedings,,,,,Aditi Kulkarni; Allan Wang; Lynn Urbina; Aaron Steinfeld; Bernardine Dias,"In this paper, we describe the process of making a robot useful as a guide robot for people who are blind or visually impaired. For this group, the interactive audio feature of a robot assumes a very high level of importance. We have introduced some features that will help to make the robot sound natural and be more comfortable. We first addressed the question of the speaker placement to help the user determine the size and distance of the robot. After the initial meeting, user data will be retained by the robot so that their communication evolves with every interaction. The robot will also ask the users if they need to take a rest after a specified interval depending upon the user's age and the distance they need to cover. The next time they visit, all this information will be used to make the interaction more natural and customized for each individual user.",,,461,462,Ubiquitous robot; Social robot; Personal robot; Robot learning; Mobile robot; Mobile robot navigation; Computer science; Multimedia; Robot control; Robot,,,,,http://dblp.uni-trier.de/db/conf/hri/hri2016.html#KulkarniWUSD16 https://dblp.uni-trier.de/db/conf/hri/hri2016.html#KulkarniWUSD16 https://ieeexplore.ieee.org/document/7451806/ http://ieeexplore.ieee.org/document/7451806/ https://dl.acm.org/doi/10.5555/2906831.2906927,https://dblp.uni-trier.de/db/conf/hri/hri2016.html#KulkarniWUSD16,,,2343174257,,0,027-244-764-182-811; 038-215-720-360-854; 061-472-591-708-148; 080-715-271-794-190; 164-507-506-093-735,13,false,,
166-158-315-750-560,LiDAR + Camera Sensor Data Fusion On Mobiles With AI-based Virtual Sensors To Provide Situational Awareness For The Visually Impaired,2021-08-23,2021,conference proceedings article,2021 IEEE Sensors Applications Symposium (SAS),,IEEE,,Vivek Bharati,"Autonomy of the blind and visually impaired can be achieved through technological means and thereby empowering them with a sense of independence. Mobile phones are ubiquitous and can access artificial intelligence capabilities locally and in the Cloud. Navigational sensors, such as Light Detection and Ranging (LiDAR), and wide angle cameras, typically found in self-driving cars, are beginning to be incorporated into mobile phones. In this paper, we propose techniques for using mobile phone LiDAR + camera sensor data fusion along with edge + Cloud split AI to create an indoor situational awareness and navigational aid for the visually impaired. In addition to physical sensors, the system uses AI models as virtual sensors to provide the required functionality. The system enhances the image of a scene captured by a camera using distance information from the LiDAR and directional information computed by the device to provide a rich 3-D description of the space in front of the user. The system also uses a combination of sensor data fusion and geometric formulas to provide step-by-step walking instructions for the user in order to reach destinations. The user-centric system proposed here can be a valuable assistive technology for the blind and visually imnpired.",,,,,Situation awareness; Artificial intelligence; Enhanced Data Rates for GSM Evolution; Mobile phone; Navigational aid; Computer vision; Sensor fusion; Computer science; Lidar; Image sensor; Cloud computing,,,,,http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=9530102 http://xplorestaging.ieee.org/ielx7/9530004/9530006/09530102.pdf?arnumber=9530102,http://dx.doi.org/10.1109/sas51076.2021.9530102,,10.1109/sas51076.2021.9530102,3199203601,,0,006-409-052-293-645; 007-415-792-829-203; 010-595-802-848-539; 025-522-385-395-133; 027-741-662-508-101; 041-188-362-118-693; 051-444-926-333-456; 088-579-849-092-361; 128-230-459-731-903; 141-781-358-206-44X,8,false,,
168-409-424-725-248,"Workshop on Computer Vision Applications for the Visually Impaired (CVAVI 08), Satellite Workshop of the European Conference on Computer Vision (ECCV 2008) - Guiding the focus of attention of blind people with visual saliency",,2008,book chapter,,,,,Beno ˆ õt Deville; Guido Bologna; Michel Vinckenbosch; Thierry Pun,"The context of this work is the development of a mobility aid for visually impaired persons. We present here an original approach for a real time alerting system, based on the detection of visual salient parts in videos. The particularity of our approach lies in the use of a new feature map constructed from the depth gradient. A distance function is described, which takes into account both stereoscopic camera limitations and user's choices. We also report how we automatically estimate the contribution of conspicuity maps, which enables the unsupervised determination of the final saliency map. We demonstrate here that this additional depth-based feature map allows the system to detect salient regions with good accuracy in most situations, even in the presence of noisy disparity maps.",,,,,Artificial intelligence; Stereo camera; Focus (optics); Geography; Saliency map; Context (language use); Mobility aid; Salient; Visual saliency; Computer vision; Feature (computer vision),,,,,https://archive-ouverte.unige.ch/unige:47670 https://archive-ouverte.unige.ch/unige:47670/ATTACHMENT01,https://archive-ouverte.unige.ch/unige:47670,,,963908825,,0,005-000-267-388-326; 009-785-505-046-945; 013-847-555-426-159; 018-386-309-946-717; 025-330-815-941-573; 042-334-901-938-383; 046-627-582-261-33X; 053-360-092-698-380; 053-816-807-752-460; 061-026-747-504-185; 063-749-314-790-077; 068-446-370-600-681; 083-245-705-135-463; 096-717-380-007-985; 124-535-130-019-04X; 135-646-014-794-306; 180-993-031-508-787; 181-797-068-816-79X,10,false,,
168-511-383-336-908,Walking Using Touch: Design and Preliminary Prototype of a Non-Invasive ETA for the Visually Impaired,,2005,journal article,Conference proceedings : ... Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual Conference,1557170x,,United States,Ramiro Velazquez; Edwige Pissaloux; J.C. Guinot; Flavien Maingreaud,"This paper presents the design and preliminary prototype of the Intelligent Glasses, a novel non-invasive electronic travel aid (ETA) designed to assist the blind/visually impaired to navigate easily, safely and quickly among obstacles in indoor/outdoor 3D environments. The Intelligent Glasses, a joint project between the Robotics Laboratory of Paris (LRP) and the French Atomic Energy Commission (CEA), is based on a visuo-tactile system. Two mini-cameras mounted on the user's eyeglasses frame detect the obstacles ahead the walking course and translate this information to a tactile display as a map representing where the obstacles are located in the scene. The user freely explores the tactile map and is able to follow it easily, demonstrating initial feasibility of the system",7,,6821,6824,Frame (networking); Engineering; Artificial intelligence; Display device; Tactile map; Tactile display; Atomic energy commission; Non invasive; Visually impaired; Computer vision; Robotics,,,,,http://europepmc.org/abstract/MED/17281840 http://ieeexplore.ieee.org/iel5/10755/33900/01616071.pdf https://www.researchgate.net/profile/Ramiro_Velazquez2/publication/6522577_Walking_Using_Touch_Design_and_Preliminary_Prototype_of_a_Non-Invasive_ETA_for_the_Visually_Impaired/links/53d12e940cf2a7fbb2e62cb2.pdf https://pubmed.ncbi.nlm.nih.gov/17281840/ https://www.ncbi.nlm.nih.gov/pubmed/17281840 http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.ieee-000001616071,http://dx.doi.org/10.1109/iembs.2005.1616071,17281840,10.1109/iembs.2005.1616071,2130184906,,0,002-324-120-239-598; 030-127-369-356-741; 031-612-744-612-651; 057-730-863-731-566; 077-303-325-289-693; 078-736-735-476-395; 094-303-367-125-634; 106-716-865-707-337; 133-457-684-144-980; 135-646-014-794-306; 157-759-506-298-834; 191-001-801-688-935,28,false,,
168-576-633-997-31X,Robots beyond Science Fiction: mutual learning in human–robot interaction on the way to participatory approaches,2021-04-28,2021,journal article,AI & SOCIETY,09515666; 14355655,Springer Science and Business Media LLC,Germany,Astrid Weiss; Katta Spiel,"<jats:title>Abstract</jats:title><jats:p>Putting laypeople in an active role as direct expert contributors in the design of service robots becomes more and more prominent in the research fields of human–robot interaction (HRI) and social robotics (SR). Currently, though, HRI is caught in a dilemma of how to create meaningful service robots for human social environments, combining expectations shaped by popular media with technology readiness. We recapitulate traditional stakeholder involvement, including two cases in which new intelligent robots were conceptualized and realized for close interaction with humans. Thereby, we show how the <jats:italic>robot narrative</jats:italic> (impacted by science fiction, the term robot itself, and assumptions on human-like intelligence) together with aspects of <jats:italic>power balancing stakeholders</jats:italic>, such as hardware constraints and missing perspectives beyond primary users, and the <jats:italic>adaptivity of robots</jats:italic> through machine learning that creates unpredictability, pose specific challenges for participatory design processes in HRI. We conclude with thoughts on a way forward for the HRI community in developing a culture of participation that considers humans when conceptualizing, building, and using robots.</jats:p>",37,2,501,515,,,,,TU Wien,,http://dx.doi.org/10.1007/s00146-021-01209-w,,10.1007/s00146-021-01209-w,,,0,000-569-656-302-906; 000-882-063-890-336; 001-356-021-504-35X; 002-841-263-762-963; 005-290-353-503-712; 006-845-382-105-919; 007-174-955-444-291; 010-304-914-409-423; 010-944-541-987-129; 011-219-217-906-117; 011-690-056-024-459; 011-813-664-094-802; 013-187-782-852-261; 013-584-286-904-041; 014-079-893-018-745; 014-239-890-623-51X; 017-675-822-842-665; 019-286-493-032-883; 020-938-755-547-868; 022-066-005-521-746; 022-355-699-049-335; 029-743-609-302-843; 031-523-262-179-467; 032-157-460-226-964; 034-750-900-787-280; 035-060-739-680-660; 036-514-906-284-850; 036-937-327-271-857; 038-213-843-181-582; 038-632-336-668-691; 038-800-918-773-221; 040-611-088-569-158; 043-531-725-692-714; 044-366-595-182-715; 045-971-710-456-681; 046-790-997-308-638; 048-632-477-076-115; 050-741-236-398-003; 052-979-802-300-895; 053-075-356-722-784; 055-355-831-143-167; 056-867-618-412-167; 059-802-217-610-666; 060-241-997-432-124; 060-309-362-177-025; 061-180-777-562-381; 061-448-183-376-316; 065-266-241-920-534; 066-909-359-243-113; 069-788-983-285-289; 070-073-307-254-774; 070-329-329-281-673; 072-050-889-156-71X; 072-579-855-400-518; 075-741-365-972-355; 076-734-986-716-873; 080-166-816-304-691; 081-361-185-273-963; 085-191-484-133-019; 089-047-954-076-397; 089-956-718-709-832; 094-871-870-293-980; 095-288-181-794-825; 096-680-364-271-042; 100-965-379-724-691; 104-096-765-444-763; 107-075-472-189-929; 112-755-249-649-54X; 119-857-645-979-294; 123-965-494-489-791; 137-664-137-936-927; 141-343-861-864-730; 147-760-930-407-118; 157-112-636-960-622; 159-717-395-395-515; 160-589-467-354-563; 166-397-431-043-570; 166-429-981-514-836,29,true,cc-by,hybrid
168-670-604-274-48X,COMPRA: A COMPact Reactive Autonomy Framework for Subterranean MAV Based Search-And-Rescue Operations,2022-06-23,2022,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Björn Lindqvist; Christoforos Kanellakis; Sina Sharif Mansouri; Ali-akbar Agha-mohammadi; George Nikolakopoulos,,105,3,,,,,,,Horizon 2020; Interreg Nord; Horizon 2020; Lulea University of Technology,,http://dx.doi.org/10.1007/s10846-022-01665-6,,10.1007/s10846-022-01665-6,,,0,004-611-080-133-522; 009-906-981-464-070; 015-363-270-574-088; 015-643-587-460-659; 016-618-456-868-98X; 017-201-651-700-317; 017-575-182-761-622; 027-337-363-635-122; 033-082-107-358-555; 045-052-763-039-868; 049-633-720-767-065; 053-577-504-876-719; 061-155-380-714-530; 067-983-077-920-296; 103-663-967-050-123; 123-843-936-162-038; 154-486-729-901-848; 193-034-129-765-122,19,false,,
168-976-443-486-757,"Viia-hand: a Reach-and-grasp Restoration System Integrating Voice interaction, Computer vision and Auditory feedback for Blind Amputees",2023-01-01,2023,preprint,arXiv (Cornell University),,,,Chunhao Peng; Dapeng Yang; Ming Cheng; Jinghui Dai; Deyu Zhao; Li Jiang,"Visual feedback plays a crucial role in the process of amputation patients completing grasping in the field of prosthesis control. However, for blind and visually impaired (BVI) amputees, the loss of both visual and grasping abilities makes the ""easy"" reach-and-grasp task a feasible challenge. In this paper, we propose a novel multi-sensory prosthesis system helping BVI amputees with sensing, navigation and grasp operations. It combines modules of voice interaction, environmental perception, grasp guidance, collaborative control, and auditory/tactile feedback. In particular, the voice interaction module receives user instructions and invokes other functional modules according to the instructions. The environmental perception and grasp guidance module obtains environmental information through computer vision, and feedbacks the information to the user through auditory feedback modules (voice prompts and spatial sound sources) and tactile feedback modules (vibration stimulation). The prosthesis collaborative control module obtains the context information of the grasp guidance process and completes the collaborative control of grasp gestures and wrist angles of prosthesis in conjunction with the user's control intention in order to achieve stable grasp of various objects. This paper details a prototyping design (named viia-hand) and presents its preliminary experimental verification on healthy subjects completing specific reach-and-grasp tasks. Our results showed that, with the help of our new design, the subjects were able to achieve a precise reach and reliable grasp of the target objects in a relatively cluttered environment. Additionally, the system is extremely user-friendly, as users can quickly adapt to it with minimal training.",,,,,GRASP; Human–computer interaction; Gesture; Computer science; Process (computing); Auditory feedback; Perception; Haptic technology; Task (project management); Computer vision; Sensory substitution; Control (management); Artificial intelligence; Engineering; Psychology; Systems engineering; Neuroscience; Programming language; Operating system,,,,,https://arxiv.org/abs/2308.06891,http://dx.doi.org/10.48550/arxiv.2308.06891,,10.48550/arxiv.2308.06891,,,0,,0,true,other-oa,green
169-557-314-937-611,Embedded Design of Smart Helmet for Physically Impaired,2021-11-27,2021,conference proceedings article,2021 Innovations in Power and Advanced Computing Technologies (i-PACT),,IEEE,,C N Sujatha; Abhishek Gudipalli; Hari Priya M; Sahithi Reddy P; Hamsini R,"The work proposed in this paper is a prototype for visually impaired people to identify common real objects, ensure their safety in public transportation and during panic situations. Here, we are to replace blind aided stick with Smart Helmet and integrating some features. This is carried over Embedded technology with the help of Machine Learning techniques. Computer Vision algorithm is implemented on Raspberry Pi detects Objects and warns the visually impaired person through speech in order to navigate properly. TensorFlow framework is used for real-time object detection in ML. This system has Panic Alert switch which the visually impaired can press when they are in an uncomfortable scenario. When a Panic Alert is triggered, our system sends the location, which was recorded using the GPS module, to the emergency contacts through GSM. GPS receiver will communicate with satellites and them it will calculate the latitude and longitude to track the location. Flite (Festival lite) is used to convert from text to speech. The primary goal of this work is to create a Helmet with convenient size and easy to navigation aid for Visually impaired which helps in Artificial Visualization by providing facts about the environmental scenario and forceful objects around them.",,,,,Global Positioning System; Computer science; Visualization; Geographic coordinate system; GSM; Raspberry pi; Human–computer interaction; Real-time computing; Computer vision; Artificial intelligence; Simulation; Computer security; Telecommunications; Internet of Things; Geodesy; Geography,,,,,,http://dx.doi.org/10.1109/i-pact52855.2021.9696754,,10.1109/i-pact52855.2021.9696754,,,0,,0,false,,
170-387-846-090-147,CVPR Workshops - Robot vision for the visually impaired,,2010,conference proceedings article,2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops,,IEEE,,Vivek Pradeep; Gerard Medioni; James D. Weiland,"We present a head-mounted, stereo-vision based navigational assistance device for the visually impaired. The head-mounted design enables our subjects to stand and scan the scene for integrating wide-field information, compared to shoulder or waist-mounted designs in literature which require body rotations. In order to extract and maintain orientation information for creating a sense of egocentricity in blind users, we incorporate visual odometry and feature based metric-topological SLAM into our system. Using camera pose estimates with dense 3D data obtained from stereo triangulation, we build a vicinity map of the user's environment. On this map, we perform 3D traversability analysis to steer subjects away from obstacles in the path. A tactile interface consisting of microvibration motors provides cues for taking evasive action, as determined by our vision processing algorithms. We report experimental results of our system (running at 10 Hz) and conduct mobility tests with blindfolded subjects to demonstrate the usefulness of our approach over conventional navigational aids like the white cane.",,,15,22,Interface (computing); Artificial intelligence; Computer vision; Visualization; Computer science; Pose; Triangulation (computer vision); Visual odometry; Stereopsis; Simultaneous localization and mapping; Orientation (computer vision),,,,,https://ieeexplore.ieee.org/abstract/document/5543579 http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.ieee-000005543579 http://ieeexplore.ieee.org/document/5543579/ https://dblp.uni-trier.de/db/conf/cvpr/cvprw2010.html#PradeepMW10 http://dblp.uni-trier.de/db/conf/cvpr/cvprw2010.html#PradeepMW10 https://doi.org/10.1109/CVPRW.2010.5543579,http://dx.doi.org/10.1109/cvprw.2010.5543579,,10.1109/cvprw.2010.5543579,2123388120,,1,018-320-986-636-810; 028-303-905-295-179; 029-674-812-337-965; 030-560-748-556-898; 031-768-944-008-214; 034-226-774-008-192; 034-551-334-308-190; 035-081-608-239-253; 035-291-375-623-97X; 039-061-928-084-526; 043-825-697-843-020; 050-130-070-252-430; 053-574-289-747-231; 056-213-005-468-886; 056-294-604-111-077; 065-992-777-809-97X; 074-006-536-890-200; 078-736-735-476-395; 091-049-551-075-26X; 120-742-669-010-74X; 125-711-223-847-828; 128-907-188-513-761; 138-774-270-601-871; 153-697-872-076-389; 176-619-480-167-936; 181-797-068-816-79X; 193-123-800-849-774; 195-842-744-142-232,113,false,,
171-057-734-143-202,Blind Assistance System Using Machine Learning,2022-07-29,2022,book chapter,Lecture Notes in Networks and Systems,23673370; 23673389,Springer International Publishing,,Naveen Kumar; Sanjeevani Sharma; Ilin Mariam Abraham; S. Sathya Priya,"Blindness is one of the most frequent and debilitating of the various disabilities. There are million visually impaired people in the globe, according to the World Health Organization (WHO). The proposed system is designed to aid visually impaired persons with real-time obstacle detection, avoidance, indoors and out navigation, and actual position tracking. The gadget proposed is a camera-visual detection hybrid that performs well in low light as part of the recommended technique, this method is utilized to detect and avoid impediments, as well as to aid visually impaired persons in identifying the environment around them. A simple and effective method for people with visual impairments to identify things in their environment and convert them into speech for improved comprehension and navigation. Along with these, the depth estimation, which calculates the safe distance between the object and the person, allowing them to be more self-sufficient and less reliant on others. This were able to achieve this model with the help of TensorFlow and pre-trained models. The approach is suggest is dependable, inexpensive, practical, and practicable.",,,419,432,Gadget; Computer science; Artificial intelligence; Obstacle; Computer vision; Blindness; Human–computer interaction; Machine learning; Optometry; Geography; Medicine; Archaeology; Algorithm,,,,,,http://dx.doi.org/10.1007/978-3-031-12413-6_33,,10.1007/978-3-031-12413-6_33,,,0,000-130-488-383-514; 014-019-198-784-152; 019-827-712-091-154; 020-343-119-550-545; 023-309-209-821-636; 031-127-219-937-467; 038-208-649-693-158; 050-047-706-859-59X; 050-620-337-235-317; 076-848-734-723-191; 080-589-710-669-822; 100-869-464-816-806; 117-677-849-214-196; 126-998-084-890-398; 137-973-182-180-54X,7,false,,
171-161-600-965-374,Dataset and semantic based-approach for image sonification,2022-05-03,2022,journal article,Multimedia Tools and Applications,13807501; 15737721,Springer Science and Business Media LLC,Netherlands,O. K. Toffa; M. Mignotte,,82,1,1505,1518,Computer science; Sonification; Image (mathematics); Artificial intelligence; Natural language processing; Computer vision; Information retrieval; Human–computer interaction,,,,,,http://dx.doi.org/10.1007/s11042-022-12914-z,,10.1007/s11042-022-12914-z,,,0,003-300-470-930-67X; 003-527-470-732-477; 004-193-609-931-550; 008-553-939-609-320; 012-906-336-067-804; 013-598-571-665-550; 013-847-555-426-159; 016-495-889-563-113; 016-670-508-984-068; 021-961-429-561-949; 025-699-495-865-398; 028-304-495-955-73X; 042-040-350-256-541; 044-202-560-313-315; 044-661-087-466-30X; 045-871-809-765-910; 049-130-224-418-304; 049-551-659-753-473; 058-501-749-323-945; 059-149-073-001-124; 067-141-060-683-588; 083-194-118-366-374; 085-655-932-183-312; 092-256-337-350-943; 104-208-613-793-423; 104-535-881-798-843; 105-584-440-275-77X; 113-386-747-442-205; 122-822-693-327-547; 127-369-116-665-254; 127-760-331-009-593; 128-921-144-770-96X; 129-357-234-203-944; 136-720-387-952-791; 159-208-128-356-997,1,false,,
171-617-400-514-467,A Transfer Learning Approach for Smart Home Application Based on Evolutionary Algorithms,2023-01-30,2023,book chapter,Handbook of Research on AI Methods and Applications in Computer Engineering,2327039x; 23270403,IGI Global,,Mouna Afif; Riadh Ayachi; Yahia Said; Mohamed Atri,"<jats:p>Building new systems used for indoor sign recognition and indoor wayfinding assistance navigation, especially for blind and visually impaired persons, presents a very important task. Deep learning-based algorithms have revolutionized the computer vision and the artificial intelligence fields. Deep convolutional neural networks (DCNNs) are on the top of state-of-the-art algorithms which makes them very suitable to build new assistive technologies based on these architectures. Especially, the authors will develop a new indoor wayfinding assistance system using aging evolutionary algorithms AmoebaNet-A. The proposed system will be able to recognize a set of landmark signs highly recommended to assist blind and sighted persons to explore their surrounding environments. The experimental results have shown the high recognition performance results obtained by the developed work. The authors obtained a mean recognition rate for the four classes coming up to 93.46%.</jats:p>",,,434,450,Computer science; Convolutional neural network; Artificial intelligence; Landmark; Transfer of learning; Deep learning; Task (project management); Set (abstract data type); Machine learning; Human–computer interaction; Engineering; Systems engineering; Programming language,,,,,,http://dx.doi.org/10.4018/978-1-6684-6937-8.ch020,,10.4018/978-1-6684-6937-8.ch020,,,0,004-269-574-716-057; 013-422-017-213-208; 013-678-295-106-102; 017-229-017-205-09X; 022-302-465-646-744; 024-404-068-805-588; 026-400-078-913-130; 026-994-702-158-53X; 030-273-939-499-337; 034-222-058-985-816; 036-023-371-180-978; 037-182-418-585-695; 045-309-399-228-849; 048-395-635-842-481; 049-638-029-358-382; 060-396-860-794-913; 061-579-383-511-500; 066-305-886-229-375; 073-216-818-844-578; 086-731-195-129-866; 098-805-256-012-421; 106-052-130-152-052; 143-823-076-584-069,1,false,,
172-081-118-529-036,Developing Dynamic Audio Navigation UIs to Pinpoint Elements in Tactile Graphics,2022-12-18,2022,journal article,Multimodal Technologies and Interaction,24144088,MDPI AG,,Gaspar Ramôa; Vincent Schmidt; Peter König,"<jats:p>Access to complex graphical information is essential when connecting blind and visually impaired (BVI) people with the world. Tactile graphics readers enable access to graphical data through audio-tactile user interfaces (UIs), but these have yet to mature. A challenging task for blind people is locating specific elements–areas in detailed tactile graphics. To this end, we developed three audio navigation UIs that dynamically guide the user’s hand to a specific position using audio feedback. One is based on submarine sonar sounds, another relies on the target’s coordinate plan x and y-axis, and the last uses direct voice instructions. The UIs were implemented in the Tactonom Reader device, a new tactile graphic reader that enhances swell paper graphics with pinpointed audio explanations. To evaluate the effectiveness of the three different dynamic navigation UIs, we conducted a within-subject usability test that involved 13 BVI participants. Beyond comparing the effectiveness of the different UIs, we observed and recorded the interaction of the visually impaired participants with the different navigation UI to further investigate their behavioral patterns during the interaction. We observed that user interfaces that required the user to move their hand in a straight direction were more likely to provoke frustration and were often perceived as challenging for blind and visually impaired people. The analysis revealed that the voice-based navigation UI guides the participant the fastest to the target and does not require prior training. This suggests that a voice-based navigation strategy is a promising approach for designing an accessible user interface for the blind.</jats:p>",6,12,113,113,Computer science; Human–computer interaction; Usability; Audio feedback; Graphics; Visually impaired; Task (project management); Orientation and Mobility; Visualization; Multimedia; Artificial intelligence; Computer graphics (images); Engineering; Systems engineering; Electrical engineering,,,,European Union’s Horizon 2020,https://www.mdpi.com/2414-4088/6/12/113/pdf?version=1671606651 https://doi.org/10.3390/mti6120113,http://dx.doi.org/10.3390/mti6120113,,10.3390/mti6120113,,,0,003-761-087-658-628; 004-476-826-365-583; 007-903-195-941-160; 008-132-823-503-692; 010-338-677-207-718; 011-469-916-558-213; 015-837-338-452-266; 020-066-230-655-756; 024-236-322-414-073; 030-869-182-891-085; 036-596-448-417-207; 039-153-929-768-848; 045-531-232-645-330; 053-794-028-526-128; 053-935-024-701-570; 059-398-290-385-221; 068-929-505-096-491; 071-475-460-546-691; 077-772-042-977-332; 077-804-947-867-772; 078-884-122-434-197; 091-198-402-622-345; 098-484-574-497-992; 105-881-219-704-097; 112-483-691-545-843; 119-369-540-197-963; 119-639-482-636-039; 130-481-377-119-217; 143-360-205-659-883; 149-376-162-623-147; 160-830-353-436-227; 199-568-191-735-973,5,true,cc-by,gold
172-720-668-066-935,A real-time image captioning framework using computer vision to help the visually impaired,2023-12-22,2023,journal article,Multimedia Tools and Applications,15737721; 13807501,Springer Science and Business Media LLC,Netherlands,K. M. Safiya; R. Pandian,,83,20,59413,59438,Closed captioning; Computer science; Human–computer interaction; Graphics; Computer graphics; Artificial intelligence; Multimedia; Deep learning; Natural language processing; Image (mathematics); Computer graphics (images),,,,,,http://dx.doi.org/10.1007/s11042-023-17849-7,,10.1007/s11042-023-17849-7,,,0,013-687-284-638-685; 017-273-949-968-386; 026-081-113-022-953; 027-085-622-004-352; 031-140-944-655-16X; 034-070-451-642-49X; 037-441-555-622-134; 037-965-005-631-252; 044-736-216-421-399; 044-794-504-733-771; 046-139-459-260-232; 047-842-004-336-158; 049-087-839-311-107; 055-858-990-745-91X; 055-987-474-114-135; 056-167-810-394-870; 067-581-971-344-797; 069-147-563-736-864; 077-203-301-877-10X; 087-544-572-997-54X; 090-275-865-129-004; 092-430-734-516-135; 092-846-649-193-238; 099-078-179-204-304; 109-604-924-381-203; 115-953-715-917-123; 120-283-816-993-089; 123-182-818-651-011; 126-998-084-890-398; 136-994-461-404-282; 142-696-686-705-643; 149-984-952-788-543; 150-761-211-802-544; 171-400-510-506-102; 177-263-285-971-015; 183-005-644-287-65X; 185-312-638-200-637; 189-601-565-136-908,0,false,,
172-906-793-877-345,Design of a GPS based Virtual Eye for the Blind People,,2014,,,,,,Rupali Kale,Till date blind people struggle a lot to live their miserable life. Their problems have made them to lose their hope to live in this competing society. They seek help from others to guide them whole day. This project aims to make the blind person fully independent in all aspects. The proposed system is based on Global Positioning System (GPS) and Obstacle detection and object avoidance technologies .The aim of the overall system is to provide a low cost and efficient navigation aid for blind which gives a sense of artificial vision by providing information about the environment scenario of static and dynamic object around them.,,,,,Human–computer interaction; Artificial intelligence; Assisted GPS; Geography; Obstacle; Artificial vision; Navigation aid; Computer vision; Global Positioning System; Object (computer science),,,,,http://inpressco.com/wp-content/uploads/2014/06/Paper1892162-2164.pdf,http://inpressco.com/wp-content/uploads/2014/06/Paper1892162-2164.pdf,,,2183599639,,0,005-433-245-500-691; 024-369-065-483-244; 060-448-827-026-324; 067-347-261-470-099; 078-736-735-476-395; 098-811-589-762-176,4,false,,
173-164-658-058-36X,ASSETS - WatchOut: Obstacle Sonification for People with Visual Impairment or Blindness,2019-10-24,2019,conference proceedings article,The 21st International ACM SIGACCESS Conference on Computers and Accessibility,,ACM,,Giorgio Presti; Dragan Ahmetovic; Mattia Ducci; Cristian Bernareggi; Luca A. Ludovico; Adriano Baratè; Federico Avanzini; Sergio Mascetti,"Independent mobility is one of the main challenges for blind or visually impaired (BVI) people. In particular, BVI people often need to identify and avoid nearby obstacles, for example a bicycle parked on the sidewalk. This is generally achieved with a combination of residual vision, hearing and haptic sensing using the white cane. However, in many cases, BVI people can only perceive obstacles at short distance (typically about 1m, i.e., the white cane detection range), in other situations obstacles are hard to detect (e.g., those elevated from the ground), while others should not be hit by the white cane (e.g., a standing person). Thus, some time and effort are required to identify the object in order to understand how to avoid it. A solution to these problems can be found in recent computer vision techniques that can run on mobile and wearable devices to detect obstacles at a distance. However, in addition to detecting obstacles, it is also necessary to convey information about them to a BVI user. This contribution presents WatchOut, a sonification technique for conveying real-time information about the main characteristics of an obstacle to a BVI person, who can then use this additional feedback to safely navigate in the environment. WatchOut was designed with a user-centric approach, involving two iterations of online questionnaires with BVI participants in order to define, improve and evaluate the sonification technique. WatchOut was implemented and tested as a module of a mobile app that detects obstacles using state-of-the-art computer vision technology. Results show that the system is considered usable, and can guide the users to avoid more than 85% of the obstacles.",,,402,413,Human–computer interaction; Obstacle; USable; Visual impairment; Computer science; Wearable technology; Object (computer science); Computer-assisted web interviewing; Obstacle avoidance; Sonification,,,,,https://air.unimi.it/handle/2434/687254 https://dblp.uni-trier.de/db/conf/assets/assets2019.html#PrestiADBLBAM19 https://air.unimi.it/bitstream/2434/687254/2/p402-presti.pdf https://dl.acm.org/citation.cfm?id=3353779 https://dl.acm.org/doi/pdf/10.1145/3308561.3353779 https://core.ac.uk/download/237177504.pdf,http://dx.doi.org/10.1145/3308561.3353779,,10.1145/3308561.3353779,2981433825,,0,000-501-534-285-334; 009-142-401-900-87X; 009-824-797-627-547; 010-719-927-108-990; 013-056-435-015-274; 013-847-555-426-159; 014-157-934-526-940; 016-577-116-408-663; 017-411-280-443-272; 022-697-627-694-941; 023-868-294-867-153; 025-764-110-104-300; 026-418-488-997-838; 028-174-049-936-114; 030-127-369-356-741; 030-807-170-587-365; 031-799-548-880-483; 034-350-308-512-672; 035-395-409-643-765; 036-280-949-293-766; 042-317-340-997-142; 046-983-906-128-801; 051-065-738-489-461; 058-347-570-370-829; 060-257-784-475-928; 064-043-611-038-916; 064-121-100-029-787; 064-309-126-323-022; 066-526-177-885-125; 076-911-364-196-257; 077-838-265-920-228; 082-060-789-804-822; 083-870-431-489-961; 086-212-029-513-470; 098-811-589-762-176; 104-535-881-798-843; 105-790-942-783-392; 112-121-560-424-373; 113-713-658-788-357; 167-673-301-993-329; 176-619-480-167-936,45,true,,green
173-566-774-754-524,Beyond Shared Autonomy: Joint Perception and Action for Human-In-The-Loop Mobile Robot Navigation Systems,2023-09-06,2023,journal article,Journal of Intelligent & Robotic Systems,09210296; 15730409,Springer Science and Business Media LLC,Netherlands,Hamed Bozorgi; Trung Dung Ngo,,109,1,,,Mobile robot; Autonomy; Human–computer interaction; Robot; Computer science; Action (physics); Mobile robot navigation; Social robot; Artificial intelligence; Engineering; Robot control; Physics; Quantum mechanics; Political science; Law,,,,,,http://dx.doi.org/10.1007/s10846-023-01942-y,,10.1007/s10846-023-01942-y,,,0,004-529-069-646-519; 005-515-885-541-95X; 005-530-228-983-868; 009-701-287-333-442; 010-734-776-904-222; 011-813-664-094-802; 012-555-347-213-748; 012-568-628-857-892; 013-253-617-441-701; 013-654-815-720-792; 019-439-559-144-445; 020-488-245-970-599; 024-659-559-166-391; 025-039-499-714-753; 027-478-644-038-075; 029-837-852-973-150; 032-059-170-916-239; 032-193-709-569-910; 032-648-896-639-66X; 037-419-892-501-287; 042-565-340-040-087; 043-569-278-273-051; 045-151-420-188-734; 046-878-751-441-785; 046-976-555-033-812; 047-111-215-399-408; 049-317-239-158-314; 054-134-308-404-945; 054-728-788-815-345; 055-262-051-052-907; 057-081-206-007-001; 057-412-997-604-067; 060-081-996-058-740; 062-332-173-592-789; 064-103-523-586-551; 064-156-450-797-807; 069-200-845-773-815; 069-211-748-028-921; 071-845-641-618-848; 074-732-692-458-416; 078-395-268-103-422; 080-885-077-391-066; 098-077-073-599-351; 099-048-960-503-145; 099-884-046-627-801; 100-432-275-559-159; 108-862-353-792-943; 126-043-354-061-599; 127-406-202-563-035; 164-934-329-584-299,0,false,,
173-728-392-408-596,Recent advancements in indoor electronic travel aids for the blind or visually impaired: a comprehensive review of technologies and implementations,2024-02-05,2024,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,In-Ju Kim,,,,,,Visually impaired; Implementation; Computer science; Human–computer interaction; Universal design; Multimedia; Telecommunications; World Wide Web; Software engineering,,,,,,http://dx.doi.org/10.1007/s10209-023-01086-8,,10.1007/s10209-023-01086-8,,,0,001-308-499-154-495; 001-325-059-112-077; 002-123-448-882-287; 002-803-890-447-084; 003-342-986-507-202; 004-216-069-855-831; 007-223-103-783-187; 007-652-375-981-035; 007-907-758-104-429; 008-529-151-900-067; 008-913-756-483-274; 011-208-264-556-881; 012-004-532-920-822; 012-370-328-340-986; 015-000-352-025-848; 015-008-890-486-381; 015-040-823-132-436; 015-714-235-272-975; 016-445-748-246-67X; 016-572-318-414-022; 017-108-666-837-168; 019-828-744-033-99X; 020-260-934-897-437; 020-745-838-621-241; 026-330-055-991-696; 026-994-702-158-53X; 027-712-485-636-459; 029-279-026-417-597; 029-696-848-270-376; 030-251-661-047-944; 034-093-615-545-728; 037-376-150-587-539; 038-326-446-864-583; 039-724-071-444-872; 040-940-319-245-132; 045-152-328-858-234; 045-895-610-715-138; 046-309-239-761-288; 047-356-385-281-782; 048-787-039-266-679; 050-258-546-938-650; 050-453-156-311-856; 054-367-982-946-305; 057-847-555-860-541; 058-846-336-967-789; 061-435-037-206-64X; 061-629-846-800-142; 062-937-085-342-503; 063-117-355-616-790; 065-032-907-680-425; 065-411-531-321-861; 065-983-543-415-033; 066-339-002-798-982; 067-066-496-849-617; 069-226-894-925-753; 070-606-003-984-347; 070-859-524-188-835; 072-480-770-780-611; 072-556-735-586-395; 080-148-076-219-168; 082-036-797-529-285; 083-671-887-012-721; 085-492-019-030-208; 085-961-783-812-300; 086-089-293-790-214; 087-716-065-499-413; 087-767-401-019-088; 088-743-350-822-814; 090-884-510-770-832; 093-548-249-489-049; 094-812-368-636-394; 096-601-741-007-688; 098-497-595-587-383; 099-334-901-486-872; 100-817-282-240-389; 101-349-523-118-790; 102-090-043-334-677; 102-263-736-254-576; 110-409-001-674-560; 111-360-462-814-708; 112-566-962-059-235; 116-715-441-276-690; 121-439-923-704-187; 123-143-273-939-602; 125-431-761-177-423; 126-391-967-174-60X; 128-585-987-688-550; 131-924-017-718-191; 133-375-862-261-006; 133-583-294-107-479; 139-618-801-962-345; 141-649-000-612-322; 149-162-653-144-382; 162-044-239-368-35X; 162-153-535-514-940; 167-450-236-434-564; 168-099-649-007-625; 169-784-348-324-119; 171-189-370-546-346; 175-661-713-504-140; 182-852-228-754-133; 186-002-073-406-20X; 189-179-024-808-221,3,false,,
173-788-818-947-867,A scientometric analysis of research on people with visual impairments in the field of HCI design: mapping the intellectual structure and evolution,2023-12-15,2023,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Lusha Huang; Baihui Chen,,,,,,Field (mathematics); Computer science; Human–computer interaction; Data science; Psychology; Cognitive science; Mathematics; Pure mathematics,,,,the Guangdong Province Colleges and Universities Young Innovative Talent Project; the Humanities and Social Sciences Research Planning Fund of the Ministry of Education in China,,http://dx.doi.org/10.1007/s10209-023-01067-x,,10.1007/s10209-023-01067-x,,,0,000-056-660-201-231; 000-318-019-697-224; 001-661-456-741-89X; 002-634-846-436-949; 003-494-222-194-245; 003-773-303-094-096; 005-191-781-331-343; 007-223-103-783-187; 007-388-534-440-157; 007-530-851-453-706; 010-475-144-749-494; 019-731-660-556-629; 026-996-022-752-657; 028-936-571-080-214; 029-659-720-492-794; 037-646-611-225-731; 037-701-987-163-355; 038-765-904-206-436; 043-740-219-623-098; 044-827-300-797-421; 046-906-287-427-917; 047-984-140-662-023; 053-714-095-143-343; 055-704-268-515-146; 057-378-990-426-265; 062-930-067-830-544; 064-264-695-662-36X; 064-400-499-357-900; 067-309-987-821-428; 075-113-904-846-298; 075-330-543-726-901; 077-938-736-353-230; 079-865-682-254-103; 083-382-172-188-183; 085-388-709-306-865; 089-796-474-980-887; 095-258-654-450-233; 099-081-268-683-922; 101-489-718-701-104; 120-456-525-352-292; 125-897-318-346-362; 127-070-672-662-621; 129-105-464-101-600; 131-366-240-217-740; 133-317-652-903-976; 137-718-696-660-630; 141-489-442-115-69X; 141-649-000-612-322; 148-846-474-483-646; 150-070-576-398-617; 160-661-689-179-127; 162-695-709-522-307; 174-741-930-721-25X; 184-116-517-417-325; 195-169-185-418-265,0,false,,
173-910-349-469-554,Haptic Interface for Remote Guidance of People with Visual Disabilities,2024-01-04,2024,book chapter,IFMBE Proceedings,16800737; 14339277; 17271983,Springer Nature Switzerland,,Angie Giovanna Figueroa-Hernandez; Camila Perdomo-Vasquez; Dayani Gomez-Escobar; Hardy Galvis-Pedraza; Juan Fernando Medina-Castañeda; Andrés Mauricio González-Vargas,"Visually impaired people experience difficulties in many daily life activities. One of them is navigation through unknown environments. Assistive alternatives such as white canes, guide dogs or guide persons are commonly available. There are also new developments on high-end automated technologies that make use of computer vision, GPS and different sensors in order to provide navigation indications for blind users. These automated technologies, however, are often expensive, and the access to guide dogs or persons is not always easy. Therefore, in this work we present the design and prototyping of a haptic interface for remote guidance of visually impaired people. With this device, any person (such as a relative or a guidance service) can help the user to move around unfamiliar environments by using the camera on the user's smartphone and transmitting tactile indications via a wearable vibration device, in order to not interfere with the users' hearing and provide a safe and comfortable experience.",,,681,689,Haptic technology; Human–computer interaction; Visually impaired; Wearable computer; Interface (matter); Global Positioning System; Computer science; Assistive technology; Multimedia; User interface; Simulation; Embedded system; Telecommunications; Operating system; Bubble; Maximum bubble pressure method; Parallel computing,,,,,,http://dx.doi.org/10.1007/978-3-031-49407-9_67,,10.1007/978-3-031-49407-9_67,,,0,003-634-369-751-95X; 020-935-356-605-174; 111-017-691-591-466; 130-362-324-653-748; 153-471-712-074-630,0,false,,
174-111-964-871-989,Smart Glasses for Sign Reading as Mobility Aids for the Blind Using a Light Communication System,,2020,conference proceedings article,"2020 17th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON)",,IEEE,,Apiched Audomphon; Anya Apavatjrut,"Vision impairment directly impacts the quality of life since the absence of vision limits proficiency i n various activities, especially those that require mobility. Mobility aid tools can provide useful information for movement and orientation. Many assistive tools have previously been proposed to serve obstacle detection, traffic pattern interpretation, and orientation assistance. However, many of these appliances are expensive and complex since they usually require high computational cost from image processing, obstacle detection, virtual reality, etc. In this work, we want to propose a design of a low cost and low complexity mobility aid platform. Our platform is composed of a pair of smart glasses and an illuminated sign. The light emit from the sign are modulated to issue warnings and instructions. The pair of smart glasses equipped with a photodetector is used as an optical signal receiver. This can interpret the sign and issue a warning through an earphone. Our objective is to make the system user-friendly. It is initially intended to be deployed in a school for the blind to facilitate the movement of the students within and between the buildings.",,,615,618,Communications system; Human–computer interaction; Sign (mathematics); Virtual reality; Airfield traffic pattern; Obstacle; Mobility aid; Computer science; Reading (process); Orientation (computer vision),,,,,https://ieeexplore.ieee.org/document/9158250,http://dx.doi.org/10.1109/ecti-con49241.2020.9158250,,10.1109/ecti-con49241.2020.9158250,3047152252,,1,003-952-680-546-290; 015-672-084-912-775; 017-411-280-443-272; 055-432-120-713-465; 078-736-735-476-395,2,false,,
174-741-930-721-25X,Recent trends in computer vision-driven scene understanding for VI/blind users: a systematic mapping,2022-02-06,2022,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Mohammad Moeen Valipoor; Angélica de Antonio,"<jats:title>Abstract</jats:title><jats:p>During the past years, the development of assistive technologies for visually impaired (VI)/blind people has helped address various challenges in their lives by providing services such as obstacle detection, indoor/outdoor navigation, scene description, text reading, facial recognition and so on. This systematic mapping review is mainly focused on the scene understanding aspect (e.g., object recognition and obstacle detection) of assistive solutions. It provides guidance for researchers in this field to understand the advances during the last four and a half years. This is because deep learning techniques together with computer vision have become more powerful and accurate than ever in tasks like object detection. These advancements can bring a radical change in the development of high-quality assistive technologies for VI/blind users. Additionally, an overview of the current challenges and a comparison between different solutions is provided to indicate the pros and cons of existing approaches.</jats:p>",22,3,983,1005,Obstacle; Computer science; Field (mathematics); Assistive technology; Human–computer interaction; Reading (process); Object (grammar); Artificial intelligence; Quality (philosophy); Computer vision; Multimedia; Philosophy; Mathematics; Epistemology; Political science; Pure mathematics; Law,,,,Universidad Politécnica de Madrid,https://link.springer.com/content/pdf/10.1007/s10209-022-00868-w.pdf https://doi.org/10.1007/s10209-022-00868-w,http://dx.doi.org/10.1007/s10209-022-00868-w,,10.1007/s10209-022-00868-w,,,0,000-991-663-378-566; 002-123-448-882-287; 002-199-393-457-486; 004-216-069-855-831; 005-191-781-331-343; 005-642-690-871-424; 006-342-835-630-050; 007-223-103-783-187; 007-913-857-265-43X; 008-927-524-692-138; 009-829-120-033-790; 010-461-705-230-16X; 010-475-144-749-494; 010-971-810-882-418; 011-797-116-120-363; 015-040-823-132-436; 016-445-748-246-67X; 016-446-586-220-032; 016-645-836-018-745; 018-738-631-000-272; 019-981-648-422-910; 020-233-013-143-936; 020-341-613-871-193; 021-814-705-252-502; 025-522-385-395-133; 025-541-192-404-658; 025-959-426-726-624; 026-303-765-898-687; 026-459-942-328-409; 026-994-702-158-53X; 027-026-193-850-672; 028-115-943-729-849; 028-174-049-936-114; 028-326-261-380-168; 028-855-346-335-263; 029-313-431-507-058; 029-621-724-363-881; 030-440-500-148-114; 031-218-334-653-826; 031-755-053-318-46X; 034-028-229-282-033; 034-259-321-454-457; 034-463-620-626-325; 035-115-110-416-139; 035-291-375-623-97X; 036-077-342-885-518; 039-085-681-603-182; 039-799-789-300-41X; 040-057-279-881-544; 041-872-163-351-477; 043-503-759-895-36X; 044-644-110-114-333; 045-309-399-228-849; 045-599-200-378-144; 047-271-748-480-72X; 048-301-582-552-287; 049-317-239-158-314; 049-324-523-155-079; 049-505-611-314-159; 051-514-402-816-140; 051-843-465-255-165; 053-611-818-883-600; 053-826-604-836-17X; 054-367-982-946-305; 055-743-142-452-276; 056-456-008-278-70X; 059-644-292-671-783; 060-005-186-721-788; 060-147-659-950-016; 061-629-846-800-142; 062-484-218-254-568; 064-521-070-547-235; 066-244-628-154-685; 067-066-496-849-617; 067-331-553-052-693; 069-969-331-457-455; 070-777-575-483-27X; 071-895-492-147-282; 073-180-689-038-913; 073-439-548-015-005; 075-358-894-384-661; 076-121-436-407-96X; 078-370-310-843-504; 079-310-940-223-758; 080-589-710-669-822; 081-475-451-419-124; 084-196-758-656-594; 084-740-593-674-175; 085-492-019-030-208; 085-961-783-812-300; 086-059-110-390-983; 092-452-392-581-46X; 094-072-138-016-877; 095-211-223-584-688; 097-823-942-919-19X; 098-497-595-587-383; 098-833-820-740-613; 100-817-282-240-389; 101-176-260-587-612; 102-251-935-371-523; 104-873-566-419-489; 109-485-258-269-762; 110-020-005-480-063; 110-831-712-365-194; 111-772-107-673-112; 114-906-535-117-147; 118-540-529-844-833; 120-693-281-620-586; 121-439-923-704-187; 122-246-600-364-064; 124-385-928-276-562; 125-608-691-280-273; 126-154-517-654-703; 132-271-253-341-075; 132-502-766-442-360; 133-416-782-758-694; 136-732-042-076-069; 137-973-182-180-54X; 139-555-713-331-986; 141-649-000-612-322; 147-855-184-779-455; 156-120-400-966-933; 158-368-277-657-984; 161-012-758-615-452; 163-506-338-712-945; 164-332-313-558-557; 169-784-348-324-119; 173-164-658-058-36X; 174-289-314-292-190; 177-522-114-459-74X; 180-082-280-146-564; 183-947-709-586-992; 184-116-517-417-325,12,true,cc-by,hybrid
174-896-898-137-163,COMPARATIVE ANALYSIS OF SMARTPHONE-BASED TIFLOTECHNICAL NAVIGATION SOLUTIONS,,2023,journal article,Scientific notes of Taurida National V.I. Vernadsky University. Series: Technical Sciences,26635941; 2663595x,Publishing House Helvetica (Publications),,O.V. Khapchenko; O.M. Lysenko,"SMARTPHONE-BASED TIFLOTECHNICAL NAVIGATION SOLUTIONSIn recent years, significant progress has been observed in the development and implementation of assistive technologies aimed at improving the daily lives of blind and visually impaired individuals.Recent research and applications have been analyzed regarding the functional capabilities of modern smartphone-based assistive technologies for the blind and visually impaired.These technologies are based on Wi-Fi/Bluetooth beacons, tag markers in the form of special color images, artificial intelligence, computer vision, and utilize vibration and auditory feedback for user interaction with visual impairments in assistive systems.These technologies enable the creation of solutions to enhance the quality of life for the target audience directly through smartphones, which are ubiquitous among the general population.Another important aspect of some applications is the inclusion of volunteer assistance, where volunteers can provide help through the smartphone's camera and the Internet.This assists users in accomplishing various tasks, including navigation, clothing style assessment, finding lost items, describing photos, artwork, solving computer-related issues, making purchases, determining the shelf life of products, and accessing public transportation schedules, among others.A comparative analysis has been provided for existing applications, considering their popularity based on user ratings, download counts, and their functionality, especially in daily life or navigation contexts.Additionally, the supported platforms have been indicated.It has been established that navigation functionality is in high demand, and the most popular applications involve volunteers who can assist users with visual impairments in solving various issues using their smartphone cameras.The importance of classifying modern smartphone-based assistive technologies has been substantiated and a classification has been proposed.This classification will enable developers to access structured information regarding the functional capabilities and application areas of existing solutions, thereby contributing to a better understanding of user needs.Consequently, this will empower developers and researchers to identify the direction for the development and creation of their own assistive solutions for individuals with visual impairments.",,5,83,88,Computer science; Human–computer interaction,,,,,,http://dx.doi.org/10.32782/2663-5941/2023.5/14,,10.32782/2663-5941/2023.5/14,,,0,,0,true,,gold
175-595-172-295-418,StairNet: visual recognition of stairs for human-robot locomotion.,2024-02-15,2024,journal article,Biomedical engineering online,1475925x,Springer Science and Business Media LLC,United Kingdom,Andrew Garrett Kurbis; Dmytro Kuzmenko; Bogdan Ivanyuk-Skulskiy; Alex Mihailidis; Brokoslaw Laschowski,"Human-robot walking with prosthetic legs and exoskeletons, especially over complex terrains, such as stairs, remains a significant challenge. Egocentric vision has the unique potential to detect the walking environment prior to physical interactions, which can improve transitions to and from stairs. This motivated us to develop the StairNet initiative to support the development of new deep learning models for visual perception of real-world stair environments. In this study, we present a comprehensive overview of the StairNet initiative and key research to date. First, we summarize the development of our large-scale data set with over 515,000 manually labeled images. We then provide a summary and detailed comparison of the performances achieved with different algorithms (i.e., 2D and 3D CNN, hybrid CNN and LSTM, and ViT networks), training methods (i.e., supervised learning with and without temporal data, and semi-supervised learning with unlabeled images), and deployment methods (i.e., mobile and embedded computing), using the StairNet data set. Finally, we discuss the challenges and future directions. To date, our StairNet models have consistently achieved high classification accuracy (i.e., up to 98.8%) with different designs, offering trade-offs between model accuracy and size. When deployed on mobile devices with GPU and NPU accelerators, our deep learning models achieved inference speeds up to 2.8 ms. In comparison, when deployed on our custom-designed CPU-powered smart glasses, our models yielded slower inference speeds of 1.5 s, presenting a trade-off between human-centered design and performance. Overall, the results of numerous experiments presented herein provide consistent evidence that StairNet can be an effective platform to develop and study new deep learning models for visual perception of human-robot walking environments, with an emphasis on stair recognition. This research aims to support the development of next-generation vision-based control systems for robotic prosthetic legs, exoskeletons, and other mobility assistive technologies.",23,1,20,,Computer science; Stairs; Deep learning; Artificial intelligence; Software deployment; Set (abstract data type); Inference; Machine learning; Robot; Mobile device; Convolutional neural network; Human–computer interaction; Engineering; Civil engineering; Programming language; Operating system,Computer vision; Deep learning; Exoskeletons; Prosthetics; Wearable robotics,Humans; Robotics; Locomotion; Walking; Algorithms; Leg,,AGE-WELL; Vector Institute; The Schroeder Institute for Brain Innovation and Recovery,https://biomedical-engineering-online.biomedcentral.com/counter/pdf/10.1186/s12938-024-01216-0 https://doi.org/10.1186/s12938-024-01216-0,http://dx.doi.org/10.1186/s12938-024-01216-0,38360664,10.1186/s12938-024-01216-0,,PMC10870468,0,004-022-734-013-891; 004-269-574-716-057; 010-585-504-629-160; 011-265-433-636-69X; 016-342-084-117-207; 017-687-895-109-163; 020-234-330-544-797; 030-207-600-694-277; 033-462-555-538-255; 039-754-164-971-919; 041-039-012-956-559; 053-094-537-530-97X; 054-071-407-783-498; 057-503-147-514-990; 059-149-073-001-124; 061-615-187-060-031; 063-340-181-226-419; 072-445-523-896-250; 081-215-770-563-077; 088-964-750-537-077; 090-107-930-854-825; 094-795-558-864-441; 095-026-225-469-646; 097-280-368-102-653; 097-624-013-243-859; 108-665-680-546-990; 117-937-084-066-168; 133-107-863-402-471; 139-479-571-608-422; 140-921-627-035-630; 145-465-421-586-985; 145-727-076-843-131; 151-213-994-396-341; 174-309-921-484-43X; 187-510-832-227-85X,1,true,"CC BY, CC0",gold
175-795-617-988-169,Evolution of the Information-Retrieval System for Blind and Visually-Impaired People,,2003,journal article,International Journal of Speech Technology,13812416; 15728110,Springer Science and Business Media LLC,Netherlands,Simon Dobrišek; Jerneja Gros; Boštjan Vesnicer; Nikola Pavešić#x,,6,3,301,309,World Wide Web; Set (psychology); Information retrieval; Text corpus; Spoken language; Computer science; Plain text; Multimedia; Reading (process); Function (engineering); XML; User interface,,,,,http://doi.org/10.1023/A%3A1023474405658 http://dx.doi.org/10.1023/A%3A1023474405658 https://doi.org/10.1023/A%3A1023474405658 https://dx.doi.org/10.1023/A%3A1023474405658 https://link.springer.com/article/10.1023%2FA%3A1023474405658 https://dblp.uni-trier.de/db/journals/ijst/ijst6.html#DobrisekGVPM03,http://dx.doi.org/10.1023/a:1023474405658,,10.1023/a:1023474405658,114910198,,34,004-796-123-658-18X; 051-083-103-683-577; 057-331-970-459-344; 057-741-416-305-622; 061-092-302-989-744; 114-884-704-515-927; 199-272-845-947-13X,4,false,,
176-061-981-445-527,Design of a guiding cane for human-computer interaction using sensors,2023-12-01,2023,conference proceedings article,Third International Conference on Control and Intelligent Robotics (ICCIR 2023),,SPIE,,Huijuan Zhu,"Human society is a comprehensive group structure. As a special group, blind people occupy a large proportion in society. It is inevitable and very difficult for blind people to walk alone in their life. First, they are easy to lose their way at intersections. Second, some imperceptible obstacles will hurt them. Finally, the Blind Way occupied will make their walk more difficult. However, with the progress of computer technology, there are more and more products that can help the blind to travel, such as wearing the obstacle warning clothing on the body of the blind, or the obstacle recognition device in the form of glasses. However, most of the products are not only very complex in use, but also change the original travel habits of the blind, making them have a kind of resistance. This research is based on the concept of human-centred design from the perspective of helping the blind, with the focus on not changing the original habits of the blind, using the concept of barrier-free design as a guideline to discover the travel needs of the blind in their daily lives, and using the sensors as the collection end, and using the computer and the device for interaction, in order to complete the design of mobility products for the blind. It is hoped that the design and research in this paper will help the development of guide products and serve as a reference for future research on such products, and that more designers will pay attention to these visually impaired people and give them more and better designs in the future.",,,,,Obstacle; Clothing; Visually impaired; Perspective (graphical); Computer science; Human–computer interaction; Order (exchange); Interaction design; Internet privacy; Artificial intelligence; Business; Archaeology; Finance; Political science; Law; History,,,,,,http://dx.doi.org/10.1117/12.3011068,,10.1117/12.3011068,,,0,011-038-063-817-417,0,false,,
176-339-454-003-376,Development of Low Cost Smart Cane with GPS,2022-06-06,2022,conference proceedings article,2022 IEEE World AI IoT Congress (AIIoT),,IEEE,,Ferdaus Ahmed; Zarin Tasnim; Masud Rana; Mohammad Monirujjaman Khan,"This paper introduces a smart stick system for assisting blind people. The smart stick is a solution which helps visually impaired people to detect obstacles, waters and dangers in front of them during walking time. It identifies almost everything around them. The system is modified to perform like an artificial vision and alarm unit the system consists of two sensors: water sensor, ultrasonic sensor and microcontroller (Arduino Uno R3) to receive the sensor signals. It processes them to the short pulses to the Arduino pins where buzzers are connected. GPS and GSM are used so that if the user thinks he/she is lost then they can press a button which will send the exact location of the stick to a particular number, which will be known as EC (Emergency Contact) button. Also, there is RF Module which will help to find the stick if it is lost in a certain range. GPS navigation system in the Mobile can be used to guide the blind people to find any places. The blind people can also use an earphone to listen to the navigation directions which are coming from the mobile phone and buzzer alarm in the stick to warn by sound. In this project, a smart stick is provided which is affordable and suitable for most of the blind people, and also it is light in weight. It can be made available to all aspects of the society and to the families who need it.",,,,,Buzzer; Global Positioning System; Microcontroller; Arduino; ALARM; GSM; Assisted GPS; Computer science; Real-time computing; Embedded system; Mobile phone; Computer hardware; Telecommunications; Engineering; Electrical engineering,,,,,,http://dx.doi.org/10.1109/aiiot54504.2022.9817322,,10.1109/aiiot54504.2022.9817322,,,0,003-976-442-054-457; 015-818-725-296-28X; 030-127-369-356-741; 031-532-771-330-967; 040-533-527-422-721; 046-649-192-050-710; 047-844-248-722-855; 062-279-197-826-869; 097-390-623-922-438; 115-938-456-405-936,7,false,,
177-300-113-434-985,REAL-TIME VISION-BASED BLIND SPOT WARNING SYSTEM: EXPERIMENTS WITH MOTORCYCLES IN DAYTIME/NIGHTTIME CONDITIONS,2013-01-29,2013,journal article,International Journal of Automotive Technology,12299138; 19763832,Springer Science and Business Media LLC,South Korea,Carmen Cagigas Fernández; David Fernández Llorca; Miguel Angel Sotelo; Iván García Daza; Agustín Martínez Hellín; S. Álvarez,"This paper describes a real-time vision-based blind spot warning system that has been specially designed for motorcycles detection in both daytime and nighttime conditions. Motorcycles are fast moving and small vehicles that frequently remain unseen to other drivers, mainly in the blind-spot area. In fact, although in recent years the number of fatal accidents has decreased overall, motorcycle accidents have increased by 20%. The risks are primarily linked to the inner characteristics of this mode of travel: motorcycles are fast moving vehicles, light, unstable and fragile. These features make the motorcycle detection problem a difficult but challenging task to be solved from the computer vision point of view. In this paper we present a daytime and nighttime vision-based motorcycle and car detection system in the blind spot area using a single camera installed on the side mirror. On the one hand, daytime vehicle detection is carried out using optical flow features and Support Vector Machine-based (SVM) classification. On the other hand, nighttime vehicle detection is based on head lights detection. The proposed system warns the driver about the presence of vehicles in the blind area, including information about the position and the type of vehicle. Extensive experiments have been carried out in 172 minutes of sequences recorded in real traffic scenarios in both daytime and nighttime conditions, in the context of the Valencia MotoGP Grand Prix 2009.",14,1,113,122,Support vector machine; Optical flow; Daytime; Context (language use); Warning system; Real time vision; Single camera; Computer science; Simulation; Blind spot; Real-time computing,,,,,http://www.robesafe.com/personal/sergio.alvarez/Sitio_web/Publications_files/IJAT_blind2013.pdf https://link.springer.com/article/10.1007%2Fs12239-013-0013-3 http://dspace.kci.go.kr/handle/kci/801208 https://link.springer.com/10.1007/s12239-013-0013-3 http://www.robesafe.com/personal/sotelo/IJAT_BlindSpot2013.pdf,http://dx.doi.org/10.1007/s12239-013-0013-3,,10.1007/s12239-013-0013-3,2045438468,,1,001-776-844-357-726; 003-277-439-911-005; 003-459-195-879-615; 008-808-270-264-915; 009-682-814-103-008; 010-403-389-474-991; 012-071-088-813-667; 012-642-196-605-159; 013-287-605-098-577; 015-522-377-591-609; 021-495-309-385-383; 021-922-733-063-432; 023-438-345-162-875; 023-862-929-536-837; 025-805-944-282-152; 028-343-172-716-736; 028-497-354-807-286; 031-098-452-075-699; 033-998-331-918-289; 036-457-016-340-445; 040-958-959-854-679; 042-668-263-289-277; 050-133-054-598-873; 052-607-227-913-910; 054-077-169-458-733; 057-341-401-628-59X; 062-025-812-084-570; 064-683-024-880-936; 068-446-370-600-681; 071-502-076-874-325; 074-065-804-709-480; 108-504-953-453-881; 115-738-216-696-344; 118-948-890-515-140; 130-273-798-982-258; 135-280-492-401-497; 146-330-000-620-696; 149-605-806-731-052; 152-343-408-302-034; 178-620-097-087-192,17,true,,green
177-333-650-235-173,Accurate Localization in Dense Urban Area Using Google Street View Image,2014-12-29,2014,preprint,arXiv: Computer Vision and Pattern Recognition,,,," Salarian","Accurate information about the location and orientation of a camera in mobile devices is central to the utilization of location-based services (LBS). Most of such mobile devices rely on GPS data but this data is subject to inaccuracy due to imperfections in the quality of the signal provided by satellites. This shortcoming has spurred the research into improving the accuracy of localization. Since mobile devices have camera, a major thrust of this research has been seeks to acquire the local scene and apply image retrieval techniques by querying a GPS-tagged image database to find the best match for the acquired scene.. The techniques are however computationally demanding and unsuitable for real-time applications such as assistive technology for navigation by the blind and visually impaired which motivated out work. To overcome the high complexity of those techniques, we investigated the use of inertial sensors as an aid in image-retrieval-based approach. Armed with information of media other than images, such as data from the GPS module along with orientation sensors such as accelerometer and gyro, we sought to limit the size of the image set to c search for the best match. Specifically, data from the orientation sensors along with Dilution of precision (DOP) from GPS are used to find the angle of view and estimation of position. We present analysis of the reduction in the image set size for the search as well as simulations to demonstrate the effectiveness in a fast implementation with 98% Estimated Position Error.",,,,,Artificial intelligence; Image retrieval; Set (abstract data type); Mobile device; Dilution of precision; Computer vision; Computer science; Global Positioning System; Inertial measurement unit; Accelerometer; Orientation (computer vision),,,,,https://arxiv.org/pdf/1412.8496.pdf https://arxiv.org/abs/1412.8496 http://ui.adsabs.harvard.edu/abs/2014arXiv1412.8496S/abstract,https://arxiv.org/abs/1412.8496,,,1872118479,,0,002-345-873-500-677; 028-303-905-295-179; 043-872-717-852-728; 061-677-802-154-042; 068-446-370-600-681; 089-316-003-342-621; 098-440-015-522-27X; 105-161-175-016-528; 118-513-636-363-038; 124-510-348-671-355,2,true,,unknown
177-545-287-048-430,Handy Smart Stick: A Guide for Visually Impaired People,2023-06-01,2023,conference proceedings article,2023 8th International Conference on Communication and Electronics Systems (ICCES),,IEEE,,Mansi Gupta; Aditi Laddha; Maahi Chouhan; Yashasvi Mishra; Vitthal Gutte,"It's a perpetual human quest and a constant effort of science to develop and design equipment and aids to help physically challenged people live their lives independently and confidently. Blindness and visual impairment are prevalent disabilities that inhibit the independence of the patient and increase the reliance on family members, friends, and guide dogs for navigation and other daily duties. The proposed work outlines a reliable, low-cost, and power-efficient solution that enables safe and secure navigation for visually impaired people. Many researchers have worked on and proposed solutions to the typical problems, but those works have some shortcomings that are addressed in this study. Existing technologies have not focused on optimizing power consumption and reliability, thereby reducing the life of the system. It is vital to reduce power consumption in order to extend the operating lifetime of devices that use batteries as a power source. A methodology has been put forward to address the crucial concern of a gadget that is left on for an extended period of time without being switched off to improve the overall performance of the system. The IoT is entrusted with taking into account heterogeneous equipment that might be severely constrained by its nature and pose problems in the hardware layer of the system. So methods to make the system reliable, accurate, and resistant to failures are inscribed in this study. Additionally, existing systems aren’t economical compared to the offered features, lack automation, and lack audio feedback, particularly notifying obstacles. This study proposes audio feedback in multiple languages and automates the command execution, thereby making it effortless for impaired people to engage with the product. Also, the present state of technology faces challenges in locating the misplaced stick, which means the person has to be dependent on loved ones, friends, or caretakers to find the stick. The proposed automated and handy Smart Stick for the blind using IOT (Internet of Things) is a potent Electronic Travel Aid (ETA) that gives artificial perceptivity of vision and discerns environmental hurdles and obstacles.",,,,,Gadget; Computer science; Reliability (semiconductor); Obstacle; Blindness; Power consumption; Visually impaired; Automation; Independence (probability theory); Power (physics); Home automation; Human–computer interaction; Multimedia; Computer security; Risk analysis (engineering); Engineering; Telecommunications; Medicine; Mechanical engineering; Physics; Algorithm; Quantum mechanics; Political science; Optometry; Statistics; Mathematics; Law,,,,,,http://dx.doi.org/10.1109/icces57224.2023.10192815,,10.1109/icces57224.2023.10192815,,,0,014-004-574-193-910; 014-386-705-554-912; 025-505-942-258-138; 126-524-218-035-094,0,false,,
177-703-188-720-296,VISUAL EASE: BLIND ASSISTANCE USING IMAGE PROCESSING,2023-06-18,2023,journal article,International Research Journal of Modernization in Engineering Technology and Science,25825208,International Research Journal of Modernization in Engineering Technology and Science,,Abit Mon Rajan; Haripriya Pm; Neha Karunnattu; M Gayathri,"Eye diseases usually cause blindness and visual impairment.According to the World Healt Organization, around 40 million people are blind, while another 250 million have some form of visual impairment.They come across many troubles in their daily life, especially while navigating from one place to another on their own.They often depend on others for help to satisfy their day-to-day needs.So, it is quite a challenging task to implement a technological solution to assist them.Several technologies have been developed for the assistance of visually impaired people.One such attempt is that we would wish to make an Integrated Machine Learning System that allows the blind victims to detect and identify real-time objects and generating voice feedback.It is really difficult for blind person to move in this world.There are chances that they can get lost; in such cases it is really difficult for their family members to find them.However this project fetch the location of the person continuosly and thereby solves this problem.",,,,,Computer science; Computer vision; Artificial intelligence; Image processing; Image (mathematics),,,,,,http://dx.doi.org/10.56726/irjmets42258,,10.56726/irjmets42258,,,0,,0,true,,bronze
178-258-717-524-13X,SoCPaR - Obstacle Detection Algorithm by Stereoscopic Image Processing - Navigation Assistance for the Blind and Visually Impaired.,,2017,conference proceedings,,,,,Adil Elachhab; Mohammed Mikou,,,,13,23,Stereoscopy; Artificial intelligence; Obstacle; Navigation assistance; Visually impaired; Computer vision; Computer science; Image processing,,,,,https://dblp.uni-trier.de/db/conf/socpar/socpar2017.html#ElachhabM17,https://dblp.uni-trier.de/db/conf/socpar/socpar2017.html#ElachhabM17,,,2811242359,,0,,0,false,,
178-786-695-290-502,"International year of disabled person, in India",,1981,journal article,The Indian Journal of Pediatrics,00195456; 09737693,Springer Science and Business Media LLC,India,J. C. Jetli,,48,6,697,702,,,,,,,http://dx.doi.org/10.1007/bf02758530,,10.1007/bf02758530,,,0,,0,false,,
179-086-462-954-151,"Visually impaired object segmentation and detection using hybrid Canny edge detector, Hough transform, and improved momentum search in YOLOv7",2024-04-05,2024,journal article,"Signal, Image and Video Processing",18631703; 18631711,Springer Science and Business Media LLC,Germany,T. Sugashini; G. Balakrishnan,,18,S1,251,265,Canny edge detector; Hough transform; Artificial intelligence; Computer vision; Segmentation; Enhanced Data Rates for GSM Evolution; Detector; Computer science; Pattern recognition (psychology); Edge detection; Image segmentation; Image (mathematics); Image processing; Telecommunications,,,,,,http://dx.doi.org/10.1007/s11760-024-03149-6,,10.1007/s11760-024-03149-6,,,0,004-019-652-142-123; 009-122-927-490-379; 013-853-109-683-842; 015-958-732-809-439; 016-287-029-525-567; 017-709-713-237-995; 023-309-209-821-636; 026-994-702-158-53X; 028-143-078-747-070; 042-567-876-492-099; 066-232-160-024-413; 068-175-978-862-717; 073-873-978-329-524; 075-465-285-579-483; 083-923-091-144-911; 092-046-009-147-393; 095-920-456-586-491; 096-070-644-360-922; 108-893-411-318-038; 122-034-480-300-095; 146-761-001-613-927; 149-273-136-335-633,0,false,,
179-345-026-468-572,Innovative haptic-based system for upper limb rehabilitation in visually impaired individuals: a multilayer approach,2023-12-29,2023,journal article,Multimedia Tools and Applications,15737721; 13807501,Springer Science and Business Media LLC,Netherlands,Javier Albusac; Vanesa Herrera; Santiago Schez-Sobrino; Rubén Grande; Dorothy N. Monekosso; David Vallejo,,83,21,60537,60563,Computer science; Haptic technology; Visually impaired; Rehabilitation; Human–computer interaction; Physical medicine and rehabilitation; Simulation; Computer vision; Artificial intelligence; Physical therapy; Medicine,,,,Ministerio de Ciencia e Innovación; Universidad de Castilla-La Mancha,,http://dx.doi.org/10.1007/s11042-023-17892-4,,10.1007/s11042-023-17892-4,,,0,002-425-977-914-305; 003-503-154-468-946; 004-895-779-508-444; 007-204-850-367-28X; 010-783-368-701-785; 014-444-265-759-289; 016-766-180-985-505; 018-132-471-339-595; 020-556-183-025-517; 022-296-116-327-114; 024-208-450-600-315; 030-090-261-408-54X; 030-500-337-935-178; 038-326-446-864-583; 041-111-086-615-769; 042-513-927-076-893; 044-097-632-032-714; 045-505-911-987-050; 047-253-972-128-203; 058-322-493-945-906; 065-078-747-021-140; 066-246-145-459-483; 067-987-417-954-686; 070-832-423-342-370; 074-389-432-549-789; 080-822-806-594-165; 081-812-218-040-69X; 083-671-887-012-721; 085-006-795-415-796; 098-910-337-444-079; 101-852-082-833-016; 103-984-449-085-841; 109-210-286-601-117; 117-894-223-223-163; 121-439-923-704-187; 123-318-648-635-296; 123-907-494-825-374; 127-017-754-158-055; 139-446-773-438-846; 139-760-416-058-47X; 141-649-000-612-322,0,false,,
180-394-697-208-41X,2.5-mm articulated endoluminal forceps using a compliant mechanism.,2022-08-03,2022,journal article,International journal of computer assisted radiology and surgery,18616429; 18616410,Springer Science and Business Media LLC,Germany,Keisuke Osawa; D S V Bandara; Ryu Nakadate; Yoshihiro Nagao; Tomohiko Akahoshi; Masatoshi Eto; Jumpei Arata,"<AbstractText Label=""PURPOSE"" NlmCategory=""OBJECTIVE"">Gastrointestinal cancer can be treated using a flexible endoscope through a natural orifice. However, treatment instruments with limited degrees of freedom (DOFs) require a highly skilled operator. Articulated devices useful for endoluminal procedures, such as endoscopic submucosal dissection and biopsy, have been developed. These devices enable dexterous operation in a narrow lumen; however, they suffer from limitations such as large size and high cost. To overcome these limitations, we developed a 2.5-mm articulated forceps that can be inserted into a standard endoscope channel based on a compliant mechanism.</AbstractText>;           <AbstractText Label=""METHODS"" NlmCategory=""METHODS"">The compliant mechanism allows the device to be compact and affordable, which is possible due to its monolithic structure. The proposed mechanism consists of two segments, 1-DOF grasping and 2-DOF bending, that are actuated by tendon-sheath mechanisms. A prototype was designed based on finite element analysis results.</AbstractText>;           <AbstractText Label=""RESULTS"" NlmCategory=""RESULTS"">To confirm the effectiveness of the proposed mechanism, we fabricated the prototype using a 3D printer. A series of mechanical performance tests on the prototype revealed that it achieved the following specifications: (1) DOF: 1-DOF grasping + 2-DOF bending, (2) outer diameter: 2.5 mm, (3) length of the bending segment: 30 mm, and (4) range of motion: [Formula: see text] to [Formula: see text] (grasping) and [Formula: see text] to [Formula: see text] (bending). Finally, we performed a tissue manipulation test on an excised porcine colon and found that a piece of mucous membrane tissue was successfully resected using an electric knife while being lifted with the developed forceps.</AbstractText>;           <AbstractText Label=""CONCLUSION"" NlmCategory=""CONCLUSIONS"">The results of the evaluation experiment demonstrated a positive feasibility of the proposed mechanism, which has a simpler structure compared to those of other conventional mechanisms; furthermore, it is potentially more cost-effective and is disposable. The mechanical design, prototype implementation, and evaluations are reported in this paper.</AbstractText>;           <CopyrightInformation>© 2022. CARS.</CopyrightInformation>",18,1,1,123,Exhibition; Medicine; Medical physics; Library science; General surgery; Computer science; Art history; Art; Forceps; Bending; Endoscope; Mechanism (biology); Micromanipulator; Compliant mechanism; Biomedical engineering; Finite element method; Materials science; Simulation; Artificial intelligence; Surgery; Physics; Engineering; Structural engineering; Quantum mechanics; Composite material,Compliant mechanism; Endoluminal instrument; Endoscopic submucosal dissection; Medical robotics,Animals; Swine; Equipment Design; Endoscopes; Robotic Surgical Procedures/methods; Biopsy; Surgical Instruments,,JSTSPRING (JPMJSP2136),https://link.springer.com/content/pdf/10.1007/s11548-023-02878-2.pdf https://doi.org/10.1007/s11548-023-02878-2,http://dx.doi.org/10.1007/s11548-023-02878-2,35922706,10.1007/s11548-023-02878-2; 10.1007/s11548-022-02726-9,,PMC10233546,0,000-357-212-602-612; 000-383-645-781-985; 016-129-488-871-908; 018-413-234-954-691; 018-623-518-267-063; 019-465-127-861-087; 020-986-249-896-690; 027-517-071-753-600; 029-320-777-338-031; 030-694-372-431-678; 032-527-848-917-652; 034-220-145-851-665; 037-129-451-894-534; 038-343-755-431-233; 039-452-103-714-069; 045-417-382-189-001; 055-094-958-564-476; 062-576-760-915-096; 062-896-899-008-274; 064-254-425-503-132; 064-674-539-602-770; 066-269-700-577-406; 081-688-787-457-078; 097-816-564-073-653; 101-932-030-149-896; 110-788-855-693-821; 126-907-262-191-084; 129-825-386-184-433; 132-041-111-942-189; 149-989-298-963-126; 170-850-755-719-722,4,true,,bronze
180-431-003-644-218,"Peripheral vision contributes to implicit attentional learning: Findings from the ""mouse-eye"" paradigm.",2024-06-05,2024,journal article,"Attention, perception & psychophysics",1943393x; 19433921,Springer Science and Business Media LLC,United States,Chen Chen; Vanessa G Lee,,,,,,Peripheral vision; Psychology; Gaze; Cognitive psychology; Eye movement; Eye tracking; Implicit learning; Visual field; Quadrant (abdomen); Cognition; Neuroscience; Computer vision; Computer science; Medicine; Pathology; Psychoanalysis,Location probability learning; Peripheral vision; Spatial attention; Visual search,,,McKnight award,,http://dx.doi.org/10.3758/s13414-024-02907-5,38839714,10.3758/s13414-024-02907-5,,,0,003-207-278-501-131; 004-460-300-789-254; 004-954-497-271-300; 005-156-673-670-053; 005-811-109-952-218; 007-353-461-769-401; 010-371-814-457-102; 011-195-748-150-896; 011-637-839-179-817; 012-952-868-125-988; 014-765-258-061-891; 016-070-726-198-904; 018-026-048-977-486; 019-647-710-346-434; 020-760-706-913-870; 021-674-499-744-810; 022-050-976-266-366; 022-782-447-729-47X; 026-512-382-963-69X; 032-049-177-406-929; 033-654-762-042-902; 037-634-242-377-968; 041-633-508-813-502; 045-652-699-113-304; 051-496-664-523-923; 053-886-362-556-179; 057-687-726-361-326; 057-969-342-912-067; 060-847-379-177-02X; 062-274-194-220-463; 063-075-100-682-793; 065-073-076-341-844; 073-536-090-595-953; 077-673-258-487-507; 080-518-093-574-714; 082-252-764-511-560; 085-824-813-193-758; 092-529-857-602-359; 098-576-354-002-896; 104-903-308-567-088; 105-397-839-955-300; 117-073-022-665-338; 117-213-083-407-633; 133-874-315-350-292; 155-298-050-729-242; 189-527-766-517-413,0,false,,
180-719-271-353-911,NavCue: Context immersive navigation assistance for blind travelers,,2016,conference proceedings article,2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI),,IEEE,,Kangwei Chen; Victoria Plaza-Leiva; Byung-Cheol Min; Aaron Steinfeld; Mary Bernardine Dias,"Research in assistive systems for travelers who are blind/low vision (B/LV) has been largely focused on basic map information. We present NavCue, an intelligent system module for providing rich, multi-sensory, context-based information using speech guidance and robot physical gestures. This approach is motivated by our previous user studies with people who are blind or low vision. This rich information should enhance user location awareness and confidence when traveling through unfamiliar locations.",,,,,Gesture; Computer science; Human–computer interaction; Context (archaeology); Robot; User interface; Multimedia; Computer vision; Artificial intelligence; Paleontology; Biology; Operating system,,,,,,http://dx.doi.org/10.1109/hri.2016.7451855,,10.1109/hri.2016.7451855,,,0,154-764-681-989-082,1,false,,
180-818-479-502-06X,BarChartAnalyzer: Data Extraction and Summarization of Bar Charts from Images,2022-10-01,2022,journal article,SN Computer Science,26618907; 2662995x,Springer Science and Business Media LLC,,Siri Chandana Daggubati; Jaya Sreevalsan-Nair; Komal Dadhich,,3,6,,,Automatic summarization; Computer science; Workflow; Chart; Bar chart; Data mining; Row; Artificial intelligence; Information extraction; Information retrieval; Database; Mathematics; Statistics,,,,,,http://dx.doi.org/10.1007/s42979-022-01380-x,,10.1007/s42979-022-01380-x,,,0,001-356-619-690-863; 002-730-375-489-867; 004-269-574-716-057; 007-764-127-368-73X; 016-427-054-200-088; 016-776-429-552-596; 032-581-001-552-982; 032-781-554-160-192; 033-218-942-529-08X; 033-809-928-480-94X; 033-906-992-249-245; 037-180-555-614-190; 039-088-685-967-50X; 044-356-060-642-288; 048-737-113-503-930; 052-193-263-990-158; 054-698-856-588-03X; 057-346-507-883-890; 057-922-075-185-783; 072-849-969-003-550; 081-201-607-519-693; 087-860-414-385-50X; 094-206-454-118-462; 104-692-086-469-484; 110-872-665-849-778; 121-640-558-245-510; 131-519-582-504-904; 137-858-631-922-307; 150-404-432-089-070; 161-832-599-574-262; 187-265-335-672-059; 195-706-909-301-830,5,false,,
181-569-304-874-410,Sonification as a reliable alternative to conventional visual surgical navigation.,2023-04-12,2023,journal article,Scientific reports,20452322,Springer Science and Business Media LLC,United Kingdom,Sasan Matinfar; Mehrdad Salehi; Daniel Suter; Matthias Seibold; Shervin Dehghani; Navid Navab; Florian Wanivenhaus; Philipp Fürnstahl; Mazda Farshad; Nassir Navab,"Despite the undeniable advantages of image-guided surgical assistance systems in terms of accuracy, such systems have not yet fully met surgeons' needs or expectations regarding usability, time efficiency, and their integration into the surgical workflow. On the other hand, perceptual studies have shown that presenting independent but causally correlated information via multimodal feedback involving different sensory modalities can improve task performance. This article investigates an alternative method for computer-assisted surgical navigation, introduces a novel four-DOF sonification methodology for navigated pedicle screw placement, and discusses advanced solutions based on multisensory feedback. The proposed method comprises a novel four-DOF sonification solution for alignment tasks in four degrees of freedom based on frequency modulation synthesis. We compared the resulting accuracy and execution time of the proposed sonification method with visual navigation, which is currently considered the state of the art. We conducted a phantom study in which 17 surgeons executed the pedicle screw placement task in the lumbar spine, guided by either the proposed sonification-based or the traditional visual navigation method. The results demonstrated that the proposed method is as accurate as the state of the art while decreasing the surgeon's need to focus on visual navigation displays instead of the natural focus on surgical tools and targeted anatomy during task execution.",13,1,5930,,Sonification; Computer science; Task (project management); Human–computer interaction; Usability; Focus (optics); Workflow; Modalities; Navigation system; Artificial intelligence; Computer vision; Engineering; Social science; Physics; Optics; Database; Sociology; Systems engineering,,"Spinal Fusion/methods; Lumbar Vertebrae/surgery; Pedicle Screws; Surgery, Computer-Assisted/methods; Phantoms, Imaging",,Technische Universität München,https://www.nature.com/articles/s41598-023-32778-z.pdf https://doi.org/10.1038/s41598-023-32778-z https://www.zora.uzh.ch/id/eprint/227855/1/2022_Matinfar_Sonification_as_a_Reliable_Alternative_to_Conventional_Visual_Surgical_Navigation_arxiv.pdf http://arxiv.org/pdf/2206.15291 http://arxiv.org/abs/2206.15291,http://dx.doi.org/10.1038/s41598-023-32778-z,37045878,10.1038/s41598-023-32778-z,,PMC10097653,0,000-099-112-302-226; 000-257-098-197-44X; 001-438-228-549-925; 001-705-338-724-935; 002-620-657-668-321; 002-638-593-520-20X; 005-489-680-478-225; 005-492-130-539-772; 005-963-667-006-253; 007-388-059-490-904; 007-573-197-769-65X; 008-094-195-839-720; 009-606-694-358-561; 010-719-927-108-990; 013-895-086-072-955; 014-388-124-107-03X; 015-257-953-064-440; 015-262-213-066-885; 016-263-916-233-791; 019-924-398-169-551; 021-528-948-812-757; 022-078-114-842-66X; 022-976-350-173-140; 027-097-281-265-746; 027-278-301-642-047; 027-603-498-384-905; 028-150-598-996-527; 028-177-324-539-740; 029-654-399-730-959; 029-985-148-406-958; 030-437-137-842-381; 032-077-673-631-306; 033-167-709-130-702; 033-677-264-279-589; 037-191-453-715-699; 037-284-587-671-17X; 037-923-176-119-274; 038-148-370-446-780; 038-879-901-406-93X; 039-180-537-312-247; 041-319-185-597-538; 048-503-024-552-157; 051-564-872-610-249; 052-911-966-816-141; 053-599-755-789-721; 056-154-800-525-010; 058-031-014-647-937; 059-525-180-171-125; 059-621-672-265-849; 061-958-542-235-401; 064-187-136-547-000; 066-465-922-468-975; 068-376-015-620-038; 071-473-658-566-47X; 075-076-760-554-498; 076-011-531-861-522; 077-025-334-771-062; 077-116-898-354-559; 082-060-789-804-822; 082-960-149-965-20X; 085-991-440-860-906; 089-730-190-470-496; 092-761-174-302-337; 093-850-327-786-194; 096-198-966-272-805; 098-170-952-380-344; 099-769-895-769-931; 100-320-084-559-483; 106-988-935-251-865; 112-628-578-335-566; 119-443-361-949-915; 122-104-622-847-936; 125-276-397-918-67X; 131-786-333-753-818; 136-936-727-229-35X; 146-051-631-033-699; 157-037-812-120-044; 166-716-413-704-975; 167-973-668-285-85X; 185-530-010-249-545,11,true,"CC BY, CC BY-NC-ND",gold
181-683-309-384-305,Real-Time Speed Breaker Detection with an Edge Impulse,2024-08-08,2024,journal article,SN Computer Science,26618907,Springer Science and Business Media LLC,,S. Manjula; M. Swarna Sudha; C. A. Yogaraja; C. Muthuaruna,,5,6,,,,,,,,,http://dx.doi.org/10.1007/s42979-024-03132-5,,10.1007/s42979-024-03132-5,,,0,004-541-048-517-63X; 008-724-907-480-415; 011-273-717-035-554; 022-568-752-287-877; 025-427-425-038-216; 038-833-465-706-229; 049-374-351-253-38X; 060-773-956-506-366; 098-133-401-395-993; 101-493-350-981-292; 126-643-959-285-762; 153-487-864-888-63X; 186-609-592-356-065,0,false,,
183-114-233-653-013,An Alternative Approach to Vision Techniques - Pedestrian Navigation System based on Digital Magnetic Compass and Gyroscope Integration,,2002,,,,,,Quentin Ladetto; Bertrand Merminod,"Over the last few years, research had been conducted on how to develop basic mobility aid for visually impaired and blind people using vision and image processing techniques. However, our research at Geodetic Engineering Laboratory has taken a different view to this problem.",,,,,Engineering; Artificial intelligence; Gyroscope; Compass; Mobility aid; Pedestrian navigation system; Visually impaired; Computer vision; Geodetic datum; Image processing,,,,,https://infoscience.epfl.ch/record/29193 https://www.researchgate.net/profile/Bertrand_Merminod/publication/37409593_An_Alternative_Approach_to_Vision_Techniques_-_Pedestrian_Navigation_System_based_on_Digital_Magnetic_Compass_and_Gyroscope_Integration/links/0912f5106be4fca49e000000.pdf,https://infoscience.epfl.ch/record/29193,,,807105365,,0,045-628-323-477-10X,9,false,,
183-431-315-941-982,Smart Object and Face Detection Assistant for Visually Impaired,2023-06-16,2023,conference proceedings article,2023 International Conference on Applied Intelligence and Sustainable Computing (ICAISC),,IEEE,,G. Divya; Sankar Dasiga; B M Gurudatt; S Guruduth; A H Akshatha; G Gaurav,"The World Health Organization (WHO) estimates that 253 million people worldwide are blind or visually impaired. 217 million of them have moderate to severe visual impairment, leaving 36 million of them entirely blind. The majority of people who are visually impaired live in low- and middle-income nations, and there is evidence that vision impairment rises with age. Visually challenged people need access to a face and object detection system in order to improve their mobility and capacity for social interaction. The suggested remedy makes use of a Raspberry Pi equipped with a camera module to capture real-time video feeds, OpenCV for image processing, and machine learning techniques for object and face recognition. The proposed methodology includes YOLO Algorithm and Convolutional Neural Network Architecture in implementation. The system is designed to provide real-time feedback to the user via audio and haptic feedback to notify them of the presence of people or objects in their vicinity. The project also includes a user-friendly interface that allows the user to customize the detection settings to suit their individual needs. The system was tested on various scenarios and showed promising results in terms of accuracy and responsiveness. Overall, the proposed system provides a practical solution for visually impaired individuals to overcome their daily navigation challenges and to improve their quality of life. The results are obtained in both software and hardware implementation.",,,,,Computer science; Object detection; Face detection; Convolutional neural network; Facial recognition system; Interface (matter); Artificial intelligence; Computer vision; Object (grammar); Haptic technology; Visual impairment; Face (sociological concept); Cognitive neuroscience of visual object recognition; Human–computer interaction; Feature extraction; Pattern recognition (psychology); Psychology; Social science; Sociology; Bubble; Maximum bubble pressure method; Parallel computing; Psychiatry,,,,,,http://dx.doi.org/10.1109/icaisc58445.2023.10200356,,10.1109/icaisc58445.2023.10200356,,,0,003-828-556-159-797; 004-004-306-782-118; 010-876-983-456-294; 016-445-748-246-67X; 016-572-318-414-022; 027-588-207-376-882; 030-733-593-526-194; 043-218-833-589-684; 065-411-531-321-861; 065-437-730-773-313; 073-529-609-559-989; 085-560-687-499-839; 092-452-392-581-46X; 092-958-099-002-180; 094-812-368-636-394; 095-427-604-075-652; 102-742-753-560-565; 103-872-524-219-402; 105-095-514-938-373; 106-587-878-038-509; 115-160-934-525-891; 116-835-130-405-826; 134-386-818-280-39X; 137-397-776-731-350; 163-506-338-712-945; 184-420-180-236-188,1,false,,
183-843-227-588-534,A Survey on the Usage of Pattern Recognition and Image Analysis Methods for the Lifestyle Improvement on Low Vision and Visually Impaired People,2021-04-08,2021,journal article,Pattern Recognition and Image Analysis,10546618; 15556212,Pleiades Publishing Ltd,United States,M. Anitha; V. D. Ambeth Kumar; S. Malathi; V.D. Ashok Kumar; M. Ramakrishnan; Abhishek Kumar; Rashid Ali,,31,1,24,34,Human–computer interaction; Image (mathematics); Pattern recognition (psychology); Domain (software engineering); Key (cryptography); Braille; Visually impaired; Field (computer science); Computer science; Pattern matching,,,,,https://link.springer.com/article/10.1134/S105466182101003X,http://dx.doi.org/10.1134/s105466182101003x,,10.1134/s105466182101003x,3156036255,,0,000-371-951-472-939; 000-521-410-320-917; 001-178-976-230-954; 002-096-130-766-170; 002-477-668-318-846; 002-958-319-229-113; 003-674-350-182-407; 004-068-122-206-08X; 005-362-510-116-848; 005-619-552-121-195; 005-990-558-175-764; 006-379-827-104-885; 008-506-450-814-490; 008-985-536-094-149; 009-966-998-920-567; 011-290-458-246-254; 013-211-461-936-429; 017-979-191-198-035; 019-200-000-657-608; 019-742-546-408-36X; 020-546-099-295-320; 020-547-299-198-407; 021-381-415-885-298; 025-103-567-318-378; 026-144-225-426-042; 026-487-814-367-268; 027-720-558-803-335; 031-160-271-128-821; 034-526-083-267-19X; 034-717-896-538-278; 035-608-452-018-304; 037-900-638-753-711; 040-431-674-314-483; 041-997-100-015-950; 042-213-292-852-865; 042-572-532-008-378; 043-067-842-956-364; 044-202-560-313-315; 046-562-183-165-194; 046-906-287-427-917; 049-104-046-974-455; 049-584-080-411-184; 050-531-002-794-614; 050-843-882-722-370; 052-079-097-964-632; 054-929-213-374-703; 060-292-654-735-975; 063-960-958-496-512; 064-964-844-282-238; 065-011-455-775-091; 065-079-380-905-074; 065-682-829-846-411; 067-353-870-707-101; 071-847-336-360-376; 072-857-590-612-087; 074-959-946-597-085; 075-022-543-787-080; 078-238-527-890-59X; 079-195-779-987-944; 079-674-152-518-333; 081-283-192-235-729; 083-768-318-680-907; 087-429-137-788-635; 092-726-040-947-902; 096-219-284-474-75X; 098-811-589-762-176; 105-633-704-675-645; 105-955-884-501-141; 108-743-738-127-328; 110-077-840-962-08X; 111-614-519-353-533; 112-065-667-681-501; 113-894-768-157-834; 114-740-836-402-286; 116-460-193-426-271; 119-892-455-283-484; 121-174-541-343-573; 126-493-028-671-162; 135-910-853-481-044; 137-778-705-314-997; 138-129-013-906-541; 143-393-959-777-457; 145-003-132-204-526; 146-272-451-874-270; 152-026-077-536-223; 153-117-929-721-059; 156-735-121-230-238; 158-304-929-176-789; 160-118-618-371-117; 161-617-941-799-458; 162-756-074-890-46X; 166-380-594-104-285; 180-985-986-155-187; 182-877-079-865-665; 186-412-044-017-67X; 187-190-292-700-174; 190-746-633-042-085; 192-346-188-739-916,6,false,,
183-947-709-586-992,Cross-Safe: A Computer Vision-Based Approach to Make All Intersection-Related Pedestrian Signals Accessible for the Visually Impaired,2019-04-24,2019,book chapter,Advances in Intelligent Systems and Computing,21945357; 21945365,Springer International Publishing,,Xiang Li; Hanzhang Cui; John Ross Rizzo; Edward K. Wong; Yi Fang,"Intersections pose great challenges to blind or visually impaired travelers who aim to cross roads safely and efficiently given unpredictable traffic control. Due to decreases in vision and increasingly difficult odds when planning and negotiating dynamic environments, visually impaired travelers require devices and/or assistance (i.e. cane, talking signals) to successfully execute intersection navigation. The proposed research project is to develop a novel computer vision-based approach, named Cross-Safe, that provides accurate and accessible guidance to the visually impaired as one crosses intersections, as part of a larger unified smart wearable device. As a first step, we focused on the red-light-green-light, go-no-go problem, as accessible pedestrian signals are drastically missing from urban infrastructure in New York City. Cross-Safe leverages state-of-the-art deep learning techniques for real-time pedestrian signal detection and recognition. A portable GPU unit, the Nvidia Jetson TX2, provides mobile visual computing and a cognitive assistant provides accurate voice-based guidance. More specifically, a lighter recognition algorithm was developed and equipped for Cross-Safe, enabling robust walking signal sign detection and signal recognition. Recognized signals are conveyed to visually impaired end user by vocal guidance, providing critical information for real-time intersection navigation. Cross-Safe is also able to balance portability, recognition accuracy, computing efficiency and power consumption. A custom image library was built and developed to train, validate, and test our methodology on real traffic intersections, demonstrating the feasibility of Cross-Safe in providing safe guidance to the visually impaired at urban intersections. Subsequently, experimental results show robust preliminary findings of our detection and recognition algorithm.",,,132,146,Deep learning; Detection theory; Wearable computer; Artificial intelligence; Pedestrian; Visual computing; Computer vision; Computer science; Software portability; Intersection (aeronautics); End user,,,,,https://link.springer.com/chapter/10.1007/978-3-030-17798-0_13 https://nyuscholars.nyu.edu/en/publications/cross-safe-a-computer-vision-based-approach-to-make-all-intersect https://rd.springer.com/chapter/10.1007/978-3-030-17798-0_13,http://dx.doi.org/10.1007/978-3-030-17798-0_13,,10.1007/978-3-030-17798-0_13,2946653762,,0,005-922-828-706-875; 031-218-334-653-826; 031-967-529-225-926; 040-445-706-184-474; 047-767-493-168-900; 049-317-239-158-314; 066-133-135-091-645; 076-853-538-573-049; 099-150-824-995-115; 100-404-039-254-355; 109-617-690-629-802; 121-565-008-467-672; 125-112-083-666-544; 171-091-935-415-285; 173-062-128-326-219,23,false,,
184-116-517-417-325,Are electronic white canes better than traditional canes? A comparative study with blind and blindfolded participants,2020-02-17,2020,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Aline Darc Piculo dos Santos; Fausto Orsi Medola; Milton José Cinelli; Alejandro Rafael Garcia Ramirez; Frode Eika Sandnes,"Visually impaired individuals often rely on assistive technologies such as white canes for independent navigation. Many electronic enhancements to the traditional white cane have been proposed. However, only a few of these proof-of-concept technologies have been tested with authentic users, as most studies rely on blindfolded non-visually impaired participants or no testing with participants at all. Experiments involving blind users are usually not contrasted with the traditional white cane. This study set out to compare an ultrasound-based electronic cane with a traditional white cane. Moreover, we also compared the performance of a group of visually impaired participants (N = 10) with a group of blindfolded participants without visual impairments (N = 31). The results show that walking speed with the electronic cane is significantly slower compared to the traditional white cane. Moreover, the results show that the performance of the participants without visual impairments is significantly slower than for the visually impaired participants. No significant differences in obstacle detection rates were observed across participant groups and device types for obstacles on the ground, while 79% of the hanging obstacles were detected by the electronic cane. The results of this study thus suggest that electronic canes present only one advantage over the traditional cane, namely in its ability to detect hanging obstacles, at least without prolonged practice. Next, blindfolded participants are insufficient substitutes for blind participants who are expert cane users. The implication of this study is that research into digital white cane enhancements should include blind participants. These participants should be followed over time in longitudinal experiments to document if practice will lead to improvements that surpass the performance achieved with traditional canes.",20,1,93,103,Psychology; Set (psychology); Cane; White cane; Assistive technology; Visually impaired; Detection rate; Audiology; White (horse); Preferred walking speed,,,,CAPES; DIKU,https://paperity.org/p/232767536/are-electronic-white-canes-better-than-traditional-canes-a-comparative-study-with-blind https://oda.oslomet.no/oda-xmlui/handle/10642/9597 https://link.springer.com/content/pdf/10.1007%2Fs10209-020-00712-z.pdf https://link.springer.com/article/10.1007/s10209-020-00712-z https://repositorio.unesp.br/handle/11449/196620 https://doi.org/10.1007/s10209-020-00712-z,http://dx.doi.org/10.1007/s10209-020-00712-z,,10.1007/s10209-020-00712-z,3005685928,,0,000-319-559-523-254; 001-757-439-881-117; 002-841-567-358-302; 003-342-986-507-202; 003-687-230-494-150; 004-398-909-518-572; 004-840-196-649-693; 007-530-851-453-706; 009-645-272-092-857; 012-081-521-655-479; 013-056-435-015-274; 016-358-694-994-270; 022-098-541-147-90X; 023-680-889-449-204; 024-417-401-364-679; 025-556-452-216-701; 025-716-725-120-01X; 025-968-486-135-433; 026-922-910-952-305; 030-500-337-935-178; 030-949-214-673-542; 031-223-487-502-873; 034-936-259-439-98X; 035-155-677-944-748; 041-346-284-111-904; 045-111-552-339-892; 049-614-977-269-922; 050-813-649-265-217; 054-879-961-055-393; 056-240-268-469-390; 056-745-340-837-164; 068-247-037-195-852; 068-526-647-480-345; 069-523-876-223-951; 069-784-235-043-745; 086-986-750-179-729; 089-487-717-586-296; 093-313-084-202-372; 093-548-249-489-049; 095-008-123-692-063; 101-969-663-263-663; 118-533-294-109-774; 119-036-310-031-196; 124-419-408-954-668; 130-552-169-471-000; 133-300-802-059-627; 138-528-673-324-553; 139-233-304-780-009; 164-719-327-371-917; 176-619-480-167-936; 178-917-329-886-901,45,true,cc-by,hybrid
184-116-855-210-992,Design and development of an indoor navigation system using denoising autoencoder based convolutional neural network for visually impaired people,2022-01-13,2022,journal article,Multimedia Tools and Applications,13807501; 15737721,Springer Science and Business Media LLC,Netherlands,J. Akilandeswari; G. Jothi; A. Naveenkumar; R. S. Sabeenian; P. Iyyanar; M. E. Paramasivam,,81,3,3483,3514,Computer science; Autoencoder; Convolutional neural network; Artificial intelligence; Deep learning; Artificial neural network; Machine learning; Computer vision; Pattern recognition (psychology),,,,"Science for Equity, Empowerment and Development Division",,http://dx.doi.org/10.1007/s11042-021-11287-z,,10.1007/s11042-021-11287-z,,,0,002-754-827-662-648; 003-947-930-918-725; 005-533-594-873-638; 006-625-302-369-283; 008-139-948-663-237; 008-217-809-214-58X; 009-083-153-351-930; 012-218-804-555-800; 014-144-371-884-732; 014-619-962-804-18X; 015-714-235-272-975; 023-805-082-979-008; 025-957-275-011-293; 026-994-702-158-53X; 029-603-934-974-438; 034-456-604-262-749; 040-834-320-822-882; 047-749-445-596-16X; 051-039-727-067-90X; 053-060-553-031-26X; 057-617-234-656-319; 058-247-815-514-678; 060-309-856-681-184; 064-742-750-569-146; 069-158-777-946-983; 070-116-981-693-266; 082-945-313-450-254; 084-183-891-000-305; 087-589-230-909-529; 090-871-058-688-660; 092-520-794-435-061; 093-596-358-012-81X; 094-104-899-809-998; 095-312-384-822-002; 099-911-735-022-411; 103-845-962-344-269; 117-894-727-533-501; 141-795-765-897-353; 151-015-279-636-660; 155-722-907-958-870; 164-155-946-000-516; 165-785-755-787-443,16,false,,
184-459-008-945-490,Graph neural networks in vision-language image understanding: a survey,2024-03-29,2024,journal article,The Visual Computer,01782789; 14322315,Springer Science and Business Media LLC,Germany,Henry Senior; Gregory Slabaugh; Shanxin Yuan; Luca Rossi,"<jats:title>Abstract</jats:title><jats:p>2D image understanding is a complex problem within computer vision, but it holds the key to providing human-level scene comprehension. It goes further than identifying the objects in an image, and instead, it attempts to <jats:italic>understand</jats:italic> the scene. Solutions to this problem form the underpinning of a range of tasks, including image captioning, visual question answering (VQA), and image retrieval. Graphs provide a natural way to represent the relational arrangement between objects in an image, and thus, in recent years graph neural networks (GNNs) have become a standard component of many 2D image understanding pipelines, becoming a core architectural component, especially in the VQA group of tasks. In this survey, we review this rapidly evolving field and we provide a taxonomy of graph types used in 2D image understanding approaches, a comprehensive list of the GNN models used in this domain, and a roadmap of future potential developments. To the best of our knowledge, this is the first comprehensive survey that covers image captioning, visual question answering, and image retrieval techniques that focus on using GNNs as the main part of their architecture.; </jats:p>",,,,,Computer science; Computer graphics; Artificial intelligence; Graphics; Graph; Artificial neural network; Image (mathematics); Computer vision; Theoretical computer science; Computer graphics (images),,,,Engineering and Physical Sciences Research Council,https://link.springer.com/content/pdf/10.1007/s00371-024-03343-0.pdf https://doi.org/10.1007/s00371-024-03343-0,http://dx.doi.org/10.1007/s00371-024-03343-0,,10.1007/s00371-024-03343-0,,,0,000-406-256-405-942; 000-445-270-830-197; 003-215-340-330-223; 003-315-030-342-753; 003-690-410-102-748; 003-933-272-647-363; 004-269-574-716-057; 004-576-495-302-462; 004-752-252-092-739; 004-786-811-483-310; 005-690-223-198-425; 007-562-024-350-558; 008-001-206-027-096; 011-145-589-259-847; 011-241-717-579-539; 011-815-374-783-502; 012-327-360-339-498; 012-469-933-876-551; 015-708-026-281-567; 015-968-400-395-152; 018-162-490-700-507; 018-193-483-515-989; 019-729-680-880-124; 020-561-831-770-539; 021-896-779-935-682; 022-705-697-906-740; 024-471-586-850-231; 027-063-688-276-767; 028-828-189-727-971; 029-055-925-495-377; 033-237-618-457-25X; 035-436-048-860-761; 035-844-327-244-366; 035-894-801-763-506; 036-941-125-852-401; 037-441-555-622-134; 040-171-574-882-055; 040-965-694-890-054; 042-314-317-354-874; 046-583-027-625-542; 046-714-875-047-957; 047-777-721-582-281; 051-430-839-848-657; 052-703-789-008-699; 053-135-426-075-802; 053-911-609-575-024; 056-893-117-058-488; 058-987-530-034-391; 061-065-523-494-469; 062-629-789-436-263; 062-761-901-321-801; 067-901-313-928-061; 068-161-739-096-78X; 069-480-623-909-181; 071-916-725-154-054; 073-557-631-429-601; 076-254-404-417-740; 076-693-630-528-726; 077-677-738-939-056; 077-798-526-658-018; 078-700-778-425-016; 079-413-394-397-030; 079-682-863-778-609; 081-518-727-437-209; 081-892-798-474-277; 090-152-747-210-776; 090-432-106-031-504; 091-201-374-753-069; 093-358-263-054-546; 095-983-968-207-764; 100-340-275-038-662; 100-578-902-242-559; 100-609-810-132-503; 101-885-110-108-614; 102-195-290-100-844; 103-774-457-019-603; 106-926-759-402-770; 107-812-447-606-337; 108-555-291-612-731; 112-154-051-246-370; 113-215-238-371-852; 113-455-098-712-979; 116-788-208-218-918; 117-112-309-294-641; 118-357-955-476-257; 122-513-536-227-076; 124-729-022-006-458; 129-398-785-761-556; 133-490-470-321-640; 136-063-653-293-526; 136-644-003-961-591; 141-384-677-093-172; 141-937-172-247-874; 143-294-819-806-400; 146-056-409-127-988; 148-230-701-477-137; 151-072-888-070-317; 153-130-591-875-734; 153-365-019-090-780; 160-980-391-044-324; 163-262-796-379-002; 190-783-604-154-865; 195-706-909-301-830; 197-690-770-816-045,0,true,cc-by,hybrid
185-253-697-584-063,"How Users, Facility Managers, and Bystanders Perceive and Accept a Navigation Robot for Visually Impaired People in Public Buildings",2022-08-29,2022,conference proceedings article,2022 31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),,IEEE,,Seita Kayukawa; Daisuke Sato; Masayuki Murata; Tatsuya Ishihara; Akihiro Kosugi; Hironobu Takagi; Shigeo Morishima; Chieko Asakawa,"Autonomous navigation robots have a considerable potential to offer a new form of mobility aid to people with visual impairments. However, to deploy such robots in public buildings, it is imperative to receive acceptance from not only robot users but also people that use the buildings and managers of those facilities. Therefore, we conducted three studies to investigate the acceptance and concerns of our prototype robot, which looks like a regular suitcase. First, an online survey revealed that people could accept the robot navigating blind users. Second, in the interviews with facility managers, they were cautious about the robot's camera and the privacy of their customers. Finally, focus group sessions with legally blind participants who experienced the robot navigation revealed that the robot may cause trouble when it collides with those who may not be aware of the user's blindness. Still, many participants liked the design of the robot which assimilated into the surroundings.",,,,,Robot; Internet privacy; Blindness; Computer science; Human–computer interaction; Focus group; Psychology; Computer security; Applied psychology; Artificial intelligence; Business; Medicine; Optometry; Marketing,,,,JST-Mirai Program,,http://dx.doi.org/10.1109/ro-man53752.2022.9900717,,10.1109/ro-man53752.2022.9900717,,,0,009-569-730-697-465; 020-373-201-301-684; 023-818-209-698-899; 029-313-431-507-058; 041-202-711-448-93X; 041-492-606-932-699; 048-697-169-430-415; 058-013-370-483-44X; 060-147-659-950-016; 080-879-120-842-969; 086-869-440-167-789; 094-871-870-293-980; 122-148-523-515-519; 130-816-777-305-897; 139-177-072-876-12X; 145-051-572-342-371; 158-368-277-657-984; 196-866-598-969-196,10,false,,
186-478-329-376-424,Wearable System to Guide Crosswalk Navigation for People With Visual Impairment,2022-03-03,2022,journal article,Frontiers in Electronics,26735857,Frontiers Media SA,,Hojun Son; James Weiland,"<jats:p>Independent travelling is a significant challenge for visually impaired people in urban settings. Traditional and widely used aids such as guide dogs and long canes provide basic guidance and obstacle avoidance but are not sufficient for complex situations such as street crossing. We propose a new wearable system that can safely guide a user with visual impairment at a signalized crosswalk. Safe street crossing is an important element of fully independent travelling for people who are blind or visually impaired (BVI), but street crossing is challenging for BVI because it involves several steps reliant on vision, including scene understanding, localization, object detection, path planning, and path following. Street crossing also requires timely completion. Prior solutions for guiding BVI in crosswalks have focused on either detection of crosswalks or classifying crosswalks signs. In this paper, we demonstrate a system that performs all the functions necessary to safely guide BVI at a signalized crosswalk. Our system utilizes prior maps, similar to how autonomous vehicles are guided. The hardware components are lightweight such that they can be wearable and mobile, and all are commercially available. The system operates in real-time. Computer vision algorithms (Orbslam2) localize the user in the map and orient them to the crosswalk. The state of the crosswalk signal (don’t walk or walk) is detected (using a convolutional neural network), the user is notified (via verbal instructions) when it is safe to cross, and the user is guided (via verbal instructions) along a path towards a destination on the prior map. The system continually updates user position relative to the path and corrects the user’s trajectory with simple verbal commands. We demonstrate the system functionality in three BVI participants. With brief training, all three were able to use the system to successfully navigate a crosswalk in a safe manner.</jats:p>",2,,,,Schema crosswalk; Computer science; Computer vision; Wearable computer; Obstacle; Visually impaired; Artificial intelligence; Path (computing); Human–computer interaction; Convolutional neural network; Obstacle avoidance; Visual impairment; Pedestrian; Engineering; Embedded system; Transport engineering; Mobile robot; Psychology; Computer network; Geography; Psychiatry; Robot; Archaeology,,,,,https://www.frontiersin.org/articles/10.3389/felec.2021.790081/pdf https://doi.org/10.3389/felec.2021.790081,http://dx.doi.org/10.3389/felec.2021.790081,,10.3389/felec.2021.790081,,,0,007-223-103-783-187; 011-265-433-636-69X; 015-077-055-227-430; 015-627-652-468-711; 016-837-621-369-448; 025-670-117-400-666; 026-204-328-399-384; 031-842-427-416-191; 044-170-635-000-193; 053-219-839-728-849; 056-301-323-850-140; 056-332-008-726-803; 063-573-679-377-529; 064-053-186-811-574; 066-099-486-825-165; 068-045-162-138-560; 075-402-037-032-739; 078-102-920-859-991; 090-373-070-341-406; 116-406-441-633-492; 118-533-294-109-774; 141-649-000-612-322,2,true,cc-by,gold
186-512-780-643-036,LidSonic V2.0: A LiDAR and Deep-Learning-Based Green Assistive Edge Device to Enhance Mobility for the Visually Impaired.,2022-09-30,2022,journal article,"Sensors (Basel, Switzerland)",14248220; 14243210,Multidisciplinary Digital Publishing Institute (MDPI),Switzerland,Sahar Busaeed; Iyad Katib; Aiiad Albeshri; Juan M Corchado; Tan Yigitcanlar; Rashid Mehmood,"Over a billion people around the world are disabled, among whom 253 million are visually impaired or blind, and this number is greatly increasing due to ageing, chronic diseases, and poor environments and health. Despite many proposals, the current devices and systems lack maturity and do not completely fulfill user requirements and satisfaction. Increased research activity in this field is required in order to encourage the development, commercialization, and widespread acceptance of low-cost and affordable assistive technologies for visual impairment and other disabilities. This paper proposes a novel approach using a LiDAR with a servo motor and an ultrasonic sensor to collect data and predict objects using deep learning for environment perception and navigation. We adopted this approach using a pair of smart glasses, called LidSonic V2.0, to enable the identification of obstacles for the visually impaired. The LidSonic system consists of an Arduino Uno edge computing device integrated into the smart glasses and a smartphone app that transmits data via Bluetooth. Arduino gathers data, operates the sensors on the smart glasses, detects obstacles using simple data processing, and provides buzzer feedback to visually impaired users. The smartphone application collects data from Arduino, detects and classifies items in the spatial environment, and gives spoken feedback to the user on the detected objects. In comparison to image-processing-based glasses, LidSonic uses far less processing time and energy to classify obstacles using simple LiDAR data, according to several integer measurements. We comprehensively describe the proposed system's hardware and software design, having constructed their prototype implementations and tested them in real-world environments. Using the open platforms, WEKA and TensorFlow, the entire LidSonic system is built with affordable off-the-shelf sensors and a microcontroller board costing less than USD 80. Essentially, we provide designs of an inexpensive, miniature green device that can be built into, or mounted on, any pair of glasses or even a wheelchair to help the visually impaired. Our approach enables faster inference and decision-making using relatively low energy with smaller data sizes, as well as faster communications for edge, fog, and cloud computing.",22,19,7435,7435,Arduino; Buzzer; Computer science; Smart device; Enhanced Data Rates for GSM Evolution; Bluetooth; Human–computer interaction; Embedded system; Real-time computing; Artificial intelligence; Engineering; Wireless; Telecommunications; ALARM; Aerospace engineering,Arduino Uno; LiDAR; assistive tools; deep learning; edge computing; green computing; obstacle detection; obstacle recognition; sensors; smart app; smart mobility; sustainability; ultrasonic; visually impaired,Deep Learning; Disabled Persons; Humans; Self-Help Devices; Visually Impaired Persons; Wheelchairs,,King Abdulaziz University (RG-11-611-38),https://www.mdpi.com/1424-8220/22/19/7435/pdf?version=1664545959 https://doi.org/10.3390/s22197435 https://eprints.qut.edu.au/235416/1/115932010.pdf,http://dx.doi.org/10.3390/s22197435,36236546,10.3390/s22197435,,PMC9570831,1,000-633-867-521-635; 002-803-890-447-084; 003-245-163-059-395; 004-104-767-456-006; 005-292-615-099-772; 005-475-055-222-817; 008-684-469-671-014; 009-603-464-912-21X; 010-004-081-294-240; 010-371-675-659-125; 010-864-608-924-297; 011-737-861-055-472; 012-081-521-655-479; 015-826-771-914-288; 017-610-812-543-699; 018-843-508-574-144; 019-612-467-688-556; 020-260-934-897-437; 020-341-613-871-193; 022-109-465-026-890; 023-036-825-183-294; 023-634-415-979-265; 024-331-031-810-75X; 026-280-926-678-732; 027-588-207-376-882; 029-015-996-824-08X; 029-360-136-216-567; 030-139-354-389-282; 030-251-661-047-944; 030-884-979-386-25X; 031-975-734-302-804; 032-793-149-773-524; 033-559-787-267-678; 033-956-821-621-968; 034-936-259-439-98X; 035-569-236-178-312; 036-098-683-217-81X; 036-465-260-438-866; 037-566-006-652-399; 038-326-446-864-583; 040-220-230-984-080; 042-681-150-258-227; 045-221-412-875-746; 045-295-277-214-436; 048-190-590-775-158; 050-620-337-235-317; 053-599-747-059-040; 055-495-282-581-758; 056-997-750-983-747; 058-268-989-258-793; 059-575-844-550-706; 061-933-888-557-079; 066-246-145-459-483; 067-155-495-860-739; 068-526-647-480-345; 074-583-125-273-953; 076-097-455-446-57X; 078-021-533-108-528; 078-867-850-458-934; 081-399-721-579-995; 092-344-415-980-001; 092-726-040-947-902; 098-235-243-382-990; 099-156-291-147-080; 100-157-392-754-072; 106-247-515-834-484; 107-997-344-407-769; 108-186-632-977-341; 113-809-734-902-69X; 118-228-071-821-472; 120-274-166-967-801; 128-404-345-632-242; 130-097-955-008-505; 130-145-570-870-595; 130-780-465-918-027; 138-833-826-742-547; 146-425-395-790-21X; 177-151-062-200-794; 182-292-485-770-924; 192-718-720-786-159,10,true,cc-by,gold
186-542-567-190-290,Recognize Objects for Visually Impaired using Computer Vision,2020-03-30,2020,journal article,International Journal of Recent Technology and Engineering (IJRTE),22773878,Blue Eyes Intelligence Engineering and Sciences Engineering and Sciences Publication - BEIESP,,Deven Pawar*; Mihir Raul; Pranav Raut; Sharmila Gaikwad,"<jats:p>Visually impaired people are often unaware of dangers in front of them, even in familiar environments. Due to lack of vision either partial or complete, such people are highly dependent on the sense of hearing to perform day to day activities. One more form of vision impairment is colour blindness. Individuals with colour blindness find it hard to distinguish between colours. This research proposes making software that can help such individuals for solving the unawareness of the surrounding of the visually impaired people which allows them to have a greater awareness of their surroundings. The software needs an input device typically a camera and an audio feedback system. The camera will continuously capture images and the algorithm recognize the objects in the image and output the result using the audio feedback system. The system also proposes to include colour extraction to also correctly identify the colour of the object and a further addition is to identify individuals if enough datasets are provided. If any suspicious/dangerous objects detected in the surrounding the software will inform the user about the imminent danger. This study has analysed Faster R-CNN, SSD (Single Shot MultiBox Detector) and YOLO (You only look once) for their accuracy and rate of object detection. This research also studied different operating scenarios of the device which includes operation at night and operation in various orientations. The results of the object recognition system while using YOLO have an accuracy of 59.7% and 10fps during real-time operation, which is sufficient for assisting visually impaired people in realizing the types and localities of the objects around them.</jats:p>",8,6,5365,5369,Computer vision; Computer science; Artificial intelligence; Software; Object (grammar); Visually impaired; Blindness; Object detection; Human–computer interaction; Pattern recognition (psychology); Optometry; Medicine; Programming language,,,,,,http://dx.doi.org/10.35940/ijrte.f9579.038620,,10.35940/ijrte.f9579.038620,,,0,,1,true,,gold
186-813-328-954-953,Computer Assisted Orthopaedic Surgery,2011-05-12,2011,journal article,International Journal of Computer Assisted Radiology and Surgery,18616410; 18616429,Springer Science and Business Media LLC,Germany,,,6,S1,108,116,Orthopedic surgery; Medicine; Medical physics; General surgery; Surgery,,,,,,http://dx.doi.org/10.1007/s11548-011-0596-y,,10.1007/s11548-011-0596-y,,,0,,0,false,,
186-887-461-524-196,An Intelligent IoT Based Smart Stick For Visually Impaired Person Using Image Sensing,2023-07-06,2023,conference proceedings article,2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT),,IEEE,,Nikhil S Patankar; Bhushan Haribhau; Prithviraj Shivaji Dhorde; Harshal Pravin Patil; Rohit Vijay Maind; Yogesh S Deshmukh,"Visual impairment is a global concern affecting millions of people worldwide, with a significant proportion classified as blind. The traditional blind stick, while widely used, presents limitations such as skill requirements, costs, and extensive training. However, recent technological advancements have paved the way for innovative solutions to aid visually impaired individuals in navigating their surroundings effectively. This paper explores the development and potential of smart blind sticks, which leverage sensors, cameras, and artificial intelligence algorithms to provide enhanced assistance to users. These devices offer obstacle detection, auditory and tactile feedback, and even GPS navigation, empowering visually impaired individuals and improving their mobility and quality of life. Several research initiatives worldwide are actively contributing to the design and development of user-friendly, affordable smart blind sticks that require minimal training. With the use of an IoT stick, this research hopes to create an image of opportunity, autonomy, and certainty. To swiftly complete their everyday tasks, the proposed smart stick is designed with an obstacle recognition module, a worldwide positioning system (GPS), pit and flight of stairs detection, water detection, and a global system for mobile communication (GSM).",,,,,Global Positioning System; Obstacle; Computer science; Leverage (statistics); GSM; Human–computer interaction; Computer security; Artificial intelligence; Telecommunications; Political science; Law,,,,,,http://dx.doi.org/10.1109/icccnt56998.2023.10306645,,10.1109/icccnt56998.2023.10306645,,,0,004-809-996-572-967; 013-847-555-426-159; 014-133-707-942-665; 018-170-211-594-394; 021-093-891-392-826; 022-259-416-833-382; 034-936-259-439-98X; 035-978-612-600-220; 044-735-811-851-700; 075-007-214-295-960; 084-621-956-547-983; 096-792-788-017-415; 099-234-409-536-024; 101-637-146-335-197; 127-824-712-629-913; 141-730-500-770-00X,0,false,,
187-640-230-486-315,Image Processing and Display,2011-05-12,2011,journal article,International Journal of Computer Assisted Radiology and Surgery,18616410; 18616429,Springer Science and Business Media LLC,Germany,,,6,S1,46,52,Computer science; Computer vision; Computer graphics (images); Image processing; Artificial intelligence; Image (mathematics); Multimedia,,,,,,http://dx.doi.org/10.1007/s11548-011-0573-5,,10.1007/s11548-011-0573-5,,,0,,1,false,,
188-188-378-320-811,"""I am the follower, also the boss"": Exploring Different Levels of Autonomy and Machine Forms of Guiding Robots for the Visually Impaired",2023-04-19,2023,conference proceedings article,Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems,,ACM,,Yan Zhang; Ziang Li; Haole Guo; Luyao Wang; Qihe Chen; Wenjie Jiang; Mingming Fan; Guyue Zhou; Jiangtao Gong,"Guiding robots, in the form of canes or cars, have recently been explored to assist blind and low vision (BLV) people. Such robots can provide full or partial autonomy when guiding. However, the pros and cons of different forms and autonomy for guiding robots remain unknown. We sought to fill this gap. We designed autonomy-switchable guiding robotic cane and car. We conducted a controlled lab-study (N=12) and a field study (N=9) on BLV. Results showed that full autonomy received better walking performance and subjective ratings in the controlled study, whereas participants used more partial autonomy in the natural environment as demanding more control. Besides, the car robot has demonstrated abilities to provide a higher sense of safety and navigation efficiency compared with the cane robot. Our findings offered empirical evidence about how the BLV community perceived different machine forms and autonomy, which can inform the design of assistive robots.",4,,1,22,Autonomy; Robot; Human–computer interaction; Computer science; Control (management); Psychology; Artificial intelligence; Political science; Law,,,,National Natural Science Foundation Youth Fund,https://arxiv.org/pdf/2302.03481 https://arxiv.org/abs/2302.03481,http://dx.doi.org/10.1145/3544548.3580884,,10.1145/3544548.3580884,,,0,001-134-239-960-91X; 001-298-245-371-258; 002-742-535-444-704; 003-985-756-771-541; 004-480-237-288-729; 005-036-313-090-323; 005-515-885-541-95X; 011-527-612-257-65X; 015-174-884-664-544; 018-313-091-668-429; 020-373-201-301-684; 023-989-567-107-266; 024-335-765-549-544; 024-964-397-377-997; 029-313-431-507-058; 034-028-229-282-033; 037-062-053-955-895; 037-972-364-518-409; 042-418-744-828-17X; 042-968-713-500-442; 045-007-767-813-838; 051-121-679-387-759; 051-566-698-436-263; 055-495-282-581-758; 058-013-370-483-44X; 064-043-611-038-916; 065-426-374-084-349; 068-708-501-300-740; 071-858-943-169-534; 071-960-270-577-672; 075-827-600-908-546; 076-781-057-771-298; 080-809-254-267-492; 086-882-728-845-848; 087-838-014-190-104; 090-042-328-255-171; 093-548-249-489-049; 094-871-870-293-980; 110-595-410-145-83X; 111-772-107-673-112; 114-313-861-704-853; 127-406-202-563-035; 128-106-934-326-205; 131-571-073-455-84X; 133-941-009-855-804; 137-648-461-521-23X; 158-258-139-676-848; 172-812-405-986-623; 179-546-740-291-787; 181-561-484-223-829; 190-970-138-037-47X; 191-396-535-705-69X,8,true,,green
188-270-236-313-299,"Design of a ""Cobot Tactile Display"" for Accessing Virtual Diagrams by Blind and Visually Impaired Users.",2022-06-13,2022,journal article,"Sensors (Basel, Switzerland)",14248220; 14243210,Multidisciplinary Digital Publishing Institute (MDPI),Switzerland,Satinder Gill; Dianne T V Pawluk,"Access to graphical information plays a very significant role in today's world. Access to this information can be particularly limiting for individuals who are blind or visually impaired (BVIs). In this work, we present the design of a low-cost, mobile tactile display that also provides robotic assistance/guidance using haptic virtual fixtures in a shared control paradigm to aid in tactile diagram exploration. This work is part of a larger project intended to improve the ability of BVI users to explore tactile graphics on refreshable displays (particularly exploration time and cognitive load) through the use of robotic assistance/guidance. The particular focus of this paper is to share information related to the design and development of an affordable and compact device that may serve as a solution towards this overall goal. The proposed system uses a small omni-wheeled robot base to allow for smooth and unlimited movements in the 2D plane. Sufficient position and orientation accuracy is obtained by using a low-cost dead reckoning approach that combines data from an optical mouse sensor and inertial measurement unit. A low-cost force-sensing system and an admittance control model are used to allow shared control between the Cobot and the user, with the addition of guidance/virtual fixtures to aid in diagram exploration. Preliminary semi-structured interviews, with four blind or visually impaired participants who were allowed to use the Cobot, found that the system was easy to use and potentially useful for exploring virtual diagrams tactually.",22,12,4468,4468,Human–computer interaction; Haptic technology; Computer science; Inertial measurement unit; Robot; Focus (optics); Orientation (vector space); Control (management); Artificial intelligence; Simulation; Physics; Geometry; Mathematics; Optics,admittance control; assistive technologies; dead reckoning; haptics; tactile displays; visually impaired,Animals; Blindness/psychology; Humans; Mice; Touch; User-Computer Interface; Visually Impaired Persons,,National Science Foundation (CBET-16-5226),https://www.mdpi.com/1424-8220/22/12/4468/pdf?version=1655115420 https://doi.org/10.3390/s22124468,http://dx.doi.org/10.3390/s22124468,35746250,10.3390/s22124468,,PMC9230892,0,004-476-826-365-583; 004-728-840-717-571; 004-870-184-909-715; 009-112-077-994-837; 009-298-692-762-606; 010-490-278-693-532; 014-392-279-773-880; 034-721-267-759-029; 036-443-969-785-358; 036-666-656-008-003; 043-380-401-330-631; 044-844-064-880-948; 049-519-975-039-482; 053-827-196-953-717; 055-539-164-934-497; 074-429-189-644-041; 080-237-924-329-322; 091-743-295-631-908; 106-542-570-452-165; 148-149-362-410-895; 169-433-381-183-720,3,true,cc-by,gold
188-395-025-959-486,Advanced Visualization Engineering for Vision Disorders: A Clinically Focused Guide to Current Technology and Future Applications.,2023-10-20,2023,journal article,Annals of biomedical engineering,15739686; 00906964; 15216047,Springer Science and Business Media LLC,Netherlands,Nasif Zaman; Joshua Ong; Ethan Waisberg; Mouayad Masalkhi; Andrew G Lee; Alireza Tavakkoli; Stewart Zuckerbrod,,52,2,178,207,Visualization; Augmented reality; Visual impairment; Virtual reality; Computer science; Human–computer interaction; Medicine; Artificial intelligence; Psychiatry,Assessment; Augmented reality; Mixed reality; Rehabilitation; Simulation; Virtual reality; Vision disorders; Visualization engineering,Humans; Computer Simulation; Augmented Reality; Vision Disorders; Technology,,NASA (80NSSC20K183) United States; NASA (80NSSC20K183) United States,,http://dx.doi.org/10.1007/s10439-023-03379-8,37861913,10.1007/s10439-023-03379-8,,,0,000-379-626-639-850; 000-826-448-527-128; 001-881-924-841-758; 002-547-584-180-342; 002-816-460-898-945; 003-383-193-719-732; 003-590-526-843-953; 004-131-334-050-856; 004-150-547-613-986; 005-350-221-974-354; 005-383-990-326-993; 005-818-496-776-526; 006-722-784-433-333; 007-413-964-120-673; 007-475-950-647-654; 008-014-399-435-493; 008-679-488-115-703; 009-210-299-062-25X; 009-364-630-132-337; 010-205-820-608-938; 010-441-738-818-527; 010-544-590-476-242; 010-662-752-611-548; 012-124-923-457-763; 012-787-222-672-046; 014-236-376-159-925; 015-950-281-469-108; 016-350-570-298-365; 016-442-381-834-040; 016-724-216-939-602; 016-772-741-248-961; 017-653-894-110-255; 018-132-471-339-595; 019-292-759-705-995; 020-673-647-799-83X; 020-726-031-527-701; 021-168-481-514-759; 021-279-729-672-630; 021-317-007-724-739; 021-897-547-732-127; 023-661-677-697-640; 024-352-835-356-233; 024-519-351-202-299; 024-640-430-054-858; 025-344-539-275-013; 025-858-398-695-038; 026-397-165-951-421; 026-589-740-131-742; 027-025-557-845-539; 027-458-555-990-381; 027-633-243-292-029; 027-974-208-618-025; 028-474-346-034-808; 028-508-537-808-063; 029-036-862-321-260; 029-043-640-232-460; 031-396-859-127-61X; 031-460-021-189-044; 031-637-704-228-445; 032-561-614-028-720; 032-628-327-816-584; 032-880-698-479-50X; 032-962-450-397-606; 033-451-734-685-687; 033-855-516-195-058; 034-202-977-233-860; 034-860-154-713-059; 035-467-816-756-900; 035-850-010-579-601; 036-011-916-817-310; 036-477-020-701-125; 036-906-390-690-171; 037-283-229-308-616; 037-716-171-999-872; 038-355-147-585-261; 040-409-503-806-597; 040-419-618-479-011; 042-938-865-453-393; 044-900-441-761-797; 047-578-620-891-307; 048-831-682-583-812; 049-295-038-656-70X; 050-171-828-729-003; 051-486-129-719-685; 051-884-730-600-849; 053-912-535-687-238; 054-108-367-874-227; 054-514-701-399-569; 054-575-663-572-98X; 056-230-979-818-965; 056-246-165-770-719; 056-431-929-443-406; 056-558-679-027-597; 056-726-752-839-170; 057-474-415-776-111; 057-734-273-328-978; 059-482-749-135-704; 061-154-462-691-411; 062-285-911-785-808; 062-528-203-052-084; 063-424-060-999-544; 063-645-139-575-102; 063-920-538-482-230; 064-279-684-600-27X; 065-904-672-779-601; 065-983-543-415-033; 066-492-755-651-177; 069-306-108-765-351; 069-927-004-430-225; 070-696-701-309-850; 072-097-197-240-37X; 073-338-244-636-803; 073-913-868-419-872; 074-934-570-630-156; 075-449-002-284-694; 076-784-360-053-292; 077-302-112-072-584; 077-699-126-237-464; 078-490-764-043-25X; 080-139-047-616-629; 081-541-848-520-418; 081-631-223-943-241; 082-710-847-933-545; 083-547-257-968-122; 085-131-468-612-277; 085-196-579-365-317; 085-590-442-763-213; 086-600-689-760-850; 086-914-923-719-154; 088-308-329-242-907; 088-706-291-269-015; 093-247-732-336-641; 094-141-448-364-013; 095-188-011-558-987; 095-353-349-464-079; 096-015-305-168-686; 096-022-100-270-025; 097-770-753-968-295; 099-143-342-031-318; 099-538-814-797-458; 099-856-484-395-221; 099-865-281-566-654; 101-186-275-129-912; 101-208-225-073-636; 101-651-956-038-363; 102-251-935-371-523; 103-632-777-265-113; 108-120-155-329-947; 114-443-283-706-376; 115-693-957-791-721; 116-113-491-458-080; 116-647-164-057-438; 116-849-934-734-905; 118-627-610-290-681; 119-653-351-265-099; 120-688-485-454-579; 122-880-897-952-163; 125-130-257-902-316; 129-892-777-371-375; 129-941-718-264-869; 131-361-063-187-786; 132-757-474-504-253; 132-803-568-904-412; 133-241-888-286-347; 139-698-615-235-163; 141-103-692-062-071; 143-940-545-639-118; 147-874-047-069-098; 148-361-668-133-892; 150-408-303-967-103; 152-760-990-058-563; 159-301-608-210-846; 167-449-625-533-859; 168-500-006-220-930; 169-604-431-441-737; 173-649-768-972-49X; 174-290-391-129-109; 179-140-856-526-920; 187-126-033-783-335; 187-369-795-252-350; 188-395-025-959-486; 188-889-374-702-179; 193-714-351-654-625; 199-519-596-569-01X,13,false,,
188-606-619-905-159,Trans4Trans: Efficient Transformer for Transparent Object Segmentation to Help Visually Impaired People Navigate in the Real World,2021-07-07,2021,preprint,arXiv: Computer Vision and Pattern Recognition,,,,Jiaming Zhang; Kailun Yang; Angela Constantinescu; Kunyu Peng; Karin Müller; Rainer Stiefelhagen,"Common fully glazed facades and transparent objects present architectural barriers and impede the mobility of people with low vision or blindness, for instance, a path detected behind a glass door is inaccessible unless it is correctly perceived and reacted. However, segmenting these safety-critical objects is rarely covered by conventional assistive technologies. To tackle this issue, we construct a wearable system with a novel dual-head Transformer for Transparency (Trans4Trans) model, which is capable of segmenting general and transparent objects and performing real-time wayfinding to assist people walking alone more safely. Especially, both decoders created by our proposed Transformer Parsing Module (TPM) enable effective joint learning from different datasets. Besides, the efficient Trans4Trans model composed of symmetric transformer-based encoder and decoder, requires little computational expenses and is readily deployed on portable GPUs. Our Trans4Trans model outperforms state-of-the-art methods on the test sets of Stanford2D3D and Trans10K-v2 datasets and obtains mIoU of 45.13% and 75.14%, respectively. Through various pre-tests and a user study conducted in indoor and outdoor scenarios, the usability and reliability of our assistive system have been extensively verified.",,,,,Human–computer interaction; Encoder; Wearable computer; Reliability (computer networking); Usability; Transformer (machine learning model); Computer science; Object (computer science); Segmentation; Transparency (human–computer interaction),,,,,https://arxiv.org/abs/2107.03172 https://arxiv.org/pdf/2107.03172.pdf https://ui.adsabs.harvard.edu/abs/2021arXiv210703172Z/abstract,https://arxiv.org/abs/2107.03172,,,3181680451,,0,000-278-355-752-479; 000-368-559-878-048; 002-123-448-882-287; 004-504-602-986-917; 008-361-673-204-978; 008-972-948-454-945; 009-604-493-900-415; 012-100-716-148-822; 013-662-330-010-928; 014-198-870-580-18X; 015-773-733-169-647; 016-628-086-107-33X; 016-769-111-907-953; 019-560-051-721-517; 020-233-013-143-936; 021-176-270-241-156; 025-044-700-570-429; 026-209-853-248-181; 026-292-486-698-315; 028-671-432-434-245; 030-540-246-658-722; 031-294-750-698-550; 034-093-615-545-728; 034-893-292-526-13X; 036-842-985-307-311; 037-550-015-414-716; 039-933-184-572-951; 040-469-719-400-101; 041-206-746-097-453; 041-314-165-476-904; 042-355-987-489-330; 043-814-030-738-536; 045-599-200-378-144; 046-201-716-147-236; 046-309-239-761-288; 048-398-355-455-958; 049-551-659-753-473; 050-261-233-769-785; 051-263-461-368-647; 051-674-567-787-301; 052-118-817-856-285; 055-495-282-581-758; 058-351-247-504-446; 062-960-343-963-966; 064-095-905-386-62X; 065-374-293-809-860; 068-246-254-541-665; 068-657-742-648-601; 070-204-788-245-541; 071-391-947-989-576; 075-430-539-649-470; 078-370-310-843-504; 078-614-414-462-114; 080-793-331-795-683; 084-065-630-749-740; 084-510-831-812-921; 090-373-070-341-406; 091-114-099-663-716; 091-317-478-968-667; 095-403-198-199-994; 098-797-680-433-359; 103-728-042-717-787; 104-208-613-793-423; 114-655-190-795-896; 116-537-167-146-949; 127-240-128-835-954; 129-674-023-352-404; 131-164-745-940-832; 148-933-974-350-650; 149-094-088-552-09X; 149-488-483-135-976; 152-145-055-495-637; 164-157-821-924-300; 167-131-435-704-717; 168-302-024-970-818; 170-680-544-359-887; 173-894-217-298-055; 176-779-278-797-427; 195-707-114-185-172; 199-277-795-543-446,0,true,,unknown
189-335-712-714-159,A Robotic Travel Aid for the Blind -Attention and Custom for Safe Behavior-,,1998,,,,,,Hideo Mori; Shinji Kotani,"We have been developing Robotic Travel Aid(RoT A) ""HARUNOBU"" to guide the visually impaired in the sidewalk or campus. RoT A is a motor wheel chair equipped with vision system, sonar, differential GPS system, dead reckoning system and a portable GIS. We estimate the performance of RoT A in two viewpoints, the viewpoint of guidance and the viewpoint of safety. RoT A is superior to the guide dog in the navigation function, and is inferior to the guide dog in the mobility. It can show the route from the current location to the destination but can not walk up and down stairs. RoT A is superior to the portable navigation system in the orientation, obstacle avoidance and physical support to keep balance of walking, but is inferior in portability.",,,,,Dead reckoning; Human–computer interaction; Engineering; Stairs; Navigation system; Differential GPS; Navigation function; Simulation; Machine vision; Software portability; Obstacle avoidance,,,,,https://link.springer.com/chapter/10.1007/978-1-4471-1580-9_22,https://link.springer.com/chapter/10.1007/978-1-4471-1580-9_22,,,2529351367,,0,014-440-173-656-399; 021-495-309-385-383; 056-621-008-090-076; 072-400-524-676-429; 073-211-104-818-914; 088-470-436-866-921; 126-815-975-370-041; 139-791-546-659-910; 142-490-859-794-741,15,false,,
190-265-192-930-425,Decision Making Algorithm for Blind Navigation Assistance using Deep Learning,2022-11-09,2022,conference proceedings article,2022 1st International Conference on Computational Science and Technology (ICCST),,IEEE,,Zulaikha Beevi S; Harish Kumar P; Harish S; Lakshan S J,"Blind people face several obstacles in their daily lives and technological interventions can help overcome these obstacles. In this research, we provide an AI-based autonomous assisting device that recognizes many objects and it will provide acoustic input to the user to help visually blind people to understand the surrounding better to understand their environment better. Multiple photos of objects relevant to visually impaired people were used to build a deep-learning model. Training photos are enhanced and manually annotated to improve the trained model's resilience. A distance-measuring sensor is included which recognise the objects using computer vision. The gadget is made more inclusive by recognizing the obstacles coming out of one place to another. After stage segmentation and obstacle detection, the aural information sent to the user is adj usted to get a lot of details in minimum time and speed up video processing.",,,,,Gadget; Computer science; Obstacle; Artificial intelligence; Segmentation; Computer vision; Deep learning; Face (sociological concept); Human–computer interaction; Machine learning; Algorithm; Social science; Sociology; Political science; Law,,,,,,http://dx.doi.org/10.1109/iccst55948.2022.10040269,,10.1109/iccst55948.2022.10040269,,,0,009-191-258-917-310; 015-672-084-912-775; 037-238-441-886-162; 051-814-657-760-825; 064-043-611-038-916; 065-553-420-193-637; 067-894-235-988-508; 080-843-138-995-926; 092-790-398-615-626; 093-916-860-538-446; 096-363-956-353-392,0,false,,
190-270-234-876-815,Road Surface Classification and Obstacle Detection for Visually Impaired People,2023-05-14,2023,book chapter,Springer Proceedings in Mathematics & Statistics,21941009; 21941017,Springer International Publishing,,Shripad Bhatlawande; Yash Aney; Aatreya Gaikwad; Vedant Anantwar; Swati Shilaskar; Jyoti Madake,"This paper proposes an aid for the visually impaired which would guide or navigate them through the streets of India by helping them differentiate between the different kinds of roads, namely, tar and cement. Navigation for the visually impaired could turn out to be very dangerous due to obvious reasons. The solution presented would not just help them navigate through the roads but also make them independent to a certain extent. The system would also detect any objects in front of them and apprise the user making them a lot more aware of their surroundings. The current aids available for the visually impaired are costly. The proposed system is a lightweight, low-power embedded system that can be carried in a bag. It detects obstacles up to 4 m to avoid cognitive overload. The solution proposed in this work is detecting the different kinds of roads to guide the blind person to walk on the correct path while constantly informing them about their surroundings. For the proposed system, SIFT is used as a feature descriptor. Feature reduction is carried out using principal component analysis on seven clusters which were created using a k-means clustering algorithm. The system used four classifiers and compared their performance. The classifier used in this work is decision tree, KNN, random forest, and support vector machine (OVR) with classification accuracy coming out to be 83.03%, 83.93%, 88.54%, and 85.14%, respectively. The random forest classifier is the best performer of all.",,,57,68,Random forest; Obstacle; Computer science; Classifier (UML); Artificial intelligence; Cluster analysis; Support vector machine; Decision tree; Scale-invariant feature transform; Computer vision; Feature extraction; Pattern recognition (psychology); Geography; Archaeology,,,,,,http://dx.doi.org/10.1007/978-3-031-16178-0_6,,10.1007/978-3-031-16178-0_6,,,0,006-917-922-806-186; 012-150-309-747-559; 013-056-435-015-274; 018-170-211-594-394; 021-702-412-374-622; 031-192-624-849-592; 036-063-612-801-744; 038-885-814-347-856; 044-644-110-114-333; 046-757-273-765-185; 049-575-510-227-033; 057-867-835-220-07X; 059-647-236-855-012; 092-391-305-190-481; 108-441-581-809-764; 123-501-344-440-241; 137-835-251-108-360; 143-350-990-910-949; 176-179-447-130-414; 190-079-672-864-181,0,false,,
190-287-088-376-177,Accessibility strategies to promote inclusive mobility through multi-objective approach,2023-04-28,2023,journal article,SN Applied Sciences,25233963; 25233971,Springer Science and Business Media LLC,,Tânia Silva; David Verde; Sara Paiva; Luís Barreto; Ana I. Pereira,"<jats:title>Abstract</jats:title><jats:p>In recent decades, urban mobility has assumed a need for adaptation due to the more significant congestion experienced in cities and the growing focus on sustainability. Several solutions are proposed to help citizens move around in an urban environment. Most are not yet aware of the universal and accessible aspect that these solutions must have. This paper proposes a route support system embedded in a mobile application, Viana+Acessível, using a multi-objective approach. The application aims to promote accessible mobility within the city, contributing to physical and psychological well-being for citizens with reduced mobility, temporary or permanently, such as people with spectrum autism disorder, the visually impaired, wheelchair users, pregnant, and the elderly. For the evaluation of the algorithms, four objective measures were considered: accessibility, slope, time, and length of the paths. The tests carried out with different routing algorithms showed that the A-Star presented the fastest results in terms of execution time compared to the Dijkstra, Floyd–Warshall, and Bellman–Ford. When analysing in a multi-objective approach, time, slope and accessibility were demonstrated to be conflicting objectives. Bi-objective and tri-objective were applied and Pareto front was explored.</jats:p>;                 <jats:p><jats:bold>Graphical abstract</jats:bold></jats:p>",5,5,,,Dijkstra's algorithm; Computer science; Adaptation (eye); Sustainability; Wheelchair; Focus (optics); Routing (electronic design automation); Shortest path problem; Human–computer interaction; Psychology; Graph; Computer network; Theoretical computer science; World Wide Web; Ecology; Physics; Neuroscience; Optics; Biology,,,,Fundação para a Ciência e a Tecnologia,https://link.springer.com/content/pdf/10.1007/s42452-023-05349-0.pdf https://doi.org/10.1007/s42452-023-05349-0,http://dx.doi.org/10.1007/s42452-023-05349-0,,10.1007/s42452-023-05349-0,,,0,001-265-145-425-764; 004-243-307-888-158; 005-700-764-487-453; 010-867-013-082-530; 011-937-565-999-723; 022-987-587-478-237; 026-427-427-369-277; 027-677-692-613-849; 028-797-080-803-898; 045-886-754-687-645; 054-498-798-305-129; 056-474-781-112-538; 056-547-713-371-217; 060-326-760-135-933; 063-850-524-053-346; 064-053-186-811-574; 065-432-742-951-110; 084-278-849-565-742; 084-826-251-921-719; 085-811-345-093-576; 087-263-676-955-538; 088-936-397-073-305; 091-191-946-134-815; 099-183-098-487-204; 102-263-736-254-576; 115-277-930-324-266; 132-246-550-138-466; 159-211-002-686-429; 160-375-541-683-91X,1,true,cc-by,gold
190-466-960-857-437,Object Detection Using Image Processing for Blind Person,2024-05-31,2024,journal article,International Journal for Research in Applied Science and Engineering Technology,23219653,International Journal for Research in Applied Science and Engineering Technology (IJRASET),,Devansh Srivastava,"<jats:p>Abstract: This research presents a fully autonomous assistive technology based on artificial intelligence that can distinguish various objects and provides real-time aural cues to the user, improving comprehension for visually impaired people. Multiple photos of items that are extremely useful for the visually impaired person are used to build a deep learning model. The learned model is made more robust by manually annotating and augmenting training photos. A distance-measuring sensor is incorporated in addition to computer vision-based object identification algorithms to enhance the device's comprehension by identifying barriers during navigation. The algorithms used to process images and videos were made to accept inputs from the camera in real-time. Deep Neural Networks were utilized to predict the objects, and Google's well-known Text-To-Speech (GTTS) API module was used to precisely detect and recognize the group or category of objects and locations contained in the anticipated voice message</jats:p>",12,5,5094,5099,Computer science; Computer vision; Artificial intelligence; Object (grammar); Image processing; Image (mathematics); Computer graphics (images),,,,,,http://dx.doi.org/10.22214/ijraset.2024.62769,,10.22214/ijraset.2024.62769,,,0,,0,true,,bronze
190-584-080-486-348,A Review on Voice Navigation System for the Visually Impaired,2024-04-03,2024,journal article,INTERANTIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ENGINEERING AND MANAGEMENT,25823930,Indospace Publications,,BIMAL MOHAN,"<jats:p>A ground-breaking voice navigation system seems as a ray of light in a world where blind people must overcome enormous obstacles to navigate strange places. For those with vision impairments, safely and independently navigating in unfamiliar situations presents substantial obstacles. This system is a paradigm shift in providing safe and independent mobility assistance for the visually handicapped, as it makes use of cutting-edge technologies. The voice navigation system gives users the confidence to confidently navigate uncharted territory by seamlessly integrating computer vision and machine learning capabilities. With the use of real-time obstacle classification and recognition capabilities, the system can give users vital information about their surroundings, empowering them to make wise decisions while traveling. The system's smartphone-based platform, which provides an intuitive interface that can be accessed with voice commands, is its basis. By utilizing deep learning algorithms and pre-trained models, the system can precisely identify barriers and guide people across intricate environments. Precise positioning and route planning are made possible by the system's integration of GPS technology. Users receive up-to-date information on their location and the most efficient route to their destination via voice-activated instructions and advice. Individual user preferences are catered to by customizable options, which guarantee a tailored and user-friendly experience. A wider user base can utilize the system, democratizing access to necessary navigation tools, due to its affordability and interoperability with commonly available smartphone devices. The results of experimental tests highlight the effectiveness of the system, showing that it can reliably and accurately guide visually impaired people across a variety of surroundings. Through the promotion of increased autonomy and mobility, the voice navigation system enables users to move with unprecedented liberty and self-assurance, surmounting obstacles and opening up novel prospects.     Key Words:  : Voice navigation system, Machine learning, GPS technology, Deep learning algorithms, Navigation tools.</jats:p>",8,4,1,5,Visually impaired; Communication; Computer science; Speech recognition; Psychology; Human–computer interaction,,,,,https://ijsrem.com/download/a-review-on-voice-navigation-system-for-the-visually-impaired/?wpdmdl=29046&refresh=660e2a3145d2e1712204337 https://doi.org/10.55041/ijsrem29925,http://dx.doi.org/10.55041/ijsrem29925,,10.55041/ijsrem29925,,,0,,0,true,,bronze
190-810-215-149-609,Robust Path Back-Tracing Guidance System for Blind People,2017-09-22,2017,,,,,,German H. Flores,"Indoor positioning and navigation is an active area of research due to the widespread use of devices equipped with micro-machined electromechanical systems (MEMS) sensors such as gyroscopes, accelerometers, and magnetometers. The MEMS inertial sensors and other popular technologies can provide information to determine the position and orientation of a person relative to a known initial location. A system that is able to produce accurate location data may find multiple applications in location-based services (LBS), safe wayfinding, and other related fields. For instance, a guidance system could be used to provide travel-related information to passengers as they use public transportation, or provide safe navigational directions to people who find themselves in unfamiliar environments. The research presented in this dissertation focuses on building robust systems that provide real-time and reliable travel-related information to help blind or visually impaired travelers reach a destination safely.In this dissertation, I present two novel navigation systems and an openly accessible and annotated data set of inertial sensor time series data collected from blind people. The first system conveys travel-related information to blind passengers when using public transportation. The system makes use of pre-configured Wi-Fi access points placed in public transit vehicles and at bus stops to convey real-time, multi-modal travel-related information to any passenger, directly on his or her own smart device. The second more complex system helps blind people retrace the path taken inside a building and walk safely back to an initial location. This is ideal for situations in which a blind person is able to reach a certain location, for example with the assistance of a sighted guide, and needs to find his or her way back to the initial location. This robust path back-tracing guidance system is comprised of a turn detector based on a hidden Markov model (HMM) to robustly detect turns even in the presence of drift in the inertial measurements and noticeable body sway during gait, a step counter that uses filtered inertial sensor data to determine the number of steps walked along a path, and a path matching algorithm to track the user’s location. The step counter and turn detection models were trained on sensor data from WeAllWalk, an openly accessible and annotated data set of inertial sensor time series data created from blind walkers using a long cane or a guide dog. This system runs on a smart device and provides the guidance necessary to help a blind person retrace a path. A robust guidance system that supports safe blind wayfinding opens the doors to many blind travelers who would like to travel and move independently as they explore unfamiliar environments.",,,,,Engineering; Tracing; Location-based service; Gyroscope; Guidance system; Smart device; Simulation; Blossom algorithm; Inertial measurement unit; Real-time computing; Accelerometer,,,,,https://escholarship.org/uc/item/6nq496gw,https://escholarship.org/uc/item/6nq496gw,,,2762217577,,0,,0,false,,
191-329-883-171-76X,AAAI Spring Symposia - Portable navigations system with adaptive multimodal interface for the blind,2017-03-27,2017,conference proceedings,,,,,Jacobus Cornelius Lock; Grzegorz Cielniak; Nicola Bellotto,"Recent advances in mobile technology have the potential to radically change the quality of tools available for people with sensory impairments, in particular the blind. Nowadays almost every smart-phone and tablet is equipped with high resolutions cameras, which are typically used for photos and videos, communication purposes, games and virtual reality applications. Very little has been proposed to exploit these sensors for user localisation and navigation instead. To this end, the “Active Vision with Human-in-the-Loop for the Visually Impaired” (ActiVis) project aims to develop a novel electronic travel aid to tackle the “last 10 yards problem” and enable the autonomous navigation of blind users in unknown environments, ultimately enhancing or replacing existing solutions, such as guide dogs and white canes. This paper describes some of the key project’s challenges, in particular with respect to the design of the user interface that translate visual information from the camera to guiding instructions for the blind person, taking into account limitations due to the visual impairment and proposing a multimodal interface that embeds human-machine co-adaptation.",,,,,Interface (computing); Active vision; Virtual reality; Key (cryptography); Visual impairment; Computer science; Multimedia; Mobile technology; User interface,,,,,http://aaai.org/ocs/index.php/SSS/SSS17/paper/view/15311 https://dblp.uni-trier.de/db/conf/aaaiss/aaaiss2017.html#LockCB17 https://eprints.lincoln.ac.uk/25413/,http://aaai.org/ocs/index.php/SSS/SSS17/paper/view/15311,,,2581921333,,0,020-756-792-127-523; 027-033-143-017-928; 029-360-136-216-567; 030-714-768-787-799; 049-149-448-212-233; 055-854-110-204-122; 071-806-151-845-245; 093-925-865-515-918; 100-327-468-898-566; 103-321-955-787-750,8,false,,
192-486-333-277-813,Indoor objects detection system implementation using multi-graphic processing units,2021-09-25,2021,journal article,Cluster Computing,13867857; 15737543,Springer Science and Business Media LLC,Netherlands,Mouna Afif; Riadh Ayachi; Mohamed Atri,,25,1,469,483,,,,,,,http://dx.doi.org/10.1007/s10586-021-03419-9,,10.1007/s10586-021-03419-9,,,0,003-270-934-605-038; 003-567-756-778-880; 007-759-025-744-822; 020-233-013-143-936; 021-360-232-245-594; 021-433-934-874-859; 026-400-078-913-130; 026-994-702-158-53X; 035-277-967-833-996; 037-182-418-585-695; 041-074-700-593-708; 044-396-615-675-996; 045-309-399-228-849; 047-040-726-932-715; 049-093-905-733-529; 049-317-239-158-314; 054-876-301-922-486; 056-307-484-095-075; 061-579-383-511-500; 061-866-612-132-646; 062-800-469-258-59X; 066-835-465-051-044; 070-103-498-360-506; 070-146-203-962-535; 075-533-194-094-730; 077-689-906-335-058; 083-477-719-417-886; 085-507-633-373-250; 108-396-749-438-718; 133-703-464-031-598; 134-048-572-801-690; 138-852-374-862-575; 139-552-118-652-140; 172-226-429-921-38X,3,false,,
192-600-440-415-958,On intelligent image processing methodologies applied to navigation assistance for visually impaired,,2002,dissertation,,,,,null G.Sainarayanan,"The main objective of this thesis is to develop a computer based Navigation Assistance for Visually Impaired (NAVI) as a vision substitutive system. The hardware of the system includes a Vision sensor mounted on a headgear, a set of; Stereo earphones and a Single Board Processing System (SBPS) with batteries, duly placed in a vest. The Vision sensor is a digital video camera. The video camera captures the image of the environment. The captured image is processed, mapped; on to specially structured stereo sound patterns and sent to the earphones. A set of image processing requirements for vision substitution is identified and incorporated in; the NAVI system. The image processing, developed in this thesis, is designed to work as a model of the human vision system. To model the human vision system in image processing, two properties of human eye, namely lateral inhibition and domination of the object properties rather than background are incorporated. The image processing methodologies applied in NAVI are developed using artificial intelligent techniques. The property of lateral inhibition is incorporated using neural network based Canny edge filter. In vision substitutive system, definition of objects; and background is not easy as compared to industrial object recognition system. Therefore, three methods for object enhancement and background suppression are proposed in NAVI using fuzzy logic and neural network. The edge image and the; object enhanced image with background suppressed are integrated to produce a resultant image. The resultant image is sonified to produce stereo acoustic patterns. Blind volunteers were trained with the developed NAVI system and they were tested to identify the environment. They were able to understand the logic behind the sound in discriminating the object from background. It was also verified that the discrimination of objects by the blind through the proposed image processing methodologies is effective and easier than that of earlier efforts in this direction.",,,,,Digital image processing; Filter (signal processing); Computer graphics (images); Feature detection (computer vision); Engineering; Artificial intelligence; Human visual system model; Video camera; Computer vision; Background subtraction; Machine vision; Image processing,,,,,http://eprints.ums.edu.my/10206/,http://eprints.ums.edu.my/10206/,,,23451984,,0,,21,false,,
192-928-772-603-41X,Voice Enabled Deep Learning Based Image Captioning Solution for Guided Navigation,2023-09-01,2023,conference proceedings article,"2023 International Conference on Network, Multimedia and Information Technology (NMITCON)",,IEEE,,Senthil Kumar A; Selvaraj Kesavan; Jayakumar J; Ananda Kumar K S; Prasad Maddula,"The use of technology to assist visually impaired individuals is crucial in addressing the global issue of vision impairment. Worldwide more than billion people suffer from a vision impairment that should have been avoided or is yet unaddressed. According to the statistics, there is a significant need for solutions that can help those who are visually impaired, mainly in the middle- and low-income countries where the vision impairment population is higher. It is anticipated that population expansion and ageing will increase the likelihood that more people may get vision impairment. The efficientnetB3 deep learning algorithm will be used in this project to caption images for blind people. so, they can learn about object identification, distance, and position. This has been accomplished by utilizing advanced picture captioning techniques, efficient net B3 algorithms, and tokenization approaches, where the computer learns the scenes with various captions. The computer recognizes and forecasts any image that is acquired using the camera. The significant objects are also anticipated, and the camera's distances are determined. Following the prediction, the user receives an audio output that can be used to determine the object's position and distance. Hence, with the aid of this research, we give the blind artificial eyesight that can give them confidence when they move on their own. The aim is to step forward in addressing the global issue of vision impairment. The use of technology to assist visually impaired individuals is crucial in providing them with the tools they need to navigate their environment and live their lives with greater ease. By utilizing advanced algorithms and image captioning techniques, the quality of life can be improved for people worldwide who are affected by vision impairment. The intension is to develop an artificial vision for vision impaired people by detecting real time objects, distance and the position of it from the person using Audio Output and to develop a model for image captioning to predict the captions.",,,,,Closed captioning; Computer science; Artificial intelligence; Computer vision; Object (grammar); Population; Deep learning; Braille; Visual impairment; Identification (biology); Object detection; Image (mathematics); Pattern recognition (psychology); Psychology; Botany; Demography; Psychiatry; Sociology; Biology; Operating system,,,,,,http://dx.doi.org/10.1109/nmitcon58196.2023.10276134,,10.1109/nmitcon58196.2023.10276134,,,0,000-401-699-900-536; 010-877-486-118-563; 013-840-724-950-102; 013-892-918-718-760; 020-817-919-065-232; 025-276-014-494-377; 032-397-606-623-202; 041-379-788-632-413; 077-445-708-526-67X; 097-193-481-592-88X; 144-424-200-084-327; 188-546-951-234-316,1,false,,
193-377-909-979-556,Early diagnosis of diabetic retinopathy using unsupervised learning,2023-05-15,2023,journal article,Soft Computing,14327643; 14337479,Springer Science and Business Media LLC,Germany,M. Padmapriya; S. Pasupathy; V. Punitha,,27,13,9093,9104,Diabetic retinopathy; Artificial intelligence; Blindness; Computer science; Preprocessor; Segmentation; Retinal; Retina; Image segmentation; Computer vision; Feature extraction; Diabetes mellitus; Pattern recognition (psychology); Medicine; Ophthalmology; Optometry; Physics; Optics; Endocrinology,,,,,,http://dx.doi.org/10.1007/s00500-023-08418-z,,10.1007/s00500-023-08418-z,,,0,001-101-933-063-417; 002-611-673-379-162; 003-721-512-148-092; 004-142-318-393-043; 005-378-532-295-660; 006-361-697-702-119; 008-812-068-625-790; 012-392-948-071-77X; 012-941-865-495-664; 016-262-662-123-108; 018-935-473-975-586; 021-394-716-962-587; 023-892-093-430-238; 026-889-381-684-912; 028-864-523-679-513; 031-383-936-776-187; 032-219-892-844-417; 032-399-981-898-16X; 035-502-765-326-361; 036-905-357-045-977; 045-390-822-493-529; 046-339-325-037-381; 047-818-827-104-155; 048-708-596-714-165; 049-368-618-042-848; 051-192-660-630-227; 051-248-546-540-690; 059-300-384-421-379; 071-421-884-863-267; 074-097-650-866-235; 078-811-540-515-54X; 080-491-095-050-01X; 080-967-795-903-707; 082-874-072-101-372; 084-553-818-981-130; 084-628-739-448-294; 087-418-095-859-103; 091-810-826-008-929; 099-923-613-937-03X; 106-343-429-745-549; 108-262-649-251-207; 113-086-236-255-382; 120-224-838-277-788; 123-739-212-629-077; 130-745-014-036-435; 137-970-836-524-53X; 148-567-462-165-69X,1,false,,
193-841-188-589-641,AISI - Research Method of Blind Path Recognition Based on DCGAN,2020-09-20,2020,book,Advances in Intelligent Systems and Computing,21945357; 21945365,Springer International Publishing,,Ling Luo; Pingjun Zhang; Pengjun Hu; Liu Yang; Kuo-Chi Chang,"In order to solve the problem that there are few blind path data sets and a lot of manual data collection work in the current blind guide system, computer vision algorithm is used to automatically generate blind path images in different environments. Methods a blind path image generation method based on the depth convolution generative adversary network (DCGAN) is proposed. The method uses the characteristics of typical blind path, which is the combination of depression and bulge. The aim of long short memory network’ (LSTM) is to encode the depression part, and the aim of convolution neural network (CNN) is to encode the bulge part. The two aspects of information are combined to generate blind path images in different environments. It can effectively improve the blind path recognition rate of the instrument and improve the safe travel of the visually impaired. Conclusion generative adversarial networks (GANs) can be used to generate realistic blind image, which has certain application value in expanding blind channel recognition data, but it still needs to be improved in some details.",,,90,99,Image (mathematics); Data collection; Convolution; Artificial intelligence; Path recognition; Computer vision; Computer science; ENCODE; Convolutional neural network; Path (graph theory); Channel (digital image),,,,,https://rd.springer.com/chapter/10.1007/978-3-030-58669-0_8 https://dblp.uni-trier.de/db/conf/aisi/aisi2020.html#LuoZHYC20 https://link.springer.com/chapter/10.1007/978-3-030-58669-0_8,http://dx.doi.org/10.1007/978-3-030-58669-0_8,,10.1007/978-3-030-58669-0_8,3092358307,,0,083-395-017-129-741; 120-813-351-553-168; 155-338-218-673-080,0,false,,
193-938-957-231-259,Analysis and Implementation for a Walking Support System for Visually Impaired People,2013-04-22,2013,book chapter,Advanced Engineering and Computational Methodologies for Intelligent Mechatronics and Robotics,,IGI Global,,Eklas Hossain; Md Raisuddin Khan; Riza Muhida; Ahad Ali,"<jats:p>Visually impaired people are faced with challenges in detecting information about terrain. This paper presents a new walking support system for the blind to navigate without any assistance from others or using a guide cane. In this research, a belt, wearable around the waist, is equipped with four ultrasonic sensors and one sharp infrared sensor. Based on mathematical models, the specifications of the ultrasonic sensors are selected to identify optimum orientation of the sensors for detecting stairs and holes. These sensors are connected to a microcontroller and laptop for analyzing terrain. An algorithm capable of classifying various types of obstacles is developed. After successful tests using laptop, the microcontroller is used for the walking system, named ‘Belt for Blind’, to navigate their environment. The unit is also equipped with a servo motor and a buzzer to generate outputs that inform the user about the type of obstacle ahead. The device is light, cheap, and consumes less energy. However, this device is limited to standard pace of mobility and cannot differentiate between animate and inanimate obstacles. Further research is recommended to overcome these deficiencies to improve mobility of blind people.</jats:p>",,,179,195,Buzzer; Laptop; Microcontroller; Wearable computer; Obstacle; Computer science; Engineering; Orientation (vector space); Human–computer interaction; Simulation; Embedded system; Computer vision; Real-time computing; Electrical engineering; ALARM; Geometry; Mathematics; Law; Political science; Operating system,,,,,,http://dx.doi.org/10.4018/978-1-4666-3634-7.ch013,,10.4018/978-1-4666-3634-7.ch013,,,0,001-076-828-128-846; 007-527-228-704-510; 013-847-555-426-159; 014-157-934-526-940; 022-697-627-694-941; 030-127-369-356-741; 031-799-548-880-483; 034-350-308-512-672; 045-039-665-546-894; 046-649-192-050-710; 053-266-220-901-486; 058-443-047-291-990; 064-309-126-323-022; 065-992-777-809-97X; 073-722-974-726-137; 096-803-870-115-728; 098-811-589-762-176; 111-057-090-228-06X; 125-501-372-179-565; 127-483-973-270-088; 146-425-395-790-21X,3,false,,
194-245-004-896-301,HRI - NavCue: Context Immersive Navigation Assistance for Blind Travelers,2016-03-07,2016,conference proceedings,,,,,Kangwei Chen; Victoria Plaza-Leiva; Byung-Cheol Min; Aaron Steinfeld; Mary Bernardine Dias,"Research in assistive systems for travelers who are blind/low vision (B/LV) has been largely focused on basic map information. We present NavCue, an intelligent system module for providing rich, multi-sensory, context-based information using speech guidance and robot physical gestures. This approach is motivated by our previous user studies with people who are blind or low vision. This rich information should enhance user location awareness and confidence when traveling through unfamiliar locations.",,,559,559,Human–computer interaction; Human–robot interaction; Artificial intelligence; Gesture; Mobile robot navigation; Location awareness; Context (language use); Low vision; Navigation assistance; Computer vision; Computer science; Robot,,,,,http://dblp.uni-trier.de/db/conf/hri/hri2016.html#ChenPMSD16 https://dl.acm.org/doi/10.5555/2906831.2906978 https://dblp.uni-trier.de/db/conf/hri/hri2016.html#ChenPMSD16,https://dl.acm.org/doi/10.5555/2906831.2906978,,,2321763238,,0,154-764-681-989-082; 169-478-549-790-022,1,false,,
194-556-870-564-15X,Assistive Technology for Navigation of Visually Impaired People,2024-05-04,2024,journal article,Deleted Journal,,,,Mohammed Fayiz Ferosh,"In this paper, an assistive navigational technology was proposed for those who are blind. People who are visually impaired are more likely to be physically inactive, move more slowly, and dread falling. As a result, the majority of them lack the confidence necessary to travel on their own in strange places. Despite the fact that some technology advancements can help them, not all of them are affordable to the common man. Through our efforts, we hope to help visually impaired persons feel secure and confident while navigating new surroundings. Assistive tech- nology for navigation of visually impaired people is an affordable and efficient computer vision based blind guidance system that helps the user detect the distance to obstacles well in advance and avoid them. The suggested system also includes an emergency distress alert feature that enables users to communicate their locations in an emergency.",20,7s,989,996,Visually impaired; Assistive technology; Physical medicine and rehabilitation; Human–computer interaction; Computer science; Psychology; Computer vision; Medicine,,,,,https://journal.esrgroups.org/jes/article/download/3479/2719 https://doi.org/10.52783/jes.3479,https://journal.esrgroups.org/jes/article/download/3479/2719,,,,,0,,0,true,cc-by-nd,hybrid
194-628-370-392-62X,Robust Gesture Recognition and Classification for Visually Impaired Persons Using Growth Optimizer with Deep Stacked Autoencoder,2023-08-26,2023,journal article,Deleted Journal,,,,Mashael Maashi; Mohammed Abdullah Al-Hagery; Mohammed Rizwanullah; Azza Elneil Osman,"Visual impairment affects the major population of the world, and impaired vision people need assistance for their day-to-day activities. With the enormous growth and usage of new technologies, various devices were developed to help them with object identification in addition to navigation in the indoor and outdoor surroundings. Gesture detection and classification for blind people aims to develop technologies to assist those people to navigate their surroundings more easily. To achieve this goal, using machine learning and computer vision techniques is a better solution to classify and detect hand gestures. Such methods are utilized for finding the shape, position, and movement of the hands in real-time. With this motivation, this article presents a robust gesture recognition and classification using growth optimizer with deep stacked autoencoder (RGRC-GODSAE) model for visually impaired persons. The goal of the RGRC-GODSAE technique lies in the accurate recognition and classification of gestures to assist visually impaired persons. The RGRC-GODSAE technique follows the Gabor filter approach at the initial stage to remove noise. In addition, the RGRC-GODSAE technique uses the ShuffleNet model as a feature extractor and the GO algorithm as a hyperparameter optimizer. Finally, the deep stacked autoencoder model is exploited for the automated recognition and classification of gestures. The experimental validation of the RGRC-GODSAE technique is carried out on the benchmark dataset. The extensive comparison study showed better gesture recognition performance of the RGRC-GODSAE technique over other deep learning models.",2,2,,,Autoencoder; Gesture; Artificial intelligence; Computer science; Gesture recognition; Deep learning; Computer vision; Benchmark (surveying); Pattern recognition (psychology); Feature (linguistics); Population; Machine learning; Speech recognition; Linguistics; Philosophy; Demography; Geodesy; Sociology; Geography,,,,,https://www.scienceopen.com/document_file/6ad5ff0b-60b5-4472-80aa-0e601519498a/ScienceOpen/jdr20230029.pdf https://doi.org/10.57197/jdr-2023-0029,https://www.scienceopen.com/document_file/6ad5ff0b-60b5-4472-80aa-0e601519498a/ScienceOpen/jdr20230029.pdf,,,,,0,,0,true,cc-by,hybrid
194-918-227-208-32X,"CARS 2024-Computer Assisted Radiology and Surgery Proceedings of the 38th International Congress and Exhibition Barcelona, Spain, June 18-21, 2024.",2024-05-27,2024,editorial,International journal of computer assisted radiology and surgery,18616429; 18616410,Springer Science and Business Media LLC,Germany,,,19,Suppl 1,1,153,Exhibition; General surgery; Medicine; Medical physics; Library science; Computer science; Art history; History,,"Humans; Surgery, Computer-Assisted/methods; Spain; Congresses as Topic; Radiology",,,,http://dx.doi.org/10.1007/s11548-024-03128-9,38802571,10.1007/s11548-024-03128-9,,,0,,0,false,,
195-019-844-650-508,Smart Cane for Blind People using Raspberry PI and Arduino,2020-05-30,2020,journal article,International Journal of Recent Technology and Engineering (IJRTE),22773878,Blue Eyes Intelligence Engineering and Sciences Engineering and Sciences Publication - BEIESP,,Prutha. G; Smitha. B. M; Kruthi. S; Sahana. D. P,"<jats:p>In this paper we are introducing a stick to guide blind people to move independently. It consists of raspberry pi and Arduino for monitoring the stick. Three pairs of ultrasonic sensors are used to detect obstacle in front of the users in the range of 15cm, and water sensors are used for water detection &amp; puddles. The above sensors are interfaced in the stick. The stick can take robotic decision to move forward, backward, left and right as per obstruction detection using dc motor. Users will get alert from buzzer and vibration. Flashlight is interfaced on stick which can be noticed by others to let him pass way. Finger ring is used to give navigation using GPS. The user can maintain easily with fast response &amp; low power consumption The main purpose of this paper is to help visually impaired people for navigating independently</jats:p>",9,1,1520,1522,Buzzer; Arduino; Raspberry pi; Global Positioning System; Obstacle; Computer science; Real-time computing; Embedded system; Computer hardware; Simulation; Electrical engineering; Engineering; Operating system; Internet of Things; Geography; ALARM; Archaeology,,,,,,http://dx.doi.org/10.35940/ijrte.a2498.059120,,10.35940/ijrte.a2498.059120,,,0,,1,true,,gold
195-196-708-451-173,"Gaining insight for the design, development, deployment and distribution of assistive navigation systems for blind and visually impaired people through a detailed user requirements elicitation",2022-06-13,2022,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Paraskevi Theodorou; Apostolos Meliones,,22,3,841,867,Software deployment; Computer science; Human–computer interaction; Assistive technology; Process (computing); Requirements elicitation; User requirements document; Autonomy; Multimedia; Requirements analysis; Software engineering; Software; Political science; Law; Programming language; Operating system,,,,,,http://dx.doi.org/10.1007/s10209-022-00885-9,,10.1007/s10209-022-00885-9,,,0,002-794-722-068-681; 004-811-722-524-127; 005-191-781-331-343; 007-489-086-930-63X; 012-295-789-954-81X; 014-898-844-427-824; 015-714-235-272-975; 019-149-805-110-617; 019-150-105-799-327; 020-685-851-955-017; 021-048-235-787-915; 022-278-887-294-358; 024-876-376-113-351; 026-450-734-929-647; 031-175-485-676-104; 040-431-674-314-483; 040-660-173-610-646; 046-375-685-310-773; 048-603-106-043-151; 051-332-059-935-398; 058-468-795-176-403; 058-765-318-078-633; 065-864-368-278-26X; 067-987-417-954-686; 067-987-539-817-268; 070-685-682-720-674; 073-338-244-636-803; 075-519-293-886-898; 075-633-076-909-925; 075-891-715-156-686; 079-433-787-480-087; 085-508-730-794-369; 088-206-525-984-388; 095-065-370-712-691; 098-910-337-444-079; 102-952-660-914-374; 110-690-766-883-070; 112-641-924-870-702; 113-018-601-425-331; 121-058-362-120-720; 130-117-048-358-929; 139-233-304-780-009; 142-575-622-508-48X; 158-769-392-676-037; 164-686-897-500-348; 166-489-262-588-008; 167-521-103-760-249; 180-813-779-294-458,7,false,,
195-262-093-932-191,Smart Electronic Travel Stick for the Visually Challenged,2016-12-28,2016,journal article,American Journal of Electrical and Electronic Engineering,23287357,,,Sambandh Bhusan Dhal; Arun Agarwal; Kabita Agarwal,"Science and technology has always played a prominent role in comforting human lives. The main objective of this work is based on mollifying the disabilities of blindness by creating a microcontroller based automated hardware that can ratify a visually impaired person to detect obstacles in front of him/her instantly and guide him/her in a proper way to reach his/her destination. The hardware consists of a microcontroller integrated with Arduino Uno board, microcontroller, ultrasonic sensor, GPS module, GSM module, serial driver, buzzer, LED and other additional equipments. It acts as a better navigational tool for the visually impaired. The sensors used in this model provide information about the outside environment. GPS Technology is integrated with pre-programmed locations and it is sent to the desired number through GSM module when required. This presents a design and a system concept to provide smart electronic aid for blind people. The aim of the overall system is to produce an economic and productive navigation aid for the visually challenged which can give them a sense of artificial vision and object detection by providing information about the environmental scenario of static and dynamic objects around them. Ultrasonic sensors are used to detect obstacles from a distance so as to guide the blind perso in the right path.",4,6,177,181,Human–computer interaction; Engineering; Embedded system; Microcontroller; Object detection; Buzzer; PATH (variable); Artificial vision; Gsm module; Arduino uno; Global Positioning System,,,,,http://www.sciepub.com/AJEEE/abstract/6905 http://pubs.sciepub.com/ajeee/4/6/4/,http://www.sciepub.com/AJEEE/abstract/6905,,,2564615078,,0,,2,false,,
195-360-003-869-256,Enhancing Accessibility: Assistive Devices Pairing Mobile Apps for Visually Impaired People,2023-09-30,2023,journal article,Platform : A Journal of Engineering,26369877; 26008424,Universiti Teknologi PETRONAS,,Aisyah Ibrahim; Siti Fauziah Toha; Nor Hidayati Diyana Nordin; Mohammad Osman Tokhi,"<jats:p>Vision impairment and blindness are significant health conditions that affect a considerable portion of the global population and have significant consequences, particularly in developed nations. Its harmful consequences can have a negative influence on a person's personal quality of life, as well as limit their ability to live independently. Navigation plays a crucial role in the activities that are impacted by vision loss. This is because it is almost impossible for the individual to move safely and independently. The goal of this project is to improve mobility for visually impaired people by employing Light Detection and Ranging (LiDAR) sensors to identify obstacles and alert users via haptic feedback if there are potential obstacles in their way. In addition, this study compares two sensors, LiDAR and ultrasonic sensors, besides the implementation of the Internet of Things (IoT), which was integrated into the small, portable robot by pairing a mobile phone. This is to ensure the user may be securely guided to their destination. I-Walk is a mobile app with a voice user interface (VUI) for GPS navigation systems and emergency calling. This app has been developed to allow users safe and independent travel. The simulation result shows that even though ultrasonic sensors are cheaper and serve the same function as LiDAR, their huge standard deviation and percentage error shown in this study cannot equal LiDAR's accuracy in which LiDAR has an average error of 0.63%. In contrast, ultrasonic sensors have an average error of 2.39%. Overall, this obstacle detection and navigation system comprises robust hardware and software to assist visually impaired people effectively.Keywords: Assistive technology, blind, mini guider robot, obstacle detection, navigation, IoT</jats:p>",7,3,32,,,,,,,,http://dx.doi.org/10.61762/pajevol7iss3art24135,,10.61762/pajevol7iss3art24135,,,0,,0,true,cc-by,gold
195-623-444-440-289,Visually impaired people and public transport information,1993-05-25,1993,,,,,,D. Harris; G. Whitney,It is not possible to successfully plan and complete a journey unless a certain amount of information is available. In the case of blind and partially sighted travellers the amount of accessible information is often unnecessarily restricted. The Royal National Institute for the Blind has also been looking at other ways in which independent travel could be made easier for blind and partially sighted people. From this work the specification for the RNIB REACT system was produced and the equipment was designed and produced by GEC Marconi. The RNIB REACT system has been designed and produced to assist blind and partially sighted people in finding and using many public machines and information systems. >,,,,,Internet privacy; Engineering; Work (electrical); Information system; Partially sighted; Visually impaired; Public transport; Plan (drawing); Telecommunications,,,,,https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=274462,https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=274462,,,1634095795,,0,,1,false,,
197-213-313-918-749,Artificial Intelligence Based Visually Impaired Assist System,2022-11-10,2022,conference proceedings article,"2022 International Conference on Advances in Computing, Communication and Materials (ICACCM)",,IEEE,,Deepti Shinghal; Kshitij Shinghal; Shuchita Saxena; Amit Saxena; Nishant Saxena; Amit Sharma,"In present work, a system is proposed which is unique in a way that there is a requirement of visually impaired friendly buildings. In current scenario when a visually impaired person enters a building which is Visually Impaired (VI) friendly, an attendant hands him over braille based navigation chart or electronic guide system The proposed system automatically detects a visually impaired person makes an announcement, generates an alert message from the basket where VI person enabled braille based guide maps are kept. The system was tested and it is able to detect blind persons with good accuracy.",,,,,Computer science; Visually impaired; Artificial intelligence; Human–computer interaction; Computer vision,,,,,,http://dx.doi.org/10.1109/icaccm56405.2022.10009591,,10.1109/icaccm56405.2022.10009591,,,0,007-069-437-291-059; 017-559-000-135-867; 019-248-094-664-19X; 019-827-712-091-154; 020-478-495-212-783; 031-643-780-327-702; 032-962-471-214-274; 051-773-212-592-656; 067-583-626-133-824; 078-561-606-050-37X; 084-616-732-951-059; 086-141-234-318-177; 108-186-632-977-341; 119-553-924-738-845; 130-593-511-201-286; 135-967-297-423-210; 166-158-315-750-560,0,false,,
197-398-944-420-377,Analysis of Obstruction Avoidance Assistants to Enhance the Mobility of Visually Impaired Person: A Systematic Review,2023-01-27,2023,conference proceedings article,2023 International Conference on Artificial Intelligence and Smart Communication (AISC),,IEEE,,Subrat Tripathi; Saurabh Singh; Tanya Tanya; Shivani Kapoor; null Kirti; Amanpreet Singh Saini,"In the field of research for assistive technology, the advancement of navigational tools for those who are blind had emerged as a major concern. This article offers valuable insights for researchers working on assistive technologies for people who are blind or visually impaired, specifically focusing on indoor and outdoor navigation systems. To create such systems, it is important to study past research in this area, starting from early investigations on electronic travel aids to the more recent utilization of advanced artificial vision models. This paper aims to provide an effective vision for researchers to use in developing technological instruments for the visually impaired. This process involves a comprehensive evaluation of various technological approaches in order to achieve the challenging goal of creating an effective navigation system. We carried out a separate systematic review because papers in this field tend to focus more on software-based computer vision solutions than on hardware. We have analyzed 191 relevant studies published between 2011 and 2022 to create a comprehensive understanding of the topic at hand. This systematic mapping process will assist researchers, engineers, and practitioners in making informed decisions by identifying shortcomings in current navigation assistance technology and proposing new and improved smart assistant applications that can ensure the safety and accuracy of direction for people who are blind or visually impaired.",,,,,Process (computing); Computer science; Field (mathematics); Assistive technology; Visually impaired; Human–computer interaction; Systematic review; Mathematics; MEDLINE; Political science; Law; Pure mathematics; Operating system,,,,,,http://dx.doi.org/10.1109/aisc56616.2023.10085416,,10.1109/aisc56616.2023.10085416,,,0,001-672-715-224-680; 002-037-819-294-847; 003-245-163-059-395; 004-772-554-982-380; 007-573-197-769-65X; 007-725-872-437-764; 009-097-030-325-625; 013-108-056-949-247; 015-440-630-508-33X; 015-944-524-719-546; 019-914-067-455-570; 020-682-621-599-375; 022-031-347-734-67X; 022-631-644-111-523; 023-438-345-162-875; 024-523-299-123-05X; 024-888-082-944-828; 025-764-110-104-300; 026-303-765-898-687; 027-720-558-803-335; 029-885-082-233-530; 031-608-060-688-477; 032-538-345-541-781; 033-627-073-872-114; 035-155-677-944-748; 036-578-364-944-588; 053-196-706-640-254; 053-215-378-840-089; 057-837-646-456-879; 057-847-555-860-541; 065-418-822-524-660; 066-627-120-919-876; 067-894-235-988-508; 069-116-650-623-84X; 071-875-464-645-382; 079-349-286-645-136; 089-822-552-618-426; 100-575-932-125-341; 106-587-878-038-509; 107-659-830-473-823; 124-851-601-621-358; 132-564-289-502-036; 134-226-557-899-385; 137-032-010-877-936; 141-649-000-612-322; 157-306-463-851-764; 159-474-331-562-499; 160-616-914-721-582; 166-810-485-231-725; 179-237-001-293-456; 186-810-372-950-411,2,false,,
197-650-658-748-04X,Development and preliminary testing of a prior knowledge-based visual navigation system for cardiac ultrasound scanning.,2023-12-21,2023,journal article,Biomedical engineering letters,2093985x; 20939868,Springer Science and Business Media LLC,Germany,Mingrui Hao; Jun Guo; Cuicui Liu; Chen Chen; Shuangyi Wang,,14,2,307,316,Sonographer; Computer science; Medical physics; Ultrasound; Cardiac Ultrasound; Set (abstract data type); Augmented reality; Computer vision; Artificial intelligence; Medicine; Radiology; Programming language,Augmented reality; Medical training; Skill learning; Ultrasound scanning; Visual navigation,,,Natural Science Foundation of China; InnoHK Program; National Natural Science Foundation of China,,http://dx.doi.org/10.1007/s13534-023-00338-z,38374906,10.1007/s13534-023-00338-z,,PMC10874367,0,001-326-191-110-786; 004-115-127-888-197; 009-987-762-746-622; 012-099-632-896-406; 016-882-078-042-664; 020-099-329-600-190; 025-266-849-429-11X; 029-601-057-140-028; 042-039-160-596-547; 050-041-685-654-650; 069-271-468-615-704; 083-098-086-485-984; 095-034-152-105-357; 100-131-055-579-693; 100-532-222-000-018; 104-045-128-180-187; 109-475-091-470-413; 129-502-263-115-124; 131-985-855-309-003; 158-800-842-814-453; 169-354-870-692-467; 176-701-598-523-863,3,false,,
198-358-526-126-482,Intelligent environments and assistive technologies for assisting visually impaired people: a systematic literature review,2024-05-03,2024,journal article,Universal Access in the Information Society,16155289; 16155297,Springer Science and Business Media LLC,Germany,Leandro Rossetti de Souza; Rosemary Francisco; João Elison da Rosa Tavares; Jorge Luis Victória Barbosa,"Intelligent environments (IE) refer to physical spaces imbued with pervasive and seamless intelligence, created to proactively support individuals in their daily routines. Developments in technologies such as the internet of things (IoT) and artificial intelligence (AI) have taken these environments from theoretical notions to practical realities. Simultaneously, the field of ambient assisted living (AAL) has made significant strides. Evolving from AT, AAL represents an application of IE that specifically seeks to enable individuals-especially those with disabilities or the elderly-to lead healthier, more independent, and dignified lives through the assistance of technology integrated within their living environments. The confluence of IE and AAL has led to the rise of innovative solutions aimed at enhancing the lives of individuals with special needs, such as the visually impaired people (VIP). This research presents a systematic literature review investigating the utilization of IE, underpinned by AAL principles, in supporting VIPs. Initially, a pool of 14,760 studies was obtained from 9 databases, all published up to December 2022. After applying specific inclusion and exclusion criteria, this pool was reduced to 101 articles. Each of these articles was reviewed, analyzed, and categorized into four functional and operating principle categories to address five research questions. The study proposes multiple taxonomies as an approach to holistically synthesize the various technologies and devices categorized in the reviewed articles. Emerging research challenges and trends in this domain are highlighted, with a substantial trend being the escalating use of deep learning (DL) techniques. These techniques have been pivotal in the development of systems focused on object detection, path recognition, and navigation for devices, particularly smartphones, geared towards enhancing the lives of VIPs.",,,,,Visually impaired; Assistive technology; Human–computer interaction; Computer science; Systematic review; Assistive device; Multimedia; Physical medicine and rehabilitation; MEDLINE; Medicine; Political science; Law,,,,,https://www.researchsquare.com/article/rs-3293649/latest.pdf https://doi.org/10.21203/rs.3.rs-3293649/v1,http://dx.doi.org/10.1007/s10209-024-01117-y,,10.1007/s10209-024-01117-y,,,0,000-259-572-898-34X; 002-318-499-750-823; 002-449-629-591-50X; 004-071-983-923-934; 005-191-781-331-343; 005-292-615-099-772; 005-892-475-648-141; 008-616-901-025-130; 009-671-283-951-337; 009-690-618-758-576; 010-475-144-749-494; 010-876-983-456-294; 012-344-135-607-729; 013-041-559-281-791; 014-012-510-609-817; 014-789-924-403-352; 015-291-279-210-205; 015-672-084-912-775; 015-944-524-719-546; 016-634-482-477-755; 017-159-670-613-140; 017-708-952-711-805; 017-893-832-693-54X; 018-165-888-033-875; 018-828-594-955-588; 019-995-754-511-586; 020-494-375-174-356; 021-215-340-461-541; 024-523-299-123-05X; 024-763-433-919-837; 025-671-579-181-87X; 027-049-361-368-415; 027-588-207-376-882; 028-087-404-356-291; 028-744-192-669-778; 032-375-119-879-051; 032-962-471-214-274; 033-639-424-237-429; 036-261-548-880-915; 043-916-813-603-766; 044-636-203-595-260; 044-912-780-668-165; 045-145-950-074-733; 045-752-119-176-922; 046-605-753-137-118; 046-631-555-227-883; 046-739-254-289-946; 048-199-650-951-338; 051-126-094-178-401; 051-893-961-323-409; 053-058-599-846-461; 053-219-839-728-849; 054-602-769-239-115; 055-068-819-449-735; 056-606-064-699-673; 057-534-818-835-800; 058-308-310-638-823; 063-670-066-881-807; 067-583-626-133-824; 068-085-675-386-70X; 069-608-818-240-967; 071-548-562-839-959; 079-254-506-729-947; 080-589-710-669-822; 083-336-948-105-959; 085-612-945-410-967; 087-805-175-265-225; 090-648-607-894-546; 090-656-986-529-548; 091-910-369-114-547; 099-334-901-486-872; 102-952-660-914-374; 103-356-406-185-802; 109-867-953-627-297; 112-566-962-059-235; 113-483-703-037-077; 113-863-831-856-32X; 115-053-089-097-274; 115-439-483-580-16X; 121-261-690-038-295; 122-752-499-927-898; 127-901-392-792-469; 130-851-230-902-026; 131-503-960-053-375; 131-802-291-182-259; 133-635-535-520-03X; 134-920-796-409-777; 135-218-000-341-751; 135-253-843-368-438; 139-014-857-960-579; 141-649-000-612-322; 145-103-385-375-073; 148-478-467-972-963; 158-368-277-657-984; 160-207-883-035-688; 162-735-017-899-473; 167-357-508-798-85X; 171-189-370-546-346; 173-312-792-945-957; 178-296-201-883-390; 187-382-466-659-359; 194-288-574-804-739,0,true,,green
198-371-206-211-238,Supplemental vibrissal extensions as an alternative to improve the tactile sensitivity of blind dogs - a preliminary approach investigation.,2024-03-01,2024,journal article,Veterinary research communications,15737446; 01657380,Springer Science and Business Media LLC,Netherlands,Manuella Oliveira Borges de Sampaio; Fabiano Montiani-Ferreira; Franz Riegler Mello; Camila Bolmann Martins; Ana Leticia Groszewicz de Souza; Mariza Bortolini; Paulo Roberto Klaumann; Bret A Moore,,48,3,1907,1914,Medicine; Audiology; Tactile perception; Perception; Surgery; Psychology; Neuroscience,Ambulation; Blind; Dogs; Tactile perception; Vibrissae; Whiskers,Animals; Dogs/physiology; Blindness/veterinary; Vibrissae/physiology; Touch/physiology; Male; Female; Dog Diseases/physiopathology,,,,http://dx.doi.org/10.1007/s11259-024-10342-y,38427268,10.1007/s11259-024-10342-y,,,0,003-457-513-696-090; 011-185-884-774-784; 020-354-087-945-561; 025-994-339-008-476; 035-923-761-547-699; 056-091-005-302-226; 061-843-602-105-298; 062-919-957-713-414; 074-819-431-805-119; 076-171-520-544-874; 084-891-961-434-481; 085-978-668-921-086; 092-931-522-154-211; 119-875-514-258-998; 131-293-328-919-208; 182-560-392-957-579; 196-863-242-795-726,0,false,,
198-446-438-690-948,Identifying the walking patterns of visually impaired people by extending white cane with smartphone sensors,2023-02-10,2023,journal article,Multimedia Tools and Applications,13807501; 15737721,Springer Science and Business Media LLC,Netherlands,Izaz Khan; Shah Khusro; Irfan Ullah,,82,17,27005,27025,Computer science; Pedestrian; Naive Bayes classifier; Random forest; Stairs; Pace; Artificial intelligence; Cane; Dead reckoning; Human–computer interaction; Table (database); Computer vision; Real-time computing; Simulation; Machine learning; Support vector machine; Global Positioning System; Data mining; Telecommunications; Biochemistry; Chemistry; Civil engineering; Sugar; Geodesy; Transport engineering; Engineering; Geography,,,,,,http://dx.doi.org/10.1007/s11042-023-14423-z,,10.1007/s11042-023-14423-z,,,0,004-098-498-677-450; 004-216-069-855-831; 006-487-017-505-503; 009-577-731-659-049; 012-833-369-160-621; 014-789-924-403-352; 018-262-121-105-137; 020-677-532-549-732; 024-888-082-944-828; 027-840-530-030-760; 028-171-260-452-285; 028-711-379-428-818; 031-520-687-911-584; 035-977-968-950-171; 038-009-428-880-783; 045-599-200-378-144; 046-375-685-310-773; 048-073-200-003-177; 052-046-438-827-353; 057-580-969-583-660; 059-088-571-732-521; 063-117-355-616-790; 068-097-134-870-978; 081-691-386-643-168; 082-036-797-529-285; 085-755-161-990-965; 090-420-681-934-605; 093-597-738-380-598; 098-048-071-223-067; 122-027-623-239-610; 127-698-186-048-283; 130-847-221-606-776; 133-228-201-498-241; 137-320-213-389-496; 138-528-673-324-553; 141-649-000-612-322; 147-576-426-453-940; 164-155-799-770-296,1,false,,
198-525-289-240-681,Navigation Agents for the Visually Impaired: A Sidewalk Simulator and Experiments,2019-10-29,2019,preprint,arXiv: Computer Vision and Pattern Recognition,,,,Martin Weiss; Simon Chamorro; Roger Girgis; Margaux Luck; Samira Ebrahimi Kahou; Joseph Paul Cohen; Derek Nowrouzezahrai; Doina Precup; Florian Golemo; Chris Pal,"Millions of blind and visually-impaired (BVI) people navigate urban environments every day, using smartphones for high-level path-planning and white canes or guide dogs for local information. However, many BVI people still struggle to travel to new places. In our endeavor to create a navigation assistant for the BVI, we found that existing Reinforcement Learning (RL) environments were unsuitable for the task. This work introduces SEVN, a sidewalk simulation environment and a neural network-based approach to creating a navigation agent. SEVN contains panoramic images with labels for house numbers, doors, and street name signs, and formulations for several navigation tasks. We study the performance of an RL algorithm (PPO) in this setting. Our policy model fuses multi-modal observations in the form of variable resolution images, visible text, and simulated GPS data to navigate to a goal door. We hope that this dataset, simulator, and experimental results will provide a foundation for further research into the creation of agents that can assist members of the BVI community with outdoor navigation.",,,,,Task (project management); Visually impaired; Computer science; Simulation; Artificial neural network; Reinforcement learning,,,,,https://arxiv.org/abs/1910.13249v1 http://ui.adsabs.harvard.edu/abs/2019arXiv191013249W/abstract https://arxiv.org/pdf/1910.13249v1 http://export.arxiv.org/pdf/1910.13249,https://arxiv.org/abs/1910.13249v1,,,2982506125,,0,006-331-277-518-553; 012-222-131-149-710; 016-602-693-582-348; 017-152-516-984-961; 034-468-880-826-73X; 042-748-773-416-723; 042-800-033-418-542; 051-839-541-246-112; 055-395-668-630-884; 056-332-008-726-803; 057-567-246-168-045; 075-519-293-886-898; 080-691-289-270-770; 081-230-818-442-836; 084-197-601-345-91X; 087-510-774-676-36X; 089-164-128-062-72X; 114-430-786-394-842; 119-716-275-108-010; 120-033-505-076-310; 124-017-217-197-252; 124-180-153-132-23X; 133-499-790-890-245; 134-198-077-256-211; 141-091-048-565-107; 144-141-067-459-117; 161-953-045-038-91X; 164-376-579-905-792; 179-415-331-453-984; 191-885-805-203-436; 195-204-145-163-609,4,true,,unknown
198-868-411-549-215,Blind Lane Detection and Following for Assistive Navigation of Vision Impaired People,2023-07-08,2023,conference proceedings article,2023 International Conference on Advanced Robotics and Mechatronics (ICARM),,IEEE,,Zhicheng Han; Jason Gu; Ying Feng,The task of safely navigating outdoor environ-ments presents significant difficulties for people with visual impairments. This paper aims to address this issue by proposing a wearable assistance system for visually impaired individuals in blind lane detection and following scenarios. The system consists of two stages: the first stage employs a fully convolutional network to detect blind lanes and an object detection network to identify obstacles. The second stage utilizes an improved artificial potential field method to achieve real-time path following based on the detection results. Our experiments involving subjects in dynamic outdoor environments demonstrate the robustness of our proposed method in navigating and avoiding obstacles under challenging outdoor conditions.,,,,,Computer science; Robustness (evolution); Artificial intelligence; Computer vision; Object detection; Convolutional neural network; Wearable computer; Visually impaired; Task (project management); Human–computer interaction; Pattern recognition (psychology); Embedded system; Engineering; Biochemistry; Chemistry; Systems engineering; Gene,,,,,,http://dx.doi.org/10.1109/icarm58088.2023.10218843,,10.1109/icarm58088.2023.10218843,,,0,002-123-448-882-287; 004-216-069-855-831; 011-737-861-055-472; 021-736-241-758-765; 027-327-337-622-425; 037-376-150-587-539; 038-326-446-864-583; 040-057-279-881-544; 041-601-720-630-715; 044-230-807-650-638; 053-094-537-530-97X; 060-479-915-055-59X; 064-490-466-346-713; 072-696-313-726-997; 082-840-767-990-238; 124-561-402-567-55X; 190-324-040-849-481,1,false,,
199-636-503-416-165,Survey of Target Parking Position Designation for Automatic Parking Systems,2023-02-15,2023,journal article,International Journal of Automotive Technology,12299138; 19763832,Springer Science and Business Media LLC,South Korea,Jae Kyu Suhr; Ho Gi Jung,,24,1,287,303,Parking guidance and information; Standardization; Position (finance); Parking space; Computer science; Commercialization; Parking lot; Sensor fusion; Field (mathematics); Space (punctuation); Transport engineering; Artificial intelligence; Engineering; Civil engineering; Mathematics; Finance; Pure mathematics; Law; Political science; Economics; Operating system,,,,,,http://dx.doi.org/10.1007/s12239-023-0025-6,,10.1007/s12239-023-0025-6,,,0,000-121-131-216-313; 000-615-876-073-88X; 002-361-675-117-167; 002-493-913-337-216; 003-294-452-992-174; 003-477-470-219-961; 003-636-286-352-418; 005-383-069-911-682; 006-716-848-256-882; 007-574-396-166-67X; 007-892-111-521-900; 013-432-163-369-103; 013-679-474-210-954; 013-936-959-973-372; 015-043-729-040-264; 016-055-604-708-720; 016-353-406-128-018; 016-430-625-798-157; 018-753-182-424-381; 021-129-774-786-705; 021-360-232-245-594; 022-403-354-681-216; 023-890-431-778-44X; 024-176-528-768-166; 027-818-045-801-962; 028-455-826-790-545; 030-141-147-955-854; 031-218-334-653-826; 031-651-588-285-346; 031-685-308-530-526; 031-887-939-912-622; 035-277-967-833-996; 035-462-419-995-817; 036-113-206-955-548; 040-528-938-897-039; 042-975-780-191-948; 043-940-096-639-212; 045-886-289-070-69X; 046-821-320-195-132; 047-732-294-970-714; 048-281-663-889-994; 048-671-285-645-960; 049-317-239-158-314; 052-167-308-999-596; 052-412-241-107-093; 053-103-752-831-400; 058-648-672-593-168; 059-974-719-243-772; 060-792-321-719-548; 061-543-728-452-549; 061-946-877-986-006; 062-159-121-882-542; 063-522-169-334-940; 063-586-573-135-065; 068-498-193-867-696; 069-480-623-909-181; 072-385-327-851-31X; 073-415-513-615-725; 074-327-969-619-763; 076-009-233-405-163; 083-146-854-451-490; 084-260-553-764-069; 087-068-122-069-573; 087-855-015-633-709; 089-753-728-933-168; 092-353-533-941-710; 094-889-584-140-794; 097-520-442-045-264; 098-926-109-358-108; 099-543-037-978-68X; 103-007-802-970-516; 114-897-779-187-561; 119-413-864-966-744; 120-936-329-066-638; 121-616-706-146-806; 125-163-061-930-068; 126-703-427-044-425; 128-402-754-907-316; 129-590-735-283-971; 132-525-140-010-63X; 137-645-032-019-408; 137-662-030-972-195; 148-021-336-493-671; 156-160-493-346-106; 157-003-360-052-611; 162-198-767-774-783; 176-247-942-059-442; 194-485-158-405-862; 199-335-216-645-652,10,false,,
199-818-033-071-648,Towards human-leveled vision systems,2024-07-30,2024,journal article,Science China Technological Sciences,16747321; 18691900; 1862281x; 10069321,Springer Science and Business Media LLC,China,JianHao Ding; TieJun Huang,,67,8,2331,2349,,,,,,,http://dx.doi.org/10.1007/s11431-024-2762-5,,10.1007/s11431-024-2762-5,,,0,000-567-600-951-131; 001-112-242-479-96X; 001-409-066-739-351; 001-967-928-067-576; 002-898-785-638-553; 002-964-307-222-687; 003-114-383-242-28X; 003-464-440-340-151; 003-507-675-866-999; 004-337-695-140-164; 004-696-619-523-432; 005-615-497-011-154; 006-642-807-907-678; 006-694-991-787-948; 007-619-251-889-048; 007-945-279-762-915; 008-217-809-214-58X; 008-306-237-874-535; 008-819-482-581-393; 008-944-320-444-882; 009-259-857-326-051; 009-277-742-482-504; 009-614-749-507-49X; 009-783-836-414-635; 010-029-694-429-444; 010-139-950-857-34X; 010-399-565-923-751; 010-757-406-152-096; 010-881-186-573-195; 010-881-916-782-475; 011-458-682-081-403; 012-170-340-093-248; 012-661-919-527-734; 012-973-663-170-26X; 013-014-936-720-478; 013-156-255-523-712; 013-513-404-652-706; 014-307-460-486-211; 015-141-132-801-613; 015-782-336-956-414; 016-559-589-261-447; 017-233-424-653-510; 018-767-650-195-426; 018-856-883-914-182; 018-999-128-116-572; 019-254-362-882-304; 019-257-809-521-235; 019-292-949-754-499; 019-410-449-993-623; 019-492-036-106-506; 020-469-578-512-242; 021-427-742-555-553; 021-673-980-633-581; 022-366-709-994-220; 023-432-166-444-132; 023-612-519-316-872; 023-681-516-146-803; 023-745-812-350-841; 023-978-655-701-261; 024-084-624-297-773; 026-130-763-246-076; 026-505-676-867-112; 027-917-994-031-860; 028-888-850-181-276; 029-148-672-721-534; 029-664-487-475-571; 029-722-542-618-479; 029-754-981-242-027; 029-783-476-235-013; 029-887-999-307-924; 030-086-055-586-423; 031-466-238-275-477; 032-635-802-353-394; 033-515-315-799-380; 034-236-791-494-531; 034-614-779-899-398; 035-009-105-141-653; 035-632-043-395-66X; 036-576-932-377-069; 036-950-396-756-838; 037-100-177-215-105; 037-646-265-142-772; 037-942-208-306-220; 038-139-446-649-396; 038-305-881-901-219; 039-108-635-618-390; 039-158-168-615-048; 039-196-378-524-037; 039-434-712-227-523; 039-901-867-982-275; 040-152-705-683-491; 041-881-735-192-673; 041-886-716-756-878; 042-178-849-030-092; 042-251-157-585-319; 046-579-780-717-330; 046-983-724-347-755; 048-214-157-508-44X; 048-266-797-651-812; 049-094-921-856-507; 049-179-394-137-633; 049-416-668-533-649; 049-769-952-902-222; 050-922-085-668-151; 052-417-277-173-757; 054-308-248-643-06X; 054-423-440-958-41X; 054-756-916-988-514; 055-239-646-409-172; 055-656-341-972-981; 055-661-462-984-985; 055-916-264-391-662; 056-169-102-526-237; 057-567-246-168-045; 058-139-253-334-350; 058-329-909-149-682; 058-780-078-856-55X; 059-326-914-410-470; 059-861-334-558-49X; 059-908-495-520-041; 060-315-821-993-357; 061-807-461-466-269; 062-822-657-347-212; 062-899-460-258-027; 062-942-641-603-206; 064-097-251-811-189; 065-516-985-240-08X; 065-715-869-899-823; 065-870-377-902-598; 065-885-904-575-800; 066-506-142-974-306; 066-974-781-302-685; 068-681-073-696-154; 068-972-362-301-26X; 072-061-291-389-950; 073-005-436-647-640; 075-615-146-752-793; 077-031-819-131-961; 077-223-748-392-772; 077-260-026-964-073; 078-118-752-753-798; 079-072-912-049-827; 079-198-854-262-85X; 079-310-940-223-758; 080-406-085-913-935; 080-455-695-912-500; 082-846-745-410-56X; 082-878-838-161-060; 082-993-800-827-035; 083-095-145-102-047; 083-194-079-342-053; 083-820-467-245-955; 084-262-224-389-068; 085-429-824-833-378; 092-043-501-664-68X; 094-385-219-750-616; 094-864-860-262-632; 096-382-173-074-665; 099-430-545-313-43X; 100-162-221-575-615; 102-252-444-735-112; 102-666-261-367-564; 103-130-325-717-852; 103-212-983-826-945; 105-273-608-699-755; 107-148-693-691-941; 108-022-398-203-696; 109-070-287-053-755; 109-456-052-162-414; 109-574-794-073-426; 110-776-778-804-242; 111-113-760-735-241; 114-223-968-443-631; 117-672-208-154-813; 119-418-108-964-586; 122-059-228-612-805; 125-399-125-257-310; 125-510-963-188-628; 130-498-386-447-600; 131-935-789-610-109; 135-031-859-451-62X; 136-558-193-815-294; 140-004-377-216-35X; 140-299-789-548-802; 140-409-895-186-774; 142-517-422-966-050; 143-904-408-309-886; 146-686-173-784-426; 157-417-333-257-718; 162-124-223-665-202; 169-560-147-410-00X; 170-260-342-315-105; 173-065-116-232-673; 176-289-883-060-405; 177-953-212-457-486; 186-013-523-611-58X; 186-154-763-652-013; 189-026-810-737-064; 193-746-843-647-364; 198-317-639-964-465,0,false,,
